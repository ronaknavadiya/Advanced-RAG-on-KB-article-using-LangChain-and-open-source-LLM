{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678dadb4",
   "metadata": {},
   "source": [
    "Install required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a9a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c290b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, multiprocess, fsspec, huggingface-hub, datasets\n",
      "\n",
      "   -------- ------------------------------- 1/5 [multiprocess]\n",
      "   -------- ------------------------------- 1/5 [multiprocess]\n",
      "  Attempting uninstall: fsspec\n",
      "   -------- ------------------------------- 1/5 [multiprocess]\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "   -------- ------------------------------- 1/5 [multiprocess]\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "   -------- ------------------------------- 1/5 [multiprocess]\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "   -------- ------------------------------- 1/5 [multiprocess]\n",
      "   ---------------- ----------------------- 2/5 [fsspec]\n",
      "   ---------------- ----------------------- 2/5 [fsspec]\n",
      "   ---------------- ----------------------- 2/5 [fsspec]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   ------------------------ --------------- 3/5 [huggingface-hub]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   -------------------------------- ------- 4/5 [datasets]\n",
      "   ---------------------------------------- 5/5 [datasets]\n",
      "\n",
      "Successfully installed datasets-4.0.0 fsspec-2025.3.0 huggingface-hub-0.34.4 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2025.3.2 requires fsspec==2025.3.2.*, but you have fsspec 2025.3.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525ea453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ecf19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98c3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b618103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f68bf925e414d26aa6a8771485ef77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"],metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc0e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
    "# This list is taken from LangChain's MarkdownTextSplitter class\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "   chunk_size=1000,\n",
    "   chunk_overlap=100,\n",
    "   add_start_index=True, # If `True`, includes chunk's start index in metadata\n",
    "   strip_whitespace=True, # If `True`, strips whitespace from the start and end of every document\n",
    "   separators=MARKDOWN_SEPARATORS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a613748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed = []\n",
    "\n",
    "for doc in RAW_KNOWLEDGE_BASE:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208a86f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 1}, page_content='Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />'),\n",
       " Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 968}, page_content='## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!'),\n",
       " Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 1805}, page_content='## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1}, page_content=\"Choosing a metric for your task\\n\\n**So you've trained your model and want to see how well it’s doing on a dataset of your choice. Where do you start?**\\n\\nThere is no “one size fits all” approach to choosing an evaluation metric, but some good guidelines to keep in mind are:\\n\\n## Categories of metrics\\n\\nThere are 3 high-level categories of metrics:\"),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 275}, page_content=\"## Categories of metrics\\n\\nThere are 3 high-level categories of metrics:\\n\\n1. *Generic metrics*, which can be applied to a variety of situations and datasets, such as precision and accuracy.\\n2. *Task-specific metrics*, which are limited to a given task, such as Machine Translation (often evaluated using metrics [BLEU](https://huggingface.co/metrics/bleu) or [ROUGE](https://huggingface.co/metrics/rouge)) or Named Entity Recognition (often evaluated with [seqeval](https://huggingface.co/metrics/seqeval)).\\n3. *Dataset-specific metrics*, which aim to measure model performance on specific benchmarks: for instance, the [GLUE benchmark](https://huggingface.co/datasets/glue) has a dedicated [evaluation metric](https://huggingface.co/metrics/glue).\\n\\nLet's look at each of these three cases:\\n\\n### Generic metrics\\n\\nMany of the metrics used in the Machine Learning community are quite generic and can be applied in a variety of tasks and datasets.\"),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1220}, page_content='This is the case for metrics like [accuracy](https://huggingface.co/metrics/accuracy) and [precision](https://huggingface.co/metrics/precision), which can be used for evaluating labeled (supervised) datasets, as well as [perplexity](https://huggingface.co/metrics/perplexity), which can be used for evaluating different kinds of (unsupervised) generative tasks.\\n\\nTo see the input structure of a given metric, you can look at its metric card. For example, in the case of [precision](https://huggingface.co/metrics/precision), the format is:'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1760}, page_content='```\\n>>> precision_metric = evaluate.load(\"precision\")\\n>>> results = precision_metric.compute(references=[0, 1], predictions=[0, 1])\\n>>> print(results)\\n{\\'precision\\': 1.0}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1930}, page_content='```\\n\\n### Task-specific metrics\\n\\nPopular ML tasks like Machine Translation and Named Entity Recognition have specific metrics that can be used to compare models. For example, a series of different metrics have been proposed for text generation, ranging from [BLEU](https://huggingface.co/metrics/bleu) and its derivatives such as [GoogleBLEU](https://huggingface.co/metrics/google_bleu) and [GLEU](https://huggingface.co/metrics/gleu), but also [ROUGE](https://huggingface.co/metrics/rouge), [MAUVE](https://huggingface.co/metrics/mauve), etc.\\n\\nYou can find the right metric for your task by:'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 2474}, page_content='You can find the right metric for your task by:\\n\\n- **Looking at the [Task pages](https://huggingface.co/tasks)** to see what metrics can be used for evaluating models for a given task.\\n- **Checking out leaderboards** on sites like [Papers With Code](https://paperswithcode.com/) (you can search by task and by dataset).\\n-  **Reading the metric cards** for the relevant metrics and see which ones are a good fit for your use case. For example, see the [BLEU metric card](https://github.com/huggingface/evaluate/tree/main/metrics/bleu) or [SQuaD metric card](https://github.com/huggingface/evaluate/tree/main/metrics/squad).\\n- **Looking at papers and blog posts** published on the topic and see what metrics they report. This can change over time, so try to pick papers from the last couple of years!\\n\\n### Dataset-specific metrics'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 3274}, page_content='### Dataset-specific metrics\\n\\nSome datasets have specific metrics associated with them -- this is especially in the case of popular benchmarks like [GLUE](https://huggingface.co/metrics/glue) and [SQuAD](https://huggingface.co/metrics/squad).\\n\\n<Tip warning={true}>\\n💡\\nGLUE is actually a collection of different subsets on different tasks, so first you need to choose the one that corresponds to the NLI task, such as mnli, which is described as “crowdsourced collection of sentence pairs with textual entailment annotations”\\n</Tip>'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 3807}, page_content='If you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use its dedicated evaluation metric. Make sure you respect the format that they require. For example, to evaluate your model on the [SQuAD](https://huggingface.co/datasets/squad) dataset, you need to feed the `question` and `context` into your model and return the `prediction_text`, which should be compared with the `references` (based on matching the `id` of the question) :'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 4281}, page_content='```\\n>>> from evaluate import load\\n>>> squad_metric = load(\"squad\")\\n>>> predictions = [{\\'prediction_text\\': \\'1976\\', \\'id\\': \\'56e10a3be3433e1400422b22\\'}]\\n>>> references = [{\\'answers\\': {\\'answer_start\\': [97], \\'text\\': [\\'1976\\']}, \\'id\\': \\'56e10a3be3433e1400422b22\\'}]\\n>>> results = squad_metric.compute(predictions=predictions, references=references)\\n>>> results\\n{\\'exact_match\\': 100.0, \\'f1\\': 100.0}\\n```\\n\\nYou can find examples of dataset structures by consulting the \"Dataset Preview\" function or the dataset card for a given dataset, and you can see how to use its dedicated evaluation function based on the metric card.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 1}, page_content='主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio 的主要特点：\\n\\n1. [添加示例输入](#example-inputs)\\n2. [传递自定义错误消息](#errors)\\n3. [添加描述内容](#descriptive-content)\\n4. [设置旗标](#flagging)\\n5. [预处理和后处理](#preprocessing-and-postprocessing)\\n6. [样式化演示](#styling)\\n7. [排队用户](#queuing)\\n8. [迭代输出](#iterative-outputs)\\n9. [进度条](#progress-bars)\\n10. [批处理函数](#batch-functions)\\n11. [在协作笔记本上运行](#colab-notebooks)\\n\\n## 示例输入\\n\\n您可以提供用户可以轻松加载到 \"Interface\" 中的示例数据。这对于演示模型期望的输入类型以及演示数据集和模型一起探索的方式非常有帮助。要加载示例数据，您可以将嵌套列表提供给 Interface 构造函数的 `examples=` 关键字参数。外部列表中的每个子列表表示一个数据样本，子列表中的每个元素表示每个输入组件的输入。有关每个组件的示例数据格式在[Docs](https://gradio.app/docs#components)中有说明。\\n\\n$code_calculator\\n$demo_calculator\\n\\n您可以将大型数据集加载到示例中，通过 Gradio 浏览和与数据集进行交互。示例将自动分页（可以通过 Interface 的 `examples_per_page` 参数进行配置）。\\n\\n继续了解示例，请参阅[更多示例](https://gradio.app/more-on-examples)指南。\\n\\n## 错误\\n\\n您希望向用户传递自定义错误消息。为此，with `gr.Error(\"custom message\")` 来显示错误消息。如果在上面的计算器示例中尝试除以零，将显示自定义错误消息的弹出模态窗口。了解有关错误的更多信息，请参阅[文档](https://gradio.app/docs#error)。\\n\\n## 描述性内容'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 950}, page_content=\"## 描述性内容\\n\\n在前面的示例中，您可能已经注意到 Interface 构造函数中的 `title=` 和 `description=` 关键字参数，帮助用户了解您的应用程序。\\n\\nInterface 构造函数中有三个参数用于指定此内容应放置在哪里：\\n\\n- `title`：接受文本，并可以将其显示在界面的顶部，也将成为页面标题。\\n- `description`：接受文本、Markdown 或 HTML，并将其放置在标题正下方。\\n- `article`：也接受文本、Markdown 或 HTML，并将其放置在界面下方。\\n\\n![annotated](/assets/guides/annotated.png)\\n\\n如果您使用的是 `Blocks` API，则可以 with `gr.Markdown(...)` 或 `gr.HTML(...)` 组件在任何位置插入文本、Markdown 或 HTML，其中描述性内容位于 `Component` 构造函数内部。\\n\\n另一个有用的关键字参数是 `label=`，它存在于每个 `Component` 中。这修改了每个 `Component` 顶部的标签文本。还可以为诸如 `Textbox` 或 `Radio` 之类的表单元素添加 `info=` 关键字参数，以提供有关其用法的进一步信息。\\n\\n```python\\ngr.Number(label='年龄', info='以年为单位，必须大于0')\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 1572}, page_content='```\\n\\n## 旗标\\n\\n默认情况下，\"Interface\" 将有一个 \"Flag\" 按钮。当用户测试您的 `Interface` 时，如果看到有趣的输出，例如错误或意外的模型行为，他们可以将输入标记为您进行查看。在由 `Interface` 构造函数的 `flagging_dir=` 参数提供的目录中，将记录标记的输入到一个 CSV 文件中。如果界面涉及文件数据，例如图像和音频组件，将创建文件夹来存储这些标记的数据。\\n\\n例如，对于上面显示的计算器界面，我们将在下面的旗标目录中存储标记的数据：\\n\\n```directory\\n+-- calculator.py\\n+-- flagged/\\n|   +-- logs.csv\\n```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nnum1,operation,num2,Output\\n5,add,7,12\\n6,subtract,1.5,4.5\\n```\\n\\n与早期显示的冷色界面相对应，我们将在下面的旗标目录中存储标记的数据：\\n\\n```directory\\n+-- sepia.py\\n+-- flagged/\\n|   +-- logs.csv\\n|   +-- im/\\n|   |   +-- 0.png\\n|   |   +-- 1.png\\n|   +-- Output/\\n|   |   +-- 0.png\\n|   |   +-- 1.png\\n```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 2169}, page_content='```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png\\n```\\n\\n如果您希望用户提供旗标原因，可以将字符串列表传递给 Interface 的 `flagging_options` 参数。用户在进行旗标时必须选择其中一个字符串，这将作为附加列保存到 CSV 中。\\n\\n## 预处理和后处理 (Preprocessing and Postprocessing)\\n\\n![annotated](/assets/img/dataflow.svg)\\n\\n如您所见，Gradio 包括可以处理各种不同数据类型的组件，例如图像、音频和视频。大多数组件都可以用作输入或输出。\\n\\n当组件用作输入时，Gradio 自动处理*预处理*，将数据从用户浏览器发送的类型（例如网络摄像头快照的 base64 表示）转换为您的函数可以接受的形式（例如 `numpy` 数组）。\\n\\n同样，当组件用作输出时，Gradio 自动处理*后处理*，将数据从函数返回的形式（例如图像路径列表）转换为可以在用户浏览器中显示的形式（例如以 base64 格式显示图像的 `Gallery`）。\\n\\n您可以使用构建图像组件时的参数控制*预处理*。例如，如果您使用以下参数实例化 `Image` 组件，它将将图像转换为 `PIL` 类型，并将其重塑为`(100, 100)`，而不管提交时的原始大小如何：\\n\\n```py\\nimg = gr.Image(shape=(100, 100), type=\"pil\")\\n```\\n\\n相反，这里我们保留图像的原始大小，但在将其转换为 numpy 数组之前反转颜色：\\n\\n```py\\nimg = gr.Image(invert_colors=True, type=\"numpy\")'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 2955}, page_content='```\\n\\n后处理要容易得多！Gradio 自动识别返回数据的格式（例如 `Image` 是 `numpy` 数组还是 `str` 文件路径？），并将其后处理为可以由浏览器显示的格式。\\n\\n请查看[文档](https://gradio.app/docs)，了解每个组件的所有与预处理相关的参数。\\n\\n## 样式 (Styling)\\n\\nGradio 主题是自定义应用程序外观和感觉的最简单方法。您可以选择多种主题或创建自己的主题。要这样做，请将 `theme=` 参数传递给 `Interface` 构造函数。例如：\\n\\n```python\\ndemo = gr.Interface(..., theme=gr.themes.Monochrome())\\n```\\n\\nGradio 带有一组预先构建的主题，您可以从 `gr.themes.*` 加载。您可以扩展这些主题或从头开始创建自己的主题 - 有关更多详细信息，请参阅[主题指南](https://gradio.app/theming-guide)。\\n\\n要增加额外的样式能力，您可以 with `css=` 关键字将任何 CSS 传递给您的应用程序。\\nGradio 应用程序的基类是 `gradio-container`，因此以下是一个更改 Gradio 应用程序背景颜色的示例：\\n\\n```python\\nwith `gr.Interface(css=\".gradio-container {background-color: red}\") as demo:\\n    ...\\n```\\n\\n## 队列 (Queuing)\\n\\n如果您的应用程序预计会有大量流量，请 with `queue()` 方法来控制处理速率。这将排队处理调用，因此一次只处理一定数量的请求。队列使用 Websockets，还可以防止网络超时，因此如果您的函数的推理时间很长（> 1 分钟），应使用队列。\\n\\nwith `Interface`：\\n\\n```python\\ndemo = gr.Interface(...).queue()\\ndemo.launch()\\n```\\n\\nwith `Blocks`：\\n\\n```python\\nwith gr.Blocks() as demo：\\n    #...\\ndemo.queue()\\ndemo.launch()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 3835}, page_content='```\\n\\nwith `Blocks`：\\n\\n```python\\nwith gr.Blocks() as demo：\\n    #...\\ndemo.queue()\\ndemo.launch()\\n```\\n\\n您可以通过以下方式控制一次处理的请求数量：\\n\\n```python\\ndemo.queue(concurrency_count=3)\\n```\\n\\n查看有关配置其他队列参数的[队列文档](/docs/#queue)。\\n\\n在 Blocks 中指定仅对某些函数进行排队：\\n\\n```python\\nwith gr.Blocks() as demo2：\\n    num1 = gr.Number()\\n    num2 = gr.Number()\\n    output = gr.Number()\\n    gr.Button(\"Add\").click(\\n        lambda a, b: a + b, [num1, num2], output)\\n    gr.Button(\"Multiply\").click(\\n        lambda a, b: a * b, [num1, num2], output, queue=True)\\ndemo2.launch()\\n```\\n\\n## 迭代输出 (Iterative Outputs)\\n\\n在某些情况下，您可能需要传输一系列输出而不是一次显示单个输出。例如，您可能有一个图像生成模型，希望显示生成的每个步骤的图像，直到最终图像。或者您可能有一个聊天机器人，它逐字逐句地流式传输响应，而不是一次返回全部响应。\\n\\n在这种情况下，您可以将**生成器**函数提供给 Gradio，而不是常规函数。在 Python 中创建生成器非常简单：函数不应该有一个单独的 `return` 值，而是应该 with `yield` 连续返回一系列值。通常，`yield` 语句放置在某种循环中。下面是一个简单示例，生成器只是简单计数到给定数字：\\n\\n```python\\ndef my_generator(x):\\n    for i in range(x):\\n        yield i'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 4732}, page_content='```\\n\\n您以与常规函数相同的方式将生成器提供给 Gradio。例如，这是一个（虚拟的）图像生成模型，它在输出图像之前生成数个步骤的噪音：\\n\\n$code_fake_diffusion\\n$demo_fake_diffusion\\n\\n请注意，我们在迭代器中添加了 `time.sleep(1)`，以创建步骤之间的人工暂停，以便您可以观察迭代器的步骤（在真实的图像生成模型中，这可能是不必要的）。\\n\\n将生成器提供给 Gradio **需要**在底层 Interface 或 Blocks 中启用队列（请参阅上面的队列部分）。\\n\\n## 进度条\\n\\nGradio 支持创建自定义进度条，以便您可以自定义和控制向用户显示的进度更新。要启用此功能，只需为方法添加一个默认值为 `gr.Progress` 实例的参数即可。然后，您可以直接调用此实例并传入 0 到 1 之间的浮点数来更新进度级别，或者 with `Progress` 实例的 `tqdm()` 方法来跟踪可迭代对象上的进度，如下所示。必须启用队列以进行进度更新。\\n\\n$code_progress_simple\\n$demo_progress_simple\\n\\n如果您 with `tqdm` 库，并且希望从函数内部的任何 `tqdm.tqdm` 自动报告进度更新，请将默认参数设置为 `gr.Progress(track_tqdm=True)`！\\n\\n## 批处理函数 (Batch Functions)\\n\\nGradio 支持传递*批处理*函数。批处理函数只是接受输入列表并返回预测列表的函数。\\n\\n例如，这是一个批处理函数，它接受两个输入列表（一个单词列表和一个整数列表），并返回修剪过的单词列表作为输出：\\n\\n```python\\nimport time\\n\\ndef trim_words(words, lens):\\n    trimmed_words = []\\n    time.sleep(5)\\n    for w, l in zip(words, lens):\\n        trimmed_words.append(w[:int(l)])\\n    return [trimmed_words]\\n    for w, l in zip(words, lens):'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 5686}, page_content='```\\n\\n使用批处理函数的优点是，如果启用了队列，Gradio 服务器可以自动*批处理*传入的请求并并行处理它们，从而可能加快演示速度。以下是 Gradio 代码的示例（请注意 `batch=True` 和 `max_batch_size=16` - 这两个参数都可以传递给事件触发器或 `Interface` 类）\\n\\nwith `Interface`：\\n\\n```python\\ndemo = gr.Interface(trim_words, [\"textbox\", \"number\"], [\"output\"],\\n                    batch=True, max_batch_size=16)\\ndemo.queue()\\ndemo.launch()\\n```\\n\\nwith `Blocks`：\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        word = gr.Textbox(label=\"word\")\\n        leng = gr.Number(label=\"leng\")\\n        output = gr.Textbox(label=\"Output\")\\n    with gr.Row():\\n        run = gr.Button()\\n\\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\\n\\ndemo.queue()\\ndemo.launch()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 6401}, page_content='```\\n\\n在上面的示例中，可以并行处理 16 个请求（总推理时间为 5 秒），而不是分别处理每个请求（总推理时间为 80 秒）。许多 Hugging Face 的 `transformers` 和 `diffusers` 模型在 Gradio 的批处理模式下自然工作：这是[使用批处理生成图像的示例演示](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\\n\\n注意：使用 Gradio 的批处理函数 **requires** 在底层 Interface 或 Blocks 中启用队列（请参阅上面的队列部分）。\\n\\n## Gradio 笔记本 (Colab Notebooks)\\n\\nGradio 可以在任何运行 Python 的地方运行，包括本地 Jupyter 笔记本和协作笔记本，如[Google Colab](https://colab.research.google.com/)。对于本地 Jupyter 笔记本和 Google Colab 笔记本，Gradio 在本地服务器上运行，您可以在浏览器中与之交互。（注意：对于 Google Colab，这是通过[服务工作器隧道](https://github.com/tensorflow/tensorboard/blob/master/docs/design/colab_integration.md)实现的，您的浏览器需要启用 cookies。）对于其他远程笔记本，Gradio 也将在服务器上运行，但您需要使用[SSH 隧道](https://coderwall.com/p/ohk6cg/remote-access-to-ipython-notebooks-via-ssh)在本地浏览器中查看应用程序。通常，更简单的选择是使用 Gradio 内置的公共链接，[在下一篇指南中讨论](/sharing-your-app/#sharing-demos)。'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Training on TPU with TensorFlow\\n\\n<Tip>\\n\\nIf you don\\'t need long explanations and just want TPU code samples to get started with, check out [our TPU example notebook!](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)\\n\\n</Tip>\\n\\n### What is a TPU?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 950}, page_content='</Tip>\\n\\n### What is a TPU?\\n\\nA TPU is a **Tensor Processing Unit.** They are hardware designed by Google, which are used to greatly speed up the tensor computations within neural networks, much like GPUs. They can be used for both network training and inference. They are generally accessed through Google’s cloud services, but small TPUs can also be accessed directly for free through Google Colab and Kaggle Kernels.\\n\\nBecause [all TensorFlow models in 🤗 Transformers are Keras models](https://huggingface.co/blog/tensorflow-philosophy), most of the methods in this document are generally applicable to TPU training for any Keras model! However, there are a few points that are specific to the HuggingFace ecosystem (hug-o-system?) of Transformers and Datasets, and we’ll make sure to flag them up when we get to them.\\n\\n### What kinds of TPU are available?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 1770}, page_content='### What kinds of TPU are available?\\n\\nNew users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between **TPU Nodes** and **TPU VMs.**\\n\\nWhen you use a **TPU Node**, you are effectively indirectly accessing a remote TPU. You will need a separate VM, which will initialize your network and data pipeline and then forward them to the remote node. When you use a TPU on Google Colab, you are accessing it in the **TPU Node** style.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 2294}, page_content='Using TPU Nodes can have some quite unexpected behaviour for people who aren’t used to them! In particular, because the TPU is located on a physically different system to the machine you’re running your Python code on, your data cannot be local to your machine - any data pipeline that loads from your machine’s internal storage will totally fail! Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU node.\\n\\n<Tip>\\n\\nIf you can fit all your data in memory as `np.ndarray` or `tf.Tensor`, then you can `fit()` on that data even when using Colab or a TPU Node, without needing to upload it to Google Cloud Storage.\\n\\n</Tip>\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 3004}, page_content='</Tip>\\n\\n<Tip>\\n\\n**🤗Specific Hugging Face Tip🤗:** The methods `Dataset.to_tf_dataset()` and its higher-level wrapper `model.prepare_tf_dataset()` , which you will see throughout our TF code examples, will both fail on a TPU Node. The reason for this is that even though they create a `tf.data.Dataset` it is not a “pure” `tf.data` pipeline and uses `tf.numpy_function` or `Dataset.from_generator()` to stream data from the underlying HuggingFace `Dataset`. This HuggingFace `Dataset` is backed by data that is on a local disc and which the remote TPU Node will not be able to read.\\n\\n</Tip>\\n\\nThe second way to access a TPU is via a **TPU VM.** When using a TPU VM, you connect directly to the machine that the TPU is attached to, much like training on a GPU VM. TPU VMs are generally easier to work with, particularly when it comes to your data pipeline. All of the above warnings do not apply to TPU VMs!'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 3908}, page_content='This is an opinionated document, so here’s our opinion: **Avoid using TPU Node if possible.** It is more confusing and more difficult to debug than TPU VMs. It is also likely to be unsupported in future - Google’s latest TPU, TPUv4, can only be accessed as a TPU VM, which suggests that TPU Nodes are increasingly going to become a “legacy” access method. However, we understand that the only free TPU access is on Colab and Kaggle Kernels, which uses TPU Node - so we’ll try to explain how to handle it if you have to! Check the [TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) for code samples that explain this in more detail.\\n\\n### What sizes of TPU are available?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 4618}, page_content='### What sizes of TPU are available?\\n\\nA single TPU (a v2-8/v3-8/v4-8) runs 8 replicas. TPUs exist in **pods** that can run hundreds or thousands of replicas simultaneously. When you use more than a single TPU but less than a whole pod (for example, a v3-32), your TPU fleet is referred to as a **pod slice.**\\n\\nWhen you access a free TPU via Colab, you generally get a single v2-8 TPU.\\n\\n### I keep hearing about this XLA thing. What’s XLA, and how does it relate to TPUs?\\n\\nXLA is an optimizing compiler, used by both TensorFlow and JAX. In JAX it is the only compiler, whereas in TensorFlow it is optional (but mandatory on TPU!). The easiest way to enable it when training a Keras model is to pass the argument `jit_compile=True` to `model.compile()`. If you don’t get any errors and performance is good, that’s a great sign that you’re ready to move to TPU!'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 5478}, page_content='Debugging on TPU is generally a bit harder than on CPU/GPU, so we recommend getting your code running on CPU/GPU with XLA first before trying it on TPU. You don’t have to train for long, of course - just for a few steps to make sure that your model and data pipeline are working like you expect them to.\\n\\n<Tip>\\n\\nXLA compiled code is usually faster - so even if you’re not planning to run on TPU, adding `jit_compile=True` can improve your performance. Be sure to note the caveats below about XLA compatibility, though!\\n\\n</Tip>\\n\\n<Tip warning={true}>\\n\\n**Tip born of painful experience:** Although using `jit_compile=True` is a good way to get a speed boost and test if your CPU/GPU code is XLA-compatible, it can actually cause a lot of problems if you leave it in when actually training on TPU. XLA compilation will happen implicitly on TPU, so remember to remove that line before actually running your code on a TPU!\\n\\n</Tip>\\n\\n### How do I make my model XLA compatible?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 6396}, page_content='</Tip>\\n\\n### How do I make my model XLA compatible?\\n\\nIn many cases, your code is probably XLA-compatible already! However, there are a few things that work in normal TensorFlow that don’t work in XLA. We’ve distilled them into three core rules below:\\n\\n<Tip>\\n\\n**🤗Specific HuggingFace Tip🤗:** We’ve put a lot of effort into rewriting our TensorFlow models and loss functions to be XLA-compatible. Our models and loss functions generally obey rule #1 and #2 by default, so you can skip over them if you’re using `transformers` models. Don’t forget about these rules when writing your own models and loss functions, though!\\n\\n</Tip>\\n\\n#### XLA Rule #1: Your code cannot have “data-dependent conditionals”\\n\\nWhat that means is that any `if` statement cannot depend on values inside a `tf.Tensor`. For example, this code block cannot be compiled with XLA!\\n\\n```python\\nif tf.reduce_sum(tensor) > 10:\\n    tensor = tensor / 2.0'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 7310}, page_content='```\\n\\nThis might seem very restrictive at first, but most neural net code doesn’t need to do this. You can often get around this restriction by using `tf.cond` (see the documentation [here](https://www.tensorflow.org/api_docs/python/tf/cond)) or by removing the conditional and finding a clever math trick with indicator variables instead, like so:\\n\\n```python\\nsum_over_10 = tf.cast(tf.reduce_sum(tensor) > 10, tf.float32)\\ntensor = tensor / (1.0 + sum_over_10)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 7769}, page_content='```\\n\\nThis code has exactly the same effect as the code above, but by avoiding a conditional, we ensure it will compile with XLA without problems!\\n\\n#### XLA Rule #2: Your code cannot have “data-dependent shapes”\\n\\nWhat this means is that the shape of all of the `tf.Tensor` objects in your code cannot depend on their values. For example, the function `tf.unique` cannot be compiled with XLA, because it returns a `tensor` containing one instance of each unique value in the input. The shape of this output will obviously be different depending on how repetitive the input `Tensor` was, and so XLA refuses to handle it!'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 8388}, page_content='In general, most neural network code obeys rule #2 by default. However, there are a few common cases where it becomes a problem. One very common one is when you use **label masking**, setting your labels to a negative value to indicate that those positions should be ignored when computing the loss. If you look at NumPy or PyTorch loss functions that support label masking, you will often see code like this that uses [boolean indexing](https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing):\\n\\n```python\\nlabel_mask = labels >= 0\\nmasked_outputs = outputs[label_mask]\\nmasked_labels = labels[label_mask]\\nloss = compute_loss(masked_outputs, masked_labels)\\nmean_loss = torch.mean(loss)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 9094}, page_content='```\\n\\nThis code is totally fine in NumPy or PyTorch, but it breaks in XLA! Why? Because the shape of `masked_outputs` and `masked_labels` depends on how many positions are masked - that makes it a **data-dependent shape.** However, just like for rule #1, we can often rewrite this code to yield exactly the same output without any data-dependent shapes.\\n\\n```python\\nlabel_mask = tf.cast(labels >= 0, tf.float32)\\nloss = compute_loss(outputs, labels)\\nloss = loss * label_mask  # Set negative label positions to 0\\nmean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 9663}, page_content='```\\n\\nHere, we avoid data-dependent shapes by computing the loss for every position, but zeroing out the masked positions in both the numerator and denominator when we calculate the mean, which yields exactly the same result as the first block while maintaining XLA compatibility. Note that we use the same trick as in rule #1 - converting a `tf.bool` to `tf.float32` and using it as an indicator variable. This is a really useful trick, so remember it if you need to convert your own code to XLA!\\n\\n#### XLA Rule #3: XLA will need to recompile your model for every different input shape it sees\\n\\nThis is the big one. What this means is that if your input shapes are very variable, XLA will have to recompile your model over and over, which will create huge performance problems. This commonly arises in NLP models, where input texts have variable lengths after tokenization. In other modalities, static shapes are more common and this rule is much less of a problem.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 10630}, page_content='How can you get around rule #3? The key is **padding** - if you pad all your inputs to the same length, and then use an `attention_mask`, you can get the same results as you’d get from variable shapes, but without any XLA issues. However, excessive padding can cause severe slowdown too - if you pad all your samples to the maximum length in the whole dataset, you might end up with batches consisting endless padding tokens, which will waste a lot of compute and memory!\\n\\nThere isn’t a perfect solution to this problem. However, you can try some tricks. One very useful trick is to **pad batches of samples up to a multiple of a number like 32 or 64 tokens.** This often only increases the number of tokens by a small amount, but it hugely reduces the number of unique input shapes, because every input shape now has to be a multiple of 32 or 64. Fewer unique input shapes means fewer XLA compilations!\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 11535}, page_content='<Tip>\\n\\n**🤗Specific HuggingFace Tip🤗:** Our tokenizers and data collators have methods that can help you here. You can use `padding=\"max_length\"` or `padding=\"longest\"` when calling tokenizers to get them to output padded data. Our tokenizers and data collators also have a `pad_to_multiple_of` argument that you can use to reduce the number of unique input shapes you see!\\n\\n</Tip>\\n\\n### How do I actually train my model on TPU?\\n\\nOnce your training is XLA-compatible and (if you’re using TPU Node / Colab) your dataset has been prepared appropriately, running on TPU is surprisingly easy! All you really need to change in your code is to add a few lines to initialize your TPU, and to ensure that your model and dataset are created inside a `TPUStrategy` scope. Take a look at [our TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) to see this in action!\\n\\n### Summary'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 12466}, page_content='### Summary\\n\\nThere was a lot in here, so let’s summarize with a quick checklist you can follow when you want to get your model ready for TPU training:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 12618}, page_content='- Make sure your code follows the three rules of XLA\\n- Compile your model with `jit_compile=True` on CPU/GPU and confirm that you can train it with XLA\\n- Either load your dataset into memory or use a TPU-compatible dataset loading approach (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Migrate your code either to Colab (with accelerator set to “TPU”) or a TPU VM on Google Cloud\\n- Add TPU initializer code (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Create your `TPUStrategy` and make sure dataset loading and model creation are inside the `strategy.scope()` (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Don’t forget to take `jit_compile=True` out again when you move to TPU!\\n- 🙏🙏🙏🥺🥺🥺\\n- Call model.fit()\\n- You did it!'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/blocks_random_slider/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef func(slider_1, slider_2):\\n    return slider_1 * 5 + slider_2\\n\\n\\nwith gr.Blocks() as demo:\\n    slider = gr.Slider(minimum=-10.2, maximum=15, label=\"Random Slider (Static)\", randomize=True)\\n    slider_1 = gr.Slider(minimum=100, maximum=200, label=\"Random Slider (Input 1)\", randomize=True)\\n    slider_2 = gr.Slider(minimum=10, maximum=23.2, label=\"Random Slider (Input 2)\", randomize=True)\\n    slider_3 = gr.Slider(value=3, label=\"Non random slider\")\\n    btn = gr.Button(\"Run\")\\n    btn.click(func, inputs=[slider_1, slider_2], outputs=gr.Number())\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 1}, page_content='Git over SSH\\n\\nYou can access and write data in repositories on huggingface.co using SSH (Secure Shell Protocol). When you connect via SSH, you authenticate using a private key file on your local machine.\\n\\nSome actions, such as pushing changes, or cloning private repositories, will require you to upload your SSH public key to your account on huggingface.co.\\n\\nYou can use a pre-existing SSH key, or generate a new one specifically for huggingface.co.\\n\\n## Checking for existing SSH keys\\n\\nIf you have an existing SSH key, you can use that key to authenticate Git operations over SSH.\\n\\nSSH keys are usually located under `~/.ssh` on Mac & Linux, and under `C:\\\\\\\\Users\\\\\\\\<username>\\\\\\\\.ssh` on Windows. List files under that directory and look for files of the form:\\n\\n- id_rsa.pub\\n- id_ecdsa.pub\\n- id_ed25519.pub\\n\\nThose files contain your SSH public key.'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 761}, page_content=\"- id_rsa.pub\\n- id_ecdsa.pub\\n- id_ed25519.pub\\n\\nThose files contain your SSH public key.\\n\\nIf you don't have such file under `~/.ssh`, you will have to [generate a new key](#generating-a-new-ssh-keypair). Otherwise, you can [add your existing SSH public key(s) to your huggingface.co account](#add-a-ssh-key-to-your-account).\\n\\n## Generating a new SSH keypair\\n\\nIf you don't have any SSH keys on your machine, you can use `ssh-keygen` to generate a new SSH key pair (public + private keys):\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 1248}, page_content='```\\n$ ssh-keygen -t ed25519 -C \"your.email@example.co\"\\n```\\n\\nWe recommend entering a passphrase when you are prompted to. A passphrase is an extra layer of security: it is a password that will be prompted whenever you use your SSH key.\\n\\nOnce your new key is generated, add it to your SSH agent with `ssh-add`:\\n\\n```\\n$ ssh-add ~/.ssh/id_ed25519'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 1590}, page_content='```\\n\\nIf you chose a different location than the default to store your SSH key, you would have to replace `~/.ssh/id_ed25519` with the file location you used.\\n\\n## Add a SSH key to your account\\n\\nTo access private repositories with SSH, or to push changes via SSH, you will need to add your SSH public key to your huggingface.co account. You can manage your SSH keys [in your user settings](https://huggingface.co/settings/keys).\\n\\nTo add a SSH key to your account, click on the \"Add SSH key\" button.\\n\\nThen, enter a name for this key (for example, \"Personal computer\"), and copy and paste the content of your **public** SSH key in the area below. The public key is located in the `~/.ssh/id_XXXX.pub` file you found or generated in the previous steps.\\n\\nClick on \"Add key\", and voilà! You have added a SSH key to your huggingface.co account.\\n\\n\\n## Testing your SSH authentication\\n\\nOnce you have added your SSH key to your huggingface.co account, you can test that the connection works as expected.'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 2583}, page_content='In a terminal, run:'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 2603}, page_content='```\\n$ ssh -T git@hf.co\\n```\\n\\nIf you see a message with your username, congrats! Everything went well, you are ready to use git over SSH.\\n\\nOtherwise, if the message states something like the following, make sure your SSH key is actually used by your SSH agent.\\n```\\nHi anonymous, welcome to Hugging Face.\\n```'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# Token classification with LayoutLMv3 (PyTorch version)\\n\\nThis directory contains a script, `run_funsd_cord.py`, that can be used to fine-tune (or evaluate) LayoutLMv3 on form understanding datasets, such as [FUNSD](https://guillaumejaume.github.io/FUNSD/) and [CORD](https://github.com/clovaai/cord).'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 896}, page_content='The script `run_funsd_cord.py` leverages the 🤗 Datasets library and the Trainer API. You can easily customize it to your needs.\\n\\n## Fine-tuning on FUNSD\\n\\nFine-tuning LayoutLMv3 for token classification on [FUNSD](https://guillaumejaume.github.io/FUNSD/) can be done as follows:\\n\\n```bash\\npython run_funsd_cord.py \\\\\\n  --model_name_or_path microsoft/layoutlmv3-base \\\\\\n  --dataset_name funsd \\\\\\n  --output_dir layoutlmv3-test \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_steps 1000 \\\\\\n  --evaluation_strategy steps \\\\\\n  --eval_steps 100 \\\\\\n  --learning_rate 1e-5 \\\\\\n  --load_best_model_at_end \\\\\\n  --metric_for_best_model \"eval_f1\" \\\\\\n  --push_to_hub \\\\\\n  --push_to_hub°model_id layoutlmv3-finetuned-funsd'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 1584}, page_content='```\\n\\n👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd. By specifying the `push_to_hub` flag, the model gets uploaded automatically to the hub (regularly), together with a model card, which includes metrics such as precision, recall and F1. Note that you can easily update the model card, as it\\'s just a README file of the respective repo on the hub.\\n\\nThere\\'s also the \"Training metrics\" [tab](https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd/tensorboard), which shows Tensorboard logs over the course of training. Pretty neat, huh?\\n\\n## Fine-tuning on CORD\\n\\nFine-tuning LayoutLMv3 for token classification on [CORD](https://github.com/clovaai/cord) can be done as follows:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 2314}, page_content='```bash\\npython run_funsd_cord.py \\\\\\n  --model_name_or_path microsoft/layoutlmv3-base \\\\\\n  --dataset_name cord \\\\\\n  --output_dir layoutlmv3-test \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_steps 1000 \\\\\\n  --evaluation_strategy steps \\\\\\n  --eval_steps 100 \\\\\\n  --learning_rate 5e-5 \\\\\\n  --load_best_model_at_end \\\\\\n  --metric_for_best_model \"eval_f1\" \\\\\\n  --push_to_hub \\\\\\n  --push_to_hub°model_id layoutlmv3-finetuned-cord'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 2721}, page_content='```\\n\\n👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-cord. Note that a model card gets generated automatically in case you specify the `push_to_hub` flag.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/03_building-with-blocks/03_state-in-blocks.md', 'start_index': 1}, page_content=\"State in Blocks\\n\\nWe covered [State in Interfaces](https://gradio.app/interface-state), this guide takes a look at state in Blocks, which works mostly the same.\\n\\n## Global State\\n\\nGlobal state in Blocks works the same as in Interface. Any variable created outside a function call is a reference shared between all users.\\n\\n## Session State\\n\\nGradio supports session **state**, where data persists across multiple submits within a page session, in Blocks apps as well. To reiterate, session data is _not_ shared between different users of your model. To store data in a session state, you need to do three things:\\n\\n1. Create a `gr.State()` object. If there is a default value to this stateful object, pass that into the constructor.\\n2. In the event listener, put the `State` object as an input and output.\\n3. In the event listener function, add the variable to the input parameters and the return value.\\n\\nLet's take a look at a game of hangman.\\n\\n$code_hangman\\n$demo_hangman\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/03_building-with-blocks/03_state-in-blocks.md', 'start_index': 901}, page_content=\"Let's take a look at a game of hangman.\\n\\n$code_hangman\\n$demo_hangman\\n\\nLet's see how we do each of the 3 steps listed above in this game:\\n\\n1. We store the used letters in `used_letters_var`. In the constructor of `State`, we set the initial value of this to `[]`, an empty list.\\n2. In `btn.click()`, we have a reference to `used_letters_var` in both the inputs and outputs.\\n3. In `guess_letter`, we pass the value of this `State` to `used_letters`, and then return an updated value of this `State` in the return statement.\\n\\nWith more complex apps, you will likely have many State variables storing session state in a single Blocks app.\\n\\nLearn more about `State` in the [docs](https://gradio.app/docs#state).\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 1}, page_content='如何使用地图组件绘制图表\\n\\nRelated spaces:\\nTags: PLOTS, MAPS\\n\\n## 简介\\n\\n本指南介绍如何使用 Gradio 的 `Plot` 组件在地图上绘制地理数据。Gradio 的 `Plot` 组件可以与 Matplotlib、Bokeh 和 Plotly 一起使用。在本指南中，我们将使用 Plotly 进行操作。Plotly 可以让开发人员轻松创建各种地图来展示他们的地理数据。点击[这里](https://plotly.com/python/maps/)查看一些示例。\\n\\n## 概述\\n\\n我们将使用纽约市的 Airbnb 数据集，该数据集托管在 kaggle 上，点击[这里](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)。我已经将其上传到 Hugging Face Hub 作为一个数据集，方便使用和下载，点击[这里](https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data)。使用这些数据，我们将在地图上绘制 Airbnb 的位置，并允许基于价格和位置进行筛选。下面是我们将要构建的演示。 ⚡️\\n\\n$demo_map_airbnb\\n\\n## 步骤 1-加载 CSV 数据 💾\\n\\n让我们首先从 Hugging Face Hub 加载纽约市的 Airbnb 数据。\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\\ndf = dataset.to_pandas()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 677}, page_content='dataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\\ndf = dataset.to_pandas()\\n\\ndef filter_map(min_price, max_price, boroughs):\\n    new_df = df[(df[\\'neighbourhood_group\\'].isin(boroughs)) &\\n            (df[\\'price\\'] > min_price) & (df[\\'price\\'] < max_price)]\\n    names = new_df[\"name\"].tolist()\\n    prices = new_df[\"price\"].tolist()\\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 1092}, page_content='```\\n\\n在上面的代码中，我们先将 CSV 数据加载到一个 pandas dataframe 中。让我们首先定义一个函数，这将作为 gradio 应用程序的预测函数。该函数将接受最低价格、最高价格范围和筛选结果地区的列表作为参数。我们可以使用传入的值 (`min_price`、`max_price` 和地区列表) 来筛选数据框并创建 `new_df`。接下来，我们将创建包含每个 Airbnb 的名称和价格的 `text_list`，以便在地图上使用作为标签。\\n\\n## 步骤 2-地图图表 🌐\\n\\nPlotly 使得处理地图变得很容易。让我们看一下下面的代码，了解如何创建地图图表。\\n\\n```python\\nimport plotly.graph_objects as go\\n\\nfig = go.Figure(go.Scattermapbox(\\n            customdata=text_list,\\n            lat=new_df[\\'latitude\\'].tolist(),\\n            lon=new_df[\\'longitude\\'].tolist(),\\n            mode=\\'markers\\',\\n            marker=go.scattermapbox.Marker(\\n                size=6\\n            ),\\n            hoverinfo=\"text\",\\n            hovertemplate=\\'<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}\\'\\n        ))\\n\\nfig.update_layout(\\n    mapbox_style=\"open-street-map\",\\n    hovermode=\\'closest\\',\\n    mapbox=dict(\\n        bearing=0,\\n        center=go.layout.mapbox.Center(\\n            lat=40.67,\\n            lon=-73.90\\n        ),\\n        pitch=0,\\n        zoom=9\\n    ),\\n)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 2088}, page_content='```\\n\\n上面的代码中，我们通过传入经纬度列表来创建一个散点图。我们还传入了名称和价格的自定义数据，以便在鼠标悬停在每个标记上时显示额外的信息。接下来，我们使用 `update_layout` 来指定其他地图设置，例如缩放和居中。\\n\\n有关使用 Mapbox 和 Plotly 创建散点图的更多信息，请点击[这里](https://plotly.com/python/scattermapbox/)。\\n\\n## 步骤 3-Gradio 应用程序 ⚡️\\n\\n我们将使用两个 `gr.Number` 组件和一个 `gr.CheckboxGroup` 组件，允许用户指定价格范围和地区位置。然后，我们将使用 `gr.Plot` 组件作为我们之前创建的 Plotly + Mapbox 地图的输出。\\n\\n```python\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\\n        btn = gr.Button(value=\"Update Filter\")\\n        map = gr.Plot()\\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\\n    btn.click(filter_map, [min_price, max_price, boroughs], map)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 3014}, page_content='```\\n\\n我们使用 `gr.Column` 和 `gr.Row` 布局这些组件，并为演示加载时和点击 \" 更新筛选 \" 按钮时添加了事件触发器，以触发地图更新新的筛选条件。\\n\\n以下是完整演示代码：\\n\\n$code_map_airbnb\\n\\n## 步骤 4-部署 Deployment 🤗\\n\\n如果你运行上面的代码，你的应用程序将在本地运行。\\n如果要获取临时共享链接，可以将 `share=True` 参数传递给 `launch`。\\n\\n但如果你想要一个永久的部署解决方案呢？\\n让我们将我们的 Gradio 应用程序部署到免费的 HuggingFace Spaces 平台。\\n\\n如果你以前没有使用过 Spaces，请按照之前的指南[这里](/using_hugging_face_integrations)。\\n\\n## 结论 🎉\\n\\n你已经完成了！这是构建地图演示所需的所有代码。\\n\\n链接到演示：[地图演示](https://huggingface.co/spaces/gradio/map_airbnb)和[完整代码](https://huggingface.co/spaces/gradio/map_airbnb/blob/main/run.py)（在 Hugging Face Spaces）'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 1}, page_content=\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) that employs [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block) to enable the network to perform dynamic channel-wise feature recalibration.\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model('seresnet152d', pretrained=True)\\n>>> model.eval()\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 458}, page_content='```\\n\\nTo load and preprocess the image:\\n\\n```py \\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 1253}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n>>> # Get imagenet class mappings\\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\n>>> urllib.request.urlretrieve(url, filename) \\n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\\n...     categories = [s.strip() for s in f.readlines()]\\n\\n>>> # Print top categories per image\\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\\n>>> for i in range(top5_prob.size(0)):\\n...     print(categories[top5_catid[i]], top5_prob[i].item())\\n>>> # prints class names and probabilities like:\\n>>> # [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 2047}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('seresnet152d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 2600}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{hu2019squeezeandexcitation,\\n      title={Squeeze-and-Excitation Networks}, \\n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\\n      year={2019},\\n      eprint={1709.01507},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 3206}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 3211}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Networks\\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\\nModels:\\n- Name: seresnet152d\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 20161904304\\n    Parameters: 66840000\\n    File Size: 268144497\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet152d\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 152\\n    Dropout: 0.2\\n    Crop Pct: '0.94'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '256'\\n    Interpolation: bicubic\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 4118}, page_content=\"Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '256'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1206\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet152d_ra2-04464dd2.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83.74%\\n      Top 5 Accuracy: 96.77%\\n- Name: seresnet50\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 5285062320\\n    Parameters: 28090000\\n    File Size: 112621903\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 5006}, page_content=\"- Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet50\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 50\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1180\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet50_ra_224-8efdb4bb.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.26%\\n      Top 5 Accuracy: 95.07%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 0}, page_content=\"--\\ntitle: poseval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  The poseval metric can be used to evaluate POS taggers. Since seqeval does not work well with POS data \\n  that is not in IOB format the poseval is an alternative. It treats each token in the dataset as independant \\n  observation and computes the precision, recall and F1-score irrespective of sentences. It uses scikit-learns's\\n  classification report to compute the scores.\\n---\\n\\n# Metric Card for peqeval\\n\\n## Metric description\"),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 544}, page_content=\"# Metric Card for peqeval\\n\\n## Metric description\\n\\nThe poseval metric can be used to evaluate POS taggers. Since seqeval does not work well with POS data (see e.g. [here](https://stackoverflow.com/questions/71327693/how-to-disable-seqeval-label-formatting-for-pos-tagging)) that is not in IOB format the poseval is an alternative. It treats each token in the dataset as independant observation and computes the precision, recall and F1-score irrespective of sentences. It uses scikit-learns's [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to compute the scores.\\n\\n\\n## How to use \\n\\nPoseval produces labelling scores along with its sufficient statistics from a source against references.\\n\\nIt takes two mandatory arguments:\\n\\n`predictions`: a list of lists of predicted labels, i.e. estimated targets as returned by a tagger.\\n\\n`references`: a list of lists of reference labels, i.e. the ground truth/target values.\"),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 1437}, page_content='`references`: a list of lists of reference labels, i.e. the ground truth/target values.\\n\\nIt can also take several optional arguments:\\n\\n`zero_division`: Which value to substitute as a metric value when encountering zero division. Should be one of [`0`,`1`,`\"warn\"`]. `\"warn\"` acts as `0`, but the warning is raised.\\n\\n\\n```python\\n>>> predictions = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'NOUN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'VERB\\', \\'SYM\\']]\\n>>> references = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'SYM\\']]\\n>>> poseval = evaluate.load(\"poseval\")\\n>>> results = poseval.compute(predictions=predictions, references=references)\\n>>> print(list(results.keys()))\\n[\\'ADP\\', \\'INTJ\\', \\'NOUN\\', \\'PROPN\\', \\'PUNCT\\', \\'SYM\\', \\'VERB\\', \\'accuracy\\', \\'macro avg\\', \\'weighted avg\\']\\n>>> print(results[\"accuracy\"])\\n0.8\\n>>> print(results[\"PROPN\"][\"recall\"])\\n0.5'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 2291}, page_content='```\\n\\n## Output values\\n\\nThis metric returns a a classification report as a dictionary with a summary of scores for overall and per type:\\n\\nOverall (weighted and macro avg):\\n\\n`accuracy`: the average [accuracy](https://huggingface.co/metrics/accuracy), on a scale between 0.0 and 1.0.\\n    \\n`precision`: the average [precision](https://huggingface.co/metrics/precision), on a scale between 0.0 and 1.0.\\n    \\n`recall`: the average [recall](https://huggingface.co/metrics/recall), on a scale between 0.0 and 1.0.\\n\\n`f1`: the average [F1 score](https://huggingface.co/metrics/f1), which is the harmonic mean of the precision and recall. It also has a scale of 0.0 to 1.0.\\n\\nPer type (e.g. `MISC`, `PER`, `LOC`,...):\\n\\n`precision`: the average [precision](https://huggingface.co/metrics/precision), on a scale between 0.0 and 1.0.\\n\\n`recall`: the average [recall](https://huggingface.co/metrics/recall), on a scale between 0.0 and 1.0.'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 3215}, page_content='`f1`: the average [F1 score](https://huggingface.co/metrics/f1), on a scale between 0.0 and 1.0.\\n\\n\\n## Examples \\n\\n```python\\n>>> predictions = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'NOUN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'VERB\\', \\'SYM\\']]\\n>>> references = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'SYM\\']]\\n>>> poseval = evaluate.load(\"poseval\")\\n>>> results = poseval.compute(predictions=predictions, references=references)\\n>>> print(list(results.keys()))\\n[\\'ADP\\', \\'INTJ\\', \\'NOUN\\', \\'PROPN\\', \\'PUNCT\\', \\'SYM\\', \\'VERB\\', \\'accuracy\\', \\'macro avg\\', \\'weighted avg\\']\\n>>> print(results[\"accuracy\"])\\n0.8\\n>>> print(results[\"PROPN\"][\"recall\"])\\n0.5'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 3865}, page_content='```\\n\\n## Limitations and bias\\n\\nIn contrast to [seqeval](https://github.com/chakki-works/seqeval), the poseval metric treats each token independently and computes the classification report over all concatenated sequences..\\n\\n\\n## Citation\\n\\n```bibtex\\n@article{scikit-learn,\\n title={Scikit-learn: Machine Learning in {P}ython},\\n author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n journal={Journal of Machine Learning Research},\\n volume={12},\\n pages={2825--2830},\\n year={2011}\\n}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 4588}, page_content='```\\n    \\n## Further References \\n- [README for seqeval at GitHub](https://github.com/chakki-works/seqeval)\\n- [Classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) \\n- [Issues with seqeval](https://stackoverflow.com/questions/71327693/how-to-disable-seqeval-label-formatting-for-pos-tagging)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 0}, page_content='--\\ntitle: \"Large Language Models: A New Moore\\'s Law?\"\\nthumbnail: /blog/assets/33_large_language_models/01_model_size.jpg\\nauthors:\\n- user: juliensimon\\n---\\n\\n# Large Language Models: A New Moore\\'s Law?\\n\\n\\n\\nA few days ago, Microsoft and NVIDIA [introduced](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) Megatron-Turing NLG 530B, a Transformer-based model hailed as \"*the world’s largest and most powerful generative language model*.\"\\n \\nThis is an impressive show of Machine Learning engineering, no doubt about it. Yet, should we be excited about this mega-model trend?  I, for one, am not. Here\\'s why.\\n\\n<kbd>\\n  <img src=\"assets/33_large_language_models/01_model_size.jpg\">\\n</kbd>\\n\\n### This is your Brain on Deep Learning'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 805}, page_content=\"### This is your Brain on Deep Learning\\n\\nResearchers estimate that the human brain contains an average of [86 billion neurons](https://pubmed.ncbi.nlm.nih.gov/19226510/) and 100 trillion synapses. It's safe to assume that not all of them are dedicated to language either. Interestingly, GPT-4 is [expected](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/) to have about 100 trillion parameters... As crude as this analogy is, shouldn't we wonder whether building language models that are about the size of the human brain is the best long-term approach?\\n\\nOf course, our brain is a marvelous device, produced by millions of years of evolution, while Deep Learning models are only a few decades old. Still, our intuition should tell us that something doesn't compute (pun intended).\\n\\n### Deep Learning, Deep Pockets?\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 1609}, page_content='### Deep Learning, Deep Pockets?\\n\\nAs you would expect, training a 530-billion parameter model on humongous text datasets requires a fair bit of infrastructure. In fact, Microsoft and NVIDIA used hundreds of DGX A100 multi-GPU servers. At $199,000 a piece, and factoring in networking equipment, hosting costs, etc., anyone looking to replicate this experiment would have to spend close to $100 million dollars. Want fries with that?\\n\\nSeriously, which organizations have business use cases that would justify spending $100 million on Deep Learning infrastructure? Or even $10 million? Very few. So who are these models for, really?\\n\\n### That Warm Feeling is your GPU Cluster'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 2241}, page_content='### That Warm Feeling is your GPU Cluster\\n\\nFor all its engineering brilliance, training Deep Learning models on GPUs is a brute force technique. According to the spec sheet, each DGX server can consume up to 6.5 kilowatts. Of course, you\\'ll need at least as much cooling power in your datacenter (or your server closet). Unless you\\'re the Starks and need to keep Winterfell warm in winter, that\\'s another problem you\\'ll have to deal with. \\n\\nIn addition, as public awareness grows on climate and social responsibility issues, organizations need to account for their carbon footprint. According to this 2019 [study](https://arxiv.org/pdf/1906.02243.pdf) from the University of Massachusetts, \"*training BERT on GPU is roughly equivalent to a trans-American flight*\".\\n\\nBERT-Large has 340 million parameters. One can only extrapolate what the footprint of Megatron-Turing could be... People who know me wouldn\\'t call me a bleeding-heart environmentalist. Still, some numbers are hard to ignore.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 3233}, page_content=\"### So?\\n\\nAm I excited by Megatron-Turing NLG 530B and whatever beast is coming next? No. Do I think that the (relatively small) benchmark improvement is worth the added cost, complexity and carbon footprint? No. Do I think that building and promoting these huge models is helping organizations understand and adopt Machine Learning ? No.\\n\\nI'm left wondering what's the point of it all. Science for the sake of science? Good old marketing? Technological supremacy? Probably a bit of each. I'll leave them to it, then.\\n\\nInstead, let me focus on pragmatic and actionable techniques that you can all use to build high quality Machine Learning solutions.\\n\\n### Use Pretrained Models\\n\\nIn the vast majority of cases, you won't need a custom model architecture. Maybe you'll *want* a custom one (which is a different thing), but there be dragons. Experts only!\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 4086}, page_content=\"A good starting point is to look for [models](https://huggingface.co/models) that have been pretrained for the task you're trying to solve (say, [summarizing English text](https://huggingface.co/models?language=en&pipeline_tag=summarization&sort=downloads)).\\n\\nThen, you should quickly try out a few models to predict your own data. If metrics tell you that one works well enough, you're done! If you need a little more accuracy, you should consider fine-tuning the model (more on this in a minute).\\n\\n### Use Smaller Models\\n\\nWhen evaluating models, you should pick the smallest one that can deliver the accuracy you need. It will predict faster and require fewer hardware resources for training and inference. Frugality goes a long way.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 4823}, page_content=\"It's nothing new either. Computer Vision practitioners will remember when [SqueezeNet](https://arxiv.org/abs/1602.07360) came out in 2017, achieving a 50x reduction in model size compared to [AlexNet](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html), while meeting or exceeding its accuracy. How clever that was!\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 5175}, page_content=\"Downsizing efforts are also under way in the Natural Language Processing community, using transfer learning techniques such as [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation). [DistilBERT](https://arxiv.org/abs/1910.01108) is perhaps its most widely known achievement. Compared to the original BERT model, it retains 97% of language understanding while being 40% smaller and 60% faster. You can try it [here](https://huggingface.co/distilbert-base-uncased). The same approach has been applied to other models, such as Facebook's [BART](https://arxiv.org/abs/1910.13461), and you can try DistilBART [here](https://huggingface.co/models?search=distilbart).\\n\\nRecent models from the [Big Science](https://bigscience.huggingface.co/) project are also very impressive. As visible in this graph included in the [research paper](https://arxiv.org/abs/2110.08207), their T0 model outperforms GPT-3 on many tasks while being 16x smaller.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 6135}, page_content='<kbd>\\n  <img src=\"assets/33_large_language_models/02_t0.png\">\\n</kbd>\\n\\nYou can try T0 [here](https://huggingface.co/bigscience/T0pp). This is the kind of research we need more of!\\n\\n### Fine-Tune Models\\n\\nIf you need to specialize a model, there should be very few reasons to train it from scratch. Instead, you should fine-tune it, that is to say train it only for a few epochs on your own data. If you\\'re short on data, maybe of one these [datasets](https://huggingface.co/datasets) can get you started.\\n\\nYou guessed it, that\\'s another way to do transfer learning, and it\\'ll help you save on everything!\\n \\n* Less data to collect, store, clean and annotate,\\n* Faster experiments and iterations,\\n* Fewer resources required in production.\\n\\nIn other words: save time, save money, save hardware resources, save the world! \\n\\nIf you need a tutorial, the Hugging Face [course](https://huggingface.co/course) will get you started in no time.\\n\\n### Use Cloud-Based Infrastructure'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 7068}, page_content='### Use Cloud-Based Infrastructure\\n\\nLike them or not, cloud companies know how to build efficient infrastructure. Sustainability studies show that cloud-based infrastructure is more energy and carbon efficient than the alternative: see [AWS](https://sustainability.aboutamazon.com/environment/the-cloud), [Azure](https://azure.microsoft.com/en-us/global-infrastructure/sustainability), and [Google](https://cloud.google.com/sustainability). Earth.org [says](https://earth.org/environmental-impact-of-cloud-computing/) that while cloud infrastructure is not perfect, \"[*it\\'s] more energy efficient than the alternative and facilitates environmentally beneficial services and economic growth.*\"'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 7762}, page_content=\"Cloud certainly has a lot going for it when it comes to ease of use, flexibility and pay as you go. It's also a little greener than you probably thought. If you're short on GPUs, why not try fine-tune your Hugging Face models on [Amazon SageMaker](https://aws.amazon.com/sagemaker/), AWS' managed service for Machine Learning? We've got [plenty of examples](https://huggingface.co/docs/sagemaker/train) for you.\\n\\n### Optimize Your Models\\n\\nFrom compilers to virtual machines, software engineers have long used tools that automatically optimize their code for whatever hardware they're running on. \\n\\nHowever, the Machine Learning community is still struggling with this topic, and for good reason. Optimizing models for size and speed is a devilishly complex task, which involves techniques such as:\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 8561}, page_content=\"* Specialized hardware that speeds up training ([Graphcore](https://www.graphcore.ai/), [Habana](https://habana.ai/)) and inference ([Google TPU](https://cloud.google.com/tpu), [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)).\\n* Pruning: remove model parameters that have little or no impact on the predicted outcome.\\n* Fusion: merge model layers (say, convolution and activation).\\n* Quantization: storing model parameters in smaller values (say, 8 bits instead of 32 bits)\\n\\nFortunately, automated tools are starting to appear, such as the [Optimum](https://huggingface.co/hardware) open source library, and [Infinity](https://huggingface.co/infinity), a containerized solution that delivers Transformers accuracy at 1-millisecond latency.\\n\\n### Conclusion \\n\\nLarge language model size has been increasing 10x every year for the last few years. This is starting to look like another [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law).\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 9527}, page_content=\"We've been there before, and we should know that this road leads to diminishing returns, higher cost, more complexity, and new risks. Exponentials tend not to end well. Remember [Meltdown and Spectre](https://meltdownattack.com/)? Do we want to find out what that looks like for AI?\\n\\nInstead of chasing trillion-parameter models (place your bets), wouldn't all be better off if we built practical and efficient solutions that all developers can use to solve real-world problems?\\n\\n*Interested in how Hugging Face can help your organization build and deploy production-grade Machine Learning solutions? Get in touch at [julsimon@huggingface.co](mailto:julsimon@huggingface.co) (no recruiters, no sales pitches, please).*\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md', 'start_index': 0}, page_content='!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# VisionTextDualEncoder\\n\\n## Overview'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md', 'start_index': 746}, page_content='-->\\n\\n# VisionTextDualEncoder\\n\\n## Overview\\n\\nThe [`VisionTextDualEncoderModel`] can be used to initialize a vision-text dual encoder model with\\nany pretrained vision autoencoding model as the vision encoder (*e.g.* [ViT](vit), [BEiT](beit), [DeiT](deit)) and any pretrained text autoencoding model as the text encoder (*e.g.* [RoBERTa](roberta), [BERT](bert)). Two projection layers are added on top of both the vision and text encoder to project the output embeddings\\nto a shared latent space. The projection layers are randomly initialized so the model should be fine-tuned on a\\ndownstream task. This model can be used to align the vision-text embeddings using CLIP like contrastive image-text\\ntraining and then can be used for zero-shot vision tasks such image-classification or retrieval.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md', 'start_index': 1538}, page_content='In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how\\nleveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement on\\nnew zero-shot vision tasks such as image classification or retrieval.\\n\\n## VisionTextDualEncoderConfig\\n\\n[[autodoc]] VisionTextDualEncoderConfig\\n\\n## VisionTextDualEncoderProcessor\\n\\n[[autodoc]] VisionTextDualEncoderProcessor\\n\\n<frameworkcontent>\\n<pt>\\n\\n## VisionTextDualEncoderModel\\n\\n[[autodoc]] VisionTextDualEncoderModel\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## FlaxVisionTextDualEncoderModel\\n\\n[[autodoc]] FlaxVisionTextDualEncoderModel\\n    - __call__\\n\\n</tf>\\n<jax>\\n\\n## TFVisionTextDualEncoderModel\\n\\n[[autodoc]] TFVisionTextDualEncoderModel\\n    - call\\n\\n</jax>\\n</frameworkcontent>'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02d_dynamic-padding.md', 'start_index': 0}, page_content='hat is dynamic padding? In the \"Batching Inputs together\" video, we have seen that to be able to group inputs of different lengths in the same batch, we need to add padding tokens to all the short inputs until they are all of the same length. Here for instance, the longest sentence is the third one, and we need to add 5, 2 and 7 pad tokens to the other to have four sentences of the same lengths. When dealing with a whole dataset, there are various padding strategies we can apply. The most obvious one is to pad all the elements of the dataset to the same length: the length of the longest sample. This will then give us batches that all have the same shape determined by the maximum sequence length. The downside is that batches composed from short sentences will have a lot of padding tokens which introduce more computations in the model we ultimately don\\'t need. To avoid this, another strategy is to pad the elements when we batch them together, to the longest sentence inside the batch.'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02d_dynamic-padding.md', 'start_index': 903}, page_content=\"is to pad the elements when we batch them together, to the longest sentence inside the batch. This way batches composed of short inputs will be smaller than the batch containing the longest sentence in the dataset. This will yield some nice speedup on CPU and GPU. The downside is that all batches will then have different shapes, which slows down training on other accelerators like TPUs. Let's see how to apply both strategies in practice. We have actually seen how to apply fixed padding in the Datasets Overview video, when we preprocessed the MRPC dataset: after loading the dataset and tokenizer, we applied the tokenization to all the dataset with padding and truncation to make all samples of length 128. As a result, if we pass this dataset to a PyTorch DataLoader, we get batches of shape batch size (here 16) by 128. To apply dynamic padding, we must defer the padding to the batch preparation, so we remove that part from our tokenize function. We still leave the truncation part so that\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02d_dynamic-padding.md', 'start_index': 1809}, page_content='so we remove that part from our tokenize function. We still leave the truncation part so that inputs that are bigger than the maximum length accepted by the model (usually 512) get truncated to that length. Then we pad our samples dynamically by using a data collator. Those classes in the Transformers library are responsible for applying all the final processing needed before forming a batch, here DataCollatorWithPadding will pad the samples to the maximum length inside the batch of sentences. We pass it to the PyTorch DataLoader as a collate function, then observe that the batches generated have various lenghs, all way below the 128 from before. Dynamic batching will almost always be faster on CPUs and GPUs, so you should apply it if you can. Remember to switch back to fixed padding however if you run your training script on TPU or need batches of fixed shapes.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Kandinsky 3'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 584}, page_content=\"# Kandinsky 3\\n\\nKandinsky 3 is created by [Vladimir Arkhipkin](https://github.com/oriBetelgeuse),[Anastasia Maltseva](https://github.com/NastyaMittseva),[Igor Pavlov](https://github.com/boomb0om),[Andrei Filatov](https://github.com/anvilarth),[Arseniy Shakhmatov](https://github.com/cene555),[Andrey Kuznetsov](https://github.com/kuznetsoffandrey),[Denis Dimitrov](https://github.com/denndimitrov), [Zein Shaheen](https://github.com/zeinsh)\\n\\nThe description from it's Github page: \\n\\n*Kandinsky 3.0 is an open-source text-to-image diffusion model built upon the Kandinsky2-x model family. In comparison to its predecessors, enhancements have been made to the text understanding and visual quality of the model, achieved by increasing the size of the text encoder and Diffusion U-Net models, respectively.*\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 1389}, page_content='Its architecture includes 3 main components:\\n1. [FLAN-UL2](https://huggingface.co/google/flan-ul2), which is an encoder decoder model based on the T5 architecture. \\n2. New U-Net architecture featuring BigGAN-deep blocks doubles depth while maintaining the same number of parameters.\\n3. Sber-MoVQGAN is a decoder proven to have superior results in image restoration.\\n\\n\\n\\nThe original codebase can be found at [ai-forever/Kandinsky-3](https://github.com/ai-forever/Kandinsky-3).\\n\\n<Tip>\\n\\nCheck out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\\n\\n</Tip>\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 2075}, page_content='</Tip>\\n\\n<Tip>\\n\\nMake sure to check out the schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## Kandinsky3Pipeline\\n\\n[[autodoc]] Kandinsky3Pipeline\\n\\t- all\\n\\t- __call__\\n\\n## Kandinsky3Img2ImgPipeline\\n\\n[[autodoc]] Kandinsky3Img2ImgPipeline\\n\\t- all\\n\\t- __call__'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 1}, page_content='Datasets server - worker\\n\\n> Workers that pre-compute and cache the response to /splits, /first-rows, /parquet, /info and /size.\\n\\n## Configuration\\n\\nUse environment variables to configure the workers. The prefix of each environment variable gives its scope.\\n\\n### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`WORKER_UVICORN_` prefix). It is used for the /healthcheck and the /metrics endpoints:\\n\\n- `WORKER_UVICORN_HOSTNAME`: the hostname. Defaults to `\"localhost\"`.\\n- `WORKER_UVICORN_NUM_WORKERS`: the number of uvicorn workers. Defaults to `2`.\\n- `WORKER_UVICORN_PORT`: the port. Defaults to `8000`.\\n\\n### Prometheus\\n\\n- `PROMETHEUS_MULTIPROC_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every uvicorn worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 986}, page_content='## Worker configuration\\n\\nSet environment variables to configure the worker.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 1063}, page_content='- `WORKER_CONTENT_MAX_BYTES`: the maximum size in bytes of the response content computed by a worker (to prevent returning big responses in the REST API). Defaults to `10_000_000`.\\n- `WORKER_DIFFICULTY_MAX`: the maximum difficulty of the jobs to process. Defaults to None.\\n- `WORKER_DIFFICULTY_MIN`: the minimum difficulty of the jobs to process. Defaults to None.\\n- `WORKER_HEARTBEAT_INTERVAL_SECONDS`: the time interval between two heartbeats. Each heartbeat updates the job \"last_heartbeat\" field in the queue. Defaults to `60` (1 minute).\\n- `WORKER_JOB_TYPES_BLOCKED`: comma-separated list of job types that will not be processed, e.g. \"dataset-config-names,dataset-split-names\". If empty, no job type is blocked. Defaults to empty.\\n- `WORKER_JOB_TYPES_ONLY`: comma-separated list of the non-blocked job types to process, e.g. \"dataset-config-names,dataset-split-names\". If empty, the worker processes all the non-blocked jobs. Defaults to empty.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 2014}, page_content='- `WORKER_KILL_LONG_JOB_INTERVAL_SECONDS`: the time interval at which the worker looks for long jobs to kill them. Defaults to `60` (1 minute).\\n- `WORKER_KILL_ZOMBIES_INTERVAL_SECONDS`: the time interval at which the worker looks for zombie jobs to kill them. Defaults to `600` (10 minutes).\\n- `WORKER_MAX_DISK_USAGE_PCT`: maximum disk usage of every storage disk in the list (in percentage) to allow a job to start. Set to 0 to disable the test. Defaults to 90.\\n- `WORKER_MAX_JOB_DURATION_SECONDS`: the maximum duration allowed for a job to run. If the job runs longer, it is killed (see `WORKER_KILL_LONG_JOB_INTERVAL_SECONDS`). Defaults to `1200` (20 minutes).\\n- `WORKER_MAX_LOAD_PCT`: maximum load of the machine (in percentage: the max between the 1m load and the 5m load divided by the number of CPUs \\\\*100) allowed to start a job. Set to 0 to disable the test. Defaults to 70.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 2898}, page_content=\"- `WORKER_MAX_MEMORY_PCT`: maximum memory (RAM + SWAP) usage of the machine (in percentage) allowed to start a job. Set to 0 to disable the test. Defaults to 80.\\n- `WORKER_MAX_MISSING_HEARTBEATS`: the number of hearbeats a job must have missed to be considered a zombie job. Defaults to `5`.\\n- `WORKER_SLEEP_SECONDS`: wait duration in seconds at each loop iteration before checking if resources are available and processing a job if any is available. Note that the loop doesn't wait just after finishing a job: the next job is immediately processed. Defaults to `15`.\\n- `WORKER_STORAGE_PATHS`: comma-separated list of paths to check for disk usage. Defaults to empty.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 3567}, page_content=\"Also, it's possible to force the parent directory in which the temporary files (as the current job state file and its associated lock file) will be created by setting `TMPDIR` to a writable directory. If not set, the worker will use the default temporary directory of the system, as described in https://docs.python.org/3/library/tempfile.html#tempfile.gettempdir.\\n\\n### Datasets based worker\\n\\nSet environment variables to configure the datasets-based worker (`DATASETS_BASED_` prefix):\\n\\n- `DATASETS_BASED_HF_DATASETS_CACHE`: directory where the `datasets` library will store the cached datasets' data. If not set, the datasets library will choose the default location. Defaults to None.\\n\\nAlso, set the modules cache configuration for the datasets-based worker. See [../../libs/libcommon/README.md](../../libs/libcommon/README.md). Note that this variable has no `DATASETS_BASED_` prefix:\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 4456}, page_content='- `HF_MODULES_CACHE`: directory where the `datasets` library will store the cached dataset scripts. If not set, the datasets library will choose the default location. Defaults to None.\\n\\nNote that both directories will be appended to `WORKER_STORAGE_PATHS` (see [../../libs/libcommon/README.md](../../libs/libcommon/README.md)) to hold the workers when the disk is full.\\n\\n### Numba library\\n\\nNumba requires setting the `NUMBA_CACHE_DIR` environment variable to a writable directory to cache the compiled functions. Required on cloud infrastructure (see https://stackoverflow.com/a/63367171/7351594):\\n\\n- `NUMBA_CACHE_DIR`: directory where the `numba` decorators (used by `librosa`) can write cache.\\n\\nNote that this directory will be appended to `WORKER_STORAGE_PATHS` (see [../../libs/libcommon/README.md](../../libs/libcommon/README.md)) to hold the workers when the disk is full.\\n\\n### Huggingface_hub library'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 5336}, page_content='### Huggingface_hub library\\n\\nIf the Hub is not https://huggingface.co (i.e., if you set the `COMMON_HF_ENDPOINT` environment variable), you must set the `HF_ENDPOINT` environment variable to the same value. See https://github.com/huggingface/datasets/pull/5196#issuecomment-1322191411 for more details:\\n\\n- `HF_ENDPOINT`: the URL of the Hub. Defaults to `https://huggingface.co`.\\n\\n### First rows worker\\n\\nSet environment variables to configure the `first-rows` worker (`FIRST_ROWS_` prefix):'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 5739}, page_content='Set environment variables to configure the `first-rows` worker (`FIRST_ROWS_` prefix):\\n\\n- `FIRST_ROWS_MAX_BYTES`: the max size of the /first-rows response in bytes. Defaults to `1_000_000` (1 MB).\\n- `FIRST_ROWS_MAX_NUMBER`: the max number of rows fetched by the worker for the split and provided in the /first-rows response. Defaults to `100`.\\n- `FIRST_ROWS_MIN_CELL_BYTES`: the minimum size in bytes of a cell when truncating the content of a row (see `FIRST_ROWS_ROWS_MAX_BYTES`). Below this limit, the cell content will not be truncated. Defaults to `100`.\\n- `FIRST_ROWS_MIN_NUMBER`: the min number of rows fetched by the worker for the split and provided in the /first-rows response. Defaults to `10`.\\n- `FIRST_ROWS_COLUMNS_MAX_NUMBER`: the max number of columns (features) provided in the /first-rows response. If the number of columns is greater than the limit, an error is returned. Defaults to `1_000`.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 6651}, page_content='Also, set the assets-related configuration for the first-rows worker. See [../../libs/libcommon/README.md](../../libs/libcommon/README.md).\\n\\n### Parquet and info worker\\n\\nSet environment variables to configure the `parquet-and-info` worker (`PARQUET_AND_INFO_` prefix):'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 6921}, page_content='- `PARQUET_AND_INFO_COMMIT_MESSAGE`: the git commit message when the worker uploads the parquet files to the Hub. Defaults to `Update parquet files`.\\n- `PARQUET_AND_INFO_COMMITTER_HF_TOKEN`: the HuggingFace token to commit the parquet files to the Hub. The token must be an app token associated with a user that has the right to 1. create the `refs/convert/parquet` branch (see `PARQUET_AND_INFO_TARGET_REVISION`) and 2. push commits to it on any dataset. [Datasets maintainers](https://huggingface.co/datasets-maintainers) members have these rights. The token must have permission to write. If not set, the worker will fail. Defaults to None.\\n- `PARQUET_AND_INFO_MAX_DATASET_SIZE_BYTES`: the maximum size in bytes of the dataset to pre-compute the parquet files. Bigger datasets, or datasets without that information, are partially streamed to get parquet files up to this value. Defaults to `100_000_000`.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 7829}, page_content='- `PARQUET_AND_INFO_MAX_EXTERNAL_DATA_FILES`: the maximum number of external files of the datasets. Bigger datasets, or datasets without that information, are partially streamed to get parquet files up to `PARQUET_AND_INFO_MAX_DATASET_SIZE_BYTES` bytes. Defaults to `10_000`.\\n- `PARQUET_AND_INFO_MAX_ROW_GROUP_BYTE_SIZE_FOR_COPY`: the maximum size in bytes of the row groups of parquet datasets that are copied to the target revision. Bigger datasets, or datasets without that information, are partially streamed to get parquet files up to `PARQUET_AND_INFO_MAX_DATASET_SIZE_BYTES` bytes. Defaults to `100_000_000`.\\n- `PARQUET_AND_INFO_SOURCE_REVISION`: the git revision of the dataset to use to prepare the parquet files. Defaults to `main`.\\n- `PARQUET_AND_INFO_TARGET_REVISION`: the git revision of the dataset where to store the parquet files. Make sure the committer token (`PARQUET_AND_INFO_COMMITTER_HF_TOKEN`) has the permission to write there. Defaults to `refs/convert/parquet`.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 8817}, page_content='- `PARQUET_AND_INFO_URL_TEMPLATE`: the URL template to build the parquet file URLs. Defaults to `/datasets/%s/resolve/%s/%s`.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 8944}, page_content='### Duckdb Index worker\\n\\nSet environment variables to configure the `duckdb-index` worker (`DUCKDB_INDEX_` prefix):'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 9061}, page_content=\"- `DUCKDB_INDEX_CACHE_DIRECTORY`: directory where the temporal duckdb index files are stored. Defaults to empty.\\n- `DUCKDB_INDEX_COMMIT_MESSAGE`: the git commit message when the worker uploads the duckdb index file to the Hub. Defaults to `Update duckdb index file`.\\n- `DUCKDB_INDEX_COMMITTER_HF_TOKEN`: the HuggingFace token to commit the duckdb index file to the Hub. The token must be an app token associated with a user that has the right to 1. create the `refs/convert/parquet` branch (see `DUCKDB_INDEX_TARGET_REVISION`) and 2. push commits to it on any dataset. [Datasets maintainers](https://huggingface.co/datasets-maintainers) members have these rights. The token must have permission to write. If not set, the worker will fail. Defaults to None.\\n- `DUCKDB_INDEX_MAX_DATASET_SIZE_BYTES`: the maximum size in bytes of the dataset's parquet files to index. Datasets with bigger size are ignored. Defaults to `100_000_000`.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 9992}, page_content='- `DUCKDB_INDEX_TARGET_REVISION`: the git revision of the dataset where to store the duckdb index file. Make sure the committer token (`DUCKDB_INDEX_COMMITTER_HF_TOKEN`) has the permission to write there. Defaults to `refs/convert/parquet`.\\n- `DUCKDB_INDEX_URL_TEMPLATE`: the URL template to build the duckdb index file URL. Defaults to `/datasets/%s/resolve/%s/%s`.\\n- `DUCKDB_INDEX_EXTENSIONS_DIRECTORY`: directory where the duckdb extensions will be downloaded. Defaults to empty.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 10476}, page_content=\"### Descriptive statistics worker\\n\\nSet environment variables to configure the `descriptive-statistics` worker (`DESCRIPTIVE_STATISTICS_` prefix):\\n\\n- `DESCRIPTIVE_STATISTICS_CACHE_DIRECTORY`: directory to which a dataset in parquet format is downloaded. Defaults to empty.\\n- `DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS`: number of histogram bins (see examples below for more info).\\n- `DESCRIPTIVE_STATISTICS_MAX_PARQUET_SIZE_BYTES`: maximum size in bytes of the dataset's parquet files to compute statistics. Datasets with bigger size are ignored. Defaults to `100_000_000`.\\n\\n#### How descriptive statistics are computed \\n\\nDescriptive statistics are currently computed for the following data types: strings, floats, and ints (including `ClassLabel` int). \\nResponse has two fields: `num_examples` and `statistics`. `statistics` field is a list of dicts with three keys: `column_name`, `column_type`, and `column_statistics`.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 11399}, page_content='`column_type` is one of the following values:\\n* `class_label` - for `datasets.ClassLabel` feature\\n* `float` - for float dtypes (\"float16\", \"float32\", \"float64\")\\n* `int` - for integer dtypes (\"int8\", \"int16\", \"int32\", \"int64\", \"uint8\", \"uint16\", \"uint32\", \"uint64\")\\n* `string_label` - for string dtypes (\"string\", \"large_string\") - if there are less than or equal to `MAX_NUM_STRING_LABELS` unique values (hardcoded in worker\\'s code, for now it\\'s 30)\\n* `string_text` - for string dtypes (\"string\", \"large_string\") - if there are more than `MAX_NUM_STRING_LABELS` unique values\\n* `bool` - for boolean dtype (\"bool\")\\n\\n`column_statistics` content depends on the feature type, see examples below.\\n##### class_label\\n\\n<details><summary>example: </summary>\\n<p>'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 12110}, page_content='<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    \"column_name\": \"class_col\",\\n    \"column_type\": \"class_label\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"no_label_count\": 0,  # number of -1 values - special value of the `datasets` lib to encode `no label` \\n        \"no_label_proportion\": 0.0,\\n        \"n_unique\": 5,  # number of unique values (excluding `no label` and nan)\\n        \"frequencies\": {   # mapping value -> its count\\n            \"this\": 19834,\\n            \"are\": 20159,\\n            \"random\": 20109,\\n            \"words\": 20172,\\n            \"test\": 19726\\n        }\\n    }\\n}'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 12752}, page_content='```\\n</p>\\n</details> \\n\\n##### float\\n\\nBin size for histogram is counted as `(max_value - min_value) / DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS`\\n\\n<details><summary>example: </summary>\\n<p>'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 12895}, page_content='<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    \"column_name\": \"delay\",\\n    \"column_type\": \"float\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": -10.206,\\n        \"max\": 8.48053,\\n        \"mean\": 2.10174,\\n        \"median\": 3.4012,\\n        \"std\": 3.12487,\\n        \"histogram\": {\\n            \"hist\": [\\n                2,\\n                34,\\n                256,\\n                15198,\\n                9037,\\n                2342,\\n                12743,\\n                45114,\\n                14904,\\n                370\\n            ],\\n            \"bin_edges\": [\\n                -10.206,\\n                -8.33734,\\n                -6.46869,\\n                -4.60004,\\n                -2.73139,\\n                -0.86273,\\n                1.00592,\\n                2.87457,\\n                4.74322,\\n                6.61188,\\n                8.48053  # includes maximum value, so len is always len(hist) + 1\\n            ]\\n        }\\n    }\\n}'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 13883}, page_content=\"```\\n</p>\\n</details> \\n\\n##### int\\n\\nAs bin edges for integer values also must be integers, bin size is counted as `np.ceil((max_value - min_value + 1) / DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS)`. Rounding up means that there might be smaller number of bins in response then provided `DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS`. The last bin's size might be smaller than that of the others if the feature's range is not divisible by the rounded bin size. \\n\\n<details><summary>examples: </summary>\\n<p>\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 14382}, page_content='```python\\n{\\n    \"column_name\": \"direction\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 0,\\n        \"max\": 1,\\n        \"mean\": 0.49925,\\n        \"median\": 0.0,\\n        \"std\": 0.5,\\n        \"histogram\": {\\n            \"hist\": [\\n                50075,\\n                49925\\n            ],\\n            \"bin_edges\": [\\n                0,\\n                1,\\n                1  # if the last value is equal to the last but one, that means that this bin includes only this value\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"hour\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 0,\\n        \"max\": 23,\\n        \"mean\": 13.44402,\\n        \"median\": 14.0,\\n        \"std\": 5.49455,\\n        \"histogram\": {\\n            \"hist\": [\\n                2694,\\n                2292,\\n                16785,\\n                16326,\\n                16346,'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 15291}, page_content='2292,\\n                16785,\\n                16326,\\n                16346,\\n                17809,\\n                16546,\\n                11202\\n            ],\\n            \"bin_edges\": [\\n                0,\\n                3,\\n                6,\\n                9,\\n                12,\\n                15,\\n                18,\\n                21,\\n                23\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"humidity\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 54,\\n        \"max\": 99,\\n        \"mean\": 83.89878,\\n        \"median\": 85.0,\\n        \"std\": 8.65174,\\n        \"histogram\": {\\n            \"hist\": [\\n                554,\\n                1662,\\n                3823,\\n                6532,\\n                12512,\\n                17536,\\n                23871,\\n                20355,\\n                12896,\\n                259\\n            ],\\n            \"bin_edges\": [\\n                54,'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 16200}, page_content='259\\n            ],\\n            \"bin_edges\": [\\n                54,\\n                59,\\n                64,\\n                69,\\n                74,\\n                79,\\n                84,\\n                89,\\n                94,\\n                99,\\n                99\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"weekday\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 0,\\n        \"max\": 6,\\n        \"mean\": 3.08063,\\n        \"median\": 3.0,\\n        \"std\": 1.90347,\\n        \"histogram\": {\\n            \"hist\": [\\n                10282,\\n                15416,\\n                15291,\\n                15201,\\n                15586,\\n                15226,\\n                12998\\n            ],\\n            \"bin_edges\": [\\n                0,\\n                1,\\n                2,\\n                3,\\n                4,\\n                5,\\n                6,\\n                6\\n            ]\\n        }\\n    }\\n}'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 17176}, page_content='```\\n\\n</p>\\n</details>\\n\\n##### string_label\\n\\nIf the number of unique values in a column (within requested split) is <= `MAX_NUM_STRING_LABELS` (currently 30), the column is considered to be a category and the categories counts are computed.\\n\\n<details><summary>examples: </summary>\\n<p>\\n\\n```python\\n{\\n    \\'column_name\\': \\'string_col\\',\\n    \\'column_type\\': \\'string_label\\',\\n    \\'column_statistics\\': \\n        {\\n            \"nan_count\": 0,\\n            \"nan_proportion\": 0.0,\\n            \"n_unique\": 5,  # number of unique values (excluding nan)\\n            \"frequencies\": {   # mapping value -> its count\\n                \"this\": 19834,\\n                \"are\": 20159,\\n                \"random\": 20109,\\n                \"words\": 20172,\\n                \"test\": 19726\\n        }\\n    }\\n}'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 17942}, page_content='```\\n</p>\\n</details>\\n\\n##### string_text\\n\\nIf the number of unique values in a column (within requested split) is > `MAX_NUM_STRING_LABELS` (currently 30), the column is considered to be text and the distribution of text **lengths** is computed.\\n\\n<details><summary>example: </summary>\\n<p>'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 18186}, page_content=\"<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    'column_name': 'text_col',\\n    'column_type': 'string_text',\\n    'column_statistics': {\\n        'max': 296,\\n        'mean': 97.46649,\\n        'median': 88.0,\\n        'min': 11,\\n        'nan_count': 0,\\n        'nan_proportion': 0.0,\\n        'std': 55.82714,\\n        'histogram': {\\n            'bin_edges': [\\n                11,\\n                40,\\n                69,\\n                98,\\n                127,\\n                156,\\n                185,\\n                214,\\n                243,\\n                272,\\n                296\\n            ],\\n            'hist': [\\n                171,\\n                224,\\n                235,\\n                180,\\n                102,\\n                99,\\n                53,\\n                28,\\n                10,\\n                2\\n               ]\\n             },\\n    }\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 19059}, page_content=\"```\\n</p>\\n</details>\\n\\n##### bool\\n\\n<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    'column_name': 'bool__nan_column', \\n    'column_type': 'bool', \\n    'column_statistics': \\n        {\\n            'nan_count': 3, \\n            'nan_proportion': 0.15, \\n            'frequencies': {\\n                'False': 7, \\n                'True': 10\\n            }\\n        }\\n}\\n```\\n</p>\\n</details>\\n\\n\\n\\n### Splits worker\\n\\nThe `splits` worker does not need any additional configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 1}, page_content='Differences between Dataset and IterableDataset\\n\\nThere are two types of dataset objects, a [`Dataset`] and an [`IterableDataset`].\\nWhichever type of dataset you choose to use or create depends on the size of the dataset.\\nIn general, an [`IterableDataset`] is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and speed advantages, while a [`Dataset`] is great for everything else.\\nThis page will compare the differences between a [`Dataset`] and an [`IterableDataset`] to help you pick the right dataset object for you.\\n\\n## Downloading and streaming\\n\\nWhen you have a regular [`Dataset`], you can access it using `my_dataset[0]`. This provides random access to the rows.\\nSuch datasets are also called \"map-style\" datasets.\\nFor example you can download ImageNet-1k like this and access any row:\\n\\n```python\\nfrom datasets import load_dataset\\n\\nimagenet = load_dataset(\"imagenet-1k\", split=\"train\")  # downloads the full dataset\\nprint(imagenet[0])'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 966}, page_content='```\\n\\nBut one caveat is that you must have the entire dataset stored on your disk or in memory, which blocks you from accessing datasets bigger than the disk.\\nBecause it can become inconvenient for big datasets, there exists another type of dataset, the [`IterableDataset`].\\nWhen you have an `IterableDataset`, you can access it using a `for` loop to load the data progressively as you iterate over the dataset.\\nThis way, only a small fraction of examples is loaded in memory, and you don\\'t write anything on disk.\\n\\nFor example, you can stream the ImageNet-1k dataset without downloading it on disk:\\n\\n```python\\nfrom datasets import load_dataset\\n\\nimagenet = load_dataset(\"imagenet-1k\", split=\"train\", streaming=True)  # will start loading the data when iterated over\\nfor example in imagenet:\\n    print(example)\\n    break'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 1785}, page_content='```\\n\\nStreaming can read online data without writing any file to disk.\\nFor example, you can stream datasets made out of multiple shards, each of which is hundreds of gigabytes like [C4](https://huggingface.co/datasets/c4), [OSCAR](https://huggingface.co/datasets/oscar) or [LAION-2B](https://huggingface.co/datasets/laion/laion2B-en).\\nLearn more about how to stream a dataset in the [Dataset Streaming Guide](./stream).\\n\\nThis is not the only difference though, because the \"lazy\" behavior of an `IterableDataset` is also present when it comes to dataset creation and processing.\\n\\n## Creating map-style datasets and iterable datasets\\n\\nYou can create a [`Dataset`] using lists or dictionaries, and the data is entirely converted to Arrow so you can easily access any row:\\n```python\\nmy_dataset = Dataset.from_dict({\"col_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]})\\nprint(my_dataset[0])'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 2659}, page_content='```\\n\\nTo create an `IterableDataset` on the other hand, you must provide a \"lazy\" way to load the data.\\nIn Python, we generally use generator functions. These functions `yield` one example at a time, which means you can\\'t access a row by slicing it like a regular `Dataset`:\\n```python\\ndef my_generator(n):\\n    for i in range(n):\\n        yield {\"col_1\": i}\\n\\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs={\"n\": 10})\\nfor example in my_iterable_dataset:\\n    print(example)\\n    break\\n```\\n\\n## Loading local files entirely and progressively\\n\\nIt is possible to convert local or remote data files to an Arrow [`Dataset`] using [`load_dataset`]:\\n```python\\ndata_files = {\"train\": [\"path/to/data.csv\"]}\\nmy_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\")\\nprint(my_dataset[0])'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 3473}, page_content='```\\n\\nHowever, this requires a conversion step from CSV to Arrow format, which takes time and disk space if your dataset is big.\\n\\nTo save disk space and skip the conversion step, you can define an `IterableDataset` by streaming from the local files directly.\\nThis way, the data is read progressively from the local files as you iterate over the dataset:\\n\\n```python\\ndata_files = {\"train\": [\"path/to/data.csv\"]}\\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\\nfor example in my_iterable_dataset:  # this reads the CSV file progressively as you iterate over the dataset\\n    print(example)\\n    break'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 4116}, page_content='```\\n\\nMany file formats are supported, like CSV, JSONL, and Parquet, as well as image and audio files.\\nYou can find more information in the corresponding guides for loading [tabular](./tabular_load), [text](./nlp_load), [vision](./image_load), and [audio](./audio_load]) datasets.\\n\\n## Eager data processing and lazy data processing\\n\\nWhen you process a [`Dataset`] object using [`Dataset.map`], the entire dataset is processed immediately and returned.\\nThis is similar to how `pandas` works for example.\\n\\n```python\\nmy_dataset = my_dataset.map(process_fn)  # process_fn is applied on all the examples of the dataset\\nprint(my_dataset[0])'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 4750}, page_content='```\\n\\nOn the other hand, due to the \"lazy\" nature of an `IterableDataset`, calling [`IterableDataset.map`] does not apply your `map` function over the full dataset.\\nInstead, your `map` function is applied on-the-fly.\\n\\nBecause of that, you can chain multiple processing steps and they will all run at once when you start iterating over the dataset:\\n\\n```python\\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_1)\\nmy_iterable_dataset = my_iterable_dataset.filter(filter_fn)\\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_2)\\n\\n# process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset\\nfor example in my_iterable_dataset:  \\n    print(example)\\n    break'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 5454}, page_content='```\\n\\n## Exact and fast approximate shuffling\\n\\nWhen you shuffle a [`Dataset`] using [`Dataset.shuffle`], you apply an exact shuffling of the dataset.\\nIt works by taking a list of indices `[0, 1, 2, ... len(my_dataset) - 1]` and shuffling this list.\\nThen, accessing `my_dataset[0]` returns the row and index defined by the first element of the indices mapping that has been shuffled:\\n```python\\nmy_dataset = my_dataset.shuffle(seed=42)\\nprint(my_dataset[0])'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 5908}, page_content=\"```\\n\\nSince we don't have random access to the rows in the case of an `IterableDataset`, we can't use a shuffled list of indices and access a row at an arbitrary position.\\nThis prevents the use of exact shuffling.\\nInstead, a fast approximate shuffling is used in [`IterableDataset.shuffle`].\\nIt uses a shuffle buffer to sample random examples iteratively from the dataset.\\nSince the dataset is still read iteratively, it provides excellent speed performance:\\n```python\\nmy_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\\nfor example in my_iterable_dataset:\\n    print(example)\\n    break\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 6517}, page_content='```\\n\\nBut using a shuffle buffer is not enough to provide a satisfactory shuffling for machine learning model training. So [`IterableDataset.shuffle`] also shuffles the dataset shards if your dataset is made of multiple files or sources:\\n\\n```python\\n# Stream from the internet\\nmy_iterable_dataset = load_dataset(\"deepmind/code_contests\", split=\"train\", streaming=True)\\nmy_iterable_dataset.n_shards  # 39\\n\\n# Stream from local files\\ndata_files = {\"train\": [f\"path/to/data_{i}.csv\" for i in range(1024)]}\\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\\nmy_iterable_dataset.n_shards  # 1024\\n\\n# From a generator function\\ndef my_generator(n, sources):\\n    for source in sources:\\n        for example_id_for_current_source in range(n):\\n            yield {\"example_id\": f\"{source}_{example_id_for_current_source}\"}'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 7370}, page_content='gen_kwargs = {\"n\": 10, \"sources\": [f\"path/to/data_{i}\" for i in range(1024)]}\\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs=gen_kwargs)\\nmy_iterable_dataset.n_shards  # 1024'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 7575}, page_content=\"```\\n\\n## Speed differences\\n\\nRegular [`Dataset`] objects are based on Arrow which provides fast random access to the rows.\\nThanks to memory mapping and the fact that Arrow is an in-memory format, reading data from disk doesn't do expensive system calls and deserialization.\\nIt provides even faster data loading when iterating using a `for` loop by iterating on contiguous Arrow record batches.\\n\\nHowever as soon as your [`Dataset`] has an indices mapping (via [`Dataset.shuffle`] for example), the speed can become 10x slower.\\nThis is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you aren't reading contiguous chunks of data anymore.\\nTo restore the speed, you'd need to rewrite the entire dataset on your disk again using [`Dataset.flatten_indices`], which removes the indices mapping.\\nThis may take a lot of time depending of the size of your dataset though:\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 8497}, page_content='```python\\nmy_dataset[0]  # fast\\nmy_dataset = my_dataset.shuffle(seed=42)\\nmy_dataset[0]  # up to 10x slower\\nmy_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\\nmy_dataset[0]  # fast again'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 8743}, page_content='```\\n\\n\\nIn this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approximate shuffling method [`IterableDataset.shuffle`].\\nIt only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal.\\nYou can also reshuffle the dataset easily:\\n\\n```python\\nfor example in enumerate(my_iterable_dataset):  # fast\\n    pass\\n\\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\\n\\nfor example in enumerate(shuffled_iterable_dataset):  # as fast as before\\n    pass\\n\\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=1337, buffer_size=100)  # reshuffling using another seed is instantaneous\\n\\nfor example in enumerate(shuffled_iterable_dataset):  # still as fast as before\\n    pass'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 9529}, page_content='```\\n\\nIf you\\'re using your dataset on multiple epochs, the effective seed to shuffle the shards order in the shuffle buffer is `seed + epoch`.\\nIt makes it easy to reshuffle a dataset between epochs:\\n```python\\nfor epoch in range(n_epochs):\\n    my_iterable_dataset.set_epoch(epoch)\\n    for example in my_iterable_dataset:  # fast + reshuffled at each epoch using `effective_seed = seed + epoch`\\n        pass\\n```\\n\\n## Switch from map-style to iterable\\n\\nIf you want to benefit from the \"lazy\" behavior of an [`IterableDataset`] or their speed advantages, you can switch your map-style [`Dataset`] to an [`IterableDataset`]:\\n```python\\nmy_iterable_dataset = my_dataset.to_iterable_dataset()\\n```\\n\\nIf you want to shuffle your dataset or [use it with a PyTorch DataLoader](./use_with_pytorch#stream-data), we recommend generating a sharded [`IterableDataset`]:\\n```python\\nmy_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=1024)\\nmy_iterable_dataset.n_shards  # 1024\\n```'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/q-learning-recap.mdx', 'start_index': 1}, page_content='Q-Learning Recap [[q-learning-recap]]\\n\\n\\n*Q-Learning* **is the RL algorithm that** :\\n\\n- Trains a *Q-function*, an **action-value function** encoded, in internal memory, by a *Q-table* **containing all the state-action pair values.**\\n\\n- Given a state and action, our Q-function **will search its Q-table for the corresponding value.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\\n\\n- When the training is done, **we have an optimal Q-function, or, equivalently, an optimal Q-table.**\\n\\n- And if we **have an optimal Q-function**, we\\nhave an optimal policy, since we **know, for each state, the best action to take.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/q-learning-recap.mdx', 'start_index': 896}, page_content='But, in the beginning,\\xa0our **Q-table is useless since it gives arbitrary values for each state-action pair\\xa0(most of the time we initialize the Q-table to 0 values)**. But, as we\\xa0explore the environment and update our Q-table it will give us a better and better approximation.\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\\n\\nThis is the Q-Learning pseudocode:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Zero-shot object detection\\n\\n[[open-in-colab]]\\n\\nTraditionally, models used for [object detection](object_detection) require labeled image datasets for training,\\nand are limited to detecting the set of classes from the training data.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 986}, page_content='Zero-shot object detection is supported by the [OWL-ViT](../model_doc/owlvit) model which uses a different approach. OWL-ViT\\nis an open-vocabulary object detector. It means that it can detect objects in images based on free-text queries without\\nthe need to fine-tune the model on labeled datasets.\\n\\nOWL-ViT leverages multi-modal representations to perform open-vocabulary detection. It combines [CLIP](../model_doc/clip) with\\nlightweight object classification and localization heads. Open-vocabulary detection is achieved by embedding free-text queries with the text encoder of CLIP and using them as input to the object classification and localization heads.\\nassociate images and their corresponding textual descriptions, and ViT processes image patches as inputs. The authors\\nof OWL-ViT first trained CLIP from scratch and then fine-tuned OWL-ViT end to end on standard object detection datasets using\\na bipartite matching loss.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 1918}, page_content='With this approach, the model can detect objects based on textual descriptions without prior training on labeled datasets.\\n\\nIn this guide, you will learn how to use OWL-ViT:\\n- to detect objects based on text prompts\\n- for batch object detection\\n- for image-guided object detection\\n\\nBefore you begin, make sure you have all the necessary libraries installed:\\n\\n```bash\\npip install -q transformers'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 2313}, page_content='```\\n\\n## Zero-shot object detection pipeline\\n\\nThe simplest way to try out inference with OWL-ViT is to use it in a [`pipeline`]. Instantiate a pipeline\\nfor zero-shot object detection from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit):\\n\\n```python\\n>>> from transformers import pipeline\\n\\n>>> checkpoint = \"google/owlvit-base-patch32\"\\n>>> detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\\n```\\n\\nNext, choose an image you\\'d like to detect objects in. Here we\\'ll use the image of astronaut Eileen Collins that is\\na part of the [NASA](https://www.nasa.gov/multimedia/imagegallery/index.html) Great Images dataset.\\n\\n```py\\n>>> import skimage\\n>>> import numpy as np\\n>>> from PIL import Image\\n\\n>>> image = skimage.data.astronaut()\\n>>> image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\\n\\n>>> image'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 3162}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\" alt=\"Astronaut Eileen Collins\"/>\\n</div>\\n\\nPass the image and the candidate object labels to look for to the pipeline.\\nHere we pass the image directly; other suitable options include a local path to an image or an image url. We also pass text descriptions for all items we want to query the image for.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 3644}, page_content='```py\\n>>> predictions = detector(\\n...     image,\\n...     candidate_labels=[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\\n... )\\n>>> predictions\\n[{\\'score\\': 0.3571370542049408,\\n  \\'label\\': \\'human face\\',\\n  \\'box\\': {\\'xmin\\': 180, \\'ymin\\': 71, \\'xmax\\': 271, \\'ymax\\': 178}},\\n {\\'score\\': 0.28099656105041504,\\n  \\'label\\': \\'nasa badge\\',\\n  \\'box\\': {\\'xmin\\': 129, \\'ymin\\': 348, \\'xmax\\': 206, \\'ymax\\': 427}},\\n {\\'score\\': 0.2110239565372467,\\n  \\'label\\': \\'rocket\\',\\n  \\'box\\': {\\'xmin\\': 350, \\'ymin\\': -1, \\'xmax\\': 468, \\'ymax\\': 288}},\\n {\\'score\\': 0.13790413737297058,\\n  \\'label\\': \\'star-spangled banner\\',\\n  \\'box\\': {\\'xmin\\': 1, \\'ymin\\': 1, \\'xmax\\': 105, \\'ymax\\': 509}},\\n {\\'score\\': 0.11950037628412247,\\n  \\'label\\': \\'nasa badge\\',\\n  \\'box\\': {\\'xmin\\': 277, \\'ymin\\': 338, \\'xmax\\': 327, \\'ymax\\': 380}},\\n {\\'score\\': 0.10649408400058746,\\n  \\'label\\': \\'rocket\\',\\n  \\'box\\': {\\'xmin\\': 358, \\'ymin\\': 64, \\'xmax\\': 424, \\'ymax\\': 280}}]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 4523}, page_content='```\\n\\nLet\\'s visualize the predictions:\\n\\n```py\\n>>> from PIL import ImageDraw\\n\\n>>> draw = ImageDraw.Draw(image)\\n\\n>>> for prediction in predictions:\\n...     box = prediction[\"box\"]\\n...     label = prediction[\"label\"]\\n...     score = prediction[\"score\"]\\n\\n...     xmin, ymin, xmax, ymax = box.values()\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\\n\\n>>> image'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 4979}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_2.png\" alt=\"Visualized predictions on NASA image\"/>\\n</div>\\n\\n## Text-prompted zero-shot object detection by hand\\n\\nNow that you\\'ve seen how to use the zero-shot object detection pipeline, let\\'s replicate the same\\nresult manually.\\n\\nStart by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit).\\nHere we\\'ll use the same checkpoint as before:\\n\\n```py\\n>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\\n\\n>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\\n>>> processor = AutoProcessor.from_pretrained(checkpoint)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 5790}, page_content='```\\n\\nLet\\'s take a different image to switch things up.\\n\\n```py\\n>>> import requests\\n\\n>>> url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\\n>>> im = Image.open(requests.get(url, stream=True).raw)\\n>>> im\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/>\\n</div>\\n\\nUse the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\\nimage for the model by resizing and normalizing it, and a [`CLIPTokenizer`] that takes care of the text inputs.\\n\\n```py\\n>>> text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\\n>>> inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 6658}, page_content='```\\n\\nPass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\\nfeeding them to the model, you need to use the [`~OwlViTImageProcessor.post_process_object_detection`] method to make sure the predicted bounding\\nboxes have the correct coordinates relative to the original image:\\n\\n```py\\n>>> import torch\\n\\n>>> with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = torch.tensor([im.size[::-1]])\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\\n\\n>>> draw = ImageDraw.Draw(im)\\n\\n>>> scores = results[\"scores\"].tolist()\\n>>> labels = results[\"labels\"].tolist()\\n>>> boxes = results[\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{text_queries[label]}: {round(score,2)}\", fill=\"white\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 7659}, page_content='>>> im'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 7666}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\\n</div>\\n\\n## Batch processing\\n\\nYou can pass multiple sets of images and text queries to search for different (or same) objects in several images.\\nLet\\'s use both an astronaut image and the beach image together.\\nFor batch processing, you should pass text queries as a nested list to the processor and images as lists of PIL images,\\nPyTorch tensors, or NumPy arrays.\\n\\n```py\\n>>> images = [image, im]\\n>>> text_queries = [\\n...     [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\\n...     [\"hat\", \"book\", \"sunglasses\", \"camera\"],\\n... ]\\n>>> inputs = processor(text=text_queries, images=images, return_tensors=\"pt\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 8508}, page_content='```\\n\\nPreviously for post-processing you passed the single image\\'s size as a tensor, but you can also pass a tuple, or, in case\\nof several images, a list of tuples. Let\\'s create predictions for the two examples, and visualize the second one (`image_idx = 1`).\\n\\n```py\\n>>> with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = [x.size[::-1] for x in images]\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\\n\\n>>> image_idx = 1\\n>>> draw = ImageDraw.Draw(images[image_idx])\\n\\n>>> scores = results[image_idx][\"scores\"].tolist()\\n>>> labels = results[image_idx][\"labels\"].tolist()\\n>>> boxes = results[image_idx][\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{text_queries[image_idx][label]}: {round(score,2)}\", fill=\"white\")\\n\\n>>> images[image_idx]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 9505}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\\n</div>\\n\\n## Image-guided object detection\\n\\nIn addition to zero-shot object detection with text queries, OWL-ViT offers image-guided object detection. This means\\nyou can use an image query to find similar objects in the target image.\\nUnlike text queries, only a single example image is allowed.\\n\\nLet\\'s take an image with two cats on a couch as a target image, and an image of a single cat\\nas a query:\\n\\n```py\\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\n>>> image_target = Image.open(requests.get(url, stream=True).raw)\\n\\n>>> query_url = \"http://images.cocodataset.org/val2017/000000524280.jpg\"\\n>>> query_image = Image.open(requests.get(query_url, stream=True).raw)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 10411}, page_content='```\\n\\nLet\\'s take a quick look at the images:\\n\\n```py\\n>>> import matplotlib.pyplot as plt\\n\\n>>> fig, ax = plt.subplots(1, 2)\\n>>> ax[0].imshow(image_target)\\n>>> ax[1].imshow(query_image)\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_5.png\" alt=\"Cats\"/>\\n</div>\\n\\nIn the preprocessing step, instead of text queries, you now need to use `query_images`:\\n\\n```py\\n>>> inputs = processor(images=image_target, query_images=query_image, return_tensors=\"pt\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 10980}, page_content='```\\n\\nFor predictions, instead of passing the inputs to the model, pass them to [`~OwlViTForObjectDetection.image_guided_detection`]. Draw the predictions\\nas before except now there are no labels.\\n\\n```py\\n>>> with torch.no_grad():\\n...     outputs = model.image_guided_detection(**inputs)\\n...     target_sizes = torch.tensor([image_target.size[::-1]])\\n...     results = processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]\\n\\n>>> draw = ImageDraw.Draw(image_target)\\n\\n>>> scores = results[\"scores\"].tolist()\\n>>> boxes = results[\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"white\", width=4)\\n\\n>>> image_target'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 11748}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_6.png\" alt=\"Cats with bounding boxes\"/>\\n</div>\\n\\nIf you\\'d like to interactively try out inference with OWL-ViT, check out this demo:\\n\\n<iframe\\n\\tsrc=\"https://adirik-owl-vit.hf.space\"\\n\\tframeborder=\"0\"\\n\\twidth=\"850\"\\n\\theight=\"450\"\\n></iframe>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 1}, page_content='Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.\\n\\n\\n### Q1: Which of the following interpretations of bias-variance tradeoff is the most accurate in the field of Reinforcement Learning?'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 394}, page_content='<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"The bias-variance tradeoff reflects how my model is able to generalize the knowledge to previously tagged data we give to the model during training time.\",\\n\\t\\t\\texplain: \"This is the traditional bias-variance tradeoff in Machine Learning. In our specific case of Reinforcement Learning, we don\\'t have previously tagged data, but only a reward signal.\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n   \\t\\t{\\n\\t\\t\\ttext: \"The bias-variance tradeoff reflects how well the reinforcement signal reflects the true reward the agent should get from the enviromment\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\t\\t\\n\\t]}\\n/>\\n\\n### Q2: Which of the following statements are true, when talking about models with bias and/or variance in RL?'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 1134}, page_content='<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"An unbiased reward signal returns rewards similar to the real / expected ones from the environment\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"A biased reward signal returns rewards similar to the real / expected ones from the environment\",\\n\\t\\t\\texplain: \"If a reward signal is biased, it means the reward signal we get differs from the real reward we should be getting from an environment\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"A reward signal with high variance has much noise in it and gets affected by, for example, stochastic (non constant) elements in the environment\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\t\\t\\n    \\t\\t{\\n\\t\\t\\ttext: \"A reward signal with low variance has much noise in it and gets affected by, for example, stochastic (non constant) elements in the environment\",'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 1990}, page_content='explain: \"If a reward signal has low variance, then it\\'s less affected by the noise of the environment and produce similar values regardless the random elements in the environment\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n\\t]}\\n/>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 2211}, page_content='### Q3: Which of the following statements are true about Monte Carlo method?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"It\\'s a sampling mechanism, which means we don\\'t analyze all the possible states, but a sample of those\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"It\\'s very resistant to stochasticity (random elements in the trajectory)\",\\n\\t\\t\\texplain: \"Monte Carlo randomly estimates everytime a sample of trajectories. However, even same trajectories can have different reward values if they contain stochastic elements\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"To reduce the impact of stochastic elements in Monte Carlo, we take `n` strategies and average them, reducing their individual impact\",\\n\\t\\t\\texplain: \"\",\\n\\t\\t\\tcorrect: true,\\n\\t\\t},\\t\\t    \\n\\t]}\\n/>\\n\\n### Q4: How would you describe, with your own words, the Actor-Critic Method (A2C)?\\n\\n<details>\\n<summary>Solution</summary>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 3073}, page_content='<details>\\n<summary>Solution</summary>\\n\\nThe idea behind Actor-Critic is that we learn two function approximations:\\n1. A `policy` that controls how our agent acts (π)\\n2. A `value` function to assist the policy update by measuring how good the action taken is (q)\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/step2.jpg\" alt=\"Actor-Critic, step 2\"/>\\n\\n</details>\\n\\n### Q5: Which of the following statements are true about the Actor-Critic Method?'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 3481}, page_content='</details>\\n\\n### Q5: Which of the following statements are true about the Actor-Critic Method?\\n\\n<Question\\n\\tchoices={[\\n   \\t\\t {\\n\\t\\t\\ttext: \"The Critic does not learn any function during the training process\",\\n\\t\\t\\texplain: \"Both the Actor and the Critic function parameters are updated during training time\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"The Actor learns a policy function, while the Critic learns a value function\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"It adds resistance to stochasticity and reduces high variance\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\t    \\n\\t]}\\n/>\\n\\n\\n\\n### Q6: What is `Advantage` in the A2C method?\\n\\n<details>\\n<summary>Solution</summary>\\n\\nInstead of using directly the Action-Value function of the Critic as it is, we could use an `Advantage` function. The idea behind an `Advantage` function is that we calculate the relative advantage of an action compared to the others possible at a state, averaging them.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 4452}, page_content='In other words: how taking that action at a state is better compared to the average value of the state\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/advantage1.jpg\" alt=\"Advantage in A2C\"/>\\n\\n</details>\\n\\nCongrats on finishing this Quiz 🥳, if you missed some elements, take time to read the chapter again to reinforce (😏) your knowledge.'),\n",
       " Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx', 'start_index': 1}, page_content='Access and read Logs\\n\\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your Endpoint. \\n\\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\\n\\nThe Container Logs are only available when your Endpoint is in the “Running” state. \\n\\n_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\\n\\n**Build Logs:**\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\\n\\n**Container Logs:**\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/examples_component/run.ipynb', 'start_index': 1}, page_content=\"Gradio Demo: examples_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\nimport os\\nos.mkdir('images')\\n!wget -q -O images/cheetah1.jpg https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/cheetah1.jpg\\n!wget -q -O images/lion.jpg https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/lion.jpg\\n!wget -q -O images/lion.webp https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/lion.webp\\n!wget -q -O images/logo.png https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/logo.png\\n```\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/examples_component/run.ipynb', 'start_index': 607}, page_content='```\\n\\n\\n```\\nimport gradio as gr\\nimport os\\n\\n\\ndef flip(i):\\n    return i.rotate(180)\\n\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        with gr.Column():\\n            img_i = gr.Image(label=\"Input Image\", type=\"pil\")\\n        with gr.Column():\\n            img_o = gr.Image(label=\"Output Image\")\\n    with gr.Row():\\n        btn = gr.Button(value=\"Flip Image\")\\n    btn.click(flip, inputs=[img_i], outputs=[img_o])\\n\\n    gr.Examples(\\n        [\\n            os.path.join(os.path.abspath(\\'\\'), \"images/cheetah1.jpg\"),\\n            os.path.join(os.path.abspath(\\'\\'), \"images/lion.jpg\"),\\n        ],\\n        img_i,\\n        img_o,\\n        flip,\\n    )\\n\\ndemo.launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/additional-readings.mdx', 'start_index': 1}, page_content='Additional Readings\\n\\nThese are **optional readings** if you want to go deeper.\\n\\n\\n## Introduction to Policy Optimization\\n\\n- [Part 3: Intro to Policy Optimization - Spinning Up documentation](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)\\n\\n\\n## Policy Gradient\\n\\n- [https://johnwlambert.github.io/policy-gradients/](https://johnwlambert.github.io/policy-gradients/)\\n- [RL - Policy Gradient Explained](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146)\\n- [Chapter 13, Policy Gradient Methods;  Reinforcement Learning, an introduction by Richard Sutton and Andrew G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)\\n\\n## Implementation\\n\\n- [PyTorch Reinforce implementation](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\\n- [Implementations from DDPG to PPO](https://github.com/MrSyee/pg-is-all-you-need)'),\n",
       " Document(metadata={'source': 'huggingface/optimum/blob/main/docs/source/onnxruntime/package_reference/quantization.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Quantization\\n\\n## ORTQuantizer\\n\\n[[autodoc]] onnxruntime.quantization.ORTQuantizer\\n    - all'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/number_component/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: number_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Blocks() as demo:\\n    gr.Number()\\n\\ndemo.launch()\\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: map_airbnb\\n### Display an interactive map of AirBnB locations with Plotly. Data is hosted on HuggingFace Datasets. \\n        \\n\\n\\n```\\n!pip install -q gradio plotly\\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 181}, page_content='```\\nimport gradio as gr\\nimport plotly.graph_objects as go\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\\ndf = dataset.to_pandas()\\n\\ndef filter_map(min_price, max_price, boroughs):\\n\\n    filtered_df = df[(df[\\'neighbourhood_group\\'].isin(boroughs)) & \\n          (df[\\'price\\'] > min_price) & (df[\\'price\\'] < max_price)]\\n    names = filtered_df[\"name\"].tolist()\\n    prices = filtered_df[\"price\"].tolist()\\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\\n    fig = go.Figure(go.Scattermapbox(\\n            customdata=text_list,\\n            lat=filtered_df[\\'latitude\\'].tolist(),\\n            lon=filtered_df[\\'longitude\\'].tolist(),\\n            mode=\\'markers\\',\\n            marker=go.scattermapbox.Marker(\\n                size=6\\n            ),\\n            hoverinfo=\"text\",\\n            hovertemplate=\\'<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}\\'\\n        ))'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 1126}, page_content='fig.update_layout(\\n        mapbox_style=\"open-street-map\",\\n        hovermode=\\'closest\\',\\n        mapbox=dict(\\n            bearing=0,\\n            center=go.layout.mapbox.Center(\\n                lat=40.67,\\n                lon=-73.90\\n            ),\\n            pitch=0,\\n            zoom=9\\n        ),\\n    )\\n\\n    return fig\\n\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\\n        btn = gr.Button(value=\"Update Filter\")\\n        map = gr.Plot()\\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\\n\\nif __name__ == \"__main__\":\\n    demo.launch()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 2063}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 1}, page_content=\"Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2Net Blocks](https://paperswithcode.com/method/res2net-block). The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model('res2net101_26w_4s', pretrained=True)\\n>>> model.eval()\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 683}, page_content='```\\n\\nTo load and preprocess the image:\\n\\n```py \\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 1478}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n>>> # Get imagenet class mappings\\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\n>>> urllib.request.urlretrieve(url, filename) \\n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\\n...     categories = [s.strip() for s in f.readlines()]\\n\\n>>> # Print top categories per image\\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\\n>>> for i in range(top5_prob.size(0)):\\n...     print(categories[top5_catid[i]], top5_prob[i].item())\\n>>> # prints class names and probabilities like:\\n>>> # [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 2272}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('res2net101_26w_4s', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 2835}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@article{Gao_2021,\\n   title={Res2Net: A New Multi-Scale Backbone Architecture},\\n   volume={43},\\n   ISSN={1939-3539},\\n   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},\\n   DOI={10.1109/tpami.2019.2938758},\\n   number={2},\\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\\n   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\\n   year={2021},\\n   month={Feb},\\n   pages={652–662}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 3721}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 3726}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: Res2Net\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale Backbone Architecture'\\n    URL: https://paperswithcode.com/paper/res2net-a-new-multi-scale-backbone\\nModels:\\n- Name: res2net101_26w_4s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 10415881200\\n    Parameters: 45210000\\n    File Size: 181456059\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net101_26w_4s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L152\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 4696}, page_content=\"Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net101_26w_4s-02a759a1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.19%\\n      Top 5 Accuracy: 94.43%\\n- Name: res2net50_14w_8s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 5403546768\\n    Parameters: 25060000\\n    File Size: 100638543\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_14w_8s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 5486}, page_content=\"Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L196\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_14w_8s-6527dddc.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.14%\\n      Top 5 Accuracy: 93.86%\\n- Name: res2net50_26w_4s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 5499974064\\n    Parameters: 25700000\\n    File Size: 103110087\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_26w_4s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 6407}, page_content=\"ID: res2net50_26w_4s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L141\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_4s-06e79181.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.99%\\n      Top 5 Accuracy: 93.85%\\n- Name: res2net50_26w_6s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 8130156528\\n    Parameters: 37050000\\n    File Size: 148603239\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 7303}, page_content=\"- SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_26w_6s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L163\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_6s-19041792.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.57%\\n      Top 5 Accuracy: 94.12%\\n- Name: res2net50_26w_8s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 10760338992\\n    Parameters: 48400000\\n    File Size: 194085165\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 8195}, page_content=\"- Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_26w_8s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L174\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_8s-2c7c9f12.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.19%\\n      Top 5 Accuracy: 94.37%\\n- Name: res2net50_48w_2s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 5375291520\\n    Parameters: 25290000\\n    File Size: 101421406\\n    Architecture:\\n    - Batch Normalization\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 9097}, page_content=\"Parameters: 25290000\\n    File Size: 101421406\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_48w_2s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L185\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_48w_2s-afed724a.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.53%\\n      Top 5 Accuracy: 93.56%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': 0}, page_content=\"ow to slice and dice a dataset. Most of the time, the data you work with won’t be perfectly prepared for training models. In this video we’ll explore various features that Datasets provides to clean up your datasets. The Datasets library provides several built-in methods that allow you to wrangle your data. In this video we'll see how you can shuffle and split your data, select the rows you're interested in, tweak the columns, and apply processing functions with the map() method. Let's start with shuffling. It is generally a good idea to apply shuffling to the training set so that your model doesn't learn any artificial ordering in the data. If you want to shuffle the whole dataset, you can apply the appropriately named shuffle() method to your dataset. You can see an example of this method in action here, where we've downloaded the training split of the SQUAD dataset and shuffled all the rows randomly.Another way to shuffle the data is to create random train and test splits. This can\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': 902}, page_content=\"rows randomly.Another way to shuffle the data is to create random train and test splits. This can be useful if you have to create your own test splits from raw data. To do this, you just apply the train_test_split method and specify how large the test split should be. In this example, we've specified that the test set should be 10% of the total dataset size. You can see that the output of train_test_split is a DatasetDict object, whose keys correspond to the new splits. Now that we know how to shuffle a dataset, let's take a look at returning the rows we're interested in. The most common way to do this is with the select method. This method expects a list or generator of the dataset's indices, and will then return a new Dataset object containing just those rows. If you want to create a random sample of rows, you can do this by chaining the shuffle and select methods together. In this example, we've created a sample of 5 elements from the SQuAD dataset. The last way to pick out\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': 1794}, page_content='this example, we\\'ve created a sample of 5 elements from the SQuAD dataset. The last way to pick out specific rows in a dataset is by applying the filter method. This method checks whether each rows fulfills some condition or not. For example, here we\\'ve created a small lambda function that checks whether the title starts with the letter \"L\". Once we apply this function with the filter method, we get a subset of the data consisting of just these titles. So far we\\'ve been talking about the rows of a dataset, but what about the columns? The Datasets library has two main methods for transforming columns: a rename_column method to change the name of a column, and a remove_columns method to delete them. You can see examples of both these method here. Some datasets have nested columns and you can expand these by applying the flatten method. For example in the SQUAD dataset, the answers column contains a text and answer_start field. If we want to promote them to their own separate columns, we'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': 2702}, page_content='a text and answer_start field. If we want to promote them to their own separate columns, we can apply flatten as shown here. Of course, no discussion of the Datasets library would be complete without mentioning the famous map method. This method applies a custom processing function to each row in the dataset. For example,here we first define a lowercase_title function that simply lowercases the text in the title column and then we feed that to the map method and voila! we now have lowercase titles. The map method can also be used to feed batches of rows to the processing function. This is especially useful for tokenization, where the tokenizers are backed by the Tokenizers library can use fast multithreading to process batches in parallel.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: question-answering\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 87}, page_content='```\\nimport gradio as gr\\n\\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\n\\nmodel_name = \"deepset/roberta-base-squad2\"\\n\\nnlp = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 237}, page_content='nlp = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\\n\\ncontext = \"The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\"\\nquestion = \"Which continent is the Amazon rainforest in?\"'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 1219}, page_content='def predict(context, question):\\n    res = nlp({\"question\": question, \"context\": context})\\n    return res[\"answer\"], res[\"score\"]\\n\\n\\ngr.Interface(\\n    predict,\\n    inputs=[\\n        gr.Textbox(lines=7, value=context, label=\"Context Paragraph\"),\\n        gr.Textbox(lines=2, value=question, label=\"Question\"),\\n    ],\\n    outputs=[gr.Textbox(label=\"Answer\"), gr.Textbox(label=\"Score\")],\\n).launch()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 1612}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/loaders/ip_adapter.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# IP-Adapter\\n\\n[IP-Adapter](https://hf.co/papers/2308.06721) is a lightweight adapter that enables prompting a diffusion model with an image. This method decouples the cross-attention layers of the image and text features. The image features are generated from an image encoder. Files generated from IP-Adapter are only ~100MBs.\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/loaders/ip_adapter.md', 'start_index': 916}, page_content='<Tip>\\n\\nLearn how to load an IP-Adapter checkpoint and image in the [IP-Adapter](../../using-diffusers/loading_adapters#ip-adapter) loading guide.\\n\\n</Tip>\\n\\n## IPAdapterMixin\\n\\n[[autodoc]] loaders.ip_adapter.IPAdapterMixin'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/source/package_reference/config.md', 'start_index': 0}, page_content='!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Configuration\\n\\n[`PeftConfigMixin`] is the base configuration class for storing the adapter configuration of a [`PeftModel`], and [`PromptLearningConfig`] is the base configuration class for soft prompt methods (p-tuning, prefix tuning, and prompt tuning). These base classes contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads.\\n\\n## PeftConfigMixin\\n\\n[[autodoc]] config.PeftConfigMixin\\n    - all\\n\\n## PeftConfig\\n\\n[[autodoc]] PeftConfig\\n    - all\\n\\n## PromptLearningConfig\\n\\n[[autodoc]] PromptLearningConfig\\n    - all'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 593}, page_content='<p align=\"center\">\\n  <picture>\\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\\n  </picture>\\n  <br/>\\n  <br/>\\n</p>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 1188}, page_content='<p align=\"center\">\\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\">\\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://huggingface.co/docs/transformers/index\">\\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/releases\">\\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 2008}, page_content='</a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">\\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\\n    </a>\\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\\n</p>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 2365}, page_content='<h4 align=\"center\">\\n    <p>\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README.md\">English</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hans.md\">简体中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">繁體中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">한국어</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_es.md\">Español</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">日本語</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">हिन्दी</a> |\\n        <b>Русский</b>\\n        <a href=\"https://github.com/huggingface/transformers//blob/main/README_te.md\">తెలుగు</a> |\\n    <p>\\n</h4>\\n\\n<h3 align=\"center\">\\n    <p>Современное машинное обучение для JAX, PyTorch и TensorFlow</p>\\n</h3>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 3215}, page_content='<h3 align=\"center\">\\n    <p>Современное машинное обучение для JAX, PyTorch и TensorFlow</p>\\n</h3>\\n\\n<h3 align=\"center\">\\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\\n</h3>\\n\\n🤗 Transformers предоставляет тысячи предварительно обученных моделей для выполнения различных задач, таких как текст, зрение и аудио.\\n\\nЭти модели могут быть применены к:\\n\\n* 📝 Тексту для таких задач, как классификация текстов, извлечение информации, ответы на вопросы, обобщение, перевод, генерация текстов на более чем 100 языках.\\n* 🖼️ Изображениям для задач классификации изображений, обнаружения объектов и сегментации.\\n* 🗣️ Аудио для задач распознавания речи и классификации аудио.\\n\\nМодели transformers также могут выполнять несколько задач, такие как ответы на табличные вопросы, распознавание оптических символов, извлечение информации из отсканированных документов, классификация видео и ответы на визуальные вопросы.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 4212}, page_content='🤗 Transformers предоставляет API для быстрой загрузки и использования предварительно обученных моделей, их тонкой настройки на собственных датасетах и последующего взаимодействия ими с сообществом на нашем [сайте](https://huggingface.co/models). В то же время каждый python модуль, определяющий архитектуру, полностью автономен и может быть модифицирован для проведения быстрых исследовательских экспериментов.\\n\\n🤗 Transformers опирается на три самые популярные библиотеки глубокого обучения - [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) и [TensorFlow](https://www.tensorflow.org/) - и легко интегрируется между ними. Это позволяет легко обучать модели с помощью одной из них, а затем загружать их для выводов с помощью другой.\\n\\n## Онлайн демонстрация'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 4974}, page_content='## Онлайн демонстрация\\n\\nБольшинство наших моделей можно протестировать непосредственно на их страницах с [сайта](https://huggingface.co/models). Мы также предлагаем [привтаный хостинг моделей, контроль версий и API для выводов](https://huggingface.co/pricing) для публичных и частных моделей.\\n\\nВот несколько примеров:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 5293}, page_content='В области NLP ( Обработка текстов на естественном языке ):\\n- [Маскированное заполнение слов с помощью BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\\n- [Распознавание сущностей с помощью Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\\n- [Генерация текста с помощью GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)\\n- [Выводы на естественном языке с помощью RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 5891}, page_content='- [Обобщение с помощью BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 6744}, page_content='- [Ответы на вопросы с помощью'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 6775}, page_content='DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+G'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 7674}, page_content='0%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 8102}, page_content='- [Перевод с помощью T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 8206}, page_content='В области компьютерного зрения:\\n- [Классификация изображений с помощью ViT](https://huggingface.co/google/vit-base-patch16-224)\\n- [Обнаружение объектов с помощью DETR](https://huggingface.co/facebook/detr-resnet-50)\\n- [Семантическая сегментация с помощью SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\\n- [Сегментация паноптикума с помощью MaskFormer](https://huggingface.co/facebook/maskformer-swin-small-coco)\\n- [Оценка глубины с помощью DPT](https://huggingface.co/docs/transformers/model_doc/dpt)\\n- [Классификация видео с помощью VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)\\n- [Универсальная сегментация с помощью OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 8953}, page_content='В области звука:\\n- [Автоматическое распознавание речи с помощью Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)\\n- [Поиск ключевых слов с помощью Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\\n- [Классификация аудиоданных с помощью траснформера аудиоспектрограмм](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)\\n\\nВ мультимодальных задачах:\\n- [Ответы на вопросы по таблице с помощью TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)\\n- [Визуальные ответы на вопросы с помощью ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)\\n- [Zero-shot классификация изображений с помощью CLIP](https://huggingface.co/openai/clip-vit-large-patch14)\\n- [Ответы на вопросы по документам с помощью LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)\\n- [Zero-shot классификация видео с помощью X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)\\n\\n\\n## 100 проектов, использующих Transformers'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 9876}, page_content='## 100 проектов, использующих Transformers\\n\\nTransformers - это не просто набор инструментов для использования предварительно обученных моделей: это сообщество проектов, созданное на его основе, и\\nHugging Face Hub. Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, инженерам и всем желающим\\nсоздавать проекты своей мечты.\\n\\nЧтобы отпраздновать 100 тысяч звезд Transformers, мы решили сделать акцент на сообществе, и создали страницу [awesome-transformers](./awesome-transformers.md), на которой перечислены 100\\nневероятных проектов, созданных с помощью transformers.\\n\\nЕсли вы являетесь владельцем или пользователем проекта, который, по вашему мнению, должен быть включен в этот список, пожалуйста, откройте PR для его добавления!\\n\\n## Если вы хотите получить индивидуальную поддержку от команды Hugging Face'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 10648}, page_content='## Если вы хотите получить индивидуальную поддержку от команды Hugging Face\\n\\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\\n</a><br>\\n\\n## Быстрый гайд\\n\\nДля использования модели на заданном входе (текст, изображение, звук, ...) мы предоставляем API `pipeline`. Конвейеры объединяют предварительно обученную модель с препроцессингом, который использовался при ее обучении. Вот как можно быстро использовать конвейер для классификации положительных и отрицательных текстов:\\n\\n```python\\n>>> from transformers import pipeline'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 11388}, page_content=\"```python\\n>>> from transformers import pipeline\\n\\n# Выделение конвейера для анализа настроений\\n>>> classifier = pipeline('sentiment-analysis')\\n>>> classifier('Мы очень рады представить конвейер в transformers.')\\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 11652}, page_content='```\\n\\nВторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейером, а третья оценивает ее на заданном тексте. Здесь ответ \"POSITIVE\" с уверенностью 99,97%.\\n\\nВо многих задачах, как в НЛП, так и в компьютерном зрении и речи, уже есть готовый `pipeline`. Например, мы можем легко извлечь обнаруженные объекты на изображении:\\n\\n``` python\\n>>> import requests\\n>>> from PIL import Image\\n>>> from transformers import pipeline\\n\\n# Скачиваем изображение с милыми котиками\\n>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\\n>>> image_data = requests.get(url, stream=True).raw\\n>>> image = Image.open(image_data)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 12342}, page_content=\"# Выделение конвейера для обнаружения объектов\\n>>> object_detector = pipeline('object-detection')\\n>>> object_detector(image)\\n[{'score': 0.9982201457023621,\\n  'label': 'remote',\\n  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\\n {'score': 0.9960021376609802,\\n  'label': 'remote',\\n  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\\n {'score': 0.9954745173454285,\\n  'label': 'couch',\\n  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\\n {'score': 0.9988006353378296,\\n  'label': 'cat',\\n  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\\n {'score': 0.9986783862113953,\\n  'label': 'cat',\\n  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 13030}, page_content='```\\n\\nЗдесь мы получаем список объектов, обнаруженных на изображении, с рамкой вокруг объекта и оценкой достоверности. Слева - исходное изображение, справа прогнозы:\\n\\n<h3 align=\"center\">\\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\" width=\"400\"></a>\\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png\" width=\"400\"></a>\\n</h3>\\n\\nПодробнее о задачах, поддерживаемых API `pipeline`, можно узнать в [этом учебном пособии](https://huggingface.co/docs/transformers/task_sum)\\n\\nВ дополнение к `pipeline`, для загрузки и использования любой из предварительно обученных моделей в заданной задаче достаточно трех строк кода. Вот версия для PyTorch:\\n```python\\n>>> from transformers import AutoTokenizer, AutoModel\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = AutoModel.from_pretrained(\"bert-base-uncased\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 13998}, page_content='>>> inputs = tokenizer(\"Привет мир!\", return_tensors=\"pt\")\\n>>> outputs = model(**inputs)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 14087}, page_content='```\\n\\nА вот эквивалентный код для TensorFlow:\\n```python\\n>>> from transformers import AutoTokenizer, TFAutoModel\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\\n\\n>>> inputs = tokenizer(\"Привет мир!\", return_tensors=\"tf\")\\n>>> outputs = model(**inputs)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 14417}, page_content='```\\n\\nТокенизатор отвечает за всю предварительную обработку, которую ожидает предварительно обученная модель, и может быть вызван непосредственно с помощью одной строки (как в приведенных выше примерах) или на списке. В результате будет получен словарь, который можно использовать в последующем коде или просто напрямую передать в модель с помощью оператора распаковки аргументов **.\\n\\nСама модель представляет собой обычный [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) или [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (в зависимости от используемого бэкенда), который можно использовать как обычно. [В этом руководстве](https://huggingface.co/docs/transformers/training) рассказывается, как интегрировать такую модель в классический цикл обучения PyTorch или TensorFlow, или как использовать наш API `Trainer` для быстрой тонкой настройки на новом датасете.\\n\\n## Почему необходимо использовать transformers?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 15353}, page_content='## Почему необходимо использовать transformers?\\n\\n1. Простые в использовании современные модели:\\n    - Высокая производительность в задачах понимания и генерации естественного языка, компьютерного зрения и аудио.\\n    - Низкий входной барьер для преподавателей и практиков.\\n    - Небольшое количество абстракций для пользователя и всего три класса для изучения.\\n    - Единый API для использования всех наших предварительно обученных моделей.\\n\\n1. Более низкие вычислительные затраты, меньший \"углеродный след\":\\n    - Исследователи могут обмениваться обученными моделями вместо того, чтобы постоянно их переобучать.\\n    - Практики могут сократить время вычислений и производственные затраты.\\n    - Десятки архитектур с более чем 60 000 предварительно обученных моделей для всех модальностей.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 16142}, page_content='1. Выбор подходящего фреймворка для каждого этапа жизни модели:\\n    - Обучение самых современных моделей за 3 строки кода.\\n    - Перемещайте одну модель между фреймворками TF2.0/PyTorch/JAX по своему усмотрению.\\n    - Беспрепятственный выбор подходящего фреймворка для обучения, оценки и производства.\\n\\n1. Легко настроить модель или пример под свои нужды:\\n    - Мы предоставляем примеры для каждой архитектуры, чтобы воспроизвести результаты, опубликованные их авторами.\\n    - Внутренние компоненты модели раскрываются максимально последовательно.\\n    - Файлы моделей можно использовать независимо от библиотеки для проведения быстрых экспериментов.\\n\\n## Почему я не должен использовать transformers?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 16843}, page_content='- Данная библиотека не является модульным набором строительных блоков для нейронных сетей. Код в файлах моделей специально не рефакторится дополнительными абстракциями, чтобы исследователи могли быстро итеративно работать с каждой из моделей, не погружаясь в дополнительные абстракции/файлы.\\n- API обучения не предназначен для работы с любой моделью, а оптимизирован для работы с моделями, предоставляемыми библиотекой. Для работы с общими циклами машинного обучения следует использовать другую библиотеку (возможно, [Accelerate](https://huggingface.co/docs/accelerate)).\\n- Несмотря на то, что мы стремимся представить как можно больше примеров использования, скрипты в нашей папке [примеров](https://github.com/huggingface/transformers/tree/main/examples) являются именно примерами. Предполагается, что они не будут работать \"из коробки\" для решения вашей конкретной задачи, и вам придется изменить несколько строк кода, чтобы адаптировать их под свои нужды.\\n\\n## Установка\\n\\n### С помощью pip'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 17804}, page_content='## Установка\\n\\n### С помощью pip\\n\\nДанный репозиторий протестирован на Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ и TensorFlow 2.6+.\\n\\nУстанавливать 🤗 Transformers следует в [виртуальной среде](https://docs.python.org/3/library/venv.html). Если вы не знакомы с виртуальными средами Python, ознакомьтесь с [руководством пользователя](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\\n\\nСначала создайте виртуальную среду с той версией Python, которую вы собираетесь использовать, и активируйте ее.\\n\\nЗатем необходимо установить хотя бы один бекенд из Flax, PyTorch или TensorFlow.\\nПожалуйста, обратитесь к страницам [TensorFlow установочная страница](https://www.tensorflow.org/install/), [PyTorch установочная страница](https://pytorch.org/get-started/locally/#start-locally) и/или [Flax](https://github.com/google/flax#quick-install) и [Jax](https://github.com/google/jax#installation), где описаны команды установки для вашей платформы.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 18771}, page_content='После установки одного из этих бэкендов 🤗 Transformers может быть установлен с помощью pip следующим образом:\\n\\n```bash\\npip install transformers'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 18915}, page_content='```\\n\\nЕсли вы хотите поиграть с примерами или вам нужен самый современный код и вы не можете ждать нового релиза, вы должны [установить библиотеку из исходного кода](https://huggingface.co/docs/transformers/installation#installing-from-source).\\n\\n### С помощью conda\\n\\nНачиная с версии Transformers v4.0.0, у нас появилсась поддержка conda: `huggingface`.\\n\\nУстановить Transformers с помощью conda можно следующим образом:\\n\\n```bash\\nconda install -c huggingface transformers'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 19385}, page_content='```\\n\\nО том, как установить Flax, PyTorch или TensorFlow с помощью conda, читайте на страницах, посвященных их установке.\\n\\n> **_ЗАМЕТКА:_** В операционной системе Windows вам может быть предложено активировать режим разработчика, чтобы воспользоваться преимуществами кэширования. Если для вас это невозможно, сообщите нам об этом [здесь](https://github.com/huggingface/huggingface_hub/issues/1062).\\n\\n## Модельные архитектуры\\n\\n**[Все контрольные точки моделей](https://huggingface.co/models)**, предоставляемые 🤗 Transformers, беспрепятственно интегрируются с huggingface.co [model hub](https://huggingface.co/models), куда они загружаются непосредственно [пользователями](https://huggingface.co/users) и [организациями](https://huggingface.co/organizations).\\n\\nТекущее количество контрольных точек: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 20284}, page_content='🤗 В настоящее время Transformers предоставляет следующие архитектуры (подробное описание каждой из них см. [здесь](https://huggingface.co/docs/transformers/model_summary)):'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 20458}, page_content='1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\\n1. **[ALIGN](https://huggingface.co/docs/transformers/model_doc/align)** (from Google Research) released with the paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 21205}, page_content='1. **[AltCLIP](https://huggingface.co/docs/transformers/model_doc/altclip)** (from BAAI) released with the paper [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679) by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.\\n1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\\n1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua University) released with the paper [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 22122}, page_content='1. **[Bark](https://huggingface.co/docs/transformers/model_doc/bark)** (from Suno) released in the repository [suno-ai/bark](https://github.com/suno-ai/bark) by Suno AI team.\\n1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\\n1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** (from École polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 22984}, page_content='1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\\n1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\\n1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 23786}, page_content='1. **[BERT For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\\n1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (from VinAI Research) released with the paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.\\n1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 24743}, page_content='1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\\n1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)** (from Microsoft Research AI4Science) released with the paper [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 25559}, page_content='1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (from Google AI) released with the paper [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.\\n1. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 26237}, page_content='1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\\n1. **[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)** (from Salesforce) released with the paper [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\\n1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** (from Salesforce) released with the paper [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 27225}, page_content='1. **[BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).\\n1. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) by Adrian de Wynter and Daniel J. Perry.\\n1. **[BridgeTower](https://huggingface.co/docs/transformers/model_doc/bridgetower)** (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 28016}, page_content='1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (from NAVER CLOVA) released with the paper [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.\\n1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 28704}, page_content='1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.\\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 29697}, page_content='1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.\\n1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\\n1. **[CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)** (from University of Göttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 30694}, page_content='1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.\\n1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (from MetaAI) released with the paper [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 31647}, page_content='1. **[Conditional DETR](https://huggingface.co/docs/transformers/model_doc/conditional_detr)** (from Microsoft Research Asia) released with the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.\\n1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\\n1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 32555}, page_content='1. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\\n1. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\\n1. **[CPM-Ant](https://huggingface.co/docs/transformers/model_doc/cpmant)** (from OpenBMB) released by the [OpenBMB](https://www.openbmb.org/).'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 33554}, page_content='1. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\\n1. **[CvT](https://huggingface.co/docs/transformers/model_doc/cvt)** (from Microsoft) released with the paper [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\\n1. **[Data2Vec](https://huggingface.co/docs/transformers/model_doc/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 34483}, page_content='1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\\n1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\\n1. **[Decision Transformer](https://huggingface.co/docs/transformers/model_doc/decision_transformer)** (from Berkeley/Facebook/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 35429}, page_content='1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)** (from SenseTime Research) released with the paper [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.\\n1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 36077}, page_content='1. **[DePlot](https://huggingface.co/docs/transformers/model_doc/deplot)** (from Google AI) released with the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.\\n1. **[DETA](https://huggingface.co/docs/transformers/model_doc/deta)** (from The University of Texas at Austin) released with the paper [NMS Strikes Back](https://arxiv.org/abs/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.\\n1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 37025}, page_content='1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\\n1. **[DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat)** (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 37623}, page_content='1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (from Meta AI) released with the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 38232}, page_content='1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 38987}, page_content='1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\\n1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\\n1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 39942}, page_content='1. **[DPT](https://huggingface.co/docs/transformers/master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\\n1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.\\n1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google Brain) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) by Mingxing Tan, Quoc V. Le.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 40790}, page_content='1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\\n1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)** (from Meta AI) released with the paper [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\\n1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 41663}, page_content='1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)** (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.\\n1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)** (from Baidu) released with the paper [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674) by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 42321}, page_content='1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (from Meta AI) are transformer protein language models.  **ESM-1b** was released with the paper [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. **ESM-1v** was released with the paper [Language models enable zero-shot prediction of the effects of mutations on protein function](https://doi.org/10.1101/2021.07.09.450648) by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. **ESM-2 and ESMFold** were released with the paper [Language models of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu,'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 43257}, page_content='by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 43420}, page_content='1. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)** (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 43820}, page_content='1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 44475}, page_content='1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 45133}, page_content='1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.\\n1. **[FLAVA](https://huggingface.co/docs/transformers/model_doc/flava)** (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.\\n1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 46102}, page_content='1. **[FocalNet](https://huggingface.co/docs/transformers/model_doc/focalnet)** (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.\\n1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\\n1. **[Fuyu](https://huggingface.co/docs/transformers/model_doc/fuyu)** (from ADEPT) Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar. Released with the paper [blog post](https://www.adept.ai/blog/fuyu-8b)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 46933}, page_content='1. **[GIT](https://huggingface.co/docs/transformers/model_doc/git)** (from Microsoft Research) released with the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.\\n1. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\\n1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 47867}, page_content='1. **[GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.\\n1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\\n1. **[GPT NeoX Japanese](https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese)** (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 48771}, page_content='1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\\n1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.\\n1. **[GPT-Sw3](https://huggingface.co/docs/transformers/model_doc/gpt-sw3)** (from AI-Sweden) released with the paper [Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf) by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 49746}, page_content=\"1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) released with the paper [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 50603}, page_content='1. **[GPTSAN-japanese](https://huggingface.co/docs/transformers/model_doc/gptsan-japanese)** released in the repository [tanreinama/GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md) by Toshiyuki Sakamoto(tanreinama).\\n1. **[Graphormer](https://huggingface.co/docs/transformers/model_doc/graphormer)** (from Microsoft) released with the paper [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.\\n1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 51486}, page_content='1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.\\n1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\\n1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 52458}, page_content='1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) released with the paper [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527) by Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh.\\n1. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 53175}, page_content='1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)** (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\\n1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)** (from Salesforce) released with the paper [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 53944}, page_content='1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)** (from OpenAI) released with the paper [Jukebox: A Generative Model for Music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.\\n1. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 54549}, page_content='1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\\n1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 55256}, page_content=\"1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\\n1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\\n1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 56174}, page_content='1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.\\n1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 56945}, page_content='1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (from The FAIR team of Meta AI) released with the paper [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX) by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 57847}, page_content='Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 58257}, page_content='1. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\\n1. **[LongT5](https://huggingface.co/docs/transformers/model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.\\n1. **[LUKE](https://huggingface.co/docs/transformers/model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 59130}, page_content='1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) by Hao Tan and Mohit Bansal.\\n1. **[M-CTC-T](https://huggingface.co/docs/transformers/model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 59718}, page_content='1. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\\n1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) released with the paper [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](https://arxiv.org/abs/2309.04662) by Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 60589}, page_content='1. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)** Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by Jörg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.\\n1. **[MarkupLM](https://huggingface.co/docs/transformers/model_doc/markuplm)** (from Microsoft Research Asia) released with the paper [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.\\n1. **[Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former)** (from FAIR and UIUC) released with the paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 61493}, page_content='1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\\n1. **[MatCha](https://huggingface.co/docs/transformers/model_doc/matcha)** (from Google AI) released with the paper [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662) by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 62174}, page_content='1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\\n1. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\\n1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (from Meta/USC/CMU/SJTU) released with the paper [Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 63163}, page_content='1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n1. **[MGP-STR](https://huggingface.co/docs/transformers/model_doc/mgp-str)** (from Alibaba Research) released with the paper [Multi-Granularity Prediction for Scene Text Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang, Cheng Da, and Cong Yao.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 64120}, page_content='1. **[mLUKE](https://huggingface.co/docs/transformers/model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\\n1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (from Facebook) released with the paper [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516) by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 64836}, page_content='1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\\n1. **[MobileNetV1](https://huggingface.co/docs/transformers/model_doc/mobilenet_v1)** (from Google Inc.) released with the paper [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\\n1. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)** (from Google Inc.) released with the paper [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 65830}, page_content='1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari.\\n1. **[MobileViTV2](https://huggingface.co/docs/transformers/model_doc/mobilevitv2)** (from Apple) released with the paper [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) by Sachin Mehta and Mohammad Rastegari.\\n1. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 66644}, page_content='1. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)** (from MosaiML) released with the repository [llm-foundry](https://github.com/mosaicml/llm-foundry/) by the MosaicML NLP Team.\\n1. **[MRA](https://huggingface.co/docs/transformers/model_doc/mra)** (from the University of Wisconsin - Madison) released with the paper [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.\\n1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 67477}, page_content='1. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)** (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.\\n1. **[MVP](https://huggingface.co/docs/transformers/model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\\n1. **[NAT](https://huggingface.co/docs/transformers/model_doc/nat)** (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 68309}, page_content='1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)** (from Huawei Noah’s Ark Lab) released with the paper [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.\\n1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.\\n1. **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 69142}, page_content='1. **[Nyströmformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\\n1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\\n1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (from [s-JoL](https://huggingface.co/s-JoL)) released on GitHub (now removed).'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 69977}, page_content='1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\\n1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 70716}, page_content='1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.\\n1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (from Google) released with the paper [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) by Jason Phang, Yao Zhao, and Peter J. Liu.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 71287}, page_content='1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.\\n1. **[Persimmon](https://huggingface.co/docs/transformers/main/model_doc/persimmon)** (from ADEPT) released in a [blog post](https://www.adept.ai/blog/persimmon-8b) by Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 72052}, page_content='1. **[Phi](https://huggingface.co/docs/main/transformers/model_doc/phi)** (from Microsoft Research) released with the papers - [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.\\n1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 73014}, page_content='1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (from Google) released with the paper [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.\\n1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 73688}, page_content='1. **[PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.\\n1. **[Pop2Piano](https://huggingface.co/docs/transformers/model_doc/pop2piano)** released with the paper [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi and Kyogu Lee.\\n1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 74595}, page_content='1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)** (from Nanjing University, The University of Hong Kong etc.) released with the paper [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/pdf/2102.12122.pdf) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.\\n1. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 75302}, page_content='1. **[RAG](https://huggingface.co/docs/transformers/model_doc/rag)** (from Facebook) released with the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.\\n1. **[REALM](https://huggingface.co/docs/transformers/model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.\\n1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 76241}, page_content='1. **[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)** (from META Platforms) released with the paper [Designing Network Design Space](https://arxiv.org/abs/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.\\n1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder.\\n1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 77072}, page_content='1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\\n1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)** (from Facebook) released with the paper [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 77771}, page_content='1. **[RoCBert](https://huggingface.co/docs/transformers/model_doc/roc_bert)** (from WeChatAI) released with the paper [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.\\n1. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\\n1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (from Bo Peng), released on [this repo](https://github.com/BlinkDL/RWKV-LM) by Bo Peng.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 78542}, page_content='1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\\n1. **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)** (from Meta AI) released with the paper [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\\n1. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 79527}, page_content='1. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\\n1. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 80235}, page_content='1. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\\n1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\\n1. **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 81199}, page_content='1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 81880}, page_content='1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\\n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\\n1. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)** (from University of Würzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 82868}, page_content='1. **[SwitchTransformers](https://huggingface.co/docs/transformers/model_doc/switch_transformers)** (from Google) released with the paper [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\\n1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 83555}, page_content='1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\\n1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 84296}, page_content='1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos.\\n1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\\n1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)** (from HuggingFace).'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 85048}, page_content='1. **[TimeSformer](https://huggingface.co/docs/transformers/model_doc/timesformer)** (from Facebook) released with the paper [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Gedas Bertasius, Heng Wang, Lorenzo Torresani.\\n1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine\\n1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 85980}, page_content='1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\\n1. **[TVLT](https://huggingface.co/docs/transformers/model_doc/tvlt)** (from UNC Chapel Hill) released with the paper [TVLT: Textless Vision-Language Transformer](https://arxiv.org/abs/2209.14156) by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.\\n1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 86893}, page_content='1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (from Google Research) released with the paper [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\\n1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 87603}, page_content='1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\\n1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)** (from Peking University) released with the paper [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221) by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.\\n1. **[VAN](https://huggingface.co/docs/transformers/model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 88551}, page_content='1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\\n1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 89189}, page_content='1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 89942}, page_content='1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\\n1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)** (from Meta AI) released with the paper [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.\\n1. **[ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 90936}, page_content='1. **[ViTMatte](https://huggingface.co/docs/transformers/main/model_doc/vitmatte)** (from HUST-VL) rreleased with the paper [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.\\n1. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)** (from Meta AI) released with the paper [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.\\n1. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)** (from Kakao Enterprise) released with the paper [Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) by Jaehyeon Kim, Jungil Kong, Juhee Son.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 91869}, page_content='1. **[ViViT](https://huggingface.co/docs/transformers/model_doc/vivit)** (from Google Research) released with the paper [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.\\n1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\\n1. **[Wav2Vec2-Conformer](https://huggingface.co/docs/transformers/model_doc/wav2vec2-conformer)** (from Facebook AI) released with the paper [FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 92775}, page_content='1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/transformers/model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\\n1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 93516}, page_content='1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\\n1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (from Microsoft Research) released with the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\\n1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)** (from Meta AI) released with the paper [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 94503}, page_content=\"1. **[XGLM](https://huggingface.co/docs/transformers/model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\\n1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 95255}, page_content='1. **[XLM-ProphetNet](https://huggingface.co/docs/transformers/model_doc/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\\n1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 96017}, page_content='1. **[XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\\n1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (from Meta AI) released with the paper [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472) by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.\\n1. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)** (from Google/CMU) released with the paper [\\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 97005}, page_content='1. **[XLS-R](https://huggingface.co/docs/transformers/model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\\n1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 97759}, page_content='1. **[YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\\n1. **[YOSO](https://huggingface.co/docs/transformers/model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 98482}, page_content='1. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in the process of adding a new model. You can find them in the [`templates`](./templates) folder of the repository. Be sure to check the [contributing guidelines](./CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 98853}, page_content='Чтобы проверить, есть ли у каждой модели реализация на Flax, PyTorch или TensorFlow, или связанный с ней токенизатор, поддерживаемый библиотекой 🤗 Tokenizers, обратитесь к [этой таблице](https://huggingface.co/docs/transformers/index#supported-frameworks).\\n\\nЭти реализации были протестированы на нескольких наборах данных (см. примеры скриптов) и должны соответствовать производительности оригинальных реализаций. Более подробную информацию о производительности можно найти в разделе \"Примеры\" [документации](https://github.com/huggingface/transformers/tree/main/examples).\\n\\n\\n## Изучи больше'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 99429}, page_content='## Изучи больше\\n\\n| Секция | Описание |\\n|-|-|\\n| [Документация](https://huggingface.co/docs/transformers/) | Полная документация по API и гайды |\\n| [Краткие описания задач](https://huggingface.co/docs/transformers/task_summary) | Задачи поддерживаются 🤗 Transformers |\\n| [Пособие по предварительной обработке](https://huggingface.co/docs/transformers/preprocessing) | Использование класса `Tokenizer` для подготовки данных для моделей |\\n| [Обучение и доработка](https://huggingface.co/docs/transformers/training) | Использование моделей, предоставляемых 🤗 Transformers, в цикле обучения PyTorch/TensorFlow и API `Trainer`. |\\n| [Быстрый тур: Тонкая настройка/скрипты использования](https://github.com/huggingface/transformers/tree/main/examples) | Примеры скриптов для тонкой настройки моделей на широком спектре задач |\\n| [Совместное использование и загрузка моделей](https://huggingface.co/docs/transformers/model_sharing) | Загружайте и делитесь с сообществом своими доработанными моделями |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 100422}, page_content='## Цитирование'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 100438}, page_content='Теперь у нас есть [статья](https://www.aclweb.org/anthology/2020.emnlp-demos.6/), которую можно цитировать для библиотеки 🤗 Transformers:\\n```bibtex\\n@inproceedings{wolf-etal-2020-transformers,\\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\\n    month = oct,\\n    year = \"2020\",\\n    address = \"Online\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 101359}, page_content='url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\\n    pages = \"38--45\"\\n}'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 101443}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Share a model'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 746}, page_content='-->\\n\\n# Share a model\\n\\nThe last two tutorials showed how you can fine-tune a model with PyTorch, Keras, and 🤗 Accelerate for distributed setups. The next step is to share your model with the community! At Hugging Face, we believe in openly sharing knowledge and resources to democratize artificial intelligence for everyone. We encourage you to consider sharing your model with the community to help others save time and resources.\\n\\nIn this tutorial, you will learn two methods for sharing a trained or fine-tuned model on the [Model Hub](https://huggingface.co/models):\\n\\n- Programmatically push your files to the Hub.\\n- Drag-and-drop your files to the Hub with the web interface.\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XvSGPZFEjDY\" title=\"YouTube video player\"\\nframeborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;\\npicture-in-picture\" allowfullscreen></iframe>\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 1677}, page_content='<Tip>\\n\\nTo share a model with the community, you need an account on [huggingface.co](https://huggingface.co/join). You can also join an existing organization or create a new one.\\n\\n</Tip>\\n\\n## Repository features\\n\\nEach repository on the Model Hub behaves like a typical GitHub repository. Our repositories offer versioning, commit history, and the ability to visualize differences.\\n\\nThe Model Hub\\'s built-in versioning is based on git and [git-lfs](https://git-lfs.github.com/). In other words, you can treat one model as one repository, enabling greater access control and scalability. Version control allows *revisions*, a method for pinning a specific version of a model with a commit hash, tag or branch.\\n\\nAs a result, you can load a specific model version with the `revision` parameter:\\n\\n```py\\n>>> model = AutoModel.from_pretrained(\\n...     \"julien-c/EsperBERTo-small\", revision=\"v2.0.1\"  # tag name, or branch name, or commit hash\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 2617}, page_content='```\\n\\nFiles are also easily edited in a repository, and you can view the commit history as well as the difference:\\n\\n![vis_diff](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis_diff.png)\\n\\n## Setup\\n\\nBefore sharing a model to the Hub, you will need your Hugging Face credentials. If you have access to a terminal, run the following command in the virtual environment where 🤗 Transformers is installed. This will store your access token in your Hugging Face cache folder (`~/.cache/` by default):\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nIf you are using a notebook like Jupyter or Colaboratory, make sure you have the [`huggingface_hub`](https://huggingface.co/docs/hub/adding-a-library) library installed. This library allows you to programmatically interact with the Hub.\\n\\n```bash\\npip install huggingface_hub'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 3453}, page_content='```\\n\\nThen use `notebook_login` to sign-in to the Hub, and follow the link [here](https://huggingface.co/settings/token) to generate a token to login with:\\n\\n```py\\n>>> from huggingface_hub import notebook_login\\n\\n>>> notebook_login()'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 3684}, page_content='```\\n\\n## Convert a model for all frameworks\\n\\nTo ensure your model can be used by someone working with a different framework, we recommend you convert and upload your model with both PyTorch and TensorFlow checkpoints. While users are still able to load your model from a different framework if you skip this step, it will be slower because 🤗 Transformers will need to convert the checkpoint on-the-fly.\\n\\nConverting a checkpoint for another framework is easy. Make sure you have PyTorch and TensorFlow installed (see [here](installation) for installation instructions), and then find the specific model for your task in the other framework. \\n\\n<frameworkcontent>\\n<pt>\\nSpecify `from_tf=True` to convert a checkpoint from TensorFlow to PyTorch:\\n\\n```py\\n>>> pt_model = DistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_tf=True)\\n>>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 4611}, page_content='```\\n</pt>\\n<tf>\\nSpecify `from_pt=True` to convert a checkpoint from PyTorch to TensorFlow:\\n\\n```py\\n>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\\n```\\n\\nThen you can save your new TensorFlow model with its new checkpoint:\\n\\n```py\\n>>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\\n```\\n</tf>\\n<jax>\\nIf a model is available in Flax, you can also convert a checkpoint from PyTorch to Flax:\\n\\n```py\\n>>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\\n...     \"path/to/awesome-name-you-picked\", from_pt=True\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 5219}, page_content='```\\n</jax>\\n</frameworkcontent>\\n\\n## Push a model during training\\n\\n<frameworkcontent>\\n<pt>\\n<Youtube id=\"Z1-XMy-GNLQ\"/>\\n\\nSharing a model to the Hub is as simple as adding an extra parameter or callback. Remember from the [fine-tuning tutorial](training), the [`TrainingArguments`] class is where you specify hyperparameters and additional training options. One of these training options includes the ability to push a model directly to the Hub. Set `push_to_hub=True` in your [`TrainingArguments`]:\\n\\n```py\\n>>> training_args = TrainingArguments(output_dir=\"my-awesome-model\", push_to_hub=True)\\n```\\n\\nPass your training arguments as usual to [`Trainer`]:\\n\\n```py\\n>>> trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     train_dataset=small_train_dataset,\\n...     eval_dataset=small_eval_dataset,\\n...     compute_metrics=compute_metrics,\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 6078}, page_content='```\\n\\nAfter you fine-tune your model, call [`~transformers.Trainer.push_to_hub`] on [`Trainer`] to push the trained model to the Hub. 🤗 Transformers will even automatically add training hyperparameters, training results and framework versions to your model card!\\n\\n```py\\n>>> trainer.push_to_hub()\\n```\\n</pt>\\n<tf>\\nShare a model to the Hub with [`PushToHubCallback`]. In the [`PushToHubCallback`] function, add:\\n\\n- An output directory for your model.\\n- A tokenizer.\\n- The `hub_model_id`, which is your Hub username and model name.\\n\\n```py\\n>>> from transformers import PushToHubCallback\\n\\n>>> push_to_hub_callback = PushToHubCallback(\\n...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 6823}, page_content='```\\n\\nAdd the callback to [`fit`](https://keras.io/api/models/model_training_apis/), and 🤗 Transformers will push the trained model to the Hub:\\n\\n```py\\n>>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\\n```\\n</tf>\\n</frameworkcontent>\\n\\n## Use the `push_to_hub` function\\n\\nYou can also call `push_to_hub` directly on your model to upload it to the Hub.\\n\\nSpecify your model name in `push_to_hub`:\\n\\n```py\\n>>> pt_model.push_to_hub(\"my-awesome-model\")\\n```\\n\\nThis creates a repository under your username with the model name `my-awesome-model`. Users can now load your model with the `from_pretrained` function:\\n\\n```py\\n>>> from transformers import AutoModel\\n\\n>>> model = AutoModel.from_pretrained(\"your_username/my-awesome-model\")\\n```\\n\\nIf you belong to an organization and want to push your model under the organization name instead, just add it to the `repo_id`:\\n\\n```py\\n>>> pt_model.push_to_hub(\"my-awesome-org/my-awesome-model\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 7804}, page_content='```\\n\\nThe `push_to_hub` function can also be used to add other files to a model repository. For example, add a tokenizer to a model repository:\\n\\n```py\\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\\n```\\n\\nOr perhaps you\\'d like to add the TensorFlow version of your fine-tuned PyTorch model:\\n\\n```py\\n>>> tf_model.push_to_hub(\"my-awesome-model\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 8143}, page_content=\"```\\n\\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\\n\\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\\n\\n## Upload with the web interface\\n\\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\\n\\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\\n\\nFrom here, add some information about your model:\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 8833}, page_content=\"From here, add some information about your model:\\n\\n- Select the **owner** of the repository. This can be yourself or any of the organizations you belong to.\\n- Pick a name for your model, which will also be the repository name.\\n- Choose whether your model is public or private.\\n- Specify the license usage for your model.\\n\\nNow click on the **Files** tab and click on the **Add file** button to upload a new file to your repository. Then drag-and-drop a file to upload and add a commit message.\\n\\n![upload_file](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/upload_file.png)\\n\\n## Add a model card\\n\\nTo make sure users understand your model's capabilities, limitations, potential biases and ethical considerations, please add a model card to your repository. The model card is defined in the `README.md` file. You can add a model card by:\\n\\n* Manually creating and uploading a `README.md` file.\\n* Clicking on the **Edit model card** button in your model repository.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 9825}, page_content=\"Take a look at the DistilBert [model card](https://huggingface.co/distilbert-base-uncased) for a good example of the type of information a model card should include. For more details about other options you can control in the `README.md` file such as a model's carbon footprint or widget examples, refer to the documentation [here](https://huggingface.co/docs/hub/models-cards).\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# LoRA\\n\\n<Tip warning={true}>\\n\\nThis is experimental and the API may change in the future.\\n\\n</Tip>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 587}, page_content='# LoRA\\n\\n<Tip warning={true}>\\n\\nThis is experimental and the API may change in the future.\\n\\n</Tip>\\n\\n[LoRA (Low-Rank Adaptation of Large Language Models)](https://hf.co/papers/2106.09685) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained. This makes training with LoRA much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), which are easier to store and share. LoRA can also be combined with other training techniques like DreamBooth to speedup training.\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 1229}, page_content='<Tip>\\n\\nLoRA is very versatile and supported for [DreamBooth](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py), [Kandinsky 2.2](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_lora_decoder.py), [Stable Diffusion XL](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py), [text-to-image](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py), and [Wuerstchen](https://github.com/huggingface/diffusers/blob/main/examples/wuerstchen/text_to_image/train_text_to_image_lora_prior.py).\\n\\n</Tip>\\n\\nThis guide will explore the [train_text_to_image_lora.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py) script to help you become more familiar with it, and how you can adapt it for your own use-case.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 2186}, page_content='Before running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install .'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 2347}, page_content='```\\n\\nNavigate to the example folder with the training script and install the required dependencies for the script you\\'re using:\\n\\n<hfoptions id=\"installation\">\\n<hfoption id=\"PyTorch\">\\n\\n```bash\\ncd examples/text_to_image\\npip install -r requirements.txt\\n```\\n\\n</hfoption>\\n<hfoption id=\"Flax\">\\n\\n```bash\\ncd examples/text_to_image\\npip install -r requirements_flax.txt\\n```\\n\\n</hfoption>\\n</hfoptions>\\n\\n<Tip>\\n\\n🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It\\'ll automatically configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\\n\\n</Tip>\\n\\nInitialize an 🤗 Accelerate environment:\\n\\n```bash\\naccelerate config\\n```\\n\\nTo setup a default 🤗 Accelerate environment without choosing any configurations:\\n\\n```bash\\naccelerate config default'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 3237}, page_content=\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```bash\\nfrom accelerate.utils import write_basic_config\\n\\nwrite_basic_config()\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 3412}, page_content=\"```\\n\\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\\n\\n<Tip>\\n\\nThe following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py) and let us know if you have any questions or concerns.\\n\\n</Tip>\\n\\n## Script parameters\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 4033}, page_content=\"</Tip>\\n\\n## Script parameters\\n\\nThe training script has many parameters to help you customize your training run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L85) function. Default values are provided for most parameters that work pretty well, but you can also set your own values in the training command if you'd like.\\n\\nFor example, to increase the number of epochs to train:\\n\\n```bash\\naccelerate launch train_text_to_image_lora.py \\\\\\n  --num_train_epochs=150 \\\\\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 4664}, page_content=\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters) training guide, so this guide just focuses on the LoRA relevant parameters:\\n\\n- `--rank`: the number of low-rank matrices to train\\n- `--learning_rate`: the default learning rate is 1e-4, but with LoRA, you can use a higher learning rate\\n\\n## Training script\\n\\nThe dataset preprocessing code and training loop are found in the [`main()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L371) function, and if you need to adapt the training script, this is where you'll make your changes.\\n\\nAs with the script parameters, a walkthrough of the training script is provided in the [Text-to-image](text2image#training-script) training guide. Instead, this guide takes a look at the LoRA relevant parts of the script.\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 5577}, page_content='The script begins by adding the [new LoRA weights](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L447) to the attention layers. This involves correctly configuring the weight size for each block in the UNet. You\\'ll see the `rank` parameter is used to create the [`~models.attention_processor.LoRAAttnProcessor`]:\\n\\n```py\\nlora_attn_procs = {}\\nfor name in unet.attn_processors.keys():\\n    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\\n    if name.startswith(\"mid_block\"):\\n        hidden_size = unet.config.block_out_channels[-1]\\n    elif name.startswith(\"up_blocks\"):\\n        block_id = int(name[len(\"up_blocks.\")])\\n        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\\n    elif name.startswith(\"down_blocks\"):\\n        block_id = int(name[len(\"down_blocks.\")])\\n        hidden_size = unet.config.block_out_channels[block_id]'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 6574}, page_content='lora_attn_procs[name] = LoRAAttnProcessor(\\n        hidden_size=hidden_size,\\n        cross_attention_dim=cross_attention_dim,\\n        rank=args.rank,\\n    )\\n\\nunet.set_attn_processor(lora_attn_procs)\\nlora_layers = AttnProcsLayers(unet.attn_processors)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 6823}, page_content=\"```\\n\\nThe [optimizer](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L519) is initialized with the `lora_layers` because these are the only weights that'll be optimized:\\n\\n```py\\noptimizer = optimizer_cls(\\n    lora_layers.parameters(),\\n    lr=args.learning_rate,\\n    betas=(args.adam_beta1, args.adam_beta2),\\n    weight_decay=args.adam_weight_decay,\\n    eps=args.adam_epsilon,\\n)\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 7290}, page_content=\"```\\n\\nAside from setting up the LoRA layers, the training script is more or less the same as train_text_to_image.py!\\n\\n## Launch the script\\n\\nOnce you've made all your changes or you're okay with the default configuration, you're ready to launch the training script! 🚀\\n\\nLet's train on the [Pokémon BLIP captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions) dataset to generate our yown Pokémon. Set the environment variables `MODEL_NAME` and `DATASET_NAME` to the model and dataset respectively. You should also specify where to save the model in `OUTPUT_DIR`, and the name of the model to save to on the Hub with `HUB_MODEL_ID`. The script creates and saves the following files to your repository:\\n\\n- saved model checkpoints\\n- `pytorch_lora_weights.safetensors` (the trained LoRA weights)\\n\\nIf you're training on more than one GPU, add the `--multi_gpu` parameter to the `accelerate launch` command.\\n\\n<Tip warning={true}>\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 8210}, page_content='<Tip warning={true}>\\n\\nA full training run takes ~5 hours on a 2080 Ti GPU with 11GB of VRAM.\\n\\n</Tip>\\n\\n```bash\\nexport MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\\nexport OUTPUT_DIR=\"/sddata/finetune/lora/pokemon\"\\nexport HUB_MODEL_ID=\"pokemon-lora\"\\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\\n\\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\\\\n  --dataset_name=$DATASET_NAME \\\\\\n  --dataloader_num_workers=8 \\\\\\n  --resolution=512 \\\\\\n  --center_crop \\\\\\n  --random_flip \\\\\\n  --train_batch_size=1 \\\\\\n  --gradient_accumulation_steps=4 \\\\\\n  --max_train_steps=15000 \\\\\\n  --learning_rate=1e-04 \\\\\\n  --max_grad_norm=1 \\\\\\n  --lr_scheduler=\"cosine\" \\\\\\n  --lr_warmup_steps=0 \\\\\\n  --output_dir=${OUTPUT_DIR} \\\\\\n  --push_to_hub \\\\\\n  --hub_model_id=${HUB_MODEL_ID} \\\\\\n  --report_to=wandb \\\\\\n  --checkpointing_steps=500 \\\\\\n  --validation_prompt=\"A pokemon with blue eyes.\" \\\\\\n  --seed=1337'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 9146}, page_content='```\\n\\nOnce training has been completed, you can use your model for inference:\\n\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\\npipeline.load_lora_weights(\"path/to/lora/model\", weight_name=\"pytorch_lora_weights.safetensors\")\\nimage = pipeline(\"A pokemon with blue eyes\").images[0]\\n```\\n\\n## Next steps\\n\\nCongratulations on training a new model with LoRA! To learn more about how to use your new model, the following guides may be helpful:\\n\\n- Learn how to [load different LoRA formats](../using-diffusers/loading_adapters#LoRA) trained using community trainers like Kohya and TheLastBen.\\n- Learn how to use and [combine multiple LoRA\\'s](../tutorials/using_peft_for_inference) with PEFT for inference.'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 0}, page_content='--\\ntitle: MAPE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  Mean Absolute Percentage Error (MAPE) is the mean percentage error difference between the predicted and actual\\n  values.\\n---\\n\\n# Metric Card for MAPE\\n\\n\\n## Metric Description\\n\\nMean Absolute Error (MAPE) is the mean of the percentage error of difference between the predicted $x_i$ and actual $y_i$ numeric values:\\n![image](https://user-images.githubusercontent.com/8100/200005316-c3975d32-8978-40f3-b541-c2ef57ec7c5b.png)\\n\\n## How to Use\\n\\nAt minimum, this metric requires predictions and references as inputs.\\n\\n```python\\n>>> mape_metric = evaluate.load(\"mape\")\\n>>> predictions = [2.5, 0.0, 2, 8]\\n>>> references = [3, -0.5, 2, 7]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 869}, page_content='```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (`n_samples`, `n_outputs`), representing the estimated target values.\\n- `references`: numeric array-like of shape (`n_samples,`) or (`n_samples`, `n_outputs`), representing the ground truth (correct) target values.\\n\\nOptional arguments:\\n- `sample_weight`: numeric array-like of shape (`n_samples,`) representing sample weights. The default is `None`.\\n- `multioutput`: `raw_values`, `uniform_average` or numeric array-like of shape (`n_outputs,`), which defines the aggregation of multiple output values. The default value is `uniform_average`.\\n  - `raw_values` returns a full set of errors in case of multioutput input.\\n  - `uniform_average` means that the errors of all outputs are averaged with uniform weight. \\n  - the array-like value defines weights used to average errors.'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 1747}, page_content=\"### Output Values\\nThis metric outputs a dictionary, containing the mean absolute error score, which is of type:\\n- `float`: if multioutput is `uniform_average` or an ndarray of weights, then the weighted average of all output errors is returned.\\n- numeric array-like of shape (`n_outputs,`): if multioutput is `raw_values`, then the score is returned for each output separately. \\n\\nEach MAPE `float` value is postive with the best value being 0.0.\\n\\nOutput Example(s):\\n```python\\n{'mape': 0.5}\"),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 2237}, page_content='```\\n\\nIf `multioutput=\"raw_values\"`:\\n```python\\n{\\'mape\\': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\n\\nExample with the `uniform_average` config:\\n```python\\n>>> mape_metric = evaluate.load(\"mape\")\\n>>> predictions = [2.5, 0.0, 2, 8]\\n>>> references = [3, -0.5, 2, 7]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)\\n>>> print(results)\\n{\\'mape\\': 0.3273...}\\n```\\n\\nExample with multi-dimensional lists, and the `raw_values` config:\\n```python\\n>>> mape_metric = evaluate.load(\"mape\", \"multilist\")\\n>>> predictions = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> references = [[0.1, 2], [-1, 2], [8, -5]]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)\\n>>> print(results)\\n{\\'mape\\': 0.8874...}\\n>>> results = mape_metric.compute(predictions=predictions, references=references, multioutput=\\'raw_values\\')\\n>>> print(results)\\n{\\'mape\\': array([1.3749..., 0.4])}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 3156}, page_content='```\\n\\n## Limitations and Bias\\nOne limitation of MAPE is that it cannot be used if the ground truth is zero or close to zero. This metric is also asymmetric in that it puts a heavier penalty on predictions less than the ground truth and a smaller penalty on predictions bigger than the ground truth and thus can lead to a bias of methods being select which under-predict if selected via this metric.\\n\\n## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n  title={Scikit-learn: Machine Learning in {P}ython},\\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n  journal={Journal of Machine Learning Research},\\n  volume={12},\\n  pages={2825--2830},\\n  year={2011}\\n}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 4063}, page_content='```\\n\\n```bibtex\\n@article{DEMYTTENAERE201638,\\n    title = {Mean Absolute Percentage Error for regression models},\\n    journal = {Neurocomputing},\\n    volume = {192},\\n    pages = {38--48},\\n    year = {2016},\\n    note = {Advances in artificial neural networks, machine learning and computational intelligence},\\n    issn = {0925-2312},\\n    doi = {https://doi.org/10.1016/j.neucom.2015.12.114},\\n    url = {https://www.sciencedirect.com/science/article/pii/S0925231216003325},\\n    author = {Arnaud {de Myttenaere} and Boris Golden and Bénédicte {Le Grand} and Fabrice Rossi},\\n}\\n```\\n\\n## Further References\\n- [Mean absolute percentage error - Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 1}, page_content=\"# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on the Inception family of architectures but incorporates [residual connections](https://paperswithcode.com/method/residual-connection) (replacing the filter concatenation stage of the Inception architecture).\\n\\nThis particular model was trained for study of adversarial examples (adversarial training).\\n\\nThe weights from this model were ported from [Tensorflow/Models](https://github.com/tensorflow/models).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True)\\nmodel.eval()\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 707}, page_content='```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 1444}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 2197}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 2825}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 3202}, page_content='```BibTeX\\n@article{DBLP:journals/corr/abs-1804-00097,\\n  author    = {Alexey Kurakin and\\n               Ian J. Goodfellow and\\n               Samy Bengio and\\n               Yinpeng Dong and\\n               Fangzhou Liao and\\n               Ming Liang and\\n               Tianyu Pang and\\n               Jun Zhu and\\n               Xiaolin Hu and\\n               Cihang Xie and\\n               Jianyu Wang and\\n               Zhishuai Zhang and\\n               Zhou Ren and\\n               Alan L. Yuille and\\n               Sangxia Huang and\\n               Yao Zhao and\\n               Yuzhe Zhao and\\n               Zhonglin Han and\\n               Junjiajia Long and\\n               Yerkebulan Berdibekov and\\n               Takuya Akiba and\\n               Seiya Tokui and\\n               Motoki Abe},\\n  title     = {Adversarial Attacks and Defences Competition},\\n  journal   = {CoRR},\\n  volume    = {abs/1804.00097},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1804.00097},\\n  archivePrefix = {arXiv},'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 4105}, page_content='year      = {2018},\\n  url       = {http://arxiv.org/abs/1804.00097},\\n  archivePrefix = {arXiv},\\n  eprint    = {1804.00097},\\n  timestamp = {Thu, 31 Oct 2019 16:31:22 +0100},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 4420}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 4425}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: Ensemble Adversarial\\n  Paper:\\n    Title: Adversarial Attacks and Defences Competition\\n    URL: https://paperswithcode.com/paper/adversarial-attacks-and-defences-competition\\nModels:\\n- Name: ens_adv_inception_resnet_v2\\n  In Collection: Ensemble Adversarial\\n  Metadata:\\n    FLOPs: 16959133120\\n    Parameters: 55850000\\n    File Size: 223774238\\n    Architecture:\\n    - 1x1 Convolution\\n    - Auxiliary Classifier\\n    - Average Pooling\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inception-v3 Module\\n    - Max Pooling\\n    - ReLU\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: ens_adv_inception_resnet_v2\\n    Crop Pct: '0.897'\\n    Image Size: '299'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/inception_resnet_v2.py#L351\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 5396}, page_content='Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ens_adv_inception_resnet_v2-2592a550.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 1.0%\\n      Top 5 Accuracy: 17.32%\\n-->'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': 1}, page_content='Using Flair at Hugging Face\\n\\n[Flair](https://github.com/flairNLP/flair) is a very simple framework for state-of-the-art NLP.\\nDeveloped by [Humboldt University of Berlin](https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en/) and friends.\\n\\n## Exploring Flair in the Hub\\n\\nYou can find `flair` models by filtering at the left of the [models page](https://huggingface.co/models?library=flair).\\n\\nAll models on the Hub come with these useful features:\\n\\n1. An automatically generated model card with a brief description.\\n2. An interactive widget you can use to play with the model directly in the browser.\\n3. An Inference API that allows you to make inference requests.\\n\\n## Installation\\n\\nTo get started, you can follow the [Flair installation guide](https://github.com/flairNLP/flair?tab=readme-ov-file#requirements-and-installation).\\nYou can also use the following one-line install through pip:\\n\\n```\\n$ pip install -U flair'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': 904}, page_content='```\\n$ pip install -U flair\\n```\\n\\n## Using existing models\\n\\nAll `flair` models can easily be loaded from the Hub:\\n\\n```py\\nfrom flair.data import Sentence\\nfrom flair.models import SequenceTagger\\n\\n# load tagger\\ntagger = SequenceTagger.load(\"flair/ner-multi\")\\n```\\n\\nOnce loaded, you can use `predict()` to perform inference:\\n\\n```py\\nsentence = Sentence(\"George Washington ging nach Washington.\")\\ntagger.predict(sentence)\\n\\n# print sentence\\nprint(sentence)\\n```\\n\\nIt outputs the following:\\n\\n```text\\nSentence[6]: \"George Washington ging nach Washington.\" → [\"George Washington\"/PER, \"Washington\"/LOC]'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': 1492}, page_content='```\\n\\nIf you want to load a specific Flair model, you can click `Use in Flair` in the model card and you will be given a working snippet!\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet1.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet1-dark.png\"/>\\n</div>\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet2.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet2-dark.png\"/>\\n</div>\\n\\n## Additional resources'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': 2323}, page_content='## Additional resources\\n\\n* Flair [repository](https://github.com/flairNLP/flair)\\n* Flair [docs](https://flairnlp.github.io/docs/intro)\\n* Official Flair [models](https://huggingface.co/flair) on the Hub (mainly trained by [@alanakbik](https://huggingface.co/alanakbik) and [@stefan-it](https://huggingface.co/stefan-it))'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/accordion/README.md', 'start_index': 1}, page_content='`@gradio/button`\\n\\n```html\\n<script>\\n\\timport { Button } from \"@gradio/button\";\\n</script>\\n\\n<button type=\"primary|secondary\" href=\"string\" on:click=\"{e.detail === href}\">\\n\\tcontent\\n</button>\\n```'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': 0}, page_content='he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library that provides an API to quickly download many public datasets and preprocess them. In this video we will explore how to do that. The downloading part is easy: with the load_dataset function, you can directly download and cache a dataset from its identifier on the Dataset hub. Here we fetch the MRPC dataset from the GLUE benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which is a sort of dictionary containing each split of our dataset. We can access each split by indexing with its name. This split is then an instance of the Dataset class, with columns (here sentence1, sentence2. label and idx) and rows. We can access a given element by its index. The amazing thing about the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': 903}, page_content='the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which means that even if your dataset is huge you won\\'t get out of RAM: only the elements you request are loaded in memory. Accessing a slice of your dataset is as easy as one element. The result is then a dictionary with list of values for each keys (here the list of labels, the list of first sentences and the list of second sentences). The features attribute of a Dataset gives us more information about its columns. In particular, we can see here it gives us the correspondence between the integers and names for the labels. 0 stands for not equivalent and 1 for equivalent. To preprocess all the elements of our dataset, we need to tokenize them. Have a look at the video \"Preprocess sentence pairs\" for a refresher, but you just have to send the two sentences to the tokenizer with some additional keyword arguments. Here we indicate a maximum length of 128 and pad inputs shorter than this length,'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': 1806}, page_content=\"arguments. Here we indicate a maximum length of 128 and pad inputs shorter than this length, truncate inputs that are longer. We put all of this in a tokenize_function that we can directly apply to all the splits in our dataset with the map method. As long as the function returns a dictionary-like object, the map method will add new columns as needed or update existing ones. To speed up preprocessing and take advantage of the fact our tokenizer is backed by Rust thanks to the Hugging Face Tokenizers library, we can process several elements at the same time to our tokenize function, using the batched=True argument. Since the tokenizer can handle list of first/second sentences, the tokenize_function does not need to change for this. You can also use multiprocessing with the map method, check out its documentation! Once this is done, we are almost ready for training: we just remove the columns we don't need anymore with the remove_columns method, rename label to labels (since the models\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': 2710}, page_content=\"we don't need anymore with the remove_columns method, rename label to labels (since the models from Hugging Face Transformers expect that) and set the output format to our desired backend: torch, tensorflow or numpy. If needed, we can also generate a short sample of a dataset using the select method.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 0}, page_content='--\\ntitle: \"Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too\"\\nthumbnail: /blog/assets/78_ml_director_insights/mantis1.png\\nauthors:\\n- user: mattupson\\n  guest: true\\n---\\n\\n# Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too\\n\\n\\n\\nHugging Face recently launched [Inference Endpoints](https://huggingface.co/inference-endpoints); which as they put it: solves transformers in production. Inference Endpoints is a managed service that allows you to:\\n\\n- Deploy (almost) any model on Hugging Face Hub\\n- To any cloud (AWS, and Azure, GCP on the way)\\n- On a range of instance types (including GPU)\\n- We’re switching some of our Machine Learning (ML) models that do inference on a CPU to this new service. This blog is about why, and why you might also want to consider it.\\n\\n## What were we doing?'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 823}, page_content='## What were we doing?\\n\\nThe models that we have switched over to Inference Endpoints were previously managed internally and were running on AWS [Elastic Container Service](https://aws.amazon.com/ecs/) (ECS) backed by [AWS Fargate](https://aws.amazon.com/fargate/). This gives you a serverless cluster which can run container based tasks. Our process was as follows:\\n\\n- Train model on a GPU instance (provisioned by [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))\\n- Upload to [Hugging Face Hub](https://huggingface.co/models)\\n- Build API to serve model [(FastAPI)](https://fastapi.tiangolo.com/)\\n- Wrap API in container [(Docker)](https://www.docker.com/)\\n- Upload container to AWS [Elastic Container Repository](https://aws.amazon.com/ecr/) (ECR)\\n- Deploy model to ECS Cluster'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 1655}, page_content='Now, you can reasonably argue that ECS was not the best approach to serving ML models, but it served us up until now, and also allowed ML models to sit alongside other container based services, so it reduced cognitive load.\\n\\n## What do we do now?\\n\\nWith Inference Endpoints, our flow looks like this:\\n\\n- Train model on a GPU instance (provisioned by  [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))\\n- Upload to [Hugging Face Hub](https://huggingface.co/models)\\n- Deploy using Hugging Face Inference Endpoints.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 2219}, page_content='So this is significantly easier. We could also use another managed service such as [SageMaker](https://aws.amazon.com/es/sagemaker/), [Seldon](https://www.seldon.io/), or [Bento ML](https://www.bentoml.com/), etc., but since we are already uploading our model to Hugging Face hub to act as a model registry, and we’re pretty invested in Hugging Face’s other tools (like transformers, and [AutoTrain](https://huggingface.co/autotrain)) using Inference Endpoints makes a lot of sense for us.\\n\\n\\n## What about Latency and Stability?\\n\\nBefore switching to Inference Endpoints we tested different CPU endpoints types using [ab](https://httpd.apache.org/docs/2.4/programs/ab.html).\\n\\nFor ECS we didn’t test so extensively, but we know that a large container had a latency of about ~200ms from an instance in the same region. The tests we did for Inference Endpoints we based on text classification model fine tuned on [RoBERTa](https://huggingface.co/roberta-base) with the following test parameters:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 3212}, page_content='- Requester region: eu-east-1\\n- Requester instance size: t3-medium\\n- Inference endpoint region: eu-east-1\\n- Endpoint Replicas: 1\\n- Concurrent connections: 1\\n- Requests: 1000 (1000 requests in 1–2 minutes even from a single connection would represent very heavy use for this particular application)\\n\\nThe following table shows latency (ms ± standard deviation and time to complete test in seconds) for four Intel Ice Lake equipped CPU endpoints.\\n\\n```bash\\nsize   |  vCPU (cores) |   Memory (GB)  |  ECS (ms) |  🤗 (ms)\\n----------------------------------------------------------------------\\nsmall  |  1            |  2             |   _       | ~ 296   \\nmedium |  2            |  4             |   _       | 156 ± 51 (158s)  \\nlarge  |  4            |   8            |   ~200    | 80 ± 30 (80s)   \\nxlarge |  8            | 16             |  _        | 43 ± 31 (43s)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 4076}, page_content='```\\nWhat we see from these results is pretty encouraging. The application that will consume these endpoints serves requests in real time, so we need as low latency as possible. We can see that the vanilla Hugging Face container was more than twice as fast as our bespoke container run on ECS — the slowest response we received from the large Inference Endpoint was just 108ms.\\n\\n## What about the cost?\\n\\nSo how much does this all cost? The table below shows a price comparison for what we were doing previously (ECS + Fargate) and using Inference Endpoints.\\n\\n```bash\\nsize   |  vCPU         |   Memory (GB)  |  ECS      |  🤗       |  % diff\\n----------------------------------------------------------------------\\nsmall  |  1            |  2             |  $ 33.18  | $ 43.80   |  0.24\\nmedium |  2            |  4             |  $ 60.38  | $ 87.61   |  0.31 \\nlarge  |  4            |  8             |  $ 114.78 | $ 175.22  |  0.34\\nxlarge |  8            | 16             |  $ 223.59 | $ 350.44  | 0.5'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 5074}, page_content='```\\n\\nWe can say a couple of things about this. Firstly, we want a managed solution to deployment, we don’t have a dedicated MLOPs team (yet), so we’re looking for a solution that helps us minimize the time we spend on deploying models, even if it costs a little more than handling the deployments ourselves.\\n\\nInference Endpoints are more expensive that what we were doing before, there’s an increased cost of between 24% and 50%. At the scale we’re currently operating, this additional cost, a difference of ~$60 a month for a large CPU instance is nothing compared to the time and cognitive load we are saving by not having to worry about APIs, and containers. If we were deploying 100s of ML microservices we would probably want to think again, but that is probably true of many approaches to hosting.\\n\\n## Some notes and caveats:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 5879}, page_content='## Some notes and caveats:\\n\\n- You can find pricing for Inference Endpoints [here](https://huggingface.co/pricing#endpoints), but a different number is displayed when you deploy a new endpoint from the [GUI](https://ui.endpoints.huggingface.co/new). I’ve used the latter, which is higher.\\n- The values that I present in the table for ECS + Fargate are an underestimate, but probably not by much. I extracted them from the [fargate pricing page](https://aws.amazon.com/fargate/pricing/) and it includes just the cost of hosting the instance. I’m not including the data ingress/egress (probably the biggest thing is downloading the model from Hugging Face hub), nor have I included the costs related to ECR.\\n\\n## Other considerations\\n\\n### Deployment Options'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 6585}, page_content='## Other considerations\\n\\n### Deployment Options\\n\\nCurrently you can deploy an Inference Endpoint from the [GUI](https://ui.endpoints.huggingface.co/new) or using a [RESTful API](https://huggingface.co/docs/inference-endpoints/api_reference). You can also make use of our command line tool [hugie](https://github.com/MantisAI/hfie) (which will be the subject of a future blog) to launch Inference Endpoints in one line of code by passing a configuration, it’s really this simple:\\n\\n```bash\\nhugie endpoint create example/development.json'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 7119}, page_content='```\\n\\nFor me, what’s lacking is a [custom terraform provider](https://www.hashicorp.com/blog/writing-custom-terraform-providers). It’s all well and good deploying an inference endpoint from a [GitHub action](https://github.com/features/actions) using hugie, as we do, but it would be better if we could use the awesome state machine that is terraform to keep track of these. I’m pretty sure that someone (if not Hugging Face) will write one soon enough — if not, we will.\\n\\n### Hosting multiple models on a single endpoint\\n\\nPhilipp Schmid posted a really nice blog about how to write a custom [Endpoint Handler](https://www.philschmid.de/multi-model-inference-endpoints) class to allow you to host multiple models on a single endpoint, potentially saving you quite a bit of money. His blog was about GPU inference, and the only real limitation is how many models you can fit into the GPU memory. I assume this will also work for CPU instances, though I’ve not tried yet.\\n\\n## To conclude…'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 8089}, page_content='## To conclude…\\n\\nWe find Hugging Face Inference Endpoints to be a very simple and convenient way to deploy transformer (and [sklearn](https://huggingface.co/scikit-learn)) models into an endpoint so they can be consumed by an application. Whilst they cost a little more than the ECS approach we were using before, it’s well worth it because it saves us time on thinking about deployment, we can concentrate on the thing we want to: building NLP solutions for our clients to help solve their problems.\\n\\n_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!_\\n\\n_This article was originally published on February 15, 2023 [in Medium](https://medium.com/mantisnlp/why-were-switching-to-hugging-face-inference-endpoints-and-maybe-you-should-too-829371dcd330)._'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 0}, page_content='--\\ntitle: ROUGE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for\\n  evaluating automatic summarization and machine translation software in natural language processing.\\n  The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\\n  \\n  Note that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\\n  \\n  This metrics is a wrapper around Google Research reimplementation of ROUGE:\\n  https://github.com/google-research/google-research/tree/master/rouge\\n---\\n\\n# Metric Card for ROUGE'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 811}, page_content='# Metric Card for ROUGE\\n\\n## Metric Description\\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\\n\\nNote that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\\n\\nThis metrics is a wrapper around the [Google Research reimplementation of ROUGE](https://github.com/google-research/google-research/tree/master/rouge)'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 1495}, page_content='## How to Use\\nAt minimum, this metric takes as input a list of predictions and a list of references:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello there\", \"general kenobi\"]\\n>>> references = [\"hello there\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references)\\n>>> print(results)\\n{\\'rouge1\\': 1.0, \\'rouge2\\': 1.0, \\'rougeL\\': 1.0, \\'rougeLsum\\': 1.0}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 1931}, page_content='```\\n\\nOne can also pass a custom tokenizer which is especially useful for non-latin languages.\\n```python\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n                            tokenizer=lambda x: x.split())\\n>>> print(results)\\n{\\'rouge1\\': 1.0, \\'rouge2\\': 1.0, \\'rougeL\\': 1.0, \\'rougeLsum\\': 1.0}\\n```\\n\\nIt can also deal with lists of references for each predictions:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello there\", \"general kenobi\"]\\n>>> references = [[\"hello\", \"there\"], [\"general kenobi\", \"general yoda\"]]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references)\\n>>> print(results)\\n{\\'rouge1\\': 0.8333, \\'rouge2\\': 0.5, \\'rougeL\\': 0.8333, \\'rougeLsum\\': 0.8333}```'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 2716}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 2725}, page_content='### Inputs\\n- **predictions** (`list`): list of predictions to score. Each prediction\\n        should be a string with tokens separated by spaces.\\n- **references** (`list` or `list[list]`): list of reference for each prediction or a list of several references per prediction. Each\\n        reference should be a string with tokens separated by spaces.\\n- **rouge_types** (`list`): A list of rouge types to calculate. Defaults to `[\\'rouge1\\', \\'rouge2\\', \\'rougeL\\', \\'rougeLsum\\']`.\\n    - Valid rouge types:\\n        - `\"rouge1\"`: unigram (1-gram) based scoring\\n        - `\"rouge2\"`: bigram (2-gram) based scoring\\n        - `\"rougeL\"`: Longest common subsequence based scoring.\\n        - `\"rougeLSum\"`: splits text using `\"\\\\n\"`\\n        - See [here](https://github.com/huggingface/datasets/issues/617) for more information\\n- **use_aggregator** (`boolean`): If True, returns aggregates. Defaults to `True`.'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 3535}, page_content='- **use_aggregator** (`boolean`): If True, returns aggregates. Defaults to `True`.\\n- **use_stemmer** (`boolean`): If `True`, uses Porter stemmer to strip word suffixes. Defaults to `False`.'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 3726}, page_content=\"### Output Values\\nThe output is a dictionary with one entry for each rouge type in the input list `rouge_types`. If `use_aggregator=False`, each dictionary entry is a list of scores, with one score for each sentence. E.g. if `rouge_types=['rouge1', 'rouge2']` and `use_aggregator=False`, the output is:\\n\\n```python\\n{'rouge1': [0.6666666666666666, 1.0], 'rouge2': [0.0, 1.0]}\"),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 4100}, page_content='```\\n\\nIf `rouge_types=[\\'rouge1\\', \\'rouge2\\']` and `use_aggregator=True`, the output is of the following format:\\n```python\\n{\\'rouge1\\': 1.0, \\'rouge2\\': 1.0}\\n```\\n\\nThe ROUGE values are in the range of 0 to 1.\\n\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nAn example without aggregation:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello goodbye\", \"ankh morpork\"]\\n>>> references = [\"goodbye\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n...                         use_aggregator=False)\\n>>> print(list(results.keys()))\\n[\\'rouge1\\', \\'rouge2\\', \\'rougeL\\', \\'rougeLsum\\']\\n>>> print(results[\"rouge1\"])\\n[0.5, 0.0]'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 4795}, page_content='```\\n\\nThe same example, but with aggregation:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello goodbye\", \"ankh morpork\"]\\n>>> references = [\"goodbye\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n...                         use_aggregator=True)\\n>>> print(list(results.keys()))\\n[\\'rouge1\\', \\'rouge2\\', \\'rougeL\\', \\'rougeLsum\\']\\n>>> print(results[\"rouge1\"])\\n0.25\\n```\\n\\nThe same example, but only calculating `rouge_1`:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello goodbye\", \"ankh morpork\"]\\n>>> references = [\"goodbye\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n...                         rouge_types=[\\'rouge_1\\'],\\n...                         use_aggregator=True)\\n>>> print(list(results.keys()))\\n[\\'rouge1\\']\\n>>> print(results[\"rouge1\"])\\n0.25'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 5729}, page_content='```\\n\\n## Limitations and Bias\\nSee [Schluter (2017)](https://aclanthology.org/E17-2007/) for an in-depth discussion of many of ROUGE\\'s limits.\\n\\n## Citation\\n```bibtex\\n@inproceedings{lin-2004-rouge,\\n    title = \"{ROUGE}: A Package for Automatic Evaluation of Summaries\",\\n    author = \"Lin, Chin-Yew\",\\n    booktitle = \"Text Summarization Branches Out\",\\n    month = jul,\\n    year = \"2004\",\\n    address = \"Barcelona, Spain\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://www.aclweb.org/anthology/W04-1013\",\\n    pages = \"74--81\",\\n}\\n```\\n\\n## Further References\\n- This metrics is a wrapper around the [Google Research reimplementation of ROUGE](https://github.com/google-research/google-research/tree/master/rouge)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# AudioLDM'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 587}, page_content='# AudioLDM\\n\\nAudioLDM was proposed in [AudioLDM: Text-to-Audio Generation with Latent Diffusion Models](https://huggingface.co/papers/2301.12503) by Haohe Liu et al. Inspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview), AudioLDM\\nis a text-to-audio _latent diffusion model (LDM)_ that learns continuous audio representations from [CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)\\nlatents. AudioLDM takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional\\nsound effects, human speech and music.\\n\\nThe abstract from the paper is:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 1232}, page_content='*Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 2136}, page_content='frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at [this https URL](https://audioldm.github.io/).*'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 2393}, page_content='The original codebase can be found at [haoheliu/AudioLDM](https://github.com/haoheliu/AudioLDM).\\n\\n## Tips\\n\\nWhen constructing a prompt, keep in mind:\\n\\n* Descriptive prompt inputs work best; you can use adjectives to describe the sound (for example, \"high quality\" or \"clear\") and make the prompt context specific (for example, \"water stream in a forest\" instead of \"stream\").\\n* It\\'s best to use general terms like \"cat\" or \"dog\" instead of specific names or abstract objects the model may not be familiar with.\\n\\nDuring inference:\\n\\n* The _quality_ of the predicted audio sample can be controlled by the `num_inference_steps` argument; higher steps give higher quality audio at the expense of slower inference.\\n* The _length_ of the predicted audio sample can be controlled by varying the `audio_length_in_s` argument.\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 3210}, page_content='<Tip>\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## AudioLDMPipeline\\n[[autodoc]] AudioLDMPipeline\\n\\t- all\\n\\t- __call__\\n\\n## AudioPipelineOutput\\n[[autodoc]] pipelines.AudioPipelineOutput'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter2/04c_character-based-tokenizers.md', 'start_index': 0}, page_content=\"efore diving in character-based tokenization, understanding why this kind of tokenization is interesting requires understanding the flaws of word-based tokenization. If you haven't seen the first video on word-based tokenization we recommend you check it out before looking at this video. Let's take a look at character-based tokenization. We now split our text into individual characters, rather than words. There are generally a lot of different words in languages, while the number of characters stays low. Here for example, for the English language that has an estimated 170,000 different words, we would need a very large vocabulary to encompass all words. With a character-based vocabulary, we can get by with only 256 characters! Even languages with a lot of different characters like the Chinese languages have dictionaries with ~20,000 different characters but more than 375,000 different words. Character-based vocabularies let us fewer different tokens than the word-based tokenization\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter2/04c_character-based-tokenizers.md', 'start_index': 898}, page_content='words. Character-based vocabularies let us fewer different tokens than the word-based tokenization dictionaries we would otherwise use. These vocabularies are also more complete than their word-based vocabularies counterparts. As our vocabulary contains all characters used in a language, even words unseen during the tokenizer training can still be tokenized, so out-of-vocabulary tokens will be less frequent. This includes the ability to correctly tokenize misspelled words, rather than discarding them as unknown straight away. However, this algorithm isn\\'t perfect either! Intuitively, characters do not hold as much information individually as a word would hold. For example, \"Let\\'s\" holds more information than \"l\". Of course, this is not true for all languages, as some languages like ideogram-based languages have a lot of information held in single characters, but for others like roman-based languages, the model will have to make sense of multiple tokens at a time to get the information'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter2/04c_character-based-tokenizers.md', 'start_index': 1801}, page_content='languages, the model will have to make sense of multiple tokens at a time to get the information held in a single word. This leads to another issue with character-based tokenizers: their sequences are translated into very large amount of tokens to be processed by the model. This can have an impact on the size of the context the model will carry around, and will reduce the size of the text we can use as input for our model. This tokenization, while it has some issues, has seen some very good results in the past and should be considered when approaching a new problem as it solves some issues encountered in the word-based algorithm.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 1}, page_content=\"Hands-on\\n\\nNow that you learned the basics of multi-agents, you're ready to train your first agents in a multi-agent system: **a 2vs2 soccer team that needs to beat the opponent team**.\\n\\nAnd you’re going to participate in AI vs. AI challenges where your trained agent will compete against other classmates’ **agents every day and be ranked on a new leaderboard.**\\n\\nTo validate this hands-on for the certification process, you just need to push a trained model. There **are no minimal results to attain to validate it.**\\n\\nFor more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 764}, page_content=\"This hands-on will be different since to get correct results **you need to train your agents from 4 hours to 8 hours**. And given the risk of timeout in Colab, we advise you to train on your computer. You don’t need a supercomputer: a simple laptop is good enough for this exercise.\\n\\nLet's get started! 🔥\\n\\n## What is AI vs. AI?\\n\\nAI vs. AI is an open-source tool we developed at Hugging Face to compete agents on the Hub against one another in a multi-agent setting. These models are then ranked in a leaderboard.\\n\\nThe idea of this tool is to have a robust evaluation tool: **by evaluating your agent with a lot of others, you’ll get a good idea of the quality of your policy.**\\n\\nMore precisely, AI vs. AI is three tools:\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 1443}, page_content='More precisely, AI vs. AI is three tools:\\n\\n- A *matchmaking process* defining the matches (which model against which) and running the model fights using a background task in the Space.\\n- A *leaderboard* getting the match history results and displaying the models’ ELO ratings: [https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos](https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos)\\n- A *Space demo* to visualize your agents playing against others: [https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos)\\n\\nIn addition to these three tools, your classmate cyllum created a 🤗 SoccerTwos Challenge Analytics where you can check the detailed match results of a model: [https://huggingface.co/spaces/cyllum/soccertwos-analytics](https://huggingface.co/spaces/cyllum/soccertwos-analytics)'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 2323}, page_content=\"We're [wrote a blog post to explain this AI vs. AI tool in detail](https://huggingface.co/blog/aivsai), but to give you the big picture it works this way:\\n\\n- Every four hours, our algorithm **fetches all the available models for a given environment (in our case ML-Agents-SoccerTwos).**\\n- It creates a **queue of matches with the matchmaking algorithm.**\\n- We simulate the match in a Unity headless process and **gather the match result** (1 if the first model won, 0.5 if it’s a draw, 0 if the second model won) in a Dataset.\\n- Then, when all matches from the matches queue are done, **we update the ELO score for each model and update the leaderboard.**\\n\\n### Competition Rules\\n\\nThis first AI vs. AI competition **is an experiment**: the goal is to improve the tool in the future with your feedback. So some **breakups can happen during the challenge**. But don't worry\\n**all the results are saved in a dataset so we can always restart the calculation correctly without losing information**.\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 3317}, page_content=\"In order for your model to get correctly evaluated against others you need to follow these rules:\\n\\n1. **You can't change the observation space or action space of the agent.** By doing that your model will not work during evaluation.\\n2. You **can't use a custom trainer for now,** you need to use the Unity MLAgents ones.\\n3. We provide executables to train your agents. You can also use the Unity Editor if you prefer **, but to avoid bugs, we advise that you use our executables**.\\n\\nWhat will make the difference during this challenge are **the hyperparameters you choose**.\\n\\nWe're constantly trying to improve our tutorials, so\\xa0**if you find some issues in this notebook**, please\\xa0[open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).\\n\\n### Chat with your classmates, share advice and ask questions on Discord\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 4089}, page_content='### Chat with your classmates, share advice and ask questions on Discord\\n\\n- We created a new channel called `ai-vs-ai-challenge` to exchange advice and ask questions.\\n- If you didn’t join the discord server yet, you can [join here](https://discord.gg/ydHrjt3WP5)\\n\\n## Step 0: Install MLAgents and download the correct executable\\n\\nWe advise you to use [conda](https://docs.conda.io/en/latest/) as a package manager and create a new environment.\\n\\nWith conda, we create a new environment called rl with **Python 3.10.12**:\\n\\n```bash\\nconda create --name rl python=3.10.12\\nconda activate rl'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 4673}, page_content='```\\n\\nTo be able to train our agents correctly and push to the Hub, we need to install ML-Agents\\n\\n```bash\\ngit clone https://github.com/Unity-Technologies/ml-agents\\n```\\n\\nWhen the cloning is done (it takes 2.63 GB), we go inside the repository and install the package\\n\\n```bash\\ncd ml-agents\\npip install -e ./ml-agents-envs\\npip install -e ./ml-agents'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 5019}, page_content='```\\n\\nFinally, you need to install git-lfs: https://git-lfs.com/\\n\\nNow that it’s installed, we need to add the environment training executable. Based on your operating system you need to download one of them, unzip it and place it in a new folder inside `ml-agents` that you call `training-envs-executables`\\n\\nAt the end your executable should be in `ml-agents/training-envs-executables/SoccerTwos`\\n\\nWindows: Download [this executable](https://drive.google.com/file/d/1sqFxbEdTMubjVktnV4C6ICjp89wLhUcP/view?usp=sharing)\\n\\nLinux (Ubuntu): Download [this executable](https://drive.google.com/file/d/1KuqBKYiXiIcU4kNMqEzhgypuFP5_45CL/view?usp=sharing)\\n\\nMac: Download [this executable](https://drive.google.com/drive/folders/1h7YB0qwjoxxghApQdEUQmk95ZwIDxrPG?usp=share_link)\\n⚠ For Mac you need also to call this `xattr -cr training-envs-executables/SoccerTwos/SoccerTwos.app` to be able to run SoccerTwos\\n\\n## Step 1: Understand the environment'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 5917}, page_content='## Step 1: Understand the environment\\n\\nThe environment is called `SoccerTwos`. The Unity MLAgents Team made it. You can find its documentation [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md#soccer-twos)\\n\\nThe goal in this environment **is to get the ball into the opponent\\'s goal while preventing the ball from entering your own goal.**\\n\\n<figure>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif\" alt=\"SoccerTwos\"/>\\n\\n<figcaption>This environment was made by the <a href=\"https://github.com/Unity-Technologies/ml-agents\"> Unity MLAgents Team</a></figcaption>\\n\\n</figure>\\n\\n### The reward function\\n\\nThe reward function is:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccerreward.png\" alt=\"SoccerTwos Reward\"/>\\n\\n### The observation space\\n\\nThe observation space is composed of vectors of size 336:'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 6818}, page_content='### The observation space\\n\\nThe observation space is composed of vectors of size 336:\\n\\n- 11 ray-casts forward distributed over 120 degrees (264 state dimensions)\\n- 3 ray-casts backward distributed over 90 degrees (72 state dimensions)\\n- Both of these ray-casts can detect 6 objects:\\n    - Ball\\n    - Blue Goal\\n    - Purple Goal\\n    - Wall\\n    - Blue Agent\\n    - Purple Agent\\n\\n### The action space\\n\\nThe action space is three discrete branches:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/socceraction.png\" alt=\"SoccerTwos Action\"/>\\n\\n## Step 2: Understand MA-POCA\\n\\nWe know how to train agents to play against others: **we can use self-play.** This is a perfect technique for a 1vs1.\\n\\nBut in our case we’re 2vs2, and each team has 2 agents. How then can we **train cooperative behavior for groups of agents?**'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 7688}, page_content='As explained in the [Unity Blog](https://blog.unity.com/technology/ml-agents-v20-release-now-supports-training-complex-cooperative-behaviors), agents typically receive a reward as a group (+1 - penalty) when the team scores a goal. This implies that **every agent on the team is rewarded even if each agent didn’t contribute the same to the win**, which makes it difficult to learn what to do independently.\\n\\nThe Unity MLAgents team developed the solution in a new multi-agent trainer called *MA-POCA (Multi-Agent POsthumous Credit Assignment)*.\\n\\nThe idea is simple but powerful: a centralized critic **processes the states of all agents in the team to estimate how well each agent is doing**. Think of this critic as a coach.\\n\\nThis allows each agent to **make decisions based only on what it perceives locally**, and **simultaneously evaluate how good its behavior is in the context of the whole group**.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 8596}, page_content='<figure>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/mapoca.png\" alt=\"MA POCA\"/>\\n\\n<figcaption>This illustrates MA-POCA’s centralized learning and decentralized execution. Source: <a href=\"https://blog.unity.com/technology/ml-agents-plays-dodgeball\">MLAgents Plays Dodgeball</a>\\n</figcaption>\\n\\n</figure>\\n\\nThe solution then is to use Self-Play with an MA-POCA trainer (called poca). The poca trainer will help us to train cooperative behavior and self-play to win against an opponent team.\\n\\nIf you want to dive deeper into this MA-POCA algorithm, you need to read the paper they published [here](https://arxiv.org/pdf/2111.05992.pdf) and the sources we put on the additional readings section.\\n\\n## Step 3: Define the config file\\n\\nWe already learned in [Unit 5](https://huggingface.co/deep-rl-course/unit5/introduction) that in ML-Agents, you define **the training hyperparameters in `config.yaml` files.**'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 9562}, page_content='There are multiple hyperparameters. To understand them better, you should read the explanations for each of them in\\xa0**[the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)**\\n\\nThe config file we’re going to use here is in  `./config/poca/SoccerTwos.yaml`. It looks like this:'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 9910}, page_content='```csharp\\nbehaviors:\\n  SoccerTwos:\\n    trainer_type: poca\\n    hyperparameters:\\n      batch_size: 2048\\n      buffer_size: 20480\\n      learning_rate: 0.0003\\n      beta: 0.005\\n      epsilon: 0.2\\n      lambd: 0.95\\n      num_epoch: 3\\n      learning_rate_schedule: constant\\n    network_settings:\\n      normalize: false\\n      hidden_units: 512\\n      num_layers: 2\\n      vis_encode_type: simple\\n    reward_signals:\\n      extrinsic:\\n        gamma: 0.99\\n        strength: 1.0\\n    keep_checkpoints: 5\\n    max_steps: 5000000\\n    time_horizon: 1000\\n    summary_freq: 10000\\n    self_play:\\n      save_steps: 50000\\n      team_change: 200000\\n      swap_steps: 2000\\n      window: 10\\n      play_against_latest_model_ratio: 0.5\\n      initial_elo: 1200.0'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 10644}, page_content='```\\n\\nCompared to Pyramids or SnowballTarget, we have new hyperparameters with a self-play part. How you modify them can be critical in getting good results.\\n\\nThe advice I can give you here is to check the explanation and recommended value for each parameters (especially self-play ones) against\\xa0**[the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md).**\\n\\nNow that you’ve modified our config file, you’re ready to train your agents.\\n\\n## Step 4: Start the training\\n\\nTo train the agents, we need to\\xa0**launch mlagents-learn and select the executable containing the environment.**\\n\\nWe define four parameters:\\n\\n1. `mlagents-learn <config>`: the path where the hyperparameter config file is.\\n2. `-env`: where the environment executable is.\\n3. `-run_id`: the name you want to give to your training run id.\\n4. `-no-graphics`: to not launch the visualization during the training.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 11587}, page_content='Depending on your hardware, 5M timesteps (the recommended value, but you can also try 10M) will take 5 to 8 hours of training. You can continue using your computer in the meantime, but I advise deactivating the computer standby mode to prevent the training from being stopped.\\n\\nDepending on the executable you use (windows, ubuntu, mac) the training command will look like this (your executable path can be different so don’t hesitate to check before running).\\n\\n```bash\\nmlagents-learn ./config/poca/SoccerTwos.yaml --env=./training-envs-executables/SoccerTwos.exe --run-id=\"SoccerTwos\" --no-graphics'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 12187}, page_content='```\\n\\nThe executable contains 8 copies of SoccerTwos.\\n\\n⚠️ It’s normal if you don’t see a big increase of ELO score (and even a decrease below 1200) before 2M timesteps, since your agents will spend most of their time moving randomly on the field before being able to goal.\\n\\n⚠️ You can stop the training with Ctrl + C but beware of typing this command only once to stop the training since MLAgents needs to generate a final .onnx file before closing the run.\\n\\n## Step 5: **Push the agent to the Hugging Face Hub**\\n\\nNow that we trained our agents, we’re\\xa0**ready to push them to the Hub to be able to participate in the AI vs. AI challenge and visualize them playing on your browser🔥.**\\n\\nTo be able to share your model with the community, there are three more steps to follow:\\n\\n1️⃣ (If it’s not already done) create an account to HF ➡\\xa0[https://huggingface.co/join](https://huggingface.co/join)\\n\\n2️⃣ Sign in and store your authentication token from the Hugging Face website.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 13078}, page_content='2️⃣ Sign in and store your authentication token from the Hugging Face website.\\n\\nCreate a new token (https://huggingface.co/settings/tokens)\\xa0**with write role**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\\n\\nCopy the token, run this, and paste the token\\n\\n```bash\\nhuggingface-cli login'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 13467}, page_content='```\\n\\nThen, we need to run `mlagents-push-to-hf`.\\n\\nAnd we define four parameters:\\n\\n1. `-run-id`: the name of the training run id.\\n2. `-local-dir`: where the agent was saved, it’s results/<run_id name>, so in my case results/First Training.\\n3. `-repo-id`: the name of the Hugging Face repo you want to create or update. It’s always <your huggingface username>/<the repo name>\\nIf the repo does not exist **it will be created automatically**\\n4. `--commit-message`: since HF repos are git repositories you need to give a commit message.\\n\\nIn my case\\n\\n```bash\\nmlagents-push-to-hf  --run-id=\"SoccerTwos\" --local-dir=\"./results/SoccerTwos\" --repo-id=\"ThomasSimonini/poca-SoccerTwos\" --commit-message=\"First Push\"`\\n```\\n\\n```bash\\nmlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id --commit-message=\"First Push\"'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 14322}, page_content=\"```\\n\\nIf everything worked you should see this at the end of the process (but with a different url 😆) :\\n\\nYour model is pushed to the Hub. You can view your model here: https://huggingface.co/ThomasSimonini/poca-SoccerTwos\\n\\nIt's the link to your model. It contains a model card that explains how to use it, your Tensorboard, and your config file. **What's awesome is that it's a git repository, which means you can have different commits, update your repository with a new push, etc.**\\n\\n## Step 6: Verify that your model is ready for AI vs AI Challenge\\n\\nNow that your model is pushed to the Hub, **it’s going to be added automatically to the AI vs AI Challenge model pool.** It can take a little bit of time before your model is added to the leaderboard given we do a run of matches every 4h.\\n\\nBut to ensure that everything works perfectly you need to check:\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 15114}, page_content='But to ensure that everything works perfectly you need to check:\\n\\n1. That you have this tag in your model: ML-Agents-SoccerTwos. This is the tag we use to select models to be added to the challenge pool. To do that go to your model and check the tags\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify1.png\" alt=\"Verify\"/>\\n\\n\\nIf it’s not the case you just need to modify the readme and add it\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify2.png\" alt=\"Verify\"/>\\n\\n2. That you have a `SoccerTwos.onnx` file\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify3.png\" alt=\"Verify\"/>\\n\\nWe strongly suggest that you create a new model when you push to the Hub if you want to train it again or train a new version.\\n\\n## Step 7: Visualize some match in our demo'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 16011}, page_content=\"## Step 7: Visualize some match in our demo\\n\\nNow that your model is part of AI vs AI Challenge, **you can visualize how good it is compared to others**: https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos\\n\\nIn order to do that, you just need to go to this demo:\\n\\n- Select your model as team blue (or team purple if you prefer) and another model to compete against. The best opponents to compare your model to are either whoever is on top of the leaderboard or the [baseline model](https://huggingface.co/unity/MLAgents-SoccerTwos)\\n\\nThe matches you see live are not used in the calculation of your result **but they are a good way to visualize how good your agent is**.\\n\\nAnd don't hesitate to share the best score your agent gets on discord in the #rl-i-made-this channel 🔥\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/sales_projections/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: sales_projections\\n\\n\\n```\\n!pip install -q gradio pandas numpy matplotlib\\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/sales_projections/run.ipynb', 'start_index': 91}, page_content='```\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nimport gradio as gr\\n\\n\\ndef sales_projections(employee_data):\\n    sales_data = employee_data.iloc[:, 1:4].astype(\"int\").to_numpy()\\n    regression_values = np.apply_along_axis(\\n        lambda row: np.array(np.poly1d(np.polyfit([0, 1, 2], row, 2))), 0, sales_data\\n    )\\n    projected_months = np.repeat(\\n        np.expand_dims(np.arange(3, 12), 0), len(sales_data), axis=0\\n    )\\n    projected_values = np.array(\\n        [\\n            month * month * regression[0] + month * regression[1] + regression[2]\\n            for month, regression in zip(projected_months, regression_values)\\n        ]\\n    )\\n    plt.plot(projected_values.T)\\n    plt.legend(employee_data[\"Name\"])\\n    return employee_data, plt.gcf(), regression_values'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/sales_projections/run.ipynb', 'start_index': 869}, page_content='demo = gr.Interface(\\n    sales_projections,\\n    gr.Dataframe(\\n        headers=[\"Name\", \"Jan Sales\", \"Feb Sales\", \"Mar Sales\"],\\n        value=[[\"Jon\", 12, 14, 18], [\"Alice\", 14, 17, 2], [\"Sana\", 8, 9.5, 12]],\\n    ),\\n    [\"dataframe\", \"plot\", \"numpy\"],\\n    description=\"Enter sales figures for employees to predict sales trajectory over year.\",\\n)\\nif __name__ == \"__main__\":\\n    demo.launch()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/sales_projections/run.ipynb', 'start_index': 1260}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 1}, page_content='Metric Card for F1\\n\\n\\n## Metric Description\\n\\nThe F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\\nF1 = 2 * (precision * recall) / (precision + recall)\\n\\n\\n## How to Use\\n\\nAt minimum, this metric requires predictions and references as input\\n\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(predictions=[0, 1], references=[0, 1])\\n>>> print(results)\\n[\"{\\'f1\\': 1.0}\"]'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 445}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 451}, page_content=\"### Inputs\\n- **predictions** (`list` of `int`): Predicted labels.\\n- **references** (`list` of `int`): Ground truth labels.\\n- **labels** (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`, and the order of the labels if `average` is `None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\\n- **pos_label** (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 1220}, page_content=\"- **average** (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\\n    - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\\n    - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\\n    - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\\n    - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. This option can result in an F-score that is not between precision and recall.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 2155}, page_content=\"- 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\\n- **sample_weight** (`list` of `float`): Sample weights Defaults to None.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 2353}, page_content=\"### Output Values\\n- **f1**(`float` or `array` of `float`): F1 score or list of f1 scores, depending on the value passed to `average`. Minimum possible value is 0. Maximum possible value is 1. Higher f1 scores are better.\\n\\nOutput Example(s):\\n```python\\n{'f1': 0.26666666666666666}\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 2632}, page_content='```\\n```python\\n{\\'f1\\': array([0.8, 0.0, 0.0])}\\n```\\n\\nThis metric outputs a dictionary, with either a single f1 score, of type `float`, or an array of f1 scores, with entries of type `float`.\\n\\n\\n#### Values from Popular Papers\\n\\n\\n\\n\\n### Examples\\n\\nExample 1-A simple binary example\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])\\n>>> print(results)\\n{\\'f1\\': 0.5}\\n```\\n\\nExample 2-The same simple binary example as in Example 1, but with `pos_label` set to `0`.\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)\\n>>> print(round(results[\\'f1\\'], 2))\\n0.67'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 3370}, page_content='```\\n\\nExample 3-The same simple binary example as in Example 1, but with `sample_weight` included.\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])\\n>>> print(round(results[\\'f1\\'], 2))\\n0.35'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 3691}, page_content='```\\n\\nExample 4-A multiclass example, with different values for the `average` input.\\n```python\\n>>> predictions = [0, 2, 1, 0, 0, 1]\\n>>> references = [0, 1, 2, 0, 1, 2]\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=\"macro\")\\n>>> print(round(results[\\'f1\\'], 2))\\n0.27\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=\"micro\")\\n>>> print(round(results[\\'f1\\'], 2))\\n0.33\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\\n>>> print(round(results[\\'f1\\'], 2))\\n0.27\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=None)\\n>>> print(results)\\n{\\'f1\\': array([0.8, 0. , 0. ])}'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 4416}, page_content='```\\n\\n\\n## Limitations and Bias\\n\\n\\n\\n## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n    title={Scikit-learn: Machine Learning in {P}ython},\\n    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n           and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n           and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n           Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n    journal={Journal of Machine Learning Research},\\n    volume={12},\\n    pages={2825--2830},\\n    year={2011}\\n}\\n```\\n\\n\\n## Further References'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# TimeSformer\\n\\n## Overview'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 746}, page_content='-->\\n\\n# TimeSformer\\n\\n## Overview\\n\\nThe TimeSformer model was proposed in [TimeSformer: Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Facebook Research.\\nThis work is a milestone in action-recognition field being the first video transformer. It inspired many transformer based video understanding and classification papers.\\n\\nThe abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 1168}, page_content='*We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named \"TimeSformer,\" adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that \"divided attention,\" where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 2069}, page_content='(at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: [this https URL](https://github.com/facebookresearch/TimeSformer).*'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 2281}, page_content='This model was contributed by [fcakyon](https://huggingface.co/fcakyon).\\nThe original code can be found [here](https://github.com/facebookresearch/TimeSformer).\\n\\n## Usage tips\\n\\nThere are many pretrained variants. Select your pretrained model based on the dataset it is trained on. Moreover,\\nthe number of input frames per clip changes based on the model size so you should consider this parameter while selecting your pretrained model.\\n\\n## Resources\\n\\n- [Video classification task guide](../tasks/video_classification)\\n\\n## TimesformerConfig\\n\\n[[autodoc]] TimesformerConfig\\n\\n## TimesformerModel\\n\\n[[autodoc]] TimesformerModel\\n    - forward\\n\\n## TimesformerForVideoClassification\\n\\n[[autodoc]] TimesformerForVideoClassification\\n    - forward'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Swin Transformer V2\\n\\n## Overview'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 746}, page_content='-->\\n\\n# Swin Transformer V2\\n\\n## Overview\\n\\nThe Swin Transformer V2 model was proposed in [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\\n\\nThe abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 1106}, page_content='*Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 2003}, page_content=\"successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time.*\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 2583}, page_content='This model was contributed by [nandwalritik](https://huggingface.co/nandwalritik).\\nThe original code can be found [here](https://github.com/microsoft/Swin-Transformer).\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Swin Transformer v2.\\n\\n<PipelineTag pipeline=\"image-classification\"/>\\n\\n- [`Swinv2ForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\\n- See also: [Image classification task guide](../tasks/image_classification)\\n\\nBesides that:\\n\\n- [`Swinv2ForMaskedImageModeling`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 3489}, page_content=\"If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n## Swinv2Config\\n\\n[[autodoc]] Swinv2Config\\n\\n## Swinv2Model\\n\\n[[autodoc]] Swinv2Model\\n    - forward\\n\\n## Swinv2ForMaskedImageModeling\\n\\n[[autodoc]] Swinv2ForMaskedImageModeling\\n    - forward\\n\\n## Swinv2ForImageClassification\\n\\n[[autodoc]] transformers.Swinv2ForImageClassification\\n    - forward\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# RemBERT\\n\\n## Overview\\n\\nThe RemBERT model was proposed in [Rethinking Embedding Coupling in Pre-trained Language Models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, Sebastian Ruder.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 988}, page_content='The abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 1035}, page_content=\"*We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art\\npre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to\\nsignificantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By\\nreallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on\\nstandard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that\\nallocating additional capacity to the output embedding provides benefits to the model that persist through the\\nfine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger\\noutput embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 1950}, page_content='Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\\nfindings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\\nnumber of parameters at the fine-tuning stage.*'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 2232}, page_content='## Usage tips\\n\\nFor fine-tuning, RemBERT can be thought of as a bigger version of mBERT with an ALBERT-like factorization of the\\nembedding layer. The embeddings are not tied in pre-training, in contrast with BERT, which enables smaller input\\nembeddings (preserved during fine-tuning) and bigger output embeddings (discarded at fine-tuning). The tokenizer is\\nalso similar to the Albert one rather than the BERT one.\\n\\n## Resources\\n\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\\n- [Multiple choice task guide](../tasks/multiple_choice)\\n\\n## RemBertConfig\\n\\n[[autodoc]] RemBertConfig\\n\\n## RemBertTokenizer'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 3061}, page_content='## RemBertConfig\\n\\n[[autodoc]] RemBertConfig\\n\\n## RemBertTokenizer\\n\\n[[autodoc]] RemBertTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## RemBertTokenizerFast\\n\\n[[autodoc]] RemBertTokenizerFast\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n<frameworkcontent>\\n<pt>\\n\\n## RemBertModel\\n\\n[[autodoc]] RemBertModel\\n    - forward\\n\\n## RemBertForCausalLM\\n\\n[[autodoc]] RemBertForCausalLM\\n    - forward\\n\\n## RemBertForMaskedLM\\n\\n[[autodoc]] RemBertForMaskedLM\\n    - forward\\n\\n## RemBertForSequenceClassification\\n\\n[[autodoc]] RemBertForSequenceClassification\\n    - forward\\n\\n## RemBertForMultipleChoice\\n\\n[[autodoc]] RemBertForMultipleChoice\\n    - forward\\n\\n## RemBertForTokenClassification\\n\\n[[autodoc]] RemBertForTokenClassification\\n    - forward\\n\\n## RemBertForQuestionAnswering\\n\\n[[autodoc]] RemBertForQuestionAnswering\\n    - forward'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 3973}, page_content='## RemBertForQuestionAnswering\\n\\n[[autodoc]] RemBertForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFRemBertModel\\n\\n[[autodoc]] TFRemBertModel\\n    - call\\n\\n## TFRemBertForMaskedLM\\n\\n[[autodoc]] TFRemBertForMaskedLM\\n    - call\\n\\n## TFRemBertForCausalLM\\n\\n[[autodoc]] TFRemBertForCausalLM\\n    - call\\n\\n## TFRemBertForSequenceClassification\\n\\n[[autodoc]] TFRemBertForSequenceClassification\\n    - call\\n\\n## TFRemBertForMultipleChoice\\n\\n[[autodoc]] TFRemBertForMultipleChoice\\n    - call\\n\\n## TFRemBertForTokenClassification\\n\\n[[autodoc]] TFRemBertForTokenClassification\\n    - call\\n\\n## TFRemBertForQuestionAnswering\\n\\n[[autodoc]] TFRemBertForQuestionAnswering\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n[[open-in-colab]]\\n\\n# Performing inference with LCM-LoRA\\n\\nLatent Consistency Models (LCM) enable quality image generation in typically 2-4 steps making it possible to use diffusion models in almost real-time settings. \\n\\nFrom the [official website](https://latent-consistency-models.github.io/):'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 806}, page_content='From the [official website](https://latent-consistency-models.github.io/):\\n\\n> LCMs can be distilled from any pre-trained Stable Diffusion (SD) in only 4,000 training steps (~32 A100 GPU Hours) for generating high quality 768 x 768 resolution images in 2~4 steps or even one step, significantly accelerating text-to-image generation. We employ LCM to distill the Dreamshaper-V7 version of SD in just 4,000 training iterations.\\n\\nFor a more technical overview of LCMs, refer to [the paper](https://huggingface.co/papers/2310.04378).'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 1337}, page_content=\"However, each model needs to be distilled separately for latent consistency distillation. The core idea with LCM-LoRA is to train just a few adapter layers, the adapter being LoRA in this case. \\nThis way, we don't have to train the full model and keep the number of trainable parameters manageable. The resulting LoRAs can then be applied to any fine-tuned version of the model without distilling them separately.\\nAdditionally, the LoRAs can be applied to image-to-image, ControlNet/T2I-Adapter, inpainting, AnimateDiff etc. \\nThe LCM-LoRA can also be combined with other LoRAs to generate styled images in very few steps (4-8).\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 1966}, page_content=\"LCM-LoRAs are available for [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5), [stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), and the [SSD-1B](https://huggingface.co/segmind/SSD-1B) model. All the checkpoints can be found in this [collection](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6).\\n\\nFor more details about LCM-LoRA, refer to [the technical report](https://huggingface.co/papers/2311.05556).\\n\\nThis guide shows how to perform inference with LCM-LoRAs for \\n- text-to-image\\n- image-to-image\\n- combined with styled LoRAs\\n- ControlNet/T2I-Adapter\\n- inpainting\\n- AnimateDiff\\n\\nBefore going through this guide, we'll take a look at the general workflow for performing inference with LCM-LoRAs.\\nLCM-LoRAs are similar to other Stable Diffusion LoRAs so they can be used with any [`DiffusionPipeline`] that supports LoRAs.\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 2931}, page_content=\"- Load the task specific pipeline and model.\\n- Set the scheduler to [`LCMScheduler`].\\n- Load the LCM-LoRA weights for the model.\\n- Reduce the `guidance_scale` between `[1.0, 2.0]` and set the `num_inference_steps` between [4, 8].\\n- Perform inference with the pipeline with the usual parameters.\\n\\nLet's look at how we can perform inference with LCM-LoRAs for different tasks.\\n\\nFirst, make sure you have [peft](https://github.com/huggingface/peft) installed, for better LoRA support.\\n\\n```bash\\npip install -U peft\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 3442}, page_content='```\\n\\n## Text-to-image\\n\\nYou\\'ll use the [`StableDiffusionXLPipeline`] with the scheduler: [`LCMScheduler`] and then load the LCM-LoRA. Together with the LCM-LoRA and the scheduler, the pipeline enables a fast inference workflow overcoming the slow iterative nature of diffusion models.\\n\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\\n    variant=\"fp16\",\\n    torch_dtype=torch.float16\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\\n\\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\\n\\ngenerator = torch.manual_seed(42)\\nimage = pipe(\\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\\n).images[0]'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 4339}, page_content=\"```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2i.png)\\n\\nNotice that we use only 4 steps for generation which is way less than what's typically used for standard SDXL.\\n\\n<Tip>\\n\\nYou may have noticed that we set `guidance_scale=1.0`, which disables classifer-free-guidance. This is because the LCM-LoRA is trained with guidance, so the batch size does not have to be doubled in this case. This leads to a faster inference time, with the drawback that negative prompts don't have any effect on the denoising process.\\n\\nYou can also use guidance with LCM-LoRA, but due to the nature of training the model is very sensitve to the `guidance_scale` values, high values can lead to artifacts in the generated images. In our experiments, we found that the best values are in the range of [1.0, 2.0].\\n\\n</Tip>\\n\\n### Inference with a fine-tuned model\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 5192}, page_content='</Tip>\\n\\n### Inference with a fine-tuned model\\n\\nAs mentioned above, the LCM-LoRA can be applied to any fine-tuned version of the model without having to distill them separately. Let\\'s look at how we can perform inference with a fine-tuned model. In this example, we\\'ll use the [animagine-xl](https://huggingface.co/Linaqruf/animagine-xl) model, which is a fine-tuned version of the SDXL model for generating anime.\\n\\n```python\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretrained(\\n    \"Linaqruf/animagine-xl\",\\n    variant=\"fp16\",\\n    torch_dtype=torch.float16\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\\n\\nprompt = \"face focus, cute, masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, night, turtleneck\"'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 6117}, page_content='generator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\\n).images[0]'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 6258}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2i_finetuned.png)\\n\\n\\n## Image-to-image\\n\\nLCM-LoRA can be applied to image-to-image tasks too. Let\\'s look at how we can perform image-to-image generation with LCMs. For this example we\\'ll use the [dreamshaper-7](https://huggingface.co/Lykon/dreamshaper-7) model and the LCM-LoRA for `stable-diffusion-v1-5 `.\\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image, LCMScheduler\\nfrom diffusers.utils import make_image_grid, load_image\\n\\npipe = AutoPipelineForImage2Image.from_pretrained(\\n    \"Lykon/dreamshaper-7\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 7044}, page_content='# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\\n\\n# prepare image\\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\\ninit_image = load_image(url)\\nprompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\\n\\n# pass prompt and image to pipeline\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt,\\n    image=init_image,\\n    num_inference_steps=4,\\n    guidance_scale=1,\\n    strength=0.6,\\n    generator=generator\\n).images[0]\\nmake_image_grid([init_image, image], rows=1, cols=2)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 7636}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_i2i.png)\\n\\n\\n<Tip>\\n\\nYou can get different results based on your prompt and the image you provide. To get the best results, we recommend trying different values for `num_inference_steps`, `strength`, and `guidance_scale` parameters and choose the best one.\\n\\n</Tip>\\n\\n\\n## Combine with styled LoRAs\\n\\nLCM-LoRA can be combined with other LoRAs to generate styled-images in very few steps (4-8). In the following example, we\\'ll use the LCM-LoRA with the [papercut LoRA](TheLastBen/Papercut_SDXL). \\nTo learn more about how to combine LoRAs, refer to [this guide](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#combine-multiple-adapters).\\n\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\\n    variant=\"fp16\",\\n    torch_dtype=torch.float16\\n).to(\"cuda\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 8635}, page_content='# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LoRAs\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\", adapter_name=\"lcm\")\\npipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\\n\\n# Combine LoRAs\\npipe.set_adapters([\"lcm\", \"papercut\"], adapter_weights=[1.0, 0.8])\\n\\nprompt = \"papercut, a cute fox\"\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]\\nimage'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 9170}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdx_lora_mix.png)\\n\\n\\n## ControlNet/T2I-Adapter\\n\\nLet\\'s look at how we can perform inference with ControlNet/T2I-Adapter and LCM-LoRA. \\n\\n### ControlNet\\nFor this example, we\\'ll use the SD-v1-5 model and the LCM-LoRA for SD-v1-5 with canny ControlNet.\\n\\n```python\\nimport torch\\nimport cv2\\nimport numpy as np\\nfrom PIL import Image\\n\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\\nfrom diffusers.utils import load_image\\n\\nimage = load_image(\\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\\n).resize((512, 512))\\n\\nimage = np.array(image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 10118}, page_content='controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    controlnet=controlnet,\\n    torch_dtype=torch.float16,\\n    safety_checker=None,\\n    variant=\"fp16\"\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\\n\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    \"the mona lisa\",\\n    image=canny_image,\\n    num_inference_steps=4,\\n    guidance_scale=1.5,\\n    controlnet_conditioning_scale=0.8,\\n    cross_attention_kwargs={\"scale\": 1},\\n    generator=generator,\\n).images[0]\\nmake_image_grid([canny_image, image], rows=1, cols=2)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 10909}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_controlnet.png)\\n\\n\\n<Tip>\\nThe inference parameters in this example might not work for all examples, so we recommend you to try different values for `num_inference_steps`, `guidance_scale`, `controlnet_conditioning_scale` and `cross_attention_kwargs` parameters and choose the best one. \\n</Tip>\\n\\n### T2I-Adapter\\n\\nThis example shows how to use the LCM-LoRA with the [Canny T2I-Adapter](TencentARC/t2i-adapter-canny-sdxl-1.0) and SDXL.\\n\\n```python\\nimport torch\\nimport cv2\\nimport numpy as np\\nfrom PIL import Image\\n\\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, LCMScheduler\\nfrom diffusers.utils import load_image, make_image_grid\\n\\n# Prepare image\\n# Detect the canny map in low resolution to avoid high-frequency details\\nimage = load_image(\\n    \"https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg\"\\n).resize((384, 384))'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 11887}, page_content='image = np.array(image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image).resize((1024, 1024))\\n\\n# load adapter\\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\\n\\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\", \\n    adapter=adapter,\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\", \\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\\n\\nprompt = \"Mystical fairy in real, magic, 4k picture, high quality\"\\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 12844}, page_content='generator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt,\\n    negative_prompt=negative_prompt,\\n    image=canny_image,\\n    num_inference_steps=4,\\n    guidance_scale=1.5, \\n    adapter_conditioning_scale=0.8, \\n    adapter_conditioning_factor=1,\\n    generator=generator,\\n).images[0]\\nmake_image_grid([canny_image, image], rows=1, cols=2)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 13185}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2iadapter.png)\\n\\n\\n## Inpainting\\n\\nLCM-LoRA can be used for inpainting as well. \\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\\nfrom diffusers.utils import load_image, make_image_grid\\n\\npipe = AutoPipelineForInpainting.from_pretrained(\\n    \"runwayml/stable-diffusion-inpainting\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\\n\\n# load base and mask image\\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 14123}, page_content='# generator = torch.Generator(\"cuda\").manual_seed(92)\\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt,\\n    image=init_image,\\n    mask_image=mask_image,\\n    generator=generator,\\n    num_inference_steps=4,\\n    guidance_scale=4, \\n).images[0]\\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 14555}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_inpainting.png)\\n\\n\\n## AnimateDiff\\n\\n[`AnimateDiff`] allows you to animate images using Stable Diffusion models. To get good results, we need to generate multiple frames (16-24), and doing this with standard SD models can be very slow. \\nLCM-LoRA can be used to speed up the process significantly, as you just need to do 4-8 steps for each frame. Let\\'s look at how we can perform animation with LCM-LoRA and AnimateDiff.\\n\\n```python\\nimport torch\\nfrom diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler\\nfrom diffusers.utils import export_to_gif\\n\\nadapter = MotionAdapter.from_pretrained(\"diffusers/animatediff-motion-adapter-v1-5\")\\npipe = AnimateDiffPipeline.from_pretrained(\\n    \"frankjoshua/toonyou_beta6\",\\n    motion_adapter=adapter,\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 15441}, page_content='# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\", adapter_name=\"lcm\")\\npipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", weight_name=\"diffusion_pytorch_model.safetensors\", adapter_name=\"motion-lora\")\\n\\npipe.set_adapters([\"lcm\", \"motion-lora\"], adapter_weights=[0.55, 1.2])\\n\\nprompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\\ngenerator = torch.manual_seed(0)\\nframes = pipe(\\n    prompt=prompt,\\n    num_inference_steps=5,\\n    guidance_scale=1.25,\\n    cross_attention_kwargs={\"scale\": 1},\\n    num_frames=24,\\n    generator=generator\\n).frames[0]\\nexport_to_gif(frames, \"animation.gif\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 16206}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_animatediff.gif)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 1}, page_content=\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https://arxiv.org/abs/2310.04378) is a method to distill a latent diffusion model to enable swift inference with minimal steps. This example demonstrates how to use latent consistency distillation to distill SDXL for inference with few timesteps.\\n\\n## Full model distillation\\n\\n### Running locally with PyTorch\\n\\n#### Installing the dependencies\\n\\nBefore running the scripts, make sure to install the library's training dependencies:\\n\\n**Important**\\n\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install -e .\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 939}, page_content=\"```\\n\\nThen cd in the example folder and run\\n```bash\\npip install -r requirements.txt\\n```\\n\\nAnd initialize an [🤗 Accelerate](https://github.com/huggingface/accelerate/) environment with:\\n\\n```bash\\naccelerate config\\n```\\n\\nOr for a default accelerate configuration without answering questions about your environment\\n\\n```bash\\naccelerate config default\\n```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom accelerate.utils import write_basic_config\\nwrite_basic_config()\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 1443}, page_content='```\\n\\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramatic speedups.\\n\\n\\n#### Example\\n\\nThe following uses the [Conceptual Captions 12M (CC12M) dataset](https://github.com/google-research-datasets/conceptual-12m) as an example, and for illustrative purposes only. For best results you may consider large and high-quality text-image datasets such as [LAION](https://laion.ai/blog/laion-400-open-dataset/). You may also need to search the hyperparameter space according to the dataset you use.\\n\\n```bash\\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\\nexport OUTPUT_DIR=\"path/to/saved/model\"'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 2086}, page_content='accelerate launch train_lcm_distill_sdxl_wds.py \\\\\\n    --pretrained_teacher_model=$MODEL_NAME \\\\\\n    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\\\\n    --output_dir=$OUTPUT_DIR \\\\\\n    --mixed_precision=fp16 \\\\\\n    --resolution=1024 \\\\\\n    --learning_rate=1e-6 --loss_type=\"huber\" --use_fix_crop_and_size --ema_decay=0.95 --adam_weight_decay=0.0 \\\\\\n    --max_train_steps=1000 \\\\\\n    --max_train_samples=4000000 \\\\\\n    --dataloader_num_workers=8 \\\\\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\\\\n    --validation_steps=200 \\\\\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\\\\n    --train_batch_size=12 \\\\\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\\\\n    --gradient_accumulation_steps=1 \\\\\\n    --use_8bit_adam \\\\\\n    --resume_from_checkpoint=latest \\\\\\n    --report_to=wandb \\\\\\n    --seed=453645634 \\\\\\n    --push_to_hub \\\\'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 3079}, page_content='```\\n\\n## LCM-LoRA\\n\\nInstead of fine-tuning the full model, we can also just train a LoRA that can be injected into any SDXL model.\\n\\n### Example\\n\\nThe following uses the [Conceptual Captions 12M (CC12M) dataset](https://github.com/google-research-datasets/conceptual-12m) as an example. For best results you may consider large and high-quality text-image datasets such as [LAION](https://laion.ai/blog/laion-400-open-dataset/).\\n\\n```bash\\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\\nexport OUTPUT_DIR=\"path/to/saved/model\"'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 3614}, page_content='accelerate launch train_lcm_distill_lora_sdxl_wds.py \\\\\\n    --pretrained_teacher_model=$MODEL_DIR \\\\\\n    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\\\\n    --output_dir=$OUTPUT_DIR \\\\\\n    --mixed_precision=fp16 \\\\\\n    --resolution=1024 \\\\\\n    --lora_rank=64 \\\\\\n    --learning_rate=1e-6 --loss_type=\"huber\" --use_fix_crop_and_size --adam_weight_decay=0.0 \\\\\\n    --max_train_steps=1000 \\\\\\n    --max_train_samples=4000000 \\\\\\n    --dataloader_num_workers=8 \\\\\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\\\\n    --validation_steps=200 \\\\\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\\\\n    --train_batch_size=12 \\\\\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\\\\n    --gradient_accumulation_steps=1 \\\\\\n    --use_8bit_adam \\\\\\n    --resume_from_checkpoint=latest \\\\\\n    --report_to=wandb \\\\\\n    --seed=453645634 \\\\'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 4514}, page_content='--resume_from_checkpoint=latest \\\\\\n    --report_to=wandb \\\\\\n    --seed=453645634 \\\\\\n    --push_to_hub \\\\\\n```'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Autoformer\\n\\n## Overview\\n\\nThe Autoformer model was proposed in [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 1000}, page_content='This model augments the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.\\n\\nThe abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 1222}, page_content='*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 2122}, page_content='progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.*'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 2696}, page_content=\"This model was contributed by [elisim](https://huggingface.co/elisim) and [kashif](https://huggingface.co/kashif).\\nThe original code can be found [here](https://github.com/thuml/Autoformer).\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)](https://huggingface.co/blog/autoformer)\\n\\n## AutoformerConfig\\n\\n[[autodoc]] AutoformerConfig\\n\\n## AutoformerModel\\n\\n[[autodoc]] AutoformerModel\\n    - forward\\n\\n## AutoformerForPrediction\\n\\n[[autodoc]] AutoformerForPrediction\\n    - forward\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 0}, page_content='--\\ntitle: \"DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub\" \\nthumbnail: /blog/assets/hub_duckdb/hub_duckdb.png\\nauthors:\\n- user: stevhliu\\n- user: lhoestq\\n- user: severo\\n---\\n\\n# DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub\\n\\n\\nThe Hugging Face Hub is dedicated to providing open access to datasets for everyone and giving users the tools to explore and understand them. You can find many of the datasets used to train popular large language models (LLMs) like [Falcon](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [MPT](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf), and [StarCoder](https://huggingface.co/datasets/bigcode/the-stack). There are tools for addressing fairness and bias in datasets like [Disaggregators](https://huggingface.co/spaces/society-ethics/disaggregators), and tools for previewing examples inside a dataset like the Dataset Viewer.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 988}, page_content='<div class=\"flex justify-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets-server/oasst1_light.png\"/>\\n</div>\\n<small>A preview of the OpenAssistant dataset with the Dataset Viewer.</small>\\n\\nWe are happy to share that we recently added another feature to help you analyze datasets on the Hub; you can run SQL queries with DuckDB on any dataset stored on the Hub! According to the 2022 [StackOverflow Developer Survey](https://survey.stackoverflow.co/2022/#section-most-popular-technologies-programming-scripting-and-markup-languages), SQL is the 3rd most popular programming language. We also wanted a fast database management system (DBMS) designed for running analytical queries, which is why we’re excited about integrating with [DuckDB](https://duckdb.org/). We hope this allows even more users to access and analyze datasets on the Hub!\\n\\n## TLDR'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 1887}, page_content='## TLDR\\n\\n[Datasets Server](https://huggingface.co/docs/datasets-server/index) **automatically converts all public datasets on the Hub to Parquet files**, that you can see by clicking on the \"Auto-converted to Parquet\" button at the top of a dataset page. You can also access the list of the Parquet files URLs with a simple HTTP call.\\n\\n```py\\nr = requests.get(\"https://datasets-server.huggingface.co/parquet?dataset=blog_authorship_corpus\")\\nj = r.json()\\nurls = [f[\\'url\\'] for f in j[\\'parquet_files\\'] if f[\\'split\\'] == \\'train\\']\\nurls\\n[\\'https://huggingface.co/datasets/blog_authorship_corpus/resolve/refs%2Fconvert%2Fparquet/blog_authorship_corpus/blog_authorship_corpus-train-00000-of-00002.parquet\\',\\n \\'https://huggingface.co/datasets/blog_authorship_corpus/resolve/refs%2Fconvert%2Fparquet/blog_authorship_corpus/blog_authorship_corpus-train-00001-of-00002.parquet\\']'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 2750}, page_content='```\\n\\nCreate a connection to DuckDB and install and load the `httpfs` extension to allow reading and writing remote files:\\n\\n```py\\nimport duckdb\\n\\nurl = \"https://huggingface.co/datasets/blog_authorship_corpus/resolve/refs%2Fconvert%2Fparquet/blog_authorship_corpus/blog_authorship_corpus-train-00000-of-00002.parquet\"\\n\\ncon = duckdb.connect()\\ncon.execute(\"INSTALL httpfs;\")\\ncon.execute(\"LOAD httpfs;\")\\n```\\n\\nOnce you’re connected, you can start writing SQL queries!\\n\\n```sql\\ncon.sql(f\"\"\"SELECT horoscope, \\n\\tcount(*), \\n\\tAVG(LENGTH(text)) AS avg_blog_length \\n\\tFROM \\'{url}\\' \\n\\tGROUP BY horoscope \\n\\tORDER BY avg_blog_length \\n\\tDESC LIMIT(5)\"\"\"\\n)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 3384}, page_content=\"```\\n\\nTo learn more, check out the [documentation](https://huggingface.co/docs/datasets-server/parquet_process).\\n\\n## From dataset to Parquet\\n\\n[Parquet](https://parquet.apache.org/docs/) files are columnar, making them more efficient to store, load and analyze. This is especially important when you're working with large datasets, which we’re seeing more and more of in the LLM era. To support this, Datasets Server automatically converts and publishes any public dataset on the Hub as Parquet files. The URL to the Parquet files can be retrieved with the [`/parquet`](https://huggingface.co/docs/datasets-server/quick_start#access-parquet-files) endpoint.\\n\\n## Analyze with DuckDB\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 4041}, page_content='## Analyze with DuckDB\\n\\nDuckDB offers super impressive performance for running complex analytical queries. It is able to execute a SQL query directly on a remote Parquet file without any overhead. With the [`httpfs`](https://duckdb.org/docs/extensions/httpfs) extension, DuckDB is able to query remote files such as datasets stored on the Hub using the URL provided from the `/parquet` endpoint. DuckDB also supports querying multiple Parquet files which is really convenient because Datasets Server shards big datasets into smaller 500MB chunks.\\n\\n## Looking forward\\n\\nKnowing what’s inside a dataset is important for developing models because it can impact model quality in all sorts of ways! By allowing users to write and execute any SQL query on Hub datasets, this is another way for us to enable open access to datasets and help users be more aware of the datasets contents. We are excited for you to try this out, and we’re looking forward to what kind of insights your analysis uncovers!'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 1}, page_content='Gradio and W&B Integration\\n\\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\\nTags: WANDB, SPACES\\nContributed by Gradio team\\n\\n## Introduction\\n\\nIn this Guide, we\\'ll walk you through:\\n\\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\\n\\n\\n## What is Wandb?\\n\\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\\n\\n<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\\n\\n## What are Hugging Face Spaces & Gradio?\\n\\n### Gradio'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 918}, page_content=\"## What are Hugging Face Spaces & Gradio?\\n\\n### Gradio\\n\\nGradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\\n\\nGet started [here](https://gradio.app/getting_started)\\n\\n### Hugging Face Spaces\\n\\nHugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\\n\\n## Setting up a Gradio Demo for JoJoGAN\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 1760}, page_content=\"## Setting up a Gradio Demo for JoJoGAN\\n\\nNow, let's walk you through how to do this on your own. We'll make the assumption that you're new to W&B and Gradio for the purposes of this tutorial.\\n\\nLet's get started!\\n\\n1. Create a W&B account\\n\\n   Follow [these quick instructions](https://app.wandb.ai/login) to create your free account if you don’t have one already. It shouldn't take more than a couple minutes. Once you're done (or if you've already got an account), next, we'll run a quick colab.\\n\\n2. Open Colab Install Gradio and W&B\\n\\n   We'll be following along with the colab provided in the JoJoGAN repo with some minor modifications to use Wandb and Gradio more effectively.\\n\\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb)\\n\\n   Install Gradio and Wandb at the top:\\n\\n```sh\\n\\npip install gradio wandb\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 2675}, page_content='```\\n\\n3. Finetune StyleGAN and W&B experiment tracking\\n\\n   This next step will open a W&B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here\\'s the code you need for that:\\n\\n   ```python\\n\\n   alpha =  1.0\\n   alpha = 1-alpha\\n\\n   preserve_color = True\\n   num_iter = 100\\n   log_interval = 50\\n\\n\\n   samples = []\\n   column_names = [\"Reference (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\\n\\n   wandb.init(project=\"JoJoGAN\")\\n   config = wandb.config\\n   config.num_iter = num_iter\\n   config.preserve_color = preserve_color\\n   wandb.log(\\n   {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\\n   step=0)\\n\\n   # load discriminator for perceptual loss\\n   discriminator = Discriminator(1024, 2).eval().to(device)\\n   ckpt = torch.load(\\'models/stylegan2-ffhq-config-f.pt\\', map_location=lambda storage, loc: storage)\\n   discriminator.load_state_dict(ckpt[\"d\"], strict=False)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 3668}, page_content='# reset generator\\n   del generator\\n   generator = deepcopy(original_generator)\\n\\n   g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\\n\\n   # Which layers to swap for generating a family of plausible real images -> fake image\\n   if preserve_color:\\n       id_swap = [9,11,15,16,17]\\n   else:\\n       id_swap = list(range(7, generator.n_latent))\\n\\n   for idx in tqdm(range(num_iter)):\\n       mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\\n       in_latent = latents.clone()\\n       in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\\n\\n       img = generator(in_latent, input_is_latent=True)\\n\\n       with torch.no_grad():\\n           real_feat = discriminator(targets)\\n       fake_feat = discriminator(img)\\n\\n       loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 4506}, page_content='loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\\n\\n\\n       wandb.log({\"loss\": loss}, step=idx)\\n       if idx % log_interval == 0:\\n           generator.eval()\\n           my_sample = generator(my_w, input_is_latent=True)\\n           generator.train()\\n           my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\\n           wandb.log(\\n           {\"Current stylization\": [wandb.Image(my_sample)]},\\n           step=idx)\\n       table_data = [\\n               wandb.Image(transforms.ToPILImage()(target_im)),\\n               wandb.Image(img),\\n               wandb.Image(my_sample),\\n           ]\\n       samples.append(table_data)\\n\\n       g_optim.zero_grad()\\n       loss.backward()\\n       g_optim.step()\\n\\n   out_table = wandb.Table(data=samples, columns=column_names)\\n   wandb.log({\"Current Samples\": out_table})'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 5388}, page_content='```\\n\\nalpha = 1.0\\nalpha = 1-alpha\\n\\npreserve_color = True\\nnum_iter = 100\\nlog_interval = 50\\n\\nsamples = []\\ncolumn_names = [\"Referece (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\\n\\nwandb.init(project=\"JoJoGAN\")\\nconfig = wandb.config\\nconfig.num_iter = num_iter\\nconfig.preserve_color = preserve_color\\nwandb.log(\\n{\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\\nstep=0)\\n\\n# load discriminator for perceptual loss\\n\\ndiscriminator = Discriminator(1024, 2).eval().to(device)\\nckpt = torch.load(\\'models/stylegan2-ffhq-config-f.pt\\', map_location=lambda storage, loc: storage)\\ndiscriminator.load_state_dict(ckpt[\"d\"], strict=False)\\n\\n# reset generator\\n\\ndel generator\\ngenerator = deepcopy(original_generator)\\n\\ng_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\\n\\n# Which layers to swap for generating a family of plausible real images -> fake image\\n\\nif preserve_color:\\nid_swap = [9,11,15,16,17]\\nelse:\\nid_swap = list(range(7, generator.n_latent))'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 6260}, page_content='if preserve_color:\\nid_swap = [9,11,15,16,17]\\nelse:\\nid_swap = list(range(7, generator.n_latent))\\n\\nfor idx in tqdm(range(num_iter)):\\nmean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\\nin_latent = latents.clone()\\nin_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\\n\\n    img = generator(in_latent, input_is_latent=True)\\n\\n    with torch.no_grad():\\n        real_feat = discriminator(targets)\\n    fake_feat = discriminator(img)\\n\\n    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 6795}, page_content='loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\\n\\n\\n    wandb.log({\"loss\": loss}, step=idx)\\n    if idx % log_interval == 0:\\n        generator.eval()\\n        my_sample = generator(my_w, input_is_latent=True)\\n        generator.train()\\n        my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\\n        wandb.log(\\n        {\"Current stylization\": [wandb.Image(my_sample)]},\\n        step=idx)\\n    table_data = [\\n            wandb.Image(transforms.ToPILImage()(target_im)),\\n            wandb.Image(img),\\n            wandb.Image(my_sample),\\n        ]\\n    samples.append(table_data)\\n\\n    g_optim.zero_grad()\\n    loss.backward()\\n    g_optim.step()\\n\\nout_table = wandb.Table(data=samples, columns=column_names)\\nwandb.log({\"Current Samples\": out_table})\\n\\n`'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 7616}, page_content='```\\n\\n4. Save, Download, and Load Model\\n\\n    Here\\'s how to save and download your model.\\n\\n```python\\n\\nfrom PIL import Image\\nimport torch\\ntorch.backends.cudnn.benchmark = True\\nfrom torchvision import transforms, utils\\nfrom util import *\\nimport math\\nimport random\\nimport numpy as np\\nfrom torch import nn, autograd, optim\\nfrom torch.nn import functional as F\\nfrom tqdm import tqdm\\nimport lpips\\nfrom model import *\\nfrom e4e_projection import projection as e4e_projection\\n\\nfrom copy import deepcopy\\nimport imageio\\n\\nimport os\\nimport sys\\nimport torchvision.transforms as transforms\\nfrom argparse import Namespace\\nfrom e4e.models.psp import pSp\\nfrom util import *\\nfrom huggingface_hub import hf_hub_download\\nfrom google.colab import files\\n\\ntorch.save({\"g\": generator.state_dict()}, \"your-model-name.pt\")\\n\\nfiles.download(\\'your-model-name.pt\\')'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 8411}, page_content='files.download(\\'your-model-name.pt\\')\\n\\nlatent_dim = 512\\ndevice=\"cuda\"\\nmodel_path_s = hf_hub_download(repo_id=\"akhaliq/jojogan-stylegan2-ffhq-config-f\", filename=\"stylegan2-ffhq-config-f.pt\")\\noriginal_generator = Generator(1024, latent_dim, 8, 2).to(device)\\nckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)\\noriginal_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\\nmean_latent = original_generator.mean_latent(10000)\\n\\ngenerator = deepcopy(original_generator)\\n\\nckpt = torch.load(\"/content/JoJoGAN/your-model-name.pt\", map_location=lambda storage, loc: storage)\\ngenerator.load_state_dict(ckpt[\"g\"], strict=False)\\ngenerator.eval()\\n\\nplt.rcParams[\\'figure.dpi\\'] = 150\\n\\n\\n\\ntransform = transforms.Compose(\\n    [\\n        transforms.Resize((1024, 1024)),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\\n    ]\\n)\\n\\n\\ndef inference(img):\\n    img.save(\\'out.jpg\\')\\n    aligned_face = align_face(\\'out.jpg\\')'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 9290}, page_content='def inference(img):\\n    img.save(\\'out.jpg\\')\\n    aligned_face = align_face(\\'out.jpg\\')\\n\\n    my_w = e4e_projection(aligned_face, \"out.pt\", device).unsqueeze(0)\\n    with torch.no_grad():\\n        my_sample = generator(my_w, input_is_latent=True)\\n\\n\\n    npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()\\n    imageio.imwrite(\\'filename.jpeg\\', npimage)\\n    return \\'filename.jpeg\\'\\n`'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 9674}, page_content='```\\n\\n5. Build a Gradio Demo\\n\\n```python\\n\\nimport gradio as gr\\n\\ntitle = \"JoJoGAN\"\\ndescription = \"Gradio Demo for JoJoGAN: One Shot Face Stylization. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\\n\\ndemo = gr.Interface(\\n    inference,\\n    gr.Image(type=\"pil\"),\\n    gr.Image(type=\"file\"),\\n    title=title,\\n    description=description\\n)\\n\\ndemo.launch(share=True)\\n```\\n\\n6. Integrate Gradio into your W&B Dashboard\\n\\n   The last step—integrating your Gradio demo with your W&B dashboard—is just one extra line:\\n\\n```python\\n\\ndemo.integrate(wandb=wandb)\\n```\\n\\n    Once you call integrate, a demo will be created and you can integrate it into your dashboard or report\\n\\n    Outside of W&B with Web components, using the gradio-app tags allows anyone can embed Gradio demos on HF spaces directly into their blogs, websites, documentation, etc.:\\n\\n```html\\n<gradio-app space=\"akhaliq/JoJoGAN\"> </gradio-app>'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 10624}, page_content='```\\n\\n7. (Optional) Embed W&B plots in your Gradio App\\n\\n   It\\'s also possible to embed W&B plots within Gradio apps. To do so, you can create a W&B Report of your plots and\\n   embed them within your Gradio app within a `gr.HTML` block.\\n\\n   The Report will need to be public and you will need to wrap the URL within an iFrame like this:\\n\\n```python\\n\\nimport gradio as gr\\n\\ndef wandb_report(url):\\n    iframe = f\\'<iframe src={url} style=\"border:none;height:1024px;width:100%\">\\'\\n    return gr.HTML(iframe)\\n\\nwith gr.Blocks() as demo:\\n    report_url = \\'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx\\'\\n    report = wandb_report(report_url)\\n\\ndemo.launch(share=True)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 11325}, page_content='```\\n\\n## Conclusion\\n\\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! Thanks for making it to the end. To recap:\\n\\n- Only one single reference image is needed for fine-tuning JoJoGAN which usually takes about 1 minute on a GPU in colab. After training, style can be applied to any input image. Read more in the paper.\\n\\n- W&B tracks experiments with just a few lines of code added to a colab and you can visualize, sort, and understand your experiments in a single, centralized dashboard.\\n\\n- Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web.\\n\\n## How to contribute Gradio demos on HF spaces on the Wandb organization'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 11940}, page_content='## How to contribute Gradio demos on HF spaces on the Wandb organization\\n\\n- Create an account on Hugging Face [here](https://huggingface.co/join).\\n- Add Gradio Demo under your username, see this [course](https://huggingface.co/course/chapter9/4?fw=pt) for setting up Gradio Demo on Hugging Face.\\n- Request to join wandb organization [here](https://huggingface.co/wandb).\\n- Once approved transfer model from your username to Wandb organization'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/duplicatebutton_component/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: duplicatebutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Blocks() as demo:\\n    gr.DuplicateButton()\\n\\ndemo.launch()\\n```'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 1}, page_content='Widget Examples\\n\\nNote that each widget example can also optionally describe the corresponding model output, directly in the `output` property. See [the spec](./models-widgets#example-outputs) for more details.\\n\\n## Natural Language Processing\\n\\n### Fill-Mask\\n\\n```yaml\\nwidget:\\n- text: \"Paris is the <mask> of France.\"\\n  example_title: \"Capital\"\\n- text: \"The goal of life is <mask>.\"\\n  example_title: \"Philosophy\"\\n```\\n\\n### Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"What\\'s my name?\"\\n  context: \"My name is Clara and I live in Berkeley.\"\\n  example_title: \"Name\"\\n- text: \"Where do I live?\"\\n  context: \"My name is Sarah and I live in London\"\\n  example_title: \"Location\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 666}, page_content='```\\n\\n### Summarization'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 690}, page_content='```yaml\\nwidget:\\n- text: \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\\n  example_title: \"Eiffel Tower\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 1462}, page_content='example_title: \"Eiffel Tower\"\\n- text: \"Laika, a dog that was the first living creature to be launched into Earth orbit, on board the Soviet artificial satellite Sputnik 2, on November 3, 1957. It was always understood that Laika would not survive the mission, but her actual fate was misrepresented for decades. Laika was a small (13 pounds [6 kg]), even-tempered, mixed-breed dog about two years of age. She was one of a number of stray dogs that were taken into the Soviet spaceflight program after being rescued from the streets. Only female dogs were used because they were considered to be anatomically better suited than males for close confinement.\"\\n  example_title: \"First in Space\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 2153}, page_content='```\\n\\n### Table Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"How many stars does the transformers repository have?\"\\n  table:\\n    Repository:\\n      - \"Transformers\"\\n      - \"Datasets\"\\n      - \"Tokenizers\"\\n    Stars:\\n      - 36542\\n      - 4512\\n      - 3934\\n    Contributors:\\n      - 651\\n      - 77\\n      - 34\\n    Programming language:\\n      - \"Python\"\\n      - \"Python\"\\n      - \"Rust, Python and NodeJS\"\\n  example_title: \"Github stars\"\\n```\\n\\n### Text Classification\\n\\n```yaml\\nwidget:\\n- text: \"I love football so much\"\\n  example_title: \"Positive\"\\n- text: \"I don\\'t really like this type of food\"\\n  example_title: \"Negative\"\\n```\\n\\n### Text Generation\\n\\n```yaml\\nwidget:\\n- text: \"My name is Julien and I like to\"\\n  example_title: \"Julien\"\\n- text: \"My name is Merve and my favorite\"\\n  example_title: \"Merve\"\\n```\\n\\n### Text2Text Generation\\n\\n```yaml\\nwidget:\\n- text: \"My name is Julien and I like to\"\\n  example_title: \"Julien\"\\n- text: \"My name is Merve and my favorite\"\\n  example_title: \"Merve\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 3130}, page_content='```\\n\\n### Token Classification\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n### Translation\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n### Zero-Shot Classification\\n\\n```yaml\\nwidget:\\n- text: \"I have a problem with my car that needs to be resolved asap!!\"\\n  candidate_labels: \"urgent, not urgent, phone, tablet, computer\"\\n  multi_class: true\\n  example_title: \"Car problem\"\\n- text: \"Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\"\\n  candidate_labels: \"mobile, website, billing, account access\"\\n  multi_class: false\\n  example_title: \"Phone issue\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 3997}, page_content='```\\n### Sentence Similarity\\n\\n```yaml\\nwidget:\\n- source_sentence: \"That is a happy person\"\\n  sentences:\\n    - \"That is a happy dog\"\\n    - \"That is a very happy person\"\\n    - \"Today is a sunny day\"\\n  example_title: \"Happy\"\\n```\\n\\n### Conversational\\n\\n```yaml\\nwidget:\\n- text: \"Hey my name is Julien! How are you?\"\\n  example_title: \"Julien\"\\n- text: \"Hey my name is Clara! How are you?\"\\n  example_title: \"Clara\"\\n```\\n\\n### Feature Extraction\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n## Audio\\n\\n### Text-to-Speech\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 4802}, page_content='```\\n\\n### Automatic Speech Recognition\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2\\n```\\n\\n### Audio-to-Audio\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2\\n```\\n\\n### Audio Classification\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 5581}, page_content='```\\n\\n### Voice Activity Detection\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2\\n```\\n\\n## Computer Vision\\n\\n### Image Classification\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\\n  example_title: Tiger\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\\n  example_title: Teapot\\n```\\n\\n### Object Detection\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\\n  example_title: Football Match\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\\n  example_title: Airport'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 6402}, page_content='```\\n\\n### Image Segmentation\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\\n  example_title: Football Match\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\\n  example_title: Airport\\n```\\n\\n### Image-to-Image\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/canny-edge.jpg\\n  prompt: Girl with Pearl Earring # `prompt` field is optional in case the underlying model supports text guidance\\n```\\n\\n### Text-to-Image\\n\\n```yaml\\nwidget:\\n- text: \"A cat playing with a ball\"\\n  example_title: \"Cat\"\\n- text: \"A dog jumping over a fence\"\\n  example_title: \"Dog\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 7084}, page_content='```\\n\\n### Document Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"What is the invoice number?\"\\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\\n- text: \"What is the purchase amount?\"\\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg\"\\n```\\n\\n### Visual Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"What animal is it?\"\\n  src: \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\"\\n- text: \"Where is it?\"\\n  src: \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\"\\n```\\n\\n### Zero-Shot Image Classification\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\\n  candidate_labels: playing music, playing sports\\n  example_title: Cat & Dog'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 7949}, page_content='```\\n\\n## Other\\n\\n### Structured Data Classification\\n\\n```yaml\\nwidget:\\n- structured_data:\\n    fixed_acidity:\\n      - 7.4\\n      - 7.8\\n      - 10.3\\n    volatile_acidity:\\n      - 0.7\\n      - 0.88\\n      - 0.32\\n    citric_acid:\\n      - 0\\n      - 0\\n      - 0.45\\n    residual_sugar:\\n      - 1.9\\n      - 2.6\\n      - 6.4\\n    chlorides:\\n      - 0.076\\n      - 0.098\\n      - 0.073\\n    free_sulfur_dioxide:\\n      - 11\\n      - 25\\n      - 5\\n    total_sulfur_dioxide:\\n      - 34\\n      - 67\\n      - 13\\n    density:\\n      - 0.9978\\n      - 0.9968\\n      - 0.9976\\n    pH:\\n      - 3.51\\n      - 3.2\\n      - 3.23\\n    sulphates:\\n      - 0.56\\n      - 0.68\\n      - 0.82\\n    alcohol:\\n      - 9.4\\n      - 9.8\\n      - 12.6\\n  example_title: \"Wine\"\\n```'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# UNet1DModel'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 587}, page_content=\"# UNet1DModel\\n\\nThe [UNet](https://huggingface.co/papers/1505.04597) model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in 🤗 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in 🤗 Diffusers, depending on it's number of dimensions and whether it is a conditional model or not. This is a 1D UNet model.\\n\\nThe abstract from the paper is:\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 1174}, page_content='*There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 2074}, page_content='image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.*'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 2268}, page_content='## UNet1DModel\\n[[autodoc]] UNet1DModel\\n\\n## UNet1DOutput\\n[[autodoc]] models.unet_1d.UNet1DOutput'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 0}, page_content='--\\ntitle: \"Building an AI WebTV\"\\nthumbnail: /blog/assets/156_ai_webtv/thumbnail.gif\\nauthors:\\n- user: jbilcke-hf\\n---\\n\\n# Building an AI WebTV\\n\\n\\nThe AI WebTV is an experimental demo to showcase the latest advancements in automatic video and music synthesis.\\n\\n👉 Watch the stream now by going to the [AI WebTV Space](https://huggingface.co/spaces/jbilcke-hf/AI-WebTV).\\n\\nIf you are using a mobile device, you can view the stream from the [Twitch mirror](https://www.twitch.tv/ai_webtv).\\n\\n![thumbnail.gif](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/thumbnail.gif)\\n\\n# Concept\\n\\nThe motivation for the AI WebTV is to demo videos generated with open-source [text-to-video models](https://huggingface.co/tasks/text-to-video) such as Zeroscope and MusicGen, in an entertaining and accessible way.\\n\\nYou can find those open-source models on the Hugging Face hub:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 838}, page_content='You can find those open-source models on the Hugging Face hub:\\n\\n- For video: [zeroscope_v2_576](https://huggingface.co/cerspense/zeroscope_v2_576w) and [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL)\\n- For music: [musicgen-melody](https://huggingface.co/facebook/musicgen-melody)\\n\\nThe individual video sequences are purposely made to be short, meaning the WebTV should be seen as a tech demo/showreel rather than an actual show (with an art direction or programming).\\n\\n# Architecture\\n\\nThe AI WebTV works by taking a sequence of [video shot](https://en.wikipedia.org/wiki/Shot_(filmmaking)) prompts and passing them to a [text-to-video model](https://huggingface.co/tasks/text-to-video) to generate a sequence of [takes](https://en.wikipedia.org/wiki/Take). \\n\\nAdditionally, a base theme and idea (written by a human) are passed through a LLM (in this case, ChatGPT), in order to generate a variety of individual prompts for each video clip.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 1799}, page_content=\"Here's a diagram of the current architecture of the AI WebTV:\\n\\n![diagram.jpg](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/diagram.jpg)\\n\\n# Implementing the pipeline\\n\\nThe WebTV is implemented in NodeJS and TypeScript, and uses various services hosted on Hugging Face.\\n\\n## The text-to-video model\\n\\nThe central video model is Zeroscope V2, a model based on [ModelScope](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis).\\n\\nZeroscope is comprised of two parts that can be chained together:\\n\\n- A first pass with [zeroscope_v2_576](https://huggingface.co/cerspense/zeroscope_v2_576w), to generate a 576x320 video clip\\n- An optional second pass with [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) to upscale the video to 1024x576\\n\\n👉\\xa0 You will need to use the same prompt for both the generation and upscaling.\\n\\n## Calling the video chain\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 2699}, page_content='## Calling the video chain\\n\\nTo make a quick prototype, the WebTV runs Zeroscope from two duplicated Hugging Face Spaces running [Gradio](https://github.com/gradio-app/gradio/), which are called using the [@gradio/client](https://www.npmjs.com/package/@gradio/client) NPM package. You can find the original spaces here:\\n\\n- [zeroscope-v2](https://huggingface.co/spaces/hysts/zeroscope-v2/tree/main) by @hysts\\n- [Zeroscope XL](https://huggingface.co/spaces/fffiloni/zeroscope-XL) by @fffiloni\\n\\nOther spaces deployed by the community can also be found if you [search for Zeroscope on the Hub](https://huggingface.co/spaces?search=zeroscope).\\n\\n👉\\xa0 Public Spaces may become overcrowded and paused at any time. If you intend to deploy your own system, please duplicate those Spaces and run them under your own account.\\n\\n## Using a model hosted on a Space'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 3511}, page_content='## Using a model hosted on a Space\\n\\nSpaces using Gradio have the ability to [expose a REST API](https://www.gradio.app/guides/sharing-your-app#api-page), which can then be called from Node using the [@gradio/client](https://www.npmjs.com/package/@gradio/client) module.\\n\\nHere is an example:\\n\\n```typescript\\nimport { client } from \"@gradio/client\"\\n\\nexport const generateVideo = async (prompt: string) => {\\n  const api = await client(\"*** URL OF THE SPACE ***\")\\n\\n  // call the \"run()\" function with an array of parameters\\n  const { data } = await api.predict(\"/run\", [\\t\\t\\n    prompt,\\n    42,\\t// seed\\t\\n    24, // nbFrames\\n    35 // nbSteps\\n  ])\\n  \\n  const { orig_name } = data[0][0]\\n\\n  const remoteUrl = `${instance}/file=${orig_name}`\\n\\n  // the file can then be downloaded and stored locally\\n}'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 4301}, page_content='```\\n\\n\\n## Post-processing\\n\\nOnce an individual take (a video clip) is upscaled, it is then passed to FILM (Frame Interpolation for Large Motion), a frame interpolation algorithm:\\n\\n- Original links: [website](https://film-net.github.io/), [source code](https://github.com/google-research/frame-interpolation)\\n- Model on Hugging Face: [/frame-interpolation-film-style](https://huggingface.co/akhaliq/frame-interpolation-film-style)\\n- A Hugging Face Space you can duplicate: [video_frame_interpolation](https://huggingface.co/spaces/fffiloni/video_frame_interpolation/blob/main/app.py) by @fffiloni\\n\\nDuring post-processing, we also add music generated with MusicGen:\\n\\n- Original links: [website](https://ai.honu.io/papers/musicgen/), [source code](https://github.com/facebookresearch/audiocraft)\\n- Hugging Face Space you can duplicate: [MusicGen](https://huggingface.co/spaces/facebook/MusicGen)\\n\\n\\n## Broadcasting the stream'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 5194}, page_content='## Broadcasting the stream\\n\\nNote: there are multiple tools you can use to create a video stream. The AI WebTV currently uses [FFmpeg](https://ffmpeg.org/documentation.html) to read a playlist made of mp4 videos files and m4a audio files.\\n\\nHere is an example of creating such a playlist:\\n\\n```typescript\\nimport { promises as fs } from \"fs\"\\nimport path from \"path\"\\n\\nconst allFiles = await fs.readdir(\"** PATH TO VIDEO FOLDER **\")\\nconst allVideos = allFiles\\n  .map(file => path.join(dir, file))\\n  .filter(filePath => filePath.endsWith(\\'.mp4\\'))\\n\\nlet playlist = \\'ffconcat version 1.0\\\\n\\'\\nallFilePaths.forEach(filePath => {\\n  playlist += `file \\'${filePath}\\'\\\\n`\\n})\\nawait fs.promises.writeFile(\"playlist.txt\", playlist)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 5904}, page_content=\"```\\n\\nThis will generate the following playlist content:\\n\\n```bash\\nffconcat version 1.0\\nfile 'video1.mp4'\\nfile 'video2.mp4'\\n...\\n```\\n\\nFFmpeg is then used again to read this playlist and send a [FLV stream](https://en.wikipedia.org/wiki/Flash_Video) to a [RTMP server](https://en.wikipedia.org/wiki/Real-Time_Messaging_Protocol). FLV is an old format but still popular in the world of real-time streaming due to its low latency.\\n\\n```bash\\nffmpeg -y -nostdin \\\\\\n  -re \\\\\\n  -f concat \\\\\\n  -safe 0 -i channel_random.txt -stream_loop -1 \\\\\\n  -loglevel error \\\\\\n  -c:v libx264 -preset veryfast -tune zerolatency \\\\\\n  -shortest \\\\\\n  -f flv rtmp://<SERVER>\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 6542}, page_content='```\\n\\nThere are many different configuration options for FFmpeg, for more information in the [official documentation](http://trac.ffmpeg.org/wiki/StreamingGuide).\\n\\nFor the RTMP server, you can find [open-source implementations on GitHub](https://github.com/topics/rtmp-server), such as the [NGINX-RTMP module](https://github.com/arut/nginx-rtmp-module).\\n\\nThe AI WebTV itself uses [node-media-server](https://github.com/illuspas/Node-Media-Server).\\n\\n💡 You can also directly stream to [one of the Twitch RTMP entrypoints](https://help.twitch.tv/s/twitch-ingest-recommendation?language=en_US). Check out the Twitch documentation for more details.\\n\\n# Observations and examples\\n\\nHere are some examples of the generated content.\\n\\nThe first thing we notice is that applying the second pass of Zeroscope XL significantly improves the quality of the image. The impact of frame interpolation is also clearly visible.\\n\\n## Characters and scene composition'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 7449}, page_content='## Characters and scene composition\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo4.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo4.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Photorealistic movie of a <strong>llama acting as a programmer, wearing glasses and a hoodie</strong>, intensely <strong>staring at a screen</strong> with lines of code, in a cozy, <strong>dimly lit room</strong>, Canon EOS, ambient lighting, high details, cinematic, trending on artstation</i></figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 8143}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo5.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo5.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>3D rendered animation showing a group of food characters <strong>forming a pyramid</strong>, with a <strong>banana</strong> standing triumphantly on top. In a city with <strong>cotton candy clouds</strong> and <strong>chocolate road</strong>, Pixar\\'s style, CGI, ambient lighting, direct sunlight, rich color scheme, ultra realistic, cinematic, photorealistic.</i></figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 8870}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo7.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo7.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Intimate <strong>close-up of a red fox, gazing into the camera with sharp eyes</strong>, ambient lighting creating a high contrast silhouette, IMAX camera, <strong>high detail</strong>, <strong>cinematic effect</strong>, golden hour, film grain.</i></figcaption>\\n</figure>\\n\\n## Simulation of dynamic scenes\\n\\nSomething truly fascinating about text-to-video models is their ability to emulate real-life phenomena they have been trained on.\\n\\nWe\\'ve seen it with large language models and their ability to synthesize convincing content that mimics human responses, but this takes things to a whole new dimension when applied to video.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 9838}, page_content='A video model predicts the next frames of a scene, which might include objects in motion such as fluids, people, animals, or vehicles. Today, this emulation isn\\'t perfect, but it will be interesting to evaluate future models (trained on larger or specialized datasets, such as animal locomotion) for their accuracy when reproducing physical phenomena, and also their ability to simulate the behavior of agents.\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo17.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo17.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Cinematic movie shot of <strong>bees energetically buzzing around a flower</strong>, sun rays illuminating the scene, captured in 4k IMAX with a soft bokeh background.</i></figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 10786}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo8.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo8.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i><strong>Dynamic footage of a grizzly bear catching a salmon in a rushing river</strong>, ambient lighting highlighting the splashing water, low angle, IMAX camera, 4K movie quality, golden hour, film grain.</i></figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 11359}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo18.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo18.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Aerial footage of a quiet morning at the coast of California, with <strong>waves gently crashing against the rocky shore</strong>. A startling sunrise illuminates the coast with vibrant colors, captured beautifully with a DJI Phantom 4 Pro. Colors and textures of the landscape come alive under the soft morning light. Film grain, cinematic, imax, movie</i></figcaption>\\n</figure>\\n\\n💡 It will be interesting to see these capabilities explored more in the future, for instance by training video models on larger video datasets covering more phenomena.\\n\\n## Styling and effects'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 12250}, page_content='## Styling and effects\\n\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo0.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo0.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>\\n<strong>3D rendered video</strong> of a friendly broccoli character wearing a hat, walking in a candy-filled city street with gingerbread houses, under a <strong>bright sun and blue skies</strong>, <strong>Pixar\\'s style</strong>, cinematic, photorealistic, movie, <strong>ambient lighting</strong>, natural lighting, <strong>CGI</strong>, wide-angle view, daytime, ultra realistic.</i>\\n</figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 13025}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo2.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo2.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i><strong>Cinematic movie</strong>, shot of an astronaut and a llama at dawn, the mountain landscape bathed in <strong>soft muted colors</strong>, early morning fog, dew glistening on fur, craggy peaks, vintage NASA suit, Canon EOS, high detailed skin, epic composition, high quality, 4K, trending on artstation, beautiful</i>\\n</figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 13713}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo1.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo1.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Panda and black cat <strong>navigating down the flowing river</strong> in a small boat, Studio Ghibli style &gt; Cinematic, beautiful composition &gt; IMAX <strong>camera panning following the boat</strong> &gt; High quality, cinematic, movie, mist effect, film grain, trending on Artstation</i>\\n</figcaption>\\n</figure>\\n\\n\\n\\n## Failure cases\\n\\n**Wrong direction:** the model sometimes has trouble with movement and direction. For instance, here the clip seems to be played in reverse. Also the modifier keyword ***green*** was not taken into account.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 14600}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"fail1.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/fail1.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Movie showing a <strong>green pumpkin</strong> falling into a bed of nails, slow-mo explosion with chunks flying all over, ambient fog adding to the dramatic lighting, filmed with IMAX camera, 8k ultra high definition, high quality, trending on artstation.</i>\\n</figcaption>\\n</figure>\\n\\n\\n**Rendering errors on realistic scenes:** sometimes we can see artifacts such as moving vertical lines or waves. It is unclear what causes this, but it may be due to the combination of keywords used.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 15426}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"fail2.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/fail2.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Film shot of a captivating flight above the Grand Canyon, ledges and plateaus etched in orange and red. <strong>Deep shadows contrast</strong> with the fiery landscape under the midday sun, shot with DJI Phantom 4 Pro. The camera rotates to capture the vastness, <strong>textures</strong> and colors, in imax quality. Film <strong>grain</strong>, cinematic, movie.</i>\\n</figcaption>\\n</figure>\\n\\n\\n**Text or objects inserted into the image:** the model sometimes injects words from the prompt into the scene, such as \"IMAX\". Mentioning \"Canon EOS\" or \"Drone footage\" in the prompt can also make those objects appear in the video.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 16392}, page_content='In the following example, we notice the word \"llama\" inserts a llama but also two occurrences of the word llama in flames.\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"fail3.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/fail3.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Movie scene of a <strong>llama</strong> acting as a firefighter, in firefighter uniform, dramatically spraying water at <strong>roaring flames</strong>, amidst a chaotic urban scene, Canon EOS, ambient lighting, high quality, award winning, highly detailed fur, cinematic, trending on artstation.</i>\\n</figcaption>\\n</figure>\\n\\n# Recommendations\\n\\nHere are some early recommendations that can be made from the previous observations:\\n\\n## Using video-specific prompt keywords'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 17285}, page_content='## Using video-specific prompt keywords\\n\\nYou may already know that if you don’t prompt a specific aspect of the image with Stable Diffusion, things like the color of clothes or the time of the day might become random, or be assigned a generic value such as a neutral mid-day light.\\n\\nThe same is true for video models: you will want to be specific about things. Examples include camera and character movement, their orientation, speed and direction. You can leave it unspecified for creative purposes (idea generation), but this might not always give you the results you want (e.g., entities animated in reverse).\\n\\n## Maintaining consistency between scenes\\n\\nIf you plan to create sequences of multiple videos, you will want to make sure you add as many details as possible in each prompt, otherwise you may lose important details from one sequence to another, such as the color.\\n\\n💡 This will also improve the quality of the image since the prompt is used for the upscaling part with Zeroscope XL.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 18282}, page_content='## Leverage frame interpolation\\n\\nFrame interpolation is a powerful tool which can repair small rendering errors and turn many defects into features, especially in scenes with a lot of animation, or where a cartoon effect is acceptable. The [FILM algorithm](https://film-net.github.io/) will smoothen out elements of a frame with previous and following events in the video clip.\\n\\nThis works great to displace the background when the camera is panning or rotating, and will also give you creative freedom, such as control over the number of frames after the generation, to make slow-motion effects.\\n\\n# Future work\\n\\nWe hope you enjoyed watching the AI WebTV stream and that it will inspire you to build more in this space.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 19003}, page_content='As this was a first trial, a lot of things were not the focus of the tech demo: generating longer and more varied sequences, adding audio (sound effects, dialogue), generating and orchestrating complex scenarios, or letting a language model agent have more control over the pipeline.\\n\\nSome of these ideas may make their way into future updates to the AI WebTV, but we also can’t wait to see what the community of researchers, engineers and builders will come up with!'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 1}, page_content='Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 217}, page_content='The summary of the model is the following:\\n\\n*Stable Diffusion is a text-to-image model that will empower billions of people to create stunning art within seconds. It is a breakthrough in speed and quality meaning that it can run on consumer GPUs. You can see some of the amazing output that has been created by this model without pre or post-processing on this page. The model itself builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all.*\\n\\n## Tips:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 1018}, page_content=\"## Tips:\\n\\n- Stable Diffusion has the same architecture as [Latent Diffusion](https://arxiv.org/abs/2112.10752) but uses a frozen CLIP Text Encoder instead of training the text encoder jointly with the diffusion model.\\n- An in-detail explanation of the Stable Diffusion model can be found under [Stable Diffusion with 🧨 Diffusers](https://huggingface.co/blog/stable_diffusion).\\n- If you don't want to rely on the Hugging Face Hub and having to pass a authentication token, you can\\ndownload the weights with `git lfs install; git clone https://huggingface.co/runwayml/stable-diffusion-v1-5` and instead pass the local path to the cloned folder to `from_pretrained` as shown below.\\n- Stable Diffusion can work with a variety of different samplers as is shown below.\\n\\n## Available Pipelines:\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 1807}, page_content='| Pipeline | Tasks | Colab\\n|---|---|:---:|\\n| [pipeline_stable_diffusion.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py) | *Text-to-Image Generation* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)\\n| [pipeline_stable_diffusion_img2img](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py) | *Image-to-Image Text-Guided Generation* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/image_2_image_using_diffusers.ipynb)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 2629}, page_content='| [pipeline_stable_diffusion_inpaint](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py) | *Text-Guided Image Inpainting* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/in_painting_with_stable_diffusion_using_diffusers.ipynb)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 3050}, page_content='## Examples:\\n\\n### Using Stable Diffusion without being logged into the Hub.\\n\\nIf you want to download the model weights using a single Python line, you need to be logged in via `huggingface-cli login`.\\n\\n```python\\nfrom diffusers import DiffusionPipeline\\n\\npipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 3382}, page_content='```\\n\\nThis however can make it difficult to build applications on top of `diffusers` as you will always have to pass the token around. A potential way to solve this issue is by downloading the weights to a local path `\"./stable-diffusion-v1-5\"`:\\n\\n```\\ngit lfs install\\ngit clone https://huggingface.co/runwayml/stable-diffusion-v1-5\\n```\\n\\nand simply passing the local path to `from_pretrained`:\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\n\\npipe = StableDiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\")\\n```\\n\\n### Text-to-Image with default PLMS scheduler\\n\\n```python\\n# make sure you\\'re logged in with `huggingface-cli login`\\nfrom diffusers import StableDiffusionPipeline\\n\\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\\npipe = pipe.to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\"\\nimage = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 4307}, page_content='```\\n\\n### Text-to-Image with DDIM scheduler\\n\\n```python\\n# make sure you\\'re logged in with `huggingface-cli login`\\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\\n\\nscheduler =  DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\\n\\npipe = StableDiffusionPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    scheduler=scheduler,\\n).to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\"\\nimage = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 4836}, page_content='```\\n\\n### Text-to-Image with K-LMS scheduler\\n\\n```python\\n# make sure you\\'re logged in with `huggingface-cli login`\\nfrom diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\\n\\nlms = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\\n\\npipe = StableDiffusionPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    scheduler=lms,\\n).to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\"\\nimage = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 5367}, page_content='```\\n\\n### CycleDiffusion using Stable Diffusion and DDIM scheduler\\n\\n```python\\nimport requests\\nimport torch\\nfrom PIL import Image\\nfrom io import BytesIO\\n\\nfrom diffusers import CycleDiffusionPipeline, DDIMScheduler\\n\\n\\n# load the scheduler. CycleDiffusion only supports stochastic schedulers.\\n\\n# load the pipeline\\n# make sure you\\'re logged in with `huggingface-cli login`\\nmodel_id_or_path = \"CompVis/stable-diffusion-v1-4\"\\nscheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\\npipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to(\"cuda\")\\n\\n# let\\'s download an initial image\\nurl = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/An%20astronaut%20riding%20a%20horse.png\"\\nresponse = requests.get(url)\\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\\ninit_image = init_image.resize((512, 512))\\ninit_image.save(\"horse.png\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 6291}, page_content='# let\\'s specify a prompt\\nsource_prompt = \"An astronaut riding a horse\"\\nprompt = \"An astronaut riding an elephant\"\\n\\n# call the pipeline\\nimage = pipe(\\n    prompt=prompt,\\n    source_prompt=source_prompt,\\n    image=init_image,\\n    num_inference_steps=100,\\n    eta=0.1,\\n    strength=0.8,\\n    guidance_scale=2,\\n    source_guidance_scale=1,\\n).images[0]\\n\\nimage.save(\"horse_to_elephant.png\")\\n\\n# let\\'s try another example\\n# See more samples at the original repo: https://github.com/ChenWu98/cycle-diffusion\\nurl = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/A%20black%20colored%20car.png\"\\nresponse = requests.get(url)\\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\\ninit_image = init_image.resize((512, 512))\\ninit_image.save(\"black.png\")\\n\\nsource_prompt = \"A black colored car\"\\nprompt = \"A blue colored car\"'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 7070}, page_content='source_prompt = \"A black colored car\"\\nprompt = \"A blue colored car\"\\n\\n# call the pipeline\\ntorch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt,\\n    source_prompt=source_prompt,\\n    image=init_image,\\n    num_inference_steps=100,\\n    eta=0.1,\\n    strength=0.85,\\n    guidance_scale=3,\\n    source_guidance_scale=1,\\n).images[0]\\n\\nimage.save(\"black_to_blue.png\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 7425}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 0}, page_content='p align=\"center\">\\n  <br/>\\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg\" width=\"376\" height=\"59\" style=\"max-width: 100%;\">\\n  <br/>\\n</p>\\n\\n<p align=\"center\">\\n    <i>Huggingface Hub के लिए आधिकारिक पायथन क्लाइंट।</i>\\n</p>'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 317}, page_content='<p align=\"center\">\\n    <a href=\"https://huggingface.co/docs/huggingface_hub/ko/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc\"></a>\\n    <a href=\"https://github.com/huggingface/huggingface_hub/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/huggingface_hub.svg\"></a>\\n    <a href=\"https://github.com/huggingface/huggingface_hub\"><img alt=\"PyPi version\" src=\"https://img.shields.io/pypi/pyversions/huggingface_hub.svg\"></a>\\n    <a href=\"https://pypi.org/project/huggingface-hub\"><img alt=\"downloads\" src=\"https://static.pepy.tech/badge/huggingface_hub/month\"></a>\\n    <a href=\"https://codecov.io/gh/huggingface/huggingface_hub\"><img alt=\"Code coverage\" src=\"https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL\"></a>\\n</p>'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 1258}, page_content='<h4 align=\"center\">\\n    <p>\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README.md\">English</a> |\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_de.md\">Deutsch</a> |\\n        <b>हिंदी</b>  |\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md\">한국어</a> |\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md\">中文（简体）</a> \\n    <p>\\n</h4>\\n\\n---\\n\\n**दस्तावेज़ीकरण**: <a href=\"https://hf.co/docs/huggingface_hub\" target=\"_blank\">https://hf.co/docs/huggingface_hub</a>\\n\\n**सोर्स कोड**: <a href=\"https://github.com/huggingface/huggingface_hub\" target=\"_blank\">https://github.com/huggingface/huggingface_hub</a>\\n\\n---\\n\\n## huggingface_hub लाइब्रेरी में आपका स्वागत है'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 1989}, page_content='---\\n\\n## huggingface_hub लाइब्रेरी में आपका स्वागत है\\n\\n`huggingface_hub` लाइब्रेरी आपको [हगिंग फेस हब](https://huggingface.co/) के साथ बातचीत करने की अनुमति देती है, जो रचनाकारों और सहयोगियों के लिए ओपन-सोर्स मशीन लर्निंग का लोकतंत्रीकरण करने वाला एक मंच है। अपनी परियोजनाओं के लिए पूर्व-प्रशिक्षित मॉडल और डेटासेट खोजें या हब पर होस्ट किए गए हजारों मशीन लर्निंग ऐप्स के साथ खेलें। आप समुदाय के साथ अपने स्वयं के मॉडल, डेटासेट और डेमो भी बना और साझा कर सकते हैं। `huggingface_hub` लाइब्रेरी पायथन के साथ इन सभी चीजों को करने का एक आसान तरीका प्रदान करती है।\\n\\n## प्रमुख विशेषताऐं'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 2547}, page_content='## प्रमुख विशेषताऐं\\n\\n- [फ़ाइलें डाउनलोड करें](https://huggingface.co/docs/huggingface_hub/en/guides/download) हब से।\\n- [फ़ाइलें अपलोड करें](https://huggingface.co/docs/huggingface_hub/en/guides/upload) हब पर।\\n- [अपनी रिपॉजिटरी प्रबंधित करें](https://huggingface.co/docs/huggingface_hub/en/guides/repository)।\\n- तैनात मॉडलों पर [अनुमान चलाएँ](https://huggingface.co/docs/huggingface_hub/en/guides/inference)।\\n- मॉडल, डेटासेट और स्पेस के लिए [खोज](https://huggingface.co/docs/huggingface_hub/en/guides/search)।\\n- [मॉडल कार्ड साझा करें](https://huggingface.co/docs/huggingface_hub/en/guides/model-cards) अपने मॉडलों का दस्तावेजीकरण करने के लिए।\\n- [समुदाय के साथ जुड़ें](https://huggingface.co/docs/huggingface_hub/en/guides/community) पीआर और टिप्पणियों के माध्यम से।\\n\\n## स्थापना\\n\\n[pip](https://pypi.org/project/huggingface-hub/) के साथ `huggingface_hub` पैकेज इंस्टॉल करें:\\n\\n```bash\\npip install huggingface_hub'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 3456}, page_content='```\\n\\nयदि आप चाहें, तो आप इसे [conda](https://huggingface.co/docs/huggingface_hub/en/installation#install-with-conda) से भी इंस्टॉल कर सकते हैं।\\n\\nपैकेज को डिफ़ॉल्ट रूप से न्यूनतम रखने के लिए, `huggingface_hub` कुछ उपयोग मामलों के लिए उपयोगी वैकल्पिक निर्भरता के साथ आता है। उदाहरण के लिए, यदि आप अनुमान के लिए संपूर्ण अनुभव चाहते हैं, तो चलाएँ:\\n\\n```bash\\npip install huggingface_hub[inference]\\n```\\n\\nअधिक इंस्टॉलेशन और वैकल्पिक निर्भरता जानने के लिए, [इंस्टॉलेशन गाइड](https://huggingface.co/docs/huggingface_hub/en/installation) देखें।\\n\\n## जल्दी शुरू\\n\\n### फ़ाइलें डाउनलोड करें\\n\\nएकल फ़ाइल डाउनलोड करें\\n\\n```py\\nfrom huggingface_hub import hf_hub_download\\n\\nhf_hub_download(repo_id=\"tiiuae/falcon-7b-instruct\", filename=\"config.json\")\\n```\\n\\nया एक संपूर्ण भंडार\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_download(\"stabilityai/stable-diffusion-2-1\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 4317}, page_content='```\\n\\nफ़ाइलें स्थानीय कैश फ़ोल्डर में डाउनलोड की जाएंगी. [this_guide] में अधिक विवरण (https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache)।\\n\\n### लॉग इन करें\\n\\nHugging Face Hub एप्लिकेशन को प्रमाणित करने के लिए टोकन का उपयोग करता है (देखें [docs](https://huggingface.co/docs/hub/security-tokens))। अपनी मशीन में लॉगिन करने के लिए, निम्नलिखित सीएलआई चलाएँ:\\n\\n```bash\\nhuggingface-cli login\\n# या कृपया इसे एक पर्यावरण चर के रूप में निर्दिष्ट करें।\\nhuggingface-cli login --token $HUGGINGFACE_TOKEN\\n```\\n\\n### एक रिपॉजिटरी बनाएं\\n\\n```py\\nfrom huggingface_hub import create_repo\\n\\ncreate_repo(repo_id=\"super-cool-model\")\\n```\\n\\n### फाइलें अपलोड करें\\n\\nएकल फ़ाइल अपलोड करें\\n\\n```py\\nfrom huggingface_hub import upload_file\\n\\nupload_file(\\n    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\",\\n    path_in_repo=\"README.md\",\\n    repo_id=\"lysandre/test-model\",\\n)'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 5174}, page_content='```\\n\\nया एक संपूर्ण फ़ोल्डर\\n\\n```py\\nfrom huggingface_hub import upload_folder\\n\\nupload_folder(\\n    folder_path=\"/path/to/local/space\",\\n    repo_id=\"username/my-cool-space\",\\n    repo_type=\"space\",\\n)'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 5369}, page_content='```\\n\\n[अपलोड गाइड](https://huggingface.co/docs/huggingface_hub/en/guides/upload) में विवरण के लिए।\\n\\n## हब से एकीकरण।\\n\\nहम मुफ्त मॉडल होस्टिंग और वर्जनिंग प्रदान करने के लिए शानदार ओपन सोर्स एमएल लाइब्रेरीज़ के साथ साझेदारी कर रहे हैं। आप मौजूदा एकीकरण [यहां](https://huggingface.co/docs/hub/libraries) पा सकते हैं।\\n\\nफायदे ये हैं:\\n\\n- पुस्तकालयों और उनके उपयोगकर्ताओं के लिए निःशुल्क मॉडल या डेटासेट होस्टिंग।\\n- गिट-आधारित दृष्टिकोण के कारण, बहुत बड़ी फ़ाइलों के साथ भी अंतर्निहित फ़ाइल संस्करणिंग।\\n- सभी मॉडलों के लिए होस्टेड अनुमान एपीआई सार्वजनिक रूप से उपलब्ध है।\\n- अपलोड किए गए मॉडलों के साथ खेलने के लिए इन-ब्राउज़र विजेट।\\n- कोई भी आपकी लाइब्रेरी के लिए एक नया मॉडल अपलोड कर सकता है, उन्हें मॉडल को खोजने योग्य बनाने के लिए बस संबंधित टैग जोड़ना होगा।\\n- तेज़ डाउनलोड! हम डाउनलोड को जियो-रेप्लिकेट करने के लिए क्लाउडफ्रंट (एक सीडीएन) का उपयोग करते हैं ताकि वे दुनिया में कहीं से भी तेजी से चमक सकें।\\n- उपयोग आँकड़े और अधिक सुविधाएँ आने वाली हैं।'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 6317}, page_content='यदि आप अपनी लाइब्रेरी को एकीकृत करना चाहते हैं, तो चर्चा शुरू करने के लिए बेझिझक एक मुद्दा खोलें। हमने ❤️ के साथ एक [चरण-दर-चरण मार्गदर्शिका](https://huggingface.co/docs/hub/adding-a-library) लिखी, जिसमें दिखाया गया कि यह एकीकरण कैसे करना है।\\n\\n## योगदान (सुविधा अनुरोध, बग, आदि) का अति स्वागत है 💙💚💛💜🧡❤️\\n\\nयोगदान के लिए हर किसी का स्वागत है और हम हर किसी के योगदान को महत्व देते हैं। कोड समुदाय की मदद करने का एकमात्र तरीका नहीं है।\\nप्रश्नों का उत्तर देना, दूसरों की मदद करना, उन तक पहुंचना और दस्तावेज़ों में सुधार करना समुदाय के लिए बेहद मूल्यवान है।\\nहमने संक्षेप में बताने के लिए एक [योगदान मार्गदर्शिका](https://github.com/huggingface/huggingface_hub/blob/main/CONTRIBUTING.md) लिखी है\\nइस भंडार में योगदान करने की शुरुआत कैसे करें।'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# LXMERT\\n\\n## Overview'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 746}, page_content='-->\\n\\n# LXMERT\\n\\n## Overview\\n\\nThe LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders\\n(one for the vision modality, one for the language modality, and then one to fuse both modalities) pretrained using a\\ncombination of masked language modeling, visual-language text alignment, ROI-feature regression, masked\\nvisual-attribute modeling, masked visual-object modeling, and visual-question answering objectives. The pretraining\\nconsists of multiple multi-modal datasets: MSCOCO, Visual-Genome + Visual-Genome Question Answering, VQA 2.0, and GQA.\\n\\nThe abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 1502}, page_content='*Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly,\\nthe alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality\\nEncoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we\\nbuild a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language\\nencoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language\\nsemantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative\\npretraining tasks: masked language modeling, masked object prediction (feature regression and label classification),\\ncross-modality matching, and image question answering. These tasks help in learning both intra-modality and'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 2422}, page_content='cross-modality relationships. After fine-tuning from our pretrained parameters, our model achieves the state-of-the-art\\nresults on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our\\npretrained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR, and improve the previous\\nbest result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel\\nmodel components and pretraining strategies significantly contribute to our strong results; and also present several\\nattention visualizations for the different encoders*'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 3063}, page_content='This model was contributed by [eltoto1219](https://huggingface.co/eltoto1219). The original code can be found [here](https://github.com/airsplay/lxmert).\\n\\n## Usage tips'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 3218}, page_content='## Usage tips\\n\\n- Bounding boxes are not necessary to be used in the visual feature embeddings, any kind of visual-spacial features\\n  will work.\\n- Both the language hidden states and the visual hidden states that LXMERT outputs are passed through the\\n  cross-modality layer, so they contain information from both modalities. To access a modality that only attends to\\n  itself, select the vision/language hidden states from the first input in the tuple.\\n- The bidirectional cross-modality encoder attention only returns attention values when the language modality is used\\n  as the input and the vision modality is used as the context vector. Further, while the cross-modality encoder\\n  contains self-attention for each respective modality and cross-attention, only the cross attention is returned and\\n  both self attention outputs are disregarded.\\n\\n## Resources\\n\\n- [Question answering task guide](../tasks/question_answering)\\n\\n## LxmertConfig\\n\\n[[autodoc]] LxmertConfig\\n\\n## LxmertTokenizer'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 4143}, page_content='## LxmertConfig\\n\\n[[autodoc]] LxmertConfig\\n\\n## LxmertTokenizer\\n\\n[[autodoc]] LxmertTokenizer\\n\\n## LxmertTokenizerFast\\n\\n[[autodoc]] LxmertTokenizerFast\\n\\n## Lxmert specific outputs\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertModelOutput\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput\\n\\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput\\n\\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\\n\\n<frameworkcontent>\\n<pt>\\n\\n## LxmertModel\\n\\n[[autodoc]] LxmertModel\\n    - forward\\n\\n## LxmertForPreTraining\\n\\n[[autodoc]] LxmertForPreTraining\\n    - forward\\n\\n## LxmertForQuestionAnswering\\n\\n[[autodoc]] LxmertForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFLxmertModel\\n\\n[[autodoc]] TFLxmertModel\\n    - call\\n\\n## TFLxmertForPreTraining\\n\\n[[autodoc]] TFLxmertForPreTraining\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 1}, page_content=\"The tokenization pipeline\\n\\nWhen calling `Tokenizer.encode` or\\n`Tokenizer.encode_batch`, the input\\ntext(s) go through the following pipeline:\\n\\n-   `normalization`\\n-   `pre-tokenization`\\n-   `model`\\n-   `post-processing`\\n\\nWe'll see in details what happens during each of those steps in detail,\\nas well as when you want to `decode <decoding>` some token ids, and how the 🤗 Tokenizers library allows you\\nto customize each of those steps to your needs. If you're already\\nfamiliar with those steps and want to learn by seeing some code, jump to\\n`our BERT from scratch example <example>`.\\n\\nFor the examples that require a `Tokenizer` we will use the tokenizer we trained in the\\n`quicktour`, which you can load with:\"),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 711}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START reload_tokenizer\",\\n\"end-before\": \"END reload_tokenizer\",\\n\"dedent\": 12}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_reload_tokenizer\",\\n\"end-before\": \"END pipeline_reload_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START reload_tokenizer\",\\n\"end-before\": \"END reload_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\n## Normalization'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 1465}, page_content='## Normalization\\n\\nNormalization is, in a nutshell, a set of operations you apply to a raw\\nstring to make it less random or \"cleaner\". Common operations include\\nstripping whitespace, removing accented characters or lowercasing all\\ntext. If you\\'re familiar with [Unicode\\nnormalization](https://unicode.org/reports/tr15), it is also a very\\ncommon normalization operation applied in most tokenizers.\\n\\nEach normalization operation is represented in the 🤗 Tokenizers library\\nby a `Normalizer`, and you can combine\\nseveral of those by using a `normalizers.Sequence`. Here is a normalizer applying NFD Unicode normalization\\nand removing accents as an example:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 2118}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START setup_normalizer\",\\n\"end-before\": \"END setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_setup_normalizer\",\\n\"end-before\": \"END pipeline_setup_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START setup_normalizer\",\\n\"end-before\": \"END setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nYou can manually test that normalizer by applying it to any string:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 2871}, page_content='You can manually test that normalizer by applying it to any string:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START test_normalizer\",\\n\"end-before\": \"END test_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_test_normalizer\",\\n\"end-before\": \"END pipeline_test_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START test_normalizer\",\\n\"end-before\": \"END test_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nWhen building a `Tokenizer`, you can\\ncustomize its normalizer by just changing the corresponding attribute:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 3796}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START replace_normalizer\",\\n\"end-before\": \"END replace_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_replace_normalizer\",\\n\"end-before\": \"END pipeline_replace_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START replace_normalizer\",\\n\"end-before\": \"END replace_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nOf course, if you change the way a tokenizer applies normalization, you\\nshould probably retrain it from scratch afterward.\\n\\n## Pre-Tokenization'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 4685}, page_content='## Pre-Tokenization\\n\\nPre-tokenization is the act of splitting a text into smaller objects\\nthat give an upper bound to what your tokens will be at the end of\\ntraining. A good way to think of this is that the pre-tokenizer will\\nsplit your text into \"words\" and then, your final tokens will be parts\\nof those words.\\n\\nAn easy way to pre-tokenize inputs is to split on spaces and\\npunctuations, which is done by the\\n`pre_tokenizers.Whitespace`\\npre-tokenizer:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 5139}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START setup_pre_tokenizer\",\\n\"end-before\": \"END setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_setup_pre_tokenizer\",\\n\"end-before\": \"END pipeline_setup_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START setup_pre_tokenizer\",\\n\"end-before\": \"END setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 5910}, page_content='The output is a list of tuples, with each tuple containing one word and\\nits span in the original sentence (which is used to determine the final\\n`offsets` of our `Encoding`). Note that splitting on\\npunctuation will split contractions like `\"I\\'m\"` in this example.\\n\\nYou can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that will\\nsplit on space, punctuation and digits, separating numbers in their\\nindividual digits:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 6364}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START combine_pre_tokenizer\",\\n\"end-before\": \"END combine_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_combine_pre_tokenizer\",\\n\"end-before\": \"END pipeline_combine_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START combine_pre_tokenizer\",\\n\"end-before\": \"END combine_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nAs we saw in the `quicktour`, you can\\ncustomize the pre-tokenizer of a `Tokenizer` by just changing the corresponding attribute:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 7277}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START replace_pre_tokenizer\",\\n\"end-before\": \"END replace_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_replace_pre_tokenizer\",\\n\"end-before\": \"END pipeline_replace_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START replace_pre_tokenizer\",\\n\"end-before\": \"END replace_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nOf course, if you change the way the pre-tokenizer, you should probably\\nretrain your tokenizer from scratch afterward.\\n\\n## Model'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 8180}, page_content='## Model\\n\\nOnce the input texts are normalized and pre-tokenized, the\\n`Tokenizer` applies the model on the\\npre-tokens. This is the part of the pipeline that needs training on your\\ncorpus (or that has been trained if you are using a pretrained\\ntokenizer).\\n\\nThe role of the model is to split your \"words\" into tokens, using the\\nrules it has learned. It\\'s also responsible for mapping those tokens to\\ntheir corresponding IDs in the vocabulary of the model.\\n\\nThis model is passed along when intializing the\\n`Tokenizer` so you already know how to\\ncustomize this part. Currently, the 🤗 Tokenizers library supports:\\n\\n-   `models.BPE`\\n-   `models.Unigram`\\n-   `models.WordLevel`\\n-   `models.WordPiece`\\n\\nFor more details about each model and its behavior, you can check\\n[here](components#models)\\n\\n## Post-Processing\\n\\nPost-processing is the last step of the tokenization pipeline, to\\nperform any additional transformation to the\\n`Encoding` before it\\'s returned, like\\nadding potential special tokens.'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 9170}, page_content='As we saw in the quick tour, we can customize the post processor of a\\n`Tokenizer` by setting the\\ncorresponding attribute. For instance, here is how we can post-process\\nto make the inputs suitable for the BERT model:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START setup_processor\",\\n\"end-before\": \"END setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_setup_processor\",\\n\"end-before\": \"END pipeline_setup_processor\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START setup_processor\",\\n\"end-before\": \"END setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 10134}, page_content=\"Note that contrarily to the pre-tokenizer or the normalizer, you don't\\nneed to retrain a tokenizer after changing its post-processor.\\n\\n## All together: a BERT tokenizer from scratch\\n\\nLet's put all those pieces together to build a BERT tokenizer. First,\\nBERT relies on WordPiece, so we instantiate a new\\n`Tokenizer` with this model:\"),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 10467}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_tokenizer\",\\n\"end-before\": \"END bert_setup_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_tokenizer\",\\n\"end-before\": \"END bert_setup_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_tokenizer\",\\n\"end-before\": \"END bert_setup_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThen we know that BERT preprocesses texts by removing accents and\\nlowercasing. We also use a unicode normalizer:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 11340}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_normalizer\",\\n\"end-before\": \"END bert_setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_normalizer\",\\n\"end-before\": \"END bert_setup_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_normalizer\",\\n\"end-before\": \"END bert_setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThe pre-tokenizer is just splitting on whitespace and punctuation:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 12105}, page_content='The pre-tokenizer is just splitting on whitespace and punctuation:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_pre_tokenizer\",\\n\"end-before\": \"END bert_setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_pre_tokenizer\",\\n\"end-before\": \"END bert_setup_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_pre_tokenizer\",\\n\"end-before\": \"END bert_setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nAnd the post-processing uses the template we saw in the previous\\nsection:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 12956}, page_content='And the post-processing uses the template we saw in the previous\\nsection:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_processor\",\\n\"end-before\": \"END bert_setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_processor\",\\n\"end-before\": \"END bert_setup_processor\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_processor\",\\n\"end-before\": \"END bert_setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nWe can use this tokenizer and train on it on wikitext like in the\\n`quicktour`:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 13790}, page_content='We can use this tokenizer and train on it on wikitext like in the\\n`quicktour`:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_train_tokenizer\",\\n\"end-before\": \"END bert_train_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_train_tokenizer\",\\n\"end-before\": \"END bert_train_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_train_tokenizer\",\\n\"end-before\": \"END bert_train_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\n## Decoding'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 14629}, page_content=\"## Decoding\\n\\nOn top of encoding the input texts, a `Tokenizer` also has an API for decoding, that is converting IDs\\ngenerated by your model back to a text. This is done by the methods\\n`Tokenizer.decode` (for one predicted text) and `Tokenizer.decode_batch` (for a batch of predictions).\\n\\nThe `decoder` will first convert the IDs back to tokens\\n(using the tokenizer's vocabulary) and remove all special tokens, then\\njoin those tokens with spaces:\"),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 15076}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START test_decoding\",\\n\"end-before\": \"END test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_test_decoding\",\\n\"end-before\": \"END pipeline_test_decoding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START test_decoding\",\\n\"end-before\": \"END test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 15811}, page_content='If you used a model that added special characters to represent subtokens\\nof a given \"word\" (like the `\"##\"` in\\nWordPiece) you will need to customize the `decoder` to treat\\nthem properly. If we take our previous `bert_tokenizer` for instance the\\ndefault decoding will give:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 16085}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_test_decoding\",\\n\"end-before\": \"END bert_test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_test_decoding\",\\n\"end-before\": \"END bert_test_decoding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_test_decoding\",\\n\"end-before\": \"END bert_test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nBut by changing it to a proper decoder, we get:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 16832}, page_content='But by changing it to a proper decoder, we get:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_proper_decoding\",\\n\"end-before\": \"END bert_proper_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_proper_decoding\",\\n\"end-before\": \"END bert_proper_decoding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_proper_decoding\",\\n\"end-before\": \"END bert_proper_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 1}, page_content=\"TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https://paperswithcode.com/method/resnet) that aim to boost accuracy while maintaining GPU training and inference efficiency.  They contain several design tricks including a SpaceToDepth stem, [Anti-Alias downsampling](https://paperswithcode.com/method/anti-alias-downsampling), In-Place Activated BatchNorm, Blocks selection and [squeeze-and-excitation layers](https://paperswithcode.com/method/squeeze-and-excitation-block).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('tresnet_l', pretrained=True)\\nmodel.eval()\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 639}, page_content='```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 1376}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 2129}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tresnet_l`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model('tresnet_l', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 2721}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{ridnik2020tresnet,\\n      title={TResNet: High Performance GPU-Dedicated Architecture}, \\n      author={Tal Ridnik and Hussam Lawen and Asaf Noy and Emanuel Ben Baruch and Gilad Sharir and Itamar Friedman},\\n      year={2020},\\n      eprint={2003.13630},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 3423}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 3428}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: TResNet\\n  Paper:\\n    Title: 'TResNet: High Performance GPU-Dedicated Architecture'\\n    URL: https://paperswithcode.com/paper/tresnet-high-performance-gpu-dedicated\\nModels:\\n- Name: tresnet_l\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 10873416792\\n    Parameters: 53456696\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_l\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 4305}, page_content=\"Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L267\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_81_5-235b486c.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81.49%\\n      Top 5 Accuracy: 95.62%\\n- Name: tresnet_l_448\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 43488238584\\n    Parameters: 53456696\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 5204}, page_content=\"- AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_l_448\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L285\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_448-940d0cd1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82.26%\\n      Top 5 Accuracy: 95.98%\\n- Name: tresnet_m\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 5733048064\\n    Parameters: 41282200\\n    File Size: 125861314\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 6096}, page_content=\"- Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    Training Time: < 24 hours\\n    ID: tresnet_m\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L261\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_80_8-dbc13962.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.8%\\n      Top 5 Accuracy: 94.86%\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 7002}, page_content=\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.8%\\n      Top 5 Accuracy: 94.86%\\n- Name: tresnet_m_448\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 22929743104\\n    Parameters: 29278464\\n    File Size: 125861314\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_m_448\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L279\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 7982}, page_content=\"Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_448-bc359d10.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81.72%\\n      Top 5 Accuracy: 95.57%\\n- Name: tresnet_xl\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 15162534034\\n    Parameters: 75646610\\n    File Size: 314378965\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_xl\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 8875}, page_content=\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L273\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_xl_82_0-a2d51b00.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82.05%\\n      Top 5 Accuracy: 95.93%\\n- Name: tresnet_xl_448\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 60641712730\\n    Parameters: 75646610\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 9798}, page_content=\"- AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_xl_448\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L291\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_448-940d0cd1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83.06%\\n      Top 5 Accuracy: 96.19%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-configure.md', 'start_index': 1}, page_content='Configure the Dataset Viewer\\n\\nThe Dataset Viewer supports many [data files formats](./datasets-adding#file-formats), from text to tabular and from image to audio formats.\\nIt also separates the train/validation/test splits based on file and folder names.\\n\\nTo configure the Dataset Viewer for your dataset, first make sure your dataset is in a [supported data format](./datasets-adding#files-formats).\\n\\n## Configure dropdowns for splits or subsets\\n\\nIn the Dataset Viewer you can view the [train/validation/test](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) splits of datasets, and sometimes additionally choose between multiple subsets (e.g. one per language).\\n\\nTo define those dropdowns, you can name the data files or their folder after their split names (train/validation/test).\\nIt is also possible to customize your splits manually using YAML.'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-configure.md', 'start_index': 875}, page_content=\"For more information, feel free to check out the documentation on [Data files Configuration](./datasets-data-files-configuration).\\n\\n## Disable the viewer\\n\\nThe dataset viewer can be disabled. To do this, add a YAML section to the dataset's `README.md` file (create one if it does not already exist) and add a `viewer` property with the value `false`.\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-configure.md', 'start_index': 1226}, page_content='```\\n---\\nviewer: false\\n---\\n```\\n\\nNote that the viewer is always disabled on the private datasets.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 0}, page_content='Author: [@vasudevgupta7](https://github.com/thevasudevgupta/)\\n\\n## Intro\\n\\nIn this project, we fine-tuned [**BigBird**](https://arxiv.org/abs/2007.14062) on [**natural-questions**](https://huggingface.co/datasets/natural_questions) dataset for **question-answering** task on long documents. **BigBird**, is a **sparse-attention based transformer** which extends Transformer based models, such as BERT to much **longer sequences**.\\n\\nRead more about BigBird at https://huggingface.co/blog/big-bird\\n\\n## Fine-tuning\\n\\n**Setup**\\n\\nYou need to install jax yourself by following the official docs ([refer this](https://github.com/google/jax#installation)). Other requirements for this project can be installed by running following command:\\n\\n```shell\\npip3 install -qr requirements.txt'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 773}, page_content=\"```\\n\\n**Download & prepare dataset**\\n\\nThe Natural Questions corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. This corpus takes ~100 GB on disk. We have used HuggingFace datasets to download & process the dataset.\\n\\n```shell\\n# just run following CMD\\npython3 prepare_natural_questions.py\\n\\n# this will download the whole dataset from HuggingFace Hub & will make it ready for training\\n# this script takes ~3 hours to process the dataset\\n```\\n\\n**Launch Training**\\n\\nWe have trained on Cloud's TPU v3-8. Each epoch took around 4.5 hours and the model got converged in just 2 epochs. You can see complete training args in [this script](bigbird_flax.py).\\n\\n```shell\\n# just run following CMD\\npython3 train.py\\n\\n# In case, you want to try hparams tuning, you can run wandb sweep\\nwandb sweep --project=bigbird sweep_flax.yaml\\nwandb agent <agent-id-obtained-by-above-CMD>\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 1755}, page_content='```\\n\\n## Evaluation\\n\\nOur evaluation script is different from the original script and we are evaluating sequences with length up to 4096 for simplicity. We managed to get the **EM score of ~55.2** using our evaluation script.\\n\\n```shell\\n# download validation-dataset first\\nmkdir natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/natural_questions-validation.arrow -P natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/dataset_info.json -P natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/state.json -P natural-questions-validation\\n\\n# simply run following command\\npython3 evaluate.py'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 2548}, page_content='```\\n\\nYou can find our checkpoint on HuggingFace Hub ([see this](https://huggingface.co/vasudevgupta/flax-bigbird-natural-questions)). In case you are interested in PyTorch BigBird fine-tuning, you can refer to [this repositary](https://github.com/thevasudevgupta/bigbird).'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 0}, page_content='--\\ntitle: Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\nthumbnail: /blog/assets/30_clip_rsicd/clip_schematic.png\\nauthors:\\n- user: arampacha\\n  guest: true\\n- user: devv\\n  guest: true\\n- user: goutham794\\n  guest: true\\n- user: cataluna84\\n  guest: true\\n- user: ghosh-r\\n  guest: true\\n- user: sujitpal\\n  guest: true\\n---\\n\\n# Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\n\\n\\n\\n## Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\n\\n<img src=\"/blog/assets/30_clip_rsicd/clip-rsicd-header-image.png\"/>\\n\\nIn July this year, [Hugging Face](https://huggingface.co/) organized a [Flax/JAX Community Week](https://github.com/huggingface/transformers/blob/master/examples/research_projects/jax-projects/README.md), and invited the community to submit projects to train Hugging Face [transformers](https://github.com/huggingface/transformers) models in the areas of Natural Language Processing (NLP) and Computer Vision (CV).'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 968}, page_content='Participants used Tensor Processing Units (TPUs) with [Flax](https://github.com/google/flax) and [JAX](https://github.com/google/jax). JAX is a linear algebra library (like `numpy`) that can do automatic differentiation ([Autograd](https://github.com/hips/autograd)) and compile down to [XLA](https://www.tensorflow.org/xla), and Flax is a neural network library and ecosystem for JAX. TPU compute time was provided free by [Google Cloud](https://cloud.google.com/), who co-sponsored the event.\\n\\nOver the next two weeks, teams participated in lectures from Hugging Face and Google, trained one or more models using JAX/Flax, shared them with the community, and provided a  [Hugging Face Spaces](https://huggingface.co/spaces) demo showcasing the capabilities of their model. Approximately 100 teams participated in the event, and it resulted in 170 models and 36 demos.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 1839}, page_content='Our team, like probably many others, is a distributed one, spanning 12 time zones. Our common thread is that we all belong to the [TWIML Slack Channel](https://twimlai.slack.com/), where we came together based on a shared interest in Artificial Intelligence (AI) and Machine Learning (ML) topics.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 2138}, page_content='We fine-tuned the [CLIP Network from OpenAI](https://openai.comclip/) with satellite images and captions from the [RSICD dataset](https://github.com/201528014227051/RSICD_optimal). The CLIP network learns visual concepts by being trained with image and caption pairs in a self-supervised manner, by using text paired with images found across the Internet. During inference, the model can predict the most relevant image given a text description or the most relevant text description given an image. CLIP is powerful enough to be used in zero-shot manner on everyday images. However, we felt that satellite images were sufficiently different from everyday images that it would be useful to fine-tune CLIP with them. Our intuition turned out to be correct, as the evaluation results (described below) shows. In this post, we describe details of our training and evaluation process, and our plans for future work on this project.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 3066}, page_content='The goal of our project was to provide a useful service and demonstrate how to use CLIP for practical use cases. Our model can be used by applications to search through large collections of satellite images using textual queries. Such queries could describe the image in totality (for example, beach, mountain, airport, baseball field, etc) or search or mention specific geographic or man-made features within these images. CLIP can similarly be fine-tuned for other domains as well, as shown by the [medclip-demo team](https://huggingface.co/spaces/flax-community/medclip-demo) for medical images.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 3666}, page_content='The ability to search through large collections of images using text queries is an immensely powerful feature, and can be used as much for social good as for malign purposes. Possible applications include national defense and anti-terrorism activities, the ability to spot and address effects of climate change before they become unmanageable, etc. Unfortunately, this power can also be misused, such as for military and police surveillance by authoritarian nation-states, so it does raise some ethical questions as well.\\n\\nYou can read about the project on our [project page](https://github.com/arampacha/CLIP-rsicd), download our [trained model](https://huggingface.co/flax-community/clip-rsicd-v2) to use for inference on your own data, or see it in action on our [demo](https://huggingface.co/spaces/sujitpal/clip-rsicd-demo).\\n\\n\\n### Training\\n\\n#### Dataset'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 4498}, page_content='### Training\\n\\n#### Dataset\\n\\nWe fine-tuned the CLIP model primarily with the [RSICD dataset](https://github.com/201528014227051/RSICD_optimal). This dataset consists of about 10,000 images collected from Google Earth, Baidu Map, MapABC, and Tianditu. It is provided freely to the research community to advance remote sensing captioning via [Exploring Models and Data for Remote Sensing Image Caption Generation](https://arxiv.org/abs/1712.0783) (Lu et al, 2017). The images are (224, 224) RGB images at various resolutions, and each image has up to 5 captions associated with it.\\n\\n<img src=\"/blog/assets/30_clip_rsicd/rsicd-images-sampling.png\"/>\\n<center><i>Some examples of images from the RSICD dataset</i></center>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 5216}, page_content='In addition, we used the [UCM Dataset](https://mega.nz/folder/wCpSzSoS#RXzIlrv--TDt3ENZdKN8JA) and the [Sydney dataset](https://mega.nz/folder/pG4yTYYA#4c4buNFLibryZnlujsrwEQ) for training, The UCM dataset is based on the UC Merced Land Use dataset. It consists of 2100 images belonging to 21 classes (100 images per class), and each image has 5 captions. The Sydney dataset contains images of Sydney, Australia from Google Earth. It contains 613 images belonging to 7 classes. Images are (500, 500) RGB and provides 5 captions for each image. We used these additional datasets because we were not sure if the RSICD dataset would be large enough to fine-tune CLIP.\\n\\n\\n#### Model'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 5883}, page_content='#### Model\\n\\nOur model is just the fine-tuned version of the original CLIP model shown below. Inputs to the model are a batch of captions and a batch of images passed through the CLIP text encoder and image encoder respectively. The training process uses [contrastive learning](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607) to learn a joint embedding representation of image and captions. In this embedding space, images and their respective captions are pushed close together, as are similar images and similar captions. Conversely, images and captions for different images, or dissimilar images and captions, are likely to be pushed further apart.\\n\\n<img src=\"/blog/assets/30_clip_rsicd/clip_schematic.png\"/>\\n<center><i>CLIP Training and Inference (Image Credit: CLIP: Connecting Text and Images (https://openai.comclip/))</i></center>\\n\\n\\n#### Data Augmentation'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 6755}, page_content=\"#### Data Augmentation\\n\\nIn order to regularize our dataset and prevent overfitting due to the size of the dataset, we used both image and text augmentation.\\n\\nImage augmentation was done inline using built-in transforms from Pytorch's [Torchvision](https://pytorch.org/vision/stable/index.html) package. The transformations used were Random Cropping, Random Resizing and Cropping, Color Jitter, and Random Horizontal and Vertical flipping.\\n\\nWe augmented the text with backtranslation to generate captions for images with less than 5 unique captions per image. The [Marian MT]((https://huggingface.co/transformers/model_doc/marian.html)) family of models from Hugging Face was used to translate the existing captions into French, Spanish, Italian, and Portuguese and back to English to fill out the captions for these images.\\n\\nAs shown in these loss plots below, image augmentation reduced overfitting significantly, and text and image augmentation reduced overfitting even further.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 7737}, page_content='<img src=\"/blog/assets/30_clip_rsicd/image-augment-loss.png\"/>\\n<img src=\"/blog/assets/30_clip_rsicd/image-text-aug-loss.png\"/>\\n<center><i>Evaluation and Training loss plots comparing (top) no augmentation vs image augmentation, and (bottom) image augmentation vs text+image augmentation</i></center>\\n\\n\\n### Evaluation\\n\\n#### Metrics\\n\\nA subset of the RSICD test set was used for evaluation. We found 30 categories of images in this subset. The evaluation was done by comparing each image with a set of 30 caption sentences of the form `\"An aerial photograph of {category}\"`. The model produced a ranked list of the 30 captions, from most relevant to least relevant. Categories corresponding to captions with the top k scores (for k=1, 3, 5, and 10) were compared with the category provided via the image file name. The scores are averaged over the entire set of images used for evaluation and reported for various values of k, as shown below.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 8678}, page_content='The `baseline` model represents the pre-trained `openai/clip-vit-base-path32` CLIP model. This model was fine-tuned with captions and images from the RSICD dataset, which resulted in a significant performance boost, as shown below.\\n\\nOur best model was trained with image and text augmentation, with batch size 1024 (128 on each of the 8 TPU cores), and the Adam optimizer with learning rate 5e-6. We trained our second base model with the same hyperparameters, except that we used the Adafactor optimizer with learning rate 1e-4. You can download either model from their model repos linked to in the table below.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 9292}, page_content='| Model-name                               | k=1   | k=3   | k=5   | k=10  |\\n| ---------------------------------------- | ----- | ----- | ----- | ----- |\\n| baseline                                 | 0.572 | 0.745 | 0.837 | 0.939 |\\n| bs128x8-lr1e-4-augs/ckpt-2               | 0.819 | 0.950 | 0.974 | 0.994 |\\n| bs128x8-lr1e-4-imgaugs/ckpt-2            | 0.812 | 0.942 | 0.970 | 0.991 |\\n| [bs128x8-lr1e-4-imgaugs-textaugs/ckpt-4](https://huggingface.co/flax-community/clip-rsicd)<sup>2</sup>   | 0.843 | 0.958 | 0.977 | 0.993 |\\n| bs128x8-lr5e-5-imgaugs-textaugs/ckpt-8   | 0.831 | 0.959 | 0.977 | 0.994 |\\n| bs128x8-lr5e-5-imgaugs/ckpt-4            | 0.746 | 0.906 | 0.956 | 0.989 |\\n| bs128x8-lr5e-5-imgaugs-textaugs-2/ckpt-4 | 0.811 | 0.945 | 0.972 | 0.993 |\\n| bs128x8-lr5e-5-imgaugs-textaugs-3/ckpt-5 | 0.823 | 0.946 | 0.971 | 0.992 |\\n| bs128x8-lr5e-5-wd02/ckpt-4               | 0.820 | 0.946 | 0.965 | 0.990 |'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 10126}, page_content='| bs128x8-lr5e-5-wd02/ckpt-4               | 0.820 | 0.946 | 0.965 | 0.990 |\\n| [bs128x8-lr5e-6-adam/ckpt-1](https://huggingface.co/flax-community/clip-rsicd-v2)<sup>1</sup> | **0.883** | **0.968** | **0.982** | **0.998** |'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 10351}, page_content='_1 - our best model, 2 - our second best model_\\n\\n\\n#### Demo\\n\\nYou can access the [CLIP-RSICD Demo](https://huggingface.co/spaces/sujitpal/clip-rsicd-demo) here. It uses our fine-tuned CLIP model to provide the following functionality:\\n\\n* Text to Image search\\n* Image to Image search\\n* Find text feature in image\\n\\nThe first two functionalities use the RSICD test set as its image corpus. They are encoded using our best fine-tuned CLIP model and stored in a [NMSLib](https://github.com/nmslib/nmslib) index which allows Approximate Nearest Neighbor based retrieval. For text-to-image and image-to-image search respectively, the query text or image are encoded with our model and matched against the image vectors in the corpus. For the third functionality, we divide the incoming image into patches and encode them, encode the queried text feature, match the text vector with each image patch vector, and return the probability of finding the feature in each patch.\\n\\n### Future Work'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 11316}, page_content='### Future Work\\n\\nWe are grateful that we have been given an opportunity to further refine our model. Some ideas we have for future work are as follows:\\n\\n1. Construct a sequence to sequence model using a CLIP encoder and a GPT-3 decoder and train it for image captioning.\\n2. Fine-tune the model on more image caption pairs from other datasets and investigate if we can improve its performance.\\n3. Investigate how fine-tuning affects the performance of model on non-RSICD image caption pairs.\\n4. Investigate the capability of the fine-tuned model to classify outside the categories it has been fine-tuned on.\\n5. Evaluate the model using other criteria such as image classification.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# 🤗 Transformers Notebooks\\n\\nYou can find here a list of the official notebooks provided by Hugging Face.\\n\\nAlso, we would like to list here interesting content created by the community.\\nIf you wrote some notebook(s) leveraging 🤗 Transformers and would like to be listed here, please open a\\nPull Request so it can be included under the Community notebooks.\\n\\n\\n## Hugging Face\\'s notebooks 🤗'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 950}, page_content=\"## Hugging Face's notebooks 🤗\\n\\n### Documentation notebooks\\n\\nYou can open any page of the documentation as a notebook in Colab (there is a button directly on said pages) but they are also listed here if you need them:\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 1168}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [Quicktour of the library](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)  | A presentation of the various APIs in Transformers |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/en/transformers_doc/quicktour.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 1816}, page_content='| [Summary of the tasks](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)  | How to run the models of the Transformers library task by task |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 2380}, page_content='| [Preprocessing data](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)  | How to use a tokenizer to preprocess your data |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 2929}, page_content='| [Fine-tuning a pretrained model](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)  | How to use the Trainer to fine-tune a pretrained model |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 3483}, page_content='| [Summary of the tokenizers](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)  | The differences between the tokenizers algorithm |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 4053}, page_content='| [Multilingual models](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)  | How to use the multilingual models of the library |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 4605}, page_content='### PyTorch Examples\\n\\n#### Natural Language Processing[[pytorch-nlp]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 4676}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 5309}, page_content='| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 5877}, page_content='| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 6475}, page_content='| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 7078}, page_content='| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/main/examples/token_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 7705}, page_content='| [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SQUAD. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 8291}, page_content='| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SWAG. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 8864}, page_content='| [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/main/examples/translation.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on WMT. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/translation.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 9420}, page_content='| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on XSUM. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 9985}, page_content='| [How to train a language model from scratch](https://github.com/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)| Highlight all the steps to effectively train Transformer model on custom data | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 10549}, page_content='| [How to generate text](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)| How to use different decoding methods for language generation with transformers | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 11102}, page_content='| [How to generate text (with constraints)](https://github.com/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)| How to guide language generation with user-provided constraints | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 11682}, page_content='| [Reformer](https://github.com/huggingface/blog/blob/main/notebooks/03_reformer.ipynb)| How Reformer pushes the limits of language modeling | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/blog/blob/main/notebooks/03_reformer.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/patrickvonplaten/blog/blob/main/notebooks/03_reformer.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 12185}, page_content='#### Computer Vision[[pytorch-cv]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 12221}, page_content='| Notebook                                                                                                                                                                   | Description                                                                                                            |                                                                                                                                                                                                            |   |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 12726}, page_content='|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------:|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 13235}, page_content='| [How to fine-tune a model on image classification (Torchvision)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                   | Show how to preprocess the data using Torchvision and fine-tune any pretrained Vision model on Image Classification    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                 | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 13923}, page_content='| [How to fine-tune a model on image classification (Albumentations)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) | Show how to preprocess the data using Albumentations and fine-tune any pretrained Vision model on Image Classification | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)  | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 14626}, page_content='| [How to fine-tune a model on image classification (Kornia)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)                 | Show how to preprocess the data using Kornia and fine-tune any pretrained Vision model on Image Classification         | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)          | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 15321}, page_content='| [How to perform zero-shot object detection with OWL-ViT](https://github.com/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)          | Show how to perform zero-shot object detection on images with text queries                                             | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 16026}, page_content='| [How to fine-tune an image captioning model](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)                                      | Show how to fine-tune BLIP for image captioning on a custom dataset                                                    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 16715}, page_content='| [How to build an image similarity system with Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)                            | Show how to build an image similarity system                                                                           | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)                     | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 17399}, page_content='| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)                     | Show how to preprocess the data and fine-tune a pretrained SegFormer model on Semantic Segmentation                    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 18088}, page_content='| [How to fine-tune a VideoMAE model on video classification](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb)          | Show how to preprocess the data and fine-tune a pretrained VideoMAE model on Video Classification                      | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/video_classification.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/video_classification.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 18762}, page_content='#### Audio[[pytorch-audio]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 18791}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to fine-tune a speech recognition model in English](https://github.com/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)| Show how to preprocess the data and fine-tune a pretrained Speech model on TIMIT | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 19492}, page_content='| [How to fine-tune a speech recognition model in any language](https://github.com/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)| Show how to preprocess the data and fine-tune a multi-lingually pretrained speech model on Common Voice | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 20162}, page_content='| [How to fine-tune a model on audio classification](https://github.com/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained Speech model on Keyword Spotting | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 20774}, page_content='#### Biological Sequences[[pytorch-bio]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 20816}, page_content='| Notebook     | Description                                                                             |   |   |\\n|:----------|:----------------------------------------------------------------------------------------|:-------------|------:|\\n| [How to fine-tune a pre-trained protein model](https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) | See how to tokenize proteins and fine-tune a large pre-trained protein \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 21679}, page_content='| [How to generate protein folds](https://github.com/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) | See how to go from protein sequence to a full protein model and PDB file                | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 22255}, page_content='| [How to fine-tune a Nucleotide Transformer model](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) | See how to tokenize DNA and fine-tune a large pre-trained DNA \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 22930}, page_content='| [Fine-tune a Nucleotide Transformer model with LoRA](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) | Train even larger DNA models in a memory-efficient way | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 23616}, page_content='#### Other modalities[[pytorch-other]]\\n\\n| Notebook     | Description                                                                             |   |   |\\n|:----------|:----------------------------------------------------------------------------------------|:-------------|------:|\\n| [Probabilistic Time Series Forecasting](https://github.com/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) | See how to train Time Series Transformer on a custom dataset                            | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) |\\n\\n#### Utility notebooks[[pytorch-utility]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 24510}, page_content='#### Utility notebooks[[pytorch-utility]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to export model to ONNX](https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb)| Highlight how to export and run inference workloads through ONNX |\\n| [How to use Benchmarks](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| How to benchmark models with transformers | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)|\\n\\n### TensorFlow Examples\\n\\n#### Natural Language Processing[[tensorflow-nlp]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 25412}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 26045}, page_content='| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 26622}, page_content='| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 27229}, page_content='| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 27841}, page_content='| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 28477}, page_content='| [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SQUAD. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 29072}, page_content='| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SWAG. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 29654}, page_content='| [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on WMT. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 30219}, page_content='| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on XSUM. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 30794}, page_content='#### Computer Vision[[tensorflow-cv]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 30833}, page_content='| Notebook                                                                                                                                                 | Description                                                                                         |   |   |\\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------|:-------------|------:|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 31382}, page_content='| [How to fine-tune a model on image classification](https://github.com/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)            | Show how to preprocess the data and fine-tune any pretrained Vision model on Image Classification   | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 32022}, page_content='| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb) | Show how to preprocess the data and fine-tune a pretrained SegFormer model on Semantic Segmentation | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 32666}, page_content='#### Biological Sequences[[tensorflow-bio]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to fine-tune a pre-trained protein model](https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | See how to tokenize proteins and fine-tune a large pre-trained protein \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) |\\n\\n#### Utility notebooks[[tensorflow-utility]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 33443}, page_content=\"#### Utility notebooks[[tensorflow-utility]]\\n\\n| Notebook     |      Description      |   |                                                                                                                                                                                      |\\n|:----------|:-------------|:-------------|------:|\\n| [How to train TF/Keras models on TPU](https://github.com/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | See how to train at high speed on Google's TPU hardware | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) |\\n\\n### Optimum notebooks\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 34320}, page_content='### Optimum notebooks\\n\\n🤗  [Optimum](https://github.com/huggingface/optimum) is an extension of 🤗 Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardwares.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 34557}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 35385}, page_content='| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| Show how to apply static, dynamic and aware training quantization on a model using [Intel Neural Compressor (INC)](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 36158}, page_content='| [How to fine-tune a model on text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)| Show how to preprocess the data and fine-tune a model on any GLUE task using [ONNX Runtime](https://github.com/microsoft/onnxruntime). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 36838}, page_content='| [How to fine-tune a model on summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)| Show how to preprocess the data and fine-tune a model on XSUM using [ONNX Runtime](https://github.com/microsoft/onnxruntime). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 37486}, page_content='## Community notebooks:\\n\\nMore notebooks developed by the community are available [here](https://hf.co/docs/transformers/community#community-notebooks).'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 0}, page_content='--\\ntitle: \"Multivariate Probabilistic Time Series Forecasting with Informer\" \\nthumbnail: /blog/assets/134_informer/thumbnail.png\\nauthors:\\n- user: elisim\\n  guest: true\\n- user: nielsr\\n- user: kashif\\n---\\n\\n# Multivariate Probabilistic Time Series Forecasting with Informer\\n\\n\\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multivariate_informer.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n## Introduction'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 617}, page_content=\"## Introduction\\n\\nA few months ago we introduced the [Time Series Transformer](https://huggingface.co/blog/time-series-transformers), which is the vanilla Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)) applied to forecasting, and showed an example for the **univariate** probabilistic forecasting task (i.e. predicting each time series' 1-d distribution individually). In this post we introduce the _Informer_ model ([Zhou, Haoyi, et al., 2021](https://arxiv.org/abs/2012.07436)), AAAI21 best paper which is [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in 🤗 Transformers. We will show how to use the Informer model for the **multivariate** probabilistic forecasting task, i.e., predicting the distribution of a future **vector** of time-series target values. Note that this will also work for the vanilla Time Series Transformer model.\\n\\n##  Multivariate Probabilistic Time Series Forecasting\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 1517}, page_content='##  Multivariate Probabilistic Time Series Forecasting\\n\\nAs far as the modeling aspect of probabilistic forecasting is concerned, the Transformer/Informer will require no change when dealing with multivariate time series. In both the univariate and multivariate setting, the model will receive a sequence of vectors and thus the only change is on the output or emission side.\\n\\nModeling the full joint conditional distribution of high dimensional data can get computationally expensive and thus methods resort to some approximation of the distribution, the easiest being to model the data as an independent distribution from the same family, or some low-rank approximation to the full covariance, etc. Here we will just resort to the independent (or diagonal) emissions which are supported for the families of distributions we have implemented [here](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils).\\n\\n## Informer - Under The Hood'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 2445}, page_content=\"## Informer - Under The Hood\\n\\nBased on the vanilla Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), Informer employs two major improvements. To understand these improvements, let's recall the drawbacks of the vanilla Transformer:\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 2699}, page_content=\"1. **Quadratic computation of canonical self-attention:** The vanilla Transformer has a computational complexity of \\\\\\\\(O(T^2 D)\\\\\\\\) where \\\\\\\\(T\\\\\\\\) is the time series length and \\\\\\\\(D\\\\\\\\) is the dimension of the hidden states. For long sequence time-series forecasting (also known as the _LSTF problem_), this might be really computationally expensive. To solve this problem, Informer employs a new self-attention mechanism called _ProbSparse_ attention, which has \\\\\\\\(O(T \\\\log T)\\\\\\\\) time and space complexity.\\n1. **Memory bottleneck when stacking layers:** When stacking \\\\\\\\(N\\\\\\\\) encoder/decoder layers, the vanilla Transformer has a memory usage of \\\\\\\\(O(N T^2)\\\\\\\\), which limits the model's capacity for long sequences. Informer uses a _Distilling_ operation, for reducing the input size between layers into its half slice. By doing so, it reduces the whole memory usage to be \\\\\\\\(O(N\\\\cdot T \\\\log T)\\\\\\\\).\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 3597}, page_content='As you can see, the motivation for the Informer model is similar to Longformer ([Beltagy et el., 2020](https://arxiv.org/abs/2004.05150)), Sparse Transformer ([Child et al., 2019](https://arxiv.org/abs/1904.10509)) and other NLP papers for reducing the quadratic complexity of the self-attention mechanism **when the input sequence is long**. Now, let\\'s dive into _ProbSparse_ attention and the _Distilling_ operation with code examples. \\n\\n### ProbSparse Attention\\n\\nThe main idea of ProbSparse is that the canonical self-attention scores form a long-tail distribution, where the \"active\" queries lie in the \"head\" scores and \"lazy\" queries lie in the \"tail\" area. By \"active\" query we mean a query \\\\\\\\(q_i\\\\\\\\) such that the dot-product \\\\\\\\(\\\\langle q_i,k_i \\\\rangle\\\\\\\\) **contributes** to the major attention, whereas a \"lazy\" query forms a dot-product which generates **trivial** attention. Here, \\\\\\\\(q_i\\\\\\\\) and \\\\\\\\(k_i\\\\\\\\) are the \\\\\\\\(i\\\\\\\\)-th rows in \\\\\\\\(Q\\\\\\\\) and \\\\\\\\(K\\\\\\\\) attention matrices respectively.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 4595}, page_content='| ![informer_full_vs_sparse_attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/informer_full_vs_sparse_attention.png) |\\n|:--:|\\n| Vanilla self attention vs ProbSparse attention from [Autoformer (Wu, Haixu, et al., 2021)](https://wuhaixu2016.github.io/pdf/NeurIPS2021_Autoformer.pdf) |\\n\\nGiven the idea of \"active\" and \"lazy\" queries, the ProbSparse attention selects the \"active\" queries, and creates a reduced query matrix \\\\\\\\(Q_{reduced}\\\\\\\\) which is used to calculate the attention weights in \\\\\\\\(O(T \\\\log T)\\\\\\\\). Let\\'s see this more in detail with a code example. \\n    \\nRecall the canonical self-attention formula:\\n\\n$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 5261}, page_content='$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nWhere \\\\\\\\(Q\\\\in \\\\mathbb{R}^{L_Q \\\\times d}\\\\\\\\), \\\\\\\\(K\\\\in \\\\mathbb{R}^{L_K \\\\times d}\\\\\\\\) and \\\\\\\\(V\\\\in \\\\mathbb{R}^{L_V \\\\times d}\\\\\\\\). Note that in practice, the input length of queries and keys are typically equivalent in the self-attention computation, i.e. \\\\\\\\(L_Q = L_K = T\\\\\\\\) where \\\\\\\\(T\\\\\\\\) is the time series length. Therefore, the \\\\\\\\(QK^T\\\\\\\\) multiplication takes \\\\\\\\(O(T^2 \\\\cdot d)\\\\\\\\) computational complexity. In ProbSparse attention, our goal is to create a new \\\\\\\\(Q_{reduce}\\\\\\\\) matrix and define:\\n\\n$$\\n\\\\textrm{ProbSparseAttention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{Q_{reduce}K^T}{\\\\sqrt{d_k}} )V\\n$$'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 5835}, page_content='$$\\n\\\\textrm{ProbSparseAttention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{Q_{reduce}K^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nwhere the \\\\\\\\(Q_{reduce}\\\\\\\\) matrix only selects the Top  \\\\\\\\(u\\\\\\\\) \"active\" queries. Here, \\\\\\\\(u = c \\\\cdot \\\\log L_Q\\\\\\\\) and \\\\\\\\(c\\\\\\\\) called the _sampling factor_ hyperparameter for the ProbSparse attention. Since \\\\\\\\(Q_{reduce}\\\\\\\\) selects only the Top \\\\\\\\(u\\\\\\\\) queries, its size is \\\\\\\\(c\\\\cdot \\\\log L_Q \\\\times d\\\\\\\\), so the multiplication \\\\\\\\(Q_{reduce}K^T\\\\\\\\) takes only \\\\\\\\(O(L_K \\\\log L_Q) = O(T \\\\log T)\\\\\\\\).\\n\\nThis is good! But how can we select the \\\\\\\\(u\\\\\\\\) \"active\" queries to create \\\\\\\\(Q_{reduce}\\\\\\\\)? Let\\'s define the _Query Sparsity Measurement_.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 6473}, page_content='#### Query Sparsity Measurement\\nQuery Sparsity Measurement \\\\\\\\(M(q_i, K)\\\\\\\\) is used for selecting the \\\\\\\\(u\\\\\\\\) \"active\" queries \\\\\\\\(q_i\\\\\\\\) in \\\\\\\\(Q\\\\\\\\) to create \\\\\\\\(Q_{reduce}\\\\\\\\). In theory, the dominant \\\\\\\\(\\\\langle q_i,k_i \\\\rangle\\\\\\\\) pairs encourage the \"active\" \\\\\\\\(q_i\\\\\\\\)\\'s probability distribution **away** from the uniform distribution as can be seen in the figure below. Hence, the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the actual queries distribution and the uniform distribution is used to define the sparsity measurement. \\n\\n| ![informer_probsparse](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/informer_probsparse.png) | \\n|:--:|\\n| The illustration of ProbSparse Attention from official [repository](https://github.com/zhouhaoyi/Informer2020)|\\n\\n\\nIn practice, the measurement is defined as:\\n\\n$$\\nM(q_i, K) = \\\\max_j \\\\frac{q_ik_j^T}{\\\\sqrt{d}}-\\\\frac{1}{L_k} \\\\sum_{j=1}^{L_k}\\\\frac{q_ik_j^T}{\\\\sqrt{d}}\\n$$'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 7474}, page_content='The important thing to understand here is when \\\\\\\\(M(q_i, K)\\\\\\\\) is larger, the query \\\\\\\\(q_i\\\\\\\\) should be in \\\\\\\\(Q_{reduce}\\\\\\\\) and vice versa.\\n\\nBut how can we calculate the term \\\\\\\\(q_ik_j^T\\\\\\\\) in non-quadratic time? Recall that most of the dot-product \\\\\\\\(\\\\langle q_i,k_i \\\\rangle\\\\\\\\) generate either way the trivial attention (i.e. long-tail distribution property), so it is enough to randomly sample a subset of keys from \\\\\\\\(K\\\\\\\\), which will be called `K_sample` in the code.\\n\\nNow, we are ready to see the code of `probsparse_attention`:\\n    \\n```python\\nfrom torch import nn\\nimport math\\n\\n\\ndef probsparse_attention(query_states, key_states, value_states, sampling_factor=5):\\n    \"\"\"\\n    Compute the probsparse self-attention.\\n    Input shape: Batch x Time x Channel'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 8239}, page_content='Note the additional `sampling_factor` input.\\n    \"\"\"\\n    # get input sizes with logs\\n    L_K = key_states.size(1)\\n    L_Q = query_states.size(1)\\n    log_L_K = np.ceil(np.log1p(L_K)).astype(\"int\").item()\\n    log_L_Q = np.ceil(np.log1p(L_Q)).astype(\"int\").item()\\n\\n    # calculate a subset of samples to slice from K and create Q_K_sample\\n    U_part = min(sampling_factor * L_Q * log_L_K, L_K)\\n\\n    # create Q_K_sample (the q_i * k_j^T term in the sparsity measurement)\\n    index_sample = torch.randint(0, L_K, (U_part,))\\n    K_sample = key_states[:, index_sample, :]\\n    Q_K_sample = torch.bmm(query_states, K_sample.transpose(1, 2))\\n\\n    # calculate the query sparsity measurement with Q_K_sample\\n    M = Q_K_sample.max(dim=-1)[0] - torch.div(Q_K_sample.sum(dim=-1), L_K)\\n\\n    # calculate u to find the Top-u queries under the sparsity measurement\\n    u = min(sampling_factor * log_L_Q, L_Q)\\n    M_top = M.topk(u, sorted=False)[1]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 9174}, page_content='# calculate Q_reduce as query_states[:, M_top]\\n    dim_for_slice = torch.arange(query_states.size(0)).unsqueeze(-1)\\n    Q_reduce = query_states[dim_for_slice, M_top]  # size: c*log_L_Q x channel\\n\\n    # and now, same as the canonical\\n    d_k = query_states.size(-1)\\n    attn_scores = torch.bmm(Q_reduce, key_states.transpose(-2, -1))  # Q_reduce x K^T\\n    attn_scores = attn_scores / math.sqrt(d_k)\\n    attn_probs = nn.functional.softmax(attn_scores, dim=-1)\\n    attn_output = torch.bmm(attn_probs, value_states)\\n\\n    return attn_output, attn_scores'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 9723}, page_content='```\\nNote that in the implementation, \\\\\\\\(U_{part}\\\\\\\\) contain \\\\\\\\(L_Q\\\\\\\\) in the calculation, for stability issues (see [this disccusion](https://discuss.huggingface.co/t/probsparse-attention-in-informer/34428) for more information).\\n\\nWe did it! Please be aware that this is only a partial implementation of the `probsparse_attention`, and the full implementation can be found in 🤗 Transformers.\\n\\n### Distilling\\n\\nBecause of the ProbSparse self-attention, the encoder’s feature map has some redundancy that can be removed. Therefore,\\nthe distilling operation is used to reduce the input size between encoder layers into its half slice, thus in theory removing this redundancy. In practice, Informer\\'s \"distilling\" operation just adds 1D convolution layers with max pooling between each of the encoder layers. Let \\\\\\\\(X_n\\\\\\\\) be the output of the \\\\\\\\(n\\\\\\\\)-th encoder layer, the distilling operation is then defined as:\\n\\n\\n$$\\nX_{n+1} = \\\\textrm{MaxPool} ( \\\\textrm{ELU}(\\\\textrm{Conv1d}(X_n))\\n$$'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 10635}, page_content=\"$$\\nX_{n+1} = \\\\textrm{MaxPool} ( \\\\textrm{ELU}(\\\\textrm{Conv1d}(X_n))\\n$$\\n\\n\\nLet's see this in code:\\n    \\n```python\\nfrom torch import nn\\n\\n# ConvLayer is a class with forward pass applying ELU and MaxPool1d\\ndef informer_encoder_forward(x_input, num_encoder_layers=3, distil=True):\\n    # Initialize the convolution layers\\n    if distil:\\n        conv_layers = nn.ModuleList([ConvLayer() for _ in range(num_encoder_layers - 1)])\\n        conv_layers.append(None)\\n    else:\\n        conv_layers = [None] * num_encoder_layers\\n    \\n    # Apply conv_layer between each encoder_layer\\n    for encoder_layer, conv_layer in zip(encoder_layers, conv_layers):\\n        output = encoder_layer(x_input)\\n        if conv_layer is not None:\\n            output = conv_layer(loutput)\\n    \\n    return output\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 11413}, page_content=\"```\\n    \\nBy reducing the input of each layer by two, we get a memory usage of \\\\\\\\(O(N\\\\cdot T \\\\log T)\\\\\\\\) instead of \\\\\\\\(O(N\\\\cdot T^2)\\\\\\\\) where \\\\\\\\(N\\\\\\\\) is the number of encoder/decoder layers. This is what we wanted!\\n    \\nThe Informer model in [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in the 🤗 Transformers library, and simply called `InformerModel`. In the sections below, we will show how to train this model on a custom multivariate time-series dataset.\\n\\n\\n## Set-up Environment\\n\\nFirst, let's install the necessary libraries: 🤗 Transformers, 🤗 Datasets, 🤗 Evaluate, 🤗 Accelerate and [GluonTS](https://github.com/awslabs/gluonts).\\n\\nAs we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches.\\n\\n\\n```python\\n!pip install -q transformers datasets evaluate accelerate gluonts ujson\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 12328}, page_content='```\\n\\n## Load Dataset\\n\\nIn this blog post, we\\'ll use the `traffic_hourly` dataset, which is available on the [Hugging Face Hub](https://huggingface.co/datasets/monash_tsf). This dataset contains the San Francisco Traffic dataset used by [Lai et al. (2017)](https://arxiv.org/abs/1703.07015). It contains 862 hourly time series showing the road occupancy rates in the range \\\\\\\\([0, 1]\\\\\\\\) on the San Francisco Bay area freeways from 2015 to 2016.\\n\\nThis dataset is part of the [Monash Time Series Forecasting](https://forecastingdata.org/) repository, a collection of time series datasets from a number of domains. It can be viewed as the [GLUE benchmark](https://gluebenchmark.com/) of time series forecasting.\\n\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"monash_tsf\", \"traffic_hourly\")'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 13136}, page_content='```\\n\\nAs can be seen, the dataset contains 3 splits: train, validation and test.\\n\\n\\n```python\\ndataset\\n\\n>>> DatasetDict({\\n        train: Dataset({\\n            features: [\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'],\\n            num_rows: 862\\n        })\\n        test: Dataset({\\n            features: [\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'],\\n            num_rows: 862\\n        })\\n        validation: Dataset({\\n            features: [\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'],\\n            num_rows: 862\\n        })\\n    })\\n```\\n\\nEach example contains a few keys, of which `start` and `target` are the most important ones. Let us have a look at the first time series in the dataset:\\n\\n\\n```python\\ntrain_example = dataset[\"train\"][0]\\ntrain_example.keys()\\n\\n>>> dict_keys([\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'])'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 14048}, page_content='```\\n\\nThe `start` simply indicates the start of the time series (as a datetime), and the `target` contains the actual values of the time series.\\n\\nThe `start` will be useful to add time related features to the time series values, as extra input to the model (such as \"month of year\"). Since we know the frequency of the data is `hourly`, we know for instance that the second value has the timestamp `2015-01-01 01:00:01`, `2015-01-01 02:00:01`, etc.\\n\\n\\n```python\\nprint(train_example[\"start\"])\\nprint(len(train_example[\"target\"]))\\n\\n>>> 2015-01-01 00:00:01\\n    17448'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 14609}, page_content='```\\n\\nThe validation set contains the same data as the training set, just for a `prediction_length` longer amount of time. This allows us to validate the model\\'s predictions against the ground truth.\\n\\nThe test set is again one `prediction_length` longer data compared to the validation set (or some multiple of `prediction_length` longer data compared to the training set for testing on multiple rolling windows).\\n\\n\\n```python\\nvalidation_example = dataset[\"validation\"][0]\\nvalidation_example.keys()\\n\\n>>> dict_keys([\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'])\\n```\\n\\nThe initial values are exactly the same as the corresponding training example. However, this example has `prediction_length=48` (48 hours, or 2 days) additional values compared to the training example. Let us verify it.\\n\\n\\n```python\\nfreq = \"1H\"\\nprediction_length = 48\\n\\nassert len(train_example[\"target\"]) + prediction_length == len(\\n    dataset[\"validation\"][0][\"target\"]\\n)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 15571}, page_content='```\\n\\nLet\\'s visualize this:\\n\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\nnum_of_samples = 150\\n\\nfigure, axes = plt.subplots()\\naxes.plot(train_example[\"target\"][-num_of_samples:], color=\"blue\")\\naxes.plot(\\n    validation_example[\"target\"][-num_of_samples - prediction_length :],\\n    color=\"red\",\\n    alpha=0.5,\\n)\\n\\nplt.show()\\n```\\n    \\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_22_0.png)\\n    \\n\\nLet\\'s split up the data:\\n\\n\\n```python\\ntrain_dataset = dataset[\"train\"]\\ntest_dataset = dataset[\"test\"]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 16124}, page_content='```\\n\\n## Update `start` to `pd.Period`\\n\\nThe first thing we\\'ll do is convert the `start` feature of each time series to a pandas `Period` index using the data\\'s `freq`:\\n\\n\\n```python\\nfrom functools import lru_cache\\n\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n@lru_cache(10_000)\\ndef convert_to_pandas_period(date, freq):\\n    return pd.Period(date, freq)\\n\\n\\ndef transform_start_field(batch, freq):\\n    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\\n    return batch\\n```\\n\\nWe now use `datasets`\\' [`set_transform`](https://huggingface.co/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.set_transform) functionality to do this on-the-fly in place:\\n\\n\\n```python\\nfrom functools import partial\\n\\ntrain_dataset.set_transform(partial(transform_start_field, freq=freq))\\ntest_dataset.set_transform(partial(transform_start_field, freq=freq))'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 17003}, page_content=\"```\\n\\nNow, let's convert the dataset into a multivariate time series using the `MultivariateGrouper` from GluonTS. This grouper will convert the individual 1-dimensional time series into a single 2D matrix.\\n\\n\\n```python\\nfrom gluonts.dataset.multivariate_grouper import MultivariateGrouper\\n\\nnum_of_variates = len(train_dataset)\\n\\ntrain_grouper = MultivariateGrouper(max_target_dim=num_of_variates)\\ntest_grouper = MultivariateGrouper(\\n    max_target_dim=num_of_variates,\\n    num_test_dates=len(test_dataset) // num_of_variates, # number of rolling test windows\\n)\\n\\nmulti_variate_train_dataset = train_grouper(train_dataset)\\nmulti_variate_test_dataset = test_grouper(test_dataset)\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 17677}, page_content='```\\n\\nNote that the target is now 2-dimensional, where the first dimension is the number of variates (number of time series) and the second is the time series values (time dimension): \\n\\n\\n```python\\nmulti_variate_train_example = multi_variate_train_dataset[0]\\nprint(\"multi_variate_train_example[\"target\"].shape =\", multi_variate_train_example[\"target\"].shape)\\n\\n>>> multi_variate_train_example[\"target\"].shape = (862, 17448)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 18098}, page_content=\"```\\n\\n## Define the Model\\n\\nNext, let's instantiate a model. The model will be trained from scratch, hence we won't use the `from_pretrained` method here, but rather randomly initialize the model from a [`config`](https://huggingface.co/docs/transformers/main/en/model_doc/informer#transformers.InformerConfig).\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 18409}, page_content='We specify a couple of additional parameters to the model:\\n- `prediction_length` (in our case, `48` hours): this is the horizon that the decoder of the Informer will learn to predict for;\\n- `context_length`: the model will set the `context_length` (input of the encoder) equal to the `prediction_length`, if no `context_length` is specified;\\n- `lags` for a given frequency: these specify an efficient \"look back\" mechanism, where we concatenate values from the past to the current values as additional features, e.g. for a `Daily` frequency we might consider a look back of `[1, 7, 30, ...]` or for `Minute` data we might consider `[1, 30, 60, 60*24, ...]` etc.;\\n- the number of time features: in our case, this will be `5` as we\\'ll add `HourOfDay`, `DayOfWeek`, ..., and `Age` features (see below).\\n\\nLet us check the default lags provided by GluonTS for the given frequency (\"hourly\"):\\n\\n\\n```python\\nfrom gluonts.time_feature import get_lags_for_frequency'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 19298}, page_content='```python\\nfrom gluonts.time_feature import get_lags_for_frequency\\n\\nlags_sequence = get_lags_for_frequency(freq)\\nprint(lags_sequence)\\n\\n>>> [1, 2, 3, 4, 5, 6, 7, 23, 24, 25, 47, 48, 49, 71, 72, 73, 95, 96, 97, 119, 120, \\n     121, 143, 144, 145, 167, 168, 169, 335, 336, 337, 503, 504, 505, 671, 672, 673, 719, 720, 721]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 19617}, page_content='```\\n\\nThis means that this would look back up to 721 hours (~30 days) for each time step, as additional features. However, the resulting feature vector would end up being of size `len(lags_sequence)*num_of_variates` which for our case will be 34480! This is not going to work so we will use our own sensible lags.\\n\\nLet us also check the default time features which GluonTS provides us:\\n\\n\\n```python\\nfrom gluonts.time_feature import time_features_from_frequency_str\\n\\ntime_features = time_features_from_frequency_str(freq)\\nprint(time_features)\\n\\n>>> [<function hour_of_day at 0x7f3809539240>, <function day_of_week at 0x7f3809539360>, <function day_of_month at 0x7f3809539480>, <function day_of_year at 0x7f38095395a0>]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 20332}, page_content='```\\n\\nIn this case, there are four additional features, namely \"hour of day\", \"day of week\", \"day of month\" and \"day of year\". This means that for each time step, we\\'ll add these features as a scalar values. For example, consider the timestamp `2015-01-01 01:00:01`. The four additional features will be:\\n\\n\\n```python\\nfrom pandas.core.arrays.period import period_array\\n\\ntimestamp = pd.Period(\"2015-01-01 01:00:01\", freq=freq)\\ntimestamp_as_index = pd.PeriodIndex(data=period_array([timestamp]))\\nadditional_features = [\\n    (time_feature.__name__, time_feature(timestamp_as_index))\\n    for time_feature in time_features\\n]\\nprint(dict(additional_features))\\n\\n>>> {\\'hour_of_day\\': array([-0.45652174]), \\'day_of_week\\': array([0.]), \\'day_of_month\\': array([-0.5]), \\'day_of_year\\': array([-0.5])}'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 21115}, page_content='```\\n\\nNote that hours and days are encoded as values between `[-0.5, 0.5]` from GluonTS. For more information about `time_features`, please see [this](https://github.com/awslabs/gluonts/blob/dev/src/gluonts/time_feature/_base.py). Besides those 4 features, we\\'ll also add an \"age\" feature as we\\'ll see later on in the data transformations.\\n\\nWe now have everything to define the model:\\n\\n\\n```python\\nfrom transformers import InformerConfig, InformerForPrediction'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 21501}, page_content='```python\\nfrom transformers import InformerConfig, InformerForPrediction\\n\\nconfig = InformerConfig(\\n    # in the multivariate setting, input_size is the number of variates in the time series per time step\\n    input_size=num_of_variates,\\n    # prediction length:\\n    prediction_length=prediction_length,\\n    # context length:\\n    context_length=prediction_length * 2,\\n    # lags value copied from 1 week before:\\n    lags_sequence=[1, 24 * 7],\\n    # we\\'ll add 5 time features (\"hour_of_day\", ..., and \"age\"):\\n    num_time_features=len(time_features) + 1,\\n    \\n    # informer params:\\n    dropout=0.1,\\n    encoder_layers=6,\\n    decoder_layers=4,\\n    # project input from num_of_variates*len(lags_sequence)+num_time_features to:\\n    d_model=64,\\n)\\n\\nmodel = InformerForPrediction(config)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 22281}, page_content=\"```\\n\\nBy default, the model uses a diagonal Student-t distribution (but this is [configurable](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils)):\\n\\n\\n```python\\nmodel.config.distribution_output\\n\\n>>> 'student_t'\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 22516}, page_content=\"```\\n\\n## Define Transformations\\n\\nNext, we define the transformations for the data, in particular for the creation of the time features (based on the dataset or universal ones).\\n\\nAgain, we'll use the GluonTS library for this. We define a `Chain` of transformations (which is a bit comparable to `torchvision.transforms.Compose` for images). It allows us to combine several transformations into a single pipeline.\\n\\n\\n```python\\nfrom gluonts.time_feature import TimeFeature\\nfrom gluonts.dataset.field_names import FieldName\\nfrom gluonts.transform import (\\n    AddAgeFeature,\\n    AddObservedValuesIndicator,\\n    AddTimeFeatures,\\n    AsNumpyArray,\\n    Chain,\\n    ExpectedNumInstanceSampler,\\n    InstanceSplitter,\\n    RemoveFields,\\n    SelectFields,\\n    SetField,\\n    TestSplitSampler,\\n    Transformation,\\n    ValidationSplitSampler,\\n    VstackFeatures,\\n    RenameFields,\\n)\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 23381}, page_content='```\\n\\nThe transformations below are annotated with comments, to explain what they do. At a high level, we will iterate over the individual time series of our dataset and add/remove fields or features:\\n\\n\\n```python\\nfrom transformers import PretrainedConfig\\n\\n\\ndef create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\\n    # create list of fields to remove later\\n    remove_field_names = []\\n    if config.num_static_real_features == 0:\\n        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\\n    if config.num_dynamic_real_features == 0:\\n        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\\n    if config.num_static_categorical_features == 0:\\n        remove_field_names.append(FieldName.FEAT_STATIC_CAT)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 24125}, page_content='return Chain(\\n        # step 1: remove static/dynamic fields if not specified\\n        [RemoveFields(field_names=remove_field_names)]\\n        # step 2: convert the data to NumPy (potentially not needed)\\n        + (\\n            [\\n                AsNumpyArray(\\n                    field=FieldName.FEAT_STATIC_CAT,\\n                    expected_ndim=1,\\n                    dtype=int,\\n                )\\n            ]\\n            if config.num_static_categorical_features > 0\\n            else []\\n        )\\n        + (\\n            [\\n                AsNumpyArray(\\n                    field=FieldName.FEAT_STATIC_REAL,\\n                    expected_ndim=1,\\n                )\\n            ]\\n            if config.num_static_real_features > 0\\n            else []\\n        )\\n        + [\\n            AsNumpyArray(\\n                field=FieldName.TARGET,\\n                # we expect an extra dim for the multivariate case:\\n                expected_ndim=1 if config.input_size == 1 else 2,\\n            ),'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 25046}, page_content=\"expected_ndim=1 if config.input_size == 1 else 2,\\n            ),\\n            # step 3: handle the NaN's by filling in the target with zero\\n            # and return the mask (which is in the observed values)\\n            # true for observed values, false for nan's\\n            # the decoder uses this mask (no loss is incurred for unobserved values)\\n            # see loss_weights inside the xxxForPrediction model\\n            AddObservedValuesIndicator(\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName.OBSERVED_VALUES,\\n            ),\\n            # step 4: add temporal features based on freq of the dataset\\n            # these serve as positional encodings\\n            AddTimeFeatures(\\n                start_field=FieldName.START,\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName.FEAT_TIME,\\n                time_features=time_features_from_frequency_str(freq),\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 25927}, page_content='time_features=time_features_from_frequency_str(freq),\\n                pred_length=config.prediction_length,\\n            ),\\n            # step 5: add another temporal feature (just a single number)\\n            # tells the model where in the life the value of the time series is\\n            # sort of running counter\\n            AddAgeFeature(\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName.FEAT_AGE,\\n                pred_length=config.prediction_length,\\n                log_scale=True,\\n            ),\\n            # step 6: vertically stack all the temporal features into the key FEAT_TIME\\n            VstackFeatures(\\n                output_field=FieldName.FEAT_TIME,\\n                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\\n                + (\\n                    [FieldName.FEAT_DYNAMIC_REAL]\\n                    if config.num_dynamic_real_features > 0\\n                    else []\\n                ),\\n            ),'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 26853}, page_content='else []\\n                ),\\n            ),\\n            # step 7: rename to match HuggingFace names\\n            RenameFields(\\n                mapping={\\n                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\\n                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\\n                    FieldName.FEAT_TIME: \"time_features\",\\n                    FieldName.TARGET: \"values\",\\n                    FieldName.OBSERVED_VALUES: \"observed_mask\",\\n                }\\n            ),\\n        ]\\n    )'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 27372}, page_content='```\\n\\n## Define `InstanceSplitter`\\n\\nFor training/validation/testing we next create an `InstanceSplitter` which is used to sample windows from the dataset (as, remember, we can\\'t pass the entire history of values to the model due to time- and memory constraints).\\n\\nThe instance splitter samples random `context_length` sized and subsequent `prediction_length` sized windows from the data, and appends a `past_` or `future_` key to any temporal keys in `time_series_fields` for the respective windows. The instance splitter can be configured into three different modes:\\n1. `mode=\"train\"`: Here we sample the context and prediction length windows randomly from the dataset given to it (the training dataset)\\n2. `mode=\"validation\"`: Here we sample the very last context length window and prediction window from the dataset given to it (for the back-testing or validation likelihood calculations)\\n3. `mode=\"test\"`: Here we sample the very last context length window only (for the prediction use case)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 28369}, page_content='```python\\nfrom gluonts.transform.sampler import InstanceSampler\\nfrom typing import Optional\\n\\n\\ndef create_instance_splitter(\\n    config: PretrainedConfig,\\n    mode: str,\\n    train_sampler: Optional[InstanceSampler] = None,\\n    validation_sampler: Optional[InstanceSampler] = None,\\n) -> Transformation:\\n    assert mode in [\"train\", \"validation\", \"test\"]\\n\\n    instance_sampler = {\\n        \"train\": train_sampler\\n        or ExpectedNumInstanceSampler(\\n            num_instances=1.0, min_future=config.prediction_length\\n        ),\\n        \"validation\": validation_sampler\\n        or ValidationSplitSampler(min_future=config.prediction_length),\\n        \"test\": TestSplitSampler(),\\n    }[mode]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 29061}, page_content='return InstanceSplitter(\\n        target_field=\"values\",\\n        is_pad_field=FieldName.IS_PAD,\\n        start_field=FieldName.START,\\n        forecast_start_field=FieldName.FORECAST_START,\\n        instance_sampler=instance_sampler,\\n        past_length=config.context_length + max(config.lags_sequence),\\n        future_length=config.prediction_length,\\n        time_series_fields=[\"time_features\", \"observed_mask\"],\\n    )'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 29479}, page_content='```\\n\\n## Create DataLoaders\\n\\nNext, it\\'s time to create the DataLoaders, which allow us to have batches of (input, output) pairs - or in other words (`past_values`, `future_values`).\\n\\n\\n```python\\nfrom typing import Iterable\\n\\nimport torch\\nfrom gluonts.itertools import Cached, Cyclic\\nfrom gluonts.dataset.loader import as_stacked_batches\\n\\n\\ndef create_train_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n    num_batches_per_epoch: int,\\n    shuffle_buffer_length: Optional[int] = None,\\n    cache_data: bool = True,\\n    **kwargs,\\n) -> Iterable:\\n    PREDICTION_INPUT_NAMES = [\\n        \"past_time_features\",\\n        \"past_values\",\\n        \"past_observed_mask\",\\n        \"future_time_features\",\\n    ]\\n    if config.num_static_categorical_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\\n\\n    if config.num_static_real_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 30438}, page_content='TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\\n        \"future_values\",\\n        \"future_observed_mask\",\\n    ]\\n\\n    transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(data, is_train=True)\\n    if cache_data:\\n        transformed_data = Cached(transformed_data)\\n\\n    # we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \"train\")'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 30857}, page_content='# the instance splitter will sample a window of\\n    # context length + lags + prediction length (from all the possible transformed time series, 1 in our case)\\n    # randomly from within the target time series and return an iterator.\\n    stream = Cyclic(transformed_data).stream()\\n    training_instances = instance_splitter.apply(stream)\\n    \\n    return as_stacked_batches(\\n        training_instances,\\n        batch_size=batch_size,\\n        shuffle_buffer_length=shuffle_buffer_length,\\n        field_names=TRAINING_INPUT_NAMES,\\n        output_type=torch.tensor,\\n        num_batches_per_epoch=num_batches_per_epoch,\\n    )'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 31477}, page_content='```\\n\\n\\n```python\\ndef create_backtest_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n    **kwargs,\\n):\\n    PREDICTION_INPUT_NAMES = [\\n        \"past_time_features\",\\n        \"past_values\",\\n        \"past_observed_mask\",\\n        \"future_time_features\",\\n    ]\\n    if config.num_static_categorical_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\\n\\n    if config.num_static_real_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\\n\\n    transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(data)\\n\\n    # we create a Validation Instance splitter which will sample the very last\\n    # context window seen during training only for the encoder.\\n    instance_sampler = create_instance_splitter(config, \"validation\")'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 32319}, page_content='# we apply the transformations in train mode\\n    testing_instances = instance_sampler.apply(transformed_data, is_train=True)\\n    \\n    return as_stacked_batches(\\n        testing_instances,\\n        batch_size=batch_size,\\n        output_type=torch.tensor,\\n        field_names=PREDICTION_INPUT_NAMES,\\n    )\\n\\ndef create_test_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n    **kwargs,\\n):\\n    PREDICTION_INPUT_NAMES = [\\n        \"past_time_features\",\\n        \"past_values\",\\n        \"past_observed_mask\",\\n        \"future_time_features\",\\n    ]\\n    if config.num_static_categorical_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\\n\\n    if config.num_static_real_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\\n\\n    transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(data, is_train=False)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 33247}, page_content='# We create a test Instance splitter to sample the very last\\n    # context window from the dataset provided.\\n    instance_sampler = create_instance_splitter(config, \"test\")\\n\\n    # We apply the transformations in test mode\\n    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\\n    \\n    return as_stacked_batches(\\n        testing_instances,\\n        batch_size=batch_size,\\n        output_type=torch.tensor,\\n        field_names=PREDICTION_INPUT_NAMES,\\n    )'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 33728}, page_content=\"```\\n\\n\\n```python\\ntrain_dataloader = create_train_dataloader(\\n    config=config,\\n    freq=freq,\\n    data=multi_variate_train_dataset,\\n    batch_size=256,\\n    num_batches_per_epoch=100,\\n    num_workers=2,\\n)\\n\\ntest_dataloader = create_backtest_dataloader(\\n    config=config,\\n    freq=freq,\\n    data=multi_variate_test_dataset,\\n    batch_size=32,\\n)\\n```\\n\\nLet's check the first batch:\\n\\n\\n```python\\nbatch = next(iter(train_dataloader))\\nfor k, v in batch.items():\\n    print(k, v.shape, v.type())\\n\\n>>> past_time_features torch.Size([256, 264, 5]) torch.FloatTensor\\n    past_values torch.Size([256, 264, 862]) torch.FloatTensor\\n    past_observed_mask torch.Size([256, 264, 862]) torch.FloatTensor\\n    future_time_features torch.Size([256, 48, 5]) torch.FloatTensor\\n    future_values torch.Size([256, 48, 862]) torch.FloatTensor\\n    future_observed_mask torch.Size([256, 48, 862]) torch.FloatTensor\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 34613}, page_content=\"```\\n\\nAs can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the case for NLP models), but rather `past_values`, along with `past_observed_mask`, `past_time_features` and `static_real_features`.\\n\\nThe decoder inputs consist of `future_values`, `future_observed_mask` and `future_time_features`. The `future_values` can be seen as the equivalent of `decoder_input_ids` in NLP.\\n\\nWe refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/informer#transformers.InformerModel.forward.past_values) for a detailed explanation for each of them.\\n\\n## Forward Pass\\n\\nLet's perform a single forward pass with the batch we just created:\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 35209}, page_content='## Forward Pass\\n\\nLet\\'s perform a single forward pass with the batch we just created:\\n\\n\\n```python\\n# perform forward pass\\noutputs = model(\\n    past_values=batch[\"past_values\"],\\n    past_time_features=batch[\"past_time_features\"],\\n    past_observed_mask=batch[\"past_observed_mask\"],\\n    static_categorical_features=batch[\"static_categorical_features\"]\\n    if config.num_static_categorical_features > 0\\n    else None,\\n    static_real_features=batch[\"static_real_features\"]\\n    if config.num_static_real_features > 0\\n    else None,\\n    future_values=batch[\"future_values\"],\\n    future_time_features=batch[\"future_time_features\"],\\n    future_observed_mask=batch[\"future_observed_mask\"],\\n    output_hidden_states=True,\\n)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 35922}, page_content='```\\n\\n\\n```python\\nprint(\"Loss:\", outputs.loss.item())\\n\\n>>> Loss: -1071.5718994140625'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 36005}, page_content=\"```\\n\\nNote that the model is returning a loss. This is possible as the decoder automatically shifts the `future_values` one position to the right in order to have the labels. This allows computing a loss between the predicted values and the labels. The loss is the negative log-likelihood of the predicted distribution with respect to the ground truth values and tends to negative infinity.\\n\\nAlso note that the decoder uses a causal mask to not look into the future as the values it needs to predict are in the `future_values` tensor.\\n\\n## Train the Model\\n\\nIt's time to train the model! We'll use a standard PyTorch training loop.\\n\\nWe will use the 🤗 [Accelerate](https://huggingface.co/docs/accelerate/index) library here, which automatically places the model, optimizer and dataloader on the appropriate `device`.\\n\\n\\n```python\\nfrom accelerate import Accelerator\\nfrom torch.optim import AdamW\\n\\nepochs = 25\\nloss_history = []\\n\\naccelerator = Accelerator()\\ndevice = accelerator.device\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 36896}, page_content='epochs = 25\\nloss_history = []\\n\\naccelerator = Accelerator()\\ndevice = accelerator.device\\n\\nmodel.to(device)\\noptimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\\n\\nmodel, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_dataloader,\\n)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 37196}, page_content='model.train()\\nfor epoch in range(epochs):\\n    for idx, batch in enumerate(train_dataloader):\\n        optimizer.zero_grad()\\n        outputs = model(\\n            static_categorical_features=batch[\"static_categorical_features\"].to(device)\\n            if config.num_static_categorical_features > 0\\n            else None,\\n            static_real_features=batch[\"static_real_features\"].to(device)\\n            if config.num_static_real_features > 0\\n            else None,\\n            past_time_features=batch[\"past_time_features\"].to(device),\\n            past_values=batch[\"past_values\"].to(device),\\n            future_time_features=batch[\"future_time_features\"].to(device),\\n            future_values=batch[\"future_values\"].to(device),\\n            past_observed_mask=batch[\"past_observed_mask\"].to(device),\\n            future_observed_mask=batch[\"future_observed_mask\"].to(device),\\n        )\\n        loss = outputs.loss'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 38118}, page_content='# Backpropagation\\n        accelerator.backward(loss)\\n        optimizer.step()\\n\\n        loss_history.append(loss.item())\\n        if idx % 100 == 0:\\n            print(loss.item())\\n\\n>>> -1081.978515625\\n    ...\\n    -2877.723876953125'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 38348}, page_content='```\\n\\n```python\\n# view training\\nloss_history = np.array(loss_history).reshape(-1)\\nx = range(loss_history.shape[0])\\nplt.figure(figsize=(10, 5))\\nplt.plot(x, loss_history, label=\"train\")\\nplt.title(\"Loss\", fontsize=15)\\nplt.legend(loc=\"upper right\")\\nplt.xlabel(\"iteration\")\\nplt.ylabel(\"nll\")\\nplt.show()'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 38645}, page_content=\"```\\n\\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_62_0.png)\\n    \\n\\n## Inference\\n\\nAt inference time, it's recommended to use the `generate()` method for autoregressive generation, similar to NLP models.\\n\\nForecasting involves getting data from the test instance sampler, which will sample the very last `context_length` sized window of values from each time series in the dataset, and pass it to the model. Note that we pass `future_time_features`, which are known ahead of time, to the decoder.\\n\\nThe model will autoregressively sample a certain number of values from the predicted distribution and pass them back to the decoder to return the prediction outputs:\\n\\n\\n```python\\nmodel.eval()\\n\\nforecasts_ = []\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 39368}, page_content='```python\\nmodel.eval()\\n\\nforecasts_ = []\\n\\nfor batch in test_dataloader:\\n    outputs = model.generate(\\n        static_categorical_features=batch[\"static_categorical_features\"].to(device)\\n        if config.num_static_categorical_features > 0\\n        else None,\\n        static_real_features=batch[\"static_real_features\"].to(device)\\n        if config.num_static_real_features > 0\\n        else None,\\n        past_time_features=batch[\"past_time_features\"].to(device),\\n        past_values=batch[\"past_values\"].to(device),\\n        future_time_features=batch[\"future_time_features\"].to(device),\\n        past_observed_mask=batch[\"past_observed_mask\"].to(device),\\n    )\\n    forecasts_.append(outputs.sequences.cpu().numpy())'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 40081}, page_content=\"```\\n\\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `input_size`). \\n\\nIn this case, we get `100` possible values for the next `48` hours for each of the `862` time series (for each example in the batch which is of size `1` since we only have a single multivariate time series):\\n\\n\\n```python\\nforecasts_[0].shape\\n\\n>>> (1, 100, 48, 862)\\n```\\n\\nWe'll stack them vertically, to get forecasts for all time-series in the test dataset (just in case there are more time series in the test set):\\n\\n\\n```python\\nforecasts = np.vstack(forecasts_)\\nprint(forecasts.shape)\\n\\n>>> (1, 100, 48, 862)\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 40701}, page_content='```\\n\\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values present in the test set. For that, we\\'ll use the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library, which includes the [MASE](https://huggingface.co/spaces/evaluate-metric/mase) and [sMAPE](https://huggingface.co/spaces/evaluate-metric/smape) metrics.\\n\\nWe calculate both metrics for each time series variate in the dataset:\\n\\n\\n```python\\nfrom evaluate import load\\nfrom gluonts.time_feature import get_seasonality\\n\\nmase_metric = load(\"evaluate-metric/mase\")\\nsmape_metric = load(\"evaluate-metric/smape\")\\n\\nforecast_median = np.median(forecasts, 1).squeeze(0).T\\n\\nmase_metrics = []\\nsmape_metrics = []'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 41315}, page_content='forecast_median = np.median(forecasts, 1).squeeze(0).T\\n\\nmase_metrics = []\\nsmape_metrics = []\\n\\nfor item_id, ts in enumerate(test_dataset):\\n    training_data = ts[\"target\"][:-prediction_length]\\n    ground_truth = ts[\"target\"][-prediction_length:]\\n    mase = mase_metric.compute(\\n        predictions=forecast_median[item_id],\\n        references=np.array(ground_truth),\\n        training=np.array(training_data),\\n        periodicity=get_seasonality(freq),\\n    )\\n    mase_metrics.append(mase[\"mase\"])\\n\\n    smape = smape_metric.compute(\\n        predictions=forecast_median[item_id],\\n        references=np.array(ground_truth),\\n    )\\n    smape_metrics.append(smape[\"smape\"])'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 41981}, page_content='```\\n\\n\\n```python\\nprint(f\"MASE: {np.mean(mase_metrics)}\")\\n\\n>>> MASE: 1.1913437728068093\\n\\nprint(f\"sMAPE: {np.mean(smape_metrics)}\")\\n\\n>>> sMAPE: 0.5322665081607634\\n```\\n\\n\\n```python\\nplt.scatter(mase_metrics, smape_metrics, alpha=0.2)\\nplt.xlabel(\"MASE\")\\nplt.ylabel(\"sMAPE\")\\nplt.show()'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 42259}, page_content='```\\n\\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_73_0.png)\\n    \\nTo plot the prediction for any time series variate with respect the ground truth test data we define the following helper:\\n\\n\\n```python\\nimport matplotlib.dates as mdates\\n\\n\\ndef plot(ts_index, mv_index):\\n    fig, ax = plt.subplots()\\n\\n    index = pd.period_range(\\n        start=multi_variate_test_dataset[ts_index][FieldName.START],\\n        periods=len(multi_variate_test_dataset[ts_index][FieldName.TARGET]),\\n        freq=multi_variate_test_dataset[ts_index][FieldName.START].freq,\\n    ).to_timestamp()\\n\\n    ax.xaxis.set_minor_locator(mdates.HourLocator())\\n\\n    ax.plot(\\n        index[-2 * prediction_length :],\\n        multi_variate_test_dataset[ts_index][\"target\"][mv_index, -2 * prediction_length :],\\n        label=\"actual\",\\n    )'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 43121}, page_content='ax.plot(\\n        index[-prediction_length:],\\n        forecasts[ts_index, ..., mv_index].mean(axis=0),\\n        label=\"mean\",\\n    )\\n    ax.fill_between(\\n        index[-prediction_length:],\\n        forecasts[ts_index, ..., mv_index].mean(0)\\n        - forecasts[ts_index, ..., mv_index].std(axis=0),\\n        forecasts[ts_index, ..., mv_index].mean(0)\\n        + forecasts[ts_index, ..., mv_index].std(axis=0),\\n        alpha=0.2,\\n        interpolate=True,\\n        label=\"+/- 1-std\",\\n    )\\n    ax.legend()\\n    fig.autofmt_xdate()'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 43644}, page_content='```\\n\\nFor example:\\n\\n\\n```python\\nplot(0, 344)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 43687}, page_content='```\\n\\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_77_0.png)\\n    \\n\\n## Conclusion\\n\\nHow do we compare against other models? The [Monash Time Series Repository](https://forecastingdata.org/#results) has a comparison table of test set MASE metrics which we can add to:\\n\\n|Dataset | \\tSES| \\tTheta | \\tTBATS| \\tETS\\t| (DHR-)ARIMA| \\tPR|\\tCatBoost |\\tFFNN\\t| DeepAR | \\tN-BEATS | \\tWaveNet|  Transformer (uni.) | **Informer (mv. our)**| \\n|:------------------:|:-----------------:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:--:|:--:|:--:|:--:|\\n|Traffic Hourly | 1.922\\t| 1.922\\t| 2.482 |\\t2.294|\\t2.535|\\t1.281|\\t1.571\\t|0.892|\\t0.825\\t|1.100|\\t1.066\\t| **0.821** | 1.191 |'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 44392}, page_content='As can be seen, and perhaps surprising to some, the multivariate forecasts are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations/relationships. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. We refer to [this paper](https://openreview.net/forum?id=GpW327gxLTF) for further reading. Multivariate models tend to work well when trained on a lot of data.\\n\\nSo the vanilla Transformer still performs best here! In the future, we hope to better benchmark these models in a central place to ease reproducing the results of several papers. Stay tuned for more!\\n\\n## Resources\\n\\nWe recommend to check out the [Informer docs](https://huggingface.co/docs/transformers/main/en/model_doc/informer) and the [example notebook](https://github.com/huggingface/notebooks/blob/main/examples/multivariate_informer.ipynb) linked at the top of this blog post.'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 1}, page_content='SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) that employs [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block) to enable the network to perform dynamic channel-wise feature recalibration.\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model(\\'seresnet152d\\', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 967}, page_content='```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])\\n```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 1939}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model('seresnet152d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 2537}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{hu2019squeezeandexcitation,\\n      title={Squeeze-and-Excitation Networks}, \\n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\\n      year={2019},\\n      eprint={1709.01507},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 3190}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 3195}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Networks\\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\\nModels:\\n- Name: seresnet152d\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 20161904304\\n    Parameters: 66840000\\n    File Size: 268144497\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet152d\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 152\\n    Dropout: 0.2\\n    Crop Pct: '0.94'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '256'\\n    Interpolation: bicubic\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 4102}, page_content=\"Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '256'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1206\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet152d_ra2-04464dd2.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83.74%\\n      Top 5 Accuracy: 96.77%\\n- Name: seresnet50\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 5285062320\\n    Parameters: 28090000\\n    File Size: 112621903\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 4990}, page_content=\"- Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet50\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 50\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1180\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet50_ra_224-8efdb4bb.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.26%\\n      Top 5 Accuracy: 95.07%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Kandinsky 2.2\\n\\nKandinsky 2.2 is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Vladimir Arkhipkin](https://github.com/oriBetelgeuse), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey), and [Denis Dimitrov](https://github.com/denndimitrov).'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 994}, page_content=\"The description from it's GitHub page is:\\n\\n*Kandinsky 2.2 brings substantial improvements upon its predecessor, Kandinsky 2.1, by introducing a new, more powerful image encoder - CLIP-ViT-G and the ControlNet support. The switch to CLIP-ViT-G as the image encoder significantly increases the model's capability to generate more aesthetic pictures and better understand text, thus enhancing the model's overall performance. The addition of the ControlNet mechanism allows the model to effectively control the process of generating images. This leads to more accurate and visually appealing outputs and opens new possibilities for text-guided image manipulation.*\\n\\nThe original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).\\n\\n<Tip>\\n\\nCheck out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\\n\\n</Tip>\\n\\n<Tip>\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 1974}, page_content='</Tip>\\n\\n<Tip>\\n\\nMake sure to check out the schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## KandinskyV22PriorPipeline\\n\\n[[autodoc]] KandinskyV22PriorPipeline\\n\\t- all\\n\\t- __call__\\n\\t- interpolate\\n\\n## KandinskyV22Pipeline\\n\\n[[autodoc]] KandinskyV22Pipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22CombinedPipeline\\n\\n[[autodoc]] KandinskyV22CombinedPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22ControlnetPipeline\\n\\n[[autodoc]] KandinskyV22ControlnetPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22PriorEmb2EmbPipeline\\n\\n[[autodoc]] KandinskyV22PriorEmb2EmbPipeline\\n\\t- all\\n\\t- __call__\\n\\t- interpolate\\n\\n## KandinskyV22Img2ImgPipeline\\n\\n[[autodoc]] KandinskyV22Img2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22Img2ImgCombinedPipeline'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 2873}, page_content='[[autodoc]] KandinskyV22Img2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22Img2ImgCombinedPipeline\\n\\n[[autodoc]] KandinskyV22Img2ImgCombinedPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22ControlnetImg2ImgPipeline\\n\\n[[autodoc]] KandinskyV22ControlnetImg2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22InpaintPipeline\\n\\n[[autodoc]] KandinskyV22InpaintPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22InpaintCombinedPipeline\\n\\n[[autodoc]] KandinskyV22InpaintCombinedPipeline\\n\\t- all\\n\\t- __call__'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 0}, page_content='--\\ntitle: \"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\"\\nthumbnail: blog/assets/156_huggylingo/Huggy_Lingo.png\\nauthors:\\n- user: davanstrien\\n---\\n\\n## Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\n\\n\\n\\n**tl;dr**: We\\'re using machine learning to detect the language of Hub datasets with no language metadata, and [librarian-bots](https://huggingface.co/librarian-bots) to make pull requests to add this metadata. \\n\\nThe Hugging Face Hub has become the repository where the community shares machine learning models, datasets, and applications. As the number of datasets grows, metadata becomes increasingly important as a tool for finding the right resource for your use case.\\n\\nIn this blog post, I\\'m excited to share some early experiments which seek to use machine learning to improve the metadata for datasets hosted on the Hugging Face Hub.\\n\\n### Language Metadata for Datasets on the Hub'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 925}, page_content='### Language Metadata for Datasets on the Hub\\n\\nThere are currently ~50K public datasets on the Hugging Face Hub. Metadata about the language used in a dataset can be specified using a [YAML](https://en.wikipedia.org/wiki/YAML) field at the top of the [dataset card](https://huggingface.co/docs/datasets/upload_dataset#create-a-dataset-card).\\n\\nAll public datasets specify 1,716 unique languages via a language tag in their metadata. Note that some of them will be the result of languages being specified in different ways i.e. `en` vs `eng` vs `english` vs `English`. \\n\\nFor example, the [IMDB dataset](https://huggingface.co/datasets/imdb) specifies `en` in the YAML metadata (indicating English):\\n\\n<p align=\"center\"> \\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_metadata.png\" alt=\"Screenshot of YAML metadata\"><br> \\n<em>Section of the YAML metadata for the IMDB dataset</em> \\n </p>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 1877}, page_content='It is perhaps unsurprising that English is by far the most common language for datasets on the Hub, with around 19% of datasets on the Hub listing their language as `en` (not including any variations of `en`, so the actual percentage is likely much higher).\\n\\n <p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_freq.png\" alt=\"Distribution of language tags\"><br> \\n     <em>The frequency and percentage frequency for datasets on the Hugging Face Hub</em> \\n </p> \\n\\n\\nWhat does the distribution of languages look like if we exclude English? We can see that there is a grouping of a few dominant languages and after that there is a pretty smooth fall in the frequencies at which languages appear.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 2655}, page_content='<p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_freq_distribution.png\" alt=\"Distribution of language tags\"><br> \\n     <em>Distribution of language tags for datasets on the hub excluding English.</em> \\n </p> \\n\\nHowever, there is a major caveat to this. Most datasets (around 87%) do not specify the language used; only approximately 13% of datasets include language information in their metadata.\\n\\n\\n<p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/has_lang_info_bar.png\" alt=\"Barchart\"><br> \\n     <em>The percent of datasets which have language metadata. True indicates language metadata is specified, False means no language data is listed. No card data means that there isn\\'t any metadata or it couldn\\'t be loaded by the `huggingface_hub` Python library.</em> \\n</p> \\n\\n#### Why is Language Metadata Important?'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 3591}, page_content=\"#### Why is Language Metadata Important?\\n\\nLanguage metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. \\n\\nCurrently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. \\n\\nMany people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 4421}, page_content='If we switch to the task of finding relevant machine learning models, knowing what languages were included in the training data for a model can help us find models for the language we are interested in. This relies on the dataset specifying this information. \\n\\nFinally, knowing what languages are represented on the Hub (and which are not), helps us understand the language biases of the Hub and helps inform community efforts to address gaps in particular languages. \\n\\n### Predicting the Languages of Datasets Using Machine Learning\\n\\nWe’ve already seen that many of the datasets on the Hugging Face Hub haven’t included metadata for the language used. However, since these datasets are already shared openly, perhaps we can look at the dataset and try to identify the language using machine learning.\\n\\n#### Getting the Data \\n\\nOne way we could access some examples from a dataset is by using the datasets library to download the datasets i.e. \\n\\n```python\\nfrom datasets import load_dataset'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 5366}, page_content='```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"biglam/on_the_books\")'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 5457}, page_content='```\\n\\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole dataset. We could instead try to load a sample of the dataset. However, depending on how the dataset was created, we might still end up downloading more data than we’d need onto the machine we’re working on. \\n\\nLuckily, many datasets on the Hub are available via the [datasets server](https://huggingface.co/docs/datasets-server/index). The datasets server is an API that allows us to access datasets hosted on the Hub without downloading the dataset locally. The Datasets Server powers the Datasets Viewer preview you will see for many datasets hosted on the Hub.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 6117}, page_content='For this first experiment with predicting language for datasets, we define a list of column names and data types likely to contain textual content i.e. `text` or `prompt` column names and `string` features are likely to be relevant `image` is not. This means we can avoid predicting the language for datasets where language information is less relevant, for example, image classification datasets. We use the Datasets Server to get 20 rows of text data to pass to a machine learning model (we could modify this to take more or fewer examples from the dataset). \\n\\nThis approach means that for the majority of datasets on the Hub we can quickly request the contents of likely text columns for the first 20 rows in a dataset. \\n\\n#### Predicting the Language of a Dataset'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 6842}, page_content='#### Predicting the Language of a Dataset \\n\\nOnce we have some examples of text from a dataset, we need to predict the language. There are various options here, but for this work, we used the [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model created by [Meta](https://huggingface.co/facebook) as part of the [No Language Left Behind](https://ai.facebook.com/research/no-language-left-behind/) work. This model can detect 217 languages which will likely represent the majority of languages for datasets hosted on the Hub. \\n\\nWe pass 20 examples to the model representing rows from a dataset. This results in 20 individual language predictions (one per row) for each dataset.  \\n\\nOnce we have these predictions, we do some additional filtering to determine if we will accept the predictions as a metadata suggestion. This roughly consists of:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 7756}, page_content='- Grouping the predictions for each dataset by language: some datasets return predictions for multiple languages. We group these predictions by the language predicted i.e. if a dataset returns predictions for English and Dutch, we group the English and Dutch predictions together. \\n- For datasets with multiple languages predicted, we count how many predictions we have for each language. If a language is predicted less than 20% of the time, we discard this prediction. i.e. if we have 18 predictions for English and only 2 for Dutch we discard the Dutch predictions. \\n- We calculate the mean score for all predictions for a language. If the mean score associated with a languages prediction is below 80% we discard this prediction.\\n\\n <p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/prediction-flow.png\" alt=\"Prediction workflow\"><br> \\n     <em>Diagram showing how predictions are handled.</em> \\n </p>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 8746}, page_content=\"Once we’ve done this filtering, we have a further step of deciding how to use these predictions. The fastText language prediction model returns predictions as an [ISO 639-3](https://en.wikipedia.org/wiki/ISO_639-3) code (an international standard for language codes) along with a script type. i.e. `kor_Hang` is the ISO 693-3 language code for Korean (kor) + Hangul script (Hang) a [ISO 15924](https://en.wikipedia.org/wiki/ISO_15924) code representing the script of a language.\\n\\nWe discard the script information since this isn't currently captured consistently as metadata on the Hub and, where possible, we convert the language prediction returned by the model from [ISO 639-3](https://en.wikipedia.org/wiki/ISO_639-3) to [ISO 639-1](https://en.wikipedia.org/wiki/ISO_639-1) language codes. This is largely done because these language codes have better support in the Hub UI for navigating datasets.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 9651}, page_content=\"For some ISO 639-3 codes, there is no ISO 639-1 equivalent. For these cases we manually specify a mapping if we deem it to make sense, for example Standard Arabic (`arb`) is mapped to Arabic (`ar`). Where an obvious mapping is not possible, we currently don't suggest metadata for this dataset. In future iterations of this work we may take a different approach. It is important to recognise this approach does come with downsides, since it reduces the diversity of languages which might be suggested and also relies on subjective judgments about what languages can be mapped to others. \\n\\nBut the process doesn't stop here. After all, what use is predicting the language of the datasets if we can't share that information with the rest of the community?\\n\\n### Using Librarian-Bot to Update Metadata\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 10406}, page_content=\"### Using Librarian-Bot to Update Metadata\\n\\nTo ensure this valuable language metadata is incorporated back into the Hub, we turn to Librarian-Bot! Librarian-Bot takes the language predictions generated by Meta's [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model and opens pull requests to add this information to the metadata of each respective dataset. \\n\\nThis system not only updates the datasets with language information, but also does it swiftly and efficiently, without requiring manual work from humans. If the owner of a repo decided to approve and merge the pull request, then the language metadata becomes available for all users, significantly enhancing the usability of the Hugging Face Hub. You can keep track of what the librarian-bot is doing [here](https://huggingface.co/librarian-bot/activity/community)! \\n\\n#### Next Steps\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 11305}, page_content=\"#### Next Steps \\n\\nAs the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.\\n\\nWith the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. \\n\\nAs the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Trainer'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md', 'start_index': 746}, page_content=\"-->\\n\\n# Trainer\\n\\nThe [`Trainer`] class provides an API for feature-complete training in PyTorch, and it supports distributed training on multiple GPUs/TPUs, mixed precision for [NVIDIA GPUs](https://nvidia.github.io/apex/), [AMD GPUs](https://rocm.docs.amd.com/en/latest/rocm.html), and [`torch.amp`](https://pytorch.org/docs/stable/amp.html) for PyTorch. [`Trainer`] goes hand-in-hand with the [`TrainingArguments`] class, which offers a wide range of options to customize how a model is trained. Together, these two classes provide a complete training API.\\n\\n[`Seq2SeqTrainer`] and [`Seq2SeqTrainingArguments`] inherit from the [`Trainer`] and [`TrainingArgument`] classes and they're adapted for training models for sequence-to-sequence tasks such as summarization or translation.\\n\\n<Tip warning={true}>\\n\\nThe [`Trainer`] class is optimized for 🤗 Transformers models and can have surprising behaviors\\nwhen used with other models. When using it with your own model, make sure:\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md', 'start_index': 1722}, page_content='- your model always return tuples or subclasses of [`~utils.ModelOutput`]\\n- your model can compute the loss if a `labels` argument is provided and that loss is returned as the first\\n  element of the tuple (if your model returns tuples)\\n- your model can accept multiple label arguments (use `label_names` in [`TrainingArguments`] to indicate their name to the [`Trainer`]) but none of them should be named `\"label\"`\\n\\n</Tip>\\n\\n## Trainer[[api-reference]]\\n\\n[[autodoc]] Trainer\\n    - all\\n\\n## Seq2SeqTrainer\\n\\n[[autodoc]] Seq2SeqTrainer\\n    - evaluate\\n    - predict\\n\\n## TrainingArguments\\n\\n[[autodoc]] TrainingArguments\\n    - all\\n\\n## Seq2SeqTrainingArguments\\n\\n[[autodoc]] Seq2SeqTrainingArguments\\n    - all'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: upload_button_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 81}, page_content='```\\nimport gradio as gr'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 106}, page_content='with gr.Blocks() as demo:\\n    \\n    with gr.Row():\\n        with gr.Column():\\n            upload_btn = gr.UploadButton(label=\"Upload Single File\", file_count=\"single\")\\n        with gr.Column():\\n            output_file_1 = gr.File(label=\"Upload Single File Output\", file_count=\"single\")\\n            num_load_btn_1 = gr.Number(label=\"# Load Upload Single File\", value=0)\\n            output_click_1 = gr.Number(label=\"# Click Upload Single File Output\", value=0)\\n            upload_btn.upload(lambda s,n: (s, n + 1), [upload_btn, num_load_btn_1], [output_file_1, num_load_btn_1])\\n            upload_btn.click(lambda n: (n + 1), output_click_1, [output_click_1])\\n    with gr.Row():\\n        with gr.Column():\\n            upload_btn_multiple = gr.UploadButton(label=\"Upload Multiple Files\", file_count=\"multiple\")\\n        with gr.Column():\\n            output_file_2 = gr.File(label=\"Upload Multiple Files Output\", file_count=\"multiple\")'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 950}, page_content='output_file_2 = gr.File(label=\"Upload Multiple Files Output\", file_count=\"multiple\")\\n            num_load_btn_2 = gr.Number(label=\"# Load Upload Multiple Files\", value=0)\\n            output_click_2 = gr.Number(label=\"# Click Upload Multiple Files Output\", value=0)\\n            upload_btn_multiple.upload(lambda s,n: (s, n + 1), [upload_btn_multiple, num_load_btn_2], [output_file_2, num_load_btn_2])\\n            upload_btn_multiple.click(lambda n: (n + 1), output_click_2, [output_click_2])'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 1443}, page_content='if __name__ == \"__main__\":\\n    demo.launch()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 1488}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 1}, page_content='Normalization and pre-tokenization[[normalization-and-pre-tokenization]]\\n\\n<CourseFloatingBanner chapter={6}\\n  classNames=\"absolute z-10 right-0 top-0\"\\n  notebooks={[\\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb\"},\\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb\"},\\n]} />\\n\\nBefore we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we\\'ll first take a look at the preprocessing that each tokenizer applies to text. Here\\'s a high-level overview of the steps in the tokenization pipeline:'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 795}, page_content='<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg\" alt=\"The tokenization pipeline.\">\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg\" alt=\"The tokenization pipeline.\">\\n</div>\\n\\nBefore splitting a text into subtokens (according to its model), the tokenizer performs two steps: _normalization_ and _pre-tokenization_.\\n\\n## Normalization[[normalization]]\\n\\n<Youtube id=\"4IIC2jI9CaU\"/>\\n\\nThe normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you\\'re familiar with [Unicode normalization](http://www.unicode.org/reports/tr15/) (such as NFC or NFKC), this is also something the tokenizer may apply.'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 1722}, page_content='The 🤗 Transformers `tokenizer` has an attribute called `backend_tokenizer` that provides access to the underlying tokenizer from the 🤗 Tokenizers library:\\n\\n```py\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\nprint(type(tokenizer.backend_tokenizer))'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 2028}, page_content='```\\n\\n```python out\\n<class \\'tokenizers.Tokenizer\\'>\\n```\\n\\nThe `normalizer` attribute of the `tokenizer` object has a `normalize_str()` method that we can use to see how the normalization is performed:\\n\\n```py\\nprint(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\\n```\\n\\n```python out\\n\\'hello how are u?\\''),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 2351}, page_content='```\\n\\nIn this example, since we picked the `bert-base-uncased` checkpoint, the normalization applied lowercasing and removed the accents. \\n\\n<Tip>\\n\\n✏️ **Try it out!** Load a tokenizer from the `bert-base-cased` checkpoint and pass the same example to it. What are the main differences you can see between the cased and uncased versions of the tokenizer?\\n\\n</Tip>\\n\\n## Pre-tokenization[[pre-tokenization]]\\n\\n<Youtube id=\"grlLV8AIXug\"/>\\n\\nAs we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That\\'s where the pre-tokenization step comes in. As we saw in [Chapter 2](/course/chapter2), a word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training.'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 3227}, page_content='To see how a fast tokenizer performs pre-tokenization, we can use the `pre_tokenize_str()` method of the `pre_tokenizer` attribute of the `tokenizer` object:\\n\\n```py\\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 3475}, page_content='```\\n\\n```python out\\n[(\\'Hello\\', (0, 5)), (\\',\\', (5, 6)), (\\'how\\', (7, 10)), (\\'are\\', (11, 14)), (\\'you\\', (16, 19)), (\\'?\\', (19, 20))]\\n```\\n\\nNotice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between `are` and `you` to account for that.\\n\\nSince we\\'re using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer:\\n\\n```py\\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 4243}, page_content='```\\n\\nit will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a `Ġ` symbol, enabling it to recover the original spaces if we decode the tokens:\\n\\n```python out\\n[(\\'Hello\\', (0, 5)), (\\',\\', (5, 6)), (\\'Ġhow\\', (6, 10)), (\\'Ġare\\', (10, 14)), (\\'Ġ\\', (14, 15)), (\\'Ġyou\\', (15, 19)),\\n (\\'?\\', (19, 20))]\\n```\\n\\nAlso note that unlike the BERT tokenizer, this tokenizer does not ignore the double space.\\n\\nFor a last example, let\\'s have a look at the T5 tokenizer, which is based on the SentencePiece algorithm:\\n\\n```py\\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\\n```\\n\\n```python out\\n[(\\'▁Hello,\\', (0, 6)), (\\'▁how\\', (7, 10)), (\\'▁are\\', (11, 14)), (\\'▁you?\\', (16, 20))]'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 5025}, page_content=\"```\\n\\nLike the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (`_`), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before `Hello`) and ignored the double space between `are` and `you`.\\n\\nNow that we've seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We'll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we'll examine how the three main algorithms used for subword tokenization work.\\n\\n## SentencePiece[[sentencepiece]]\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 5671}, page_content=\"## SentencePiece[[sentencepiece]]\\n\\n[SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `▁`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter7/7)), it doesn't even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).\\n\\nThe other main feature of SentencePiece is *reversible tokenization*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_`s with spaces -- this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.\\n\\n## Algorithm overview[[algorithm-overview]]\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 6601}, page_content=\"## Algorithm overview[[algorithm-overview]]\\n\\nIn the following sections, we'll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here's a quick overview of how they each work. Don't hesitate to come back to this table after reading each of the next sections if it doesn't make sense to you yet.\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 7035}, page_content='Model | BPE | WordPiece | Unigram\\n:----:|:---:|:---------:|:------:\\nTraining | Starts from a small vocabulary and learns rules to merge tokens |  Starts from a small vocabulary and learns rules to merge tokens | Starts from a large vocabulary and learns rules to remove tokens\\nTraining step | Merges the tokens corresponding to the most common pair | Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent | Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus\\nLearns | Merge rules and a vocabulary | Just a vocabulary | A vocabulary with a score for each token'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 7750}, page_content='Encoding | Splits a word into characters and applies the merges learned during training | Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word | Finds the most likely split into tokens, using the scores learned during training'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 8050}, page_content=\"Now let's dive into BPE!\"),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/howto/map_pools.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Map pools\\n\\nMap pools allow you to instantiate multiple versions of your environment on the backend, the enables higher \\nthroughput with parallelization of interaction in simulations and embodied environments.\\nUsing map pools is simple with 🤗 Simulate. First define a function that will generate your environment, we call each environment instance a \"map\".'),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/howto/map_pools.mdx', 'start_index': 946}, page_content='```\\ndef generate_map(index):\\n    root = sm.Asset(name=f\"root_{index}\")\\n    root += sm.Box(\\n        name=f\"floor_{index}\",\\n        position=[0, -0.05, 0],\\n        scaling=[10, 0.1, 10],\\n        material=sm.Material.BLUE,\\n        with_collider=True,\\n    )\\n    root += sm.Box(\\n        name=f\"wall1_{index}\",\\n        position=[-1, 0.5, 0],\\n        scaling=[0.1, 1, 5.1],\\n        material=sm.Material.GRAY75,\\n        with_collider=True,\\n    )\\n    root += sm.Box(\\n        name=f\"wall2_{index}\",\\n        position=[1, 0.5, 0],\\n        scaling=[0.1, 1, 5.1],\\n        material=sm.Material.GRAY75,\\n        with_collider=True,\\n    )\\n    root += sm.Box(\\n        name=f\"wall3_{index}\",\\n        position=[0, 0.5, 4.5],\\n        scaling=[5.9, 1, 0.1],\\n        material=sm.Material.GRAY75,\\n        with_collider=True,\\n    )\\n\\n    # add actors, sensors, reward functions etc ...\\n\\n    return root'),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/howto/map_pools.mdx', 'start_index': 1822}, page_content='```\\n\\nYou can then provide the `generate_map` method as an argument to the `sm.ParallelRLEnv` class, which will instantiate `n_maps`. \\nTraining with a subset of the maps is possible using the `n_show` option. At each environment reset, it cycles through to the next map.\\n\\n[[autodoc]] ParallelRLEnv'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 1}, page_content='@gradio/imageeditor\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story.  Thanks [@hannahblair](https://github.com/hannahblair)!\\n\\n## 0.1.5\\n\\n### Fixes\\n\\n- [#6799](https://github.com/gradio-app/gradio/pull/6799) [`c352811`](https://github.com/gradio-app/gradio/commit/c352811f76d4126613ece0a584f8c552fdd8d1f6) - Adds docstrings for `gr.WaveformOptions`, `gr.Brush`, and `gr.Eraser`, fixes examples for `ImageEditor`, and allows individual images to be used as the initial `value` for `ImageEditor`.  Thanks [@abidlabs](https://github.com/abidlabs)!\\n\\n## 0.1.4\\n\\n### Patch Changes'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 715}, page_content='## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a), [`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:\\n  - @gradio/upload@0.5.4\\n  - @gradio/client@0.9.1\\n  - @gradio/image@0.5.1\\n\\n## 0.1.3\\n\\n### Patch Changes'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 1071}, page_content='- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/gradio/commit/6a9151d5c9432c724098da7d88a539aaaf5ffe88), [`21cfb0a`](https://github.com/gradio-app/gradio/commit/21cfb0acc309bb1a392f4d8a8e42f6be864c5978), [`d76bcaa`](https://github.com/gradio-app/gradio/commit/d76bcaaaf0734aaf49a680f94ea9d4d22a602e70), [`67ddd40`](https://github.com/gradio-app/gradio/commit/67ddd40b4b70d3a37cb1637c33620f8d197dbee0), [`053bec9`](https://github.com/gradio-app/gradio/commit/053bec98be1127e083414024e02cf0bebb0b5142), [`bdf81fe`](https://github.com/gradio-app/gradio/commit/bdf81fead86e1d5a29e6b036f1fff677f6480e6b), [`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00), [`5177132`](https://github.com/gradio-app/gradio/commit/5177132d718c77f6d47869b4334afae6380394cb)]:\\n  - @gradio/image@0.5.0\\n  - @gradio/upload@0.5.3\\n  - @gradio/client@0.9.0\\n  - @gradio/wasm@0.4.0\\n  - @gradio/icons@0.3.2\\n  - @gradio/atoms@0.4.0\\n  - @gradio/statustracker@0.4.2'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 2066}, page_content='## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b639e04`](https://github.com/gradio-app/gradio/commit/b639e040741e6c0d9104271c81415d7befbd8cf3), [`206af31`](https://github.com/gradio-app/gradio/commit/206af31d7c1a31013364a44e9b40cf8df304ba50)]:\\n  - @gradio/image@0.4.2\\n  - @gradio/icons@0.3.1\\n  - @gradio/atoms@0.3.1\\n  - @gradio/statustracker@0.4.1\\n  - @gradio/upload@0.5.2\\n\\n## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\\n  - @gradio/client@0.8.2\\n  - @gradio/image@0.4.1\\n  - @gradio/upload@0.5.1\\n\\n## 0.1.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))\\n\\nA brand new component, completely separate from `Image` that provides simple editing capabilities.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 2896}, page_content='A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n- Set background images from file uploads, webcam, or just paste!\\n- Crop images with an improved cropping UI. App authors can event set specific crop size, or crop ratios (`1:1`, etc)\\n- Paint on top of any image (or no image) and erase any mistakes!\\n- The ImageEditor supports layers, confining draw and erase actions to that layer.\\n- More flexible access to data. The image component returns a composite image representing the final state of the canvas as well as providing the background and all layers as individual images.\\n- Fully customisable. All features can be enabled and disabled. Even the brush color swatches can be customised.\\n\\n<video src=\"https://user-images.githubusercontent.com/12937446/284027169-31188926-fd16-4a1c-8718-998e7aae4695.mp4\" autoplay muted></video>\\n\\n```py'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 3777}, page_content='```py\\n\\ndef fn(im):\\n    im[\"composite\"] # the full canvas\\n    im[\"background\"] # the background image\\n    im[\"layers\"] # a list of individual layers\\n\\n\\nim = gr.ImageEditor(\\n    # decide which sources you\\'d like to accept\\n    sources=[\"upload\", \"webcam\", \"clipboard\"],\\n    # set a cropsize constraint, can either be a ratio or a concrete [width, height]\\n    crop_size=\"1:1\",\\n    # enable crop (or disable it)\\n    transforms=[\"crop\"],\\n    # customise the brush\\n    brush=Brush(\\n      default_size=\"25\", # or leave it as \\'auto\\'\\n      color_mode=\"fixed\", # \\'fixed\\' hides the user swatches and colorpicker, \\'defaults\\' shows it\\n      default_color=\"hotpink\", # html names are supported\\n      colors=[\\n        \"rgba(0, 150, 150, 1)\", # rgb(a)\\n        \"#fff\", # hex rgb\\n        \"hsl(360, 120, 120)\" # in fact any valid colorstring\\n      ]\\n    ),\\n    brush=Eraser(default_size=\"25\")\\n)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 4652}, page_content='```\\n\\nThanks [@pngwn](https://github.com/pngwn)!\\n\\n### Fixes\\n\\n- [#6502](https://github.com/gradio-app/gradio/pull/6502) [`070f71c93`](https://github.com/gradio-app/gradio/commit/070f71c933d846ce8e2fe11cdd9bc0f3f897f29f) - Ensure image editor crop and draw cursor works as expected when the scroll position changes. Thanks [@pngwn](https://github.com/pngwn)!\\n\\n# @gradio/image'),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': 1}, page_content='How to contribute to simulate?\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg)](CODE_OF_CONDUCT.md)\\n\\nSimulation environments is an open source project, so all contributions and suggestions are welcome.\\n\\nYou can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, \\nimproving the documentation, fixing bugs,...\\n\\nMany thanks in advance to every contributor.\\n\\nIn order to facilitate healthy, constructive behavior in an open and inclusive community, we all respect and abide by \\nour [code of conduct](CODE_OF_CONDUCT.md).\\n\\n## How to work on an open Issue?\\nYou have the list of open Issues at: https://github.com/huggingface/simulate/issues\\n\\nSome of them may have the label `help wanted`: that means that any contributor is welcomed!\\n\\nIf you would like to work on any of the open Issues:'),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': 834}, page_content=\"If you would like to work on any of the open Issues:\\n\\n1. Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page.\\n\\n2. You can self-assign it by commenting on the Issue page with one of the keywords: `#take` or `#self-assign`.\\n\\n3. Work on your self-assigned issue and eventually create a Pull Request.\\n\\n## How to create a Pull Request?\\n1. Fork the [repository](https://github.com/huggingface/simulate) by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account.\\n\\n2. Clone your fork to your local disk, and add the base repository as a remote:\\n\\n\\t```bash\\n\\tgit clone git@github.com:<your Github handle>/simulate.git\\n\\tcd simulate\\n\\tgit remote add upstream https://github.com/huggingface/simulate.git\"),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': 1670}, page_content='```\\n\\n3. Create a new branch to hold your development changes:\\n\\n\\t```bash\\n\\tgit checkout -b a-descriptive-name-for-my-changes\\n\\t```\\n\\n\\t**do not** work on the `main` branch.\\n\\n4. Set up a development environment by running the following command in a virtual environment:\\n\\n\\t```bash\\n\\tpip install -e \".[dev]\"\\n\\t```\\n\\n   (If simulate was already installed in the virtual environment, remove\\n   it with `pip uninstall simulate` before reinstalling it in editable\\n   mode with the `-e` flag.)\\n\\n5. Develop the features on your branch. If you want to add a dataset see more in-detail instructions in the section [*How to add a dataset*](#how-to-add-a-dataset). \\n\\n6. Format your code. Run black and isort so that your newly added files look nice with the following command:\\n\\n\\t```bash\\n\\tmake style\\n\\t```\\n\\n7. Once you\\'re happy with your dataset script file, add your changes and make a commit to record your changes locally:\\n\\n\\t```bash\\n\\tgit add simulate/<your_dataset_name>\\n\\tgit commit'),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': 2634}, page_content='```\\n\\n\\tIt is a good idea to sync your copy of the code with the original\\n\\trepository regularly. This way you can quickly account for changes:\\n\\n\\t```bash\\n\\tgit fetch upstream\\n\\tgit rebase upstream/main\\n    ```\\n\\n   Push the changes to your account using:\\n\\n   ```bash\\n   git push -u origin a-descriptive-name-for-my-changes\\n   ```\\n\\n8. Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review.\\n\\n## Code of conduct\\n\\nThis project adheres to the HuggingFace [code of conduct](CODE_OF_CONDUCT.md). \\nBy participating, you are expected to uphold this code.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/chatinterface_system_prompt/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: chatinterface_system_prompt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\ndef echo(message, history, system_prompt, tokens):\\n    response = f\"System prompt: {system_prompt}\\\\n Message: {message}.\"\\n    for i in range(min(len(response), int(tokens))):\\n        time.sleep(0.05)\\n        yield response[: i+1]\\n\\ndemo = gr.ChatInterface(echo, \\n                        additional_inputs=[\\n                            gr.Textbox(\"You are helpful AI.\", label=\"System Prompt\"), \\n                            gr.Slider(10, 100)\\n                        ]\\n                       )\\n\\nif __name__ == \"__main__\":\\n    demo.queue().launch()\\n```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 1}, page_content=\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to [ResNeXt](https://paperswithcode.com/method/resnext). The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model('cspresnext50', pretrained=True)\\n>>> model.eval()\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 572}, page_content='```\\n\\nTo load and preprocess the image:\\n\\n```py \\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 1367}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n>>> # Get imagenet class mappings\\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\n>>> urllib.request.urlretrieve(url, filename) \\n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\\n...     categories = [s.strip() for s in f.readlines()]\\n\\n>>> # Print top categories per image\\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\\n>>> for i in range(top5_prob.size(0)):\\n...     print(categories[top5_catid[i]], top5_prob[i].item())\\n>>> # prints class names and probabilities like:\\n>>> # [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 2161}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspresnext50`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('cspresnext50', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 2714}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{wang2019cspnet,\\n      title={CSPNet: A New Backbone that can Enhance Learning Capability of CNN}, \\n      author={Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh},\\n      year={2019},\\n      eprint={1911.11929},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 3385}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 3390}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNeXt\\n  Paper:\\n    Title: 'CSPNet: A New Backbone that can Enhance Learning Capability of CNN'\\n    URL: https://paperswithcode.com/paper/cspnet-a-new-backbone-that-can-enhance\\nModels:\\n- Name: cspresnext50\\n  In Collection: CSP ResNeXt\\n  Metadata:\\n    FLOPs: 3962945536\\n    Parameters: 20570000\\n    File Size: 82562887\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - Max Pooling\\n    - ReLU\\n    - ResNeXt Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - Polynomial Learning Rate Decay\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 1x GPU\\n    ID: cspresnext50\\n    LR: 0.1\\n    Layers: 50\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 128\\n    Image Size: '224'\\n    Weight Decay: 0.005\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 4289}, page_content=\"Momentum: 0.9\\n    Batch Size: 128\\n    Image Size: '224'\\n    Weight Decay: 0.005\\n    Interpolation: bilinear\\n    Training Steps: 8000000\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/cspnet.py#L430\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspresnext50_ra_224-648b4713.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.05%\\n      Top 5 Accuracy: 94.94%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# MultiDiffusion\\n\\n[MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation](https://huggingface.co/papers/2302.08113) is by Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\\n\\nThe abstract from the paper is:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 816}, page_content='*Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals,'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 1710}, page_content='user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.*'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 1869}, page_content=\"You can find additional information about MultiDiffusion on the [project page](https://multidiffusion.github.io/), [original codebase](https://github.com/omerbt/MultiDiffusion), and try it out in a [demo](https://huggingface.co/spaces/weizmannscience/MultiDiffusion).\\n\\n## Tips\\n\\nWhile calling [`StableDiffusionPanoramaPipeline`], it's possible to specify the `view_batch_size` parameter to be > 1.\\nFor some GPUs with high performance, this can speedup the generation process and increase VRAM usage.\\n\\nTo generate panorama-like images make sure you pass the width parameter accordingly. We recommend a width value of 2048 which is the default.\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 2512}, page_content='Circular padding is applied to ensure there are no stitching artifacts when working with panoramas to ensure a seamless transition from the rightmost part to the leftmost part. By enabling circular padding (set `circular_padding=True`), the operation applies additional crops after the rightmost point of the image, allowing the model to \"see” the transition from the rightmost part to the leftmost part. This helps maintain visual consistency in a 360-degree sense and creates a proper “panorama” that can be viewed using 360-degree panorama viewers. When decoding latents in Stable Diffusion, circular padding is applied to ensure that the decoded latents match in the RGB space.\\n\\nFor example, without circular padding, there is a stitching artifact (default):\\n![img](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/indoor_%20no_circular_padding.png)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 3396}, page_content='But with circular padding, the right and the left parts are matching (`circular_padding=True`):\\n![img](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/indoor_%20circular_padding.png)\\n\\n<Tip>\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## StableDiffusionPanoramaPipeline\\n[[autodoc]] StableDiffusionPanoramaPipeline\\n\\t- __call__\\n\\t- all\\n\\n## StableDiffusionPipelineOutput\\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput'),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/api/actors.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Actors\\n\\n[[autodoc]] SimpleActor\\n\\n[[autodoc]] EgocentricCameraActor\\n\\n\\nUnder construction 🚧.'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02c_preprocess-sentence-pairs-pt.md', 'start_index': 0}, page_content='ow to preprocess pairs of sentences? We have seen how to tokenize single sentences and batch them together in the \"Batching inputs together\" video. If this code look unfamiliar to you, be sure to check that video again! Here we will focus on tasks that classify pairs of sentences. For instance, we may want to classify whether two texts are paraphrases or not. Here is an example taken from the Quora Question Pairs dataset, which focuses on identifying duplicate questions. In the first pair, the two questions are duplicates; in the second, they are not. Another pair classification problem is when we want to know if two sentences are logically related or not (a problem called Natural Language Inference or NLI). In this example taken from the MultiNLI dataset, we have a pair of sentences for each possible label: contradiction, neutral or entailment (which is a fancy way of saying the first sentence implies the second). So classifying pairs of sentences is a problem worth studying. In fact,'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02c_preprocess-sentence-pairs-pt.md', 'start_index': 908}, page_content=\"implies the second). So classifying pairs of sentences is a problem worth studying. In fact, in the GLUE benchmark (which is an academic benchmark for text classification), 8 of the 10 datasets are focused on tasks using pairs of sentences. That's why models like BERT are often pretrained with a dual objective: on top of the language modeling objective, they often have an objective related to sentence pairs. For instance, during pretraining, BERT is shown pairs of sentences and must predict both the value of randomly masked tokens and whether the second sentence follows from the first. Fortunately, the tokenizer from the Transformers library has a nice API to deal with pairs of sentences: you just have to pass them as two arguments to the tokenizer. On top of the input IDs and the attention mask we studied already, it returns a new field called token type IDs, which tells the model which tokens belong to the first sentence and which ones belong to the second sentence. Zooming in a\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02c_preprocess-sentence-pairs-pt.md', 'start_index': 1809}, page_content='tokens belong to the first sentence and which ones belong to the second sentence. Zooming in a little bit, here are the input IDs, aligned with the tokens they correspond to, their respective token type ID and attention mask. We can see the tokenizer also added special tokens so we have a CLS token, the tokens from the first sentence, a SEP token, the tokens from the second sentence, and a final SEP token. If we have several pairs of sentences, we can tokenize them together by passing the list of first sentences, then the list of second sentences and all the keyword arguments we studied already, like padding=True. Zooming in at the result, we can see how the tokenizer added padding to the second pair of sentences, to make the two outputs the same length, and properly dealt with token type IDS and attention masks for the two sentences. This is then all ready to pass through our model!'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 0}, page_content='!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Download files from the Hub\\n\\nThe `huggingface_hub` library provides functions to download files from the repositories\\nstored on the Hub. You can use these functions independently or integrate them into your\\nown library, making it more convenient for your users to interact with the Hub. This\\nguide will show you how to:\\n\\n* Download and cache a single file.\\n* Download and cache an entire repository.\\n* Download files to a local folder. \\n\\n## Download a single file\\n\\nThe [`hf_hub_download`] function is the main function for downloading files from the Hub.\\nIt downloads the remote file, caches it on disk (in a version-aware way), and returns its local file path.\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 835}, page_content='<Tip>\\n\\nThe returned filepath is a pointer to the HF local cache. Therefore, it is important to not modify the file to avoid\\nhaving a corrupted cache. If you are interested in getting to know more about how files are cached, please refer to our\\n[caching guide](./manage-cache).\\n\\n</Tip>\\n\\n### From latest version\\n\\nSelect the file to download using the `repo_id`, `repo_type` and `filename` parameters. By default, the file will\\nbe considered as being part of a `model` repo.\\n\\n```python\\n>>> from huggingface_hub import hf_hub_download\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\")\\n\\'/root/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade/config.json\\'\\n\\n# Download from a dataset\\n>>> hf_hub_download(repo_id=\"google/fleurs\", filename=\"fleurs.py\", repo_type=\"dataset\")\\n\\'/root/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34/fleurs.py\\''),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 1794}, page_content='```\\n\\n### From specific version\\n\\nBy default, the latest version from the `main` branch is downloaded. However, in some cases you want to download a file\\nat a particular version (e.g. from a specific branch, a PR, a tag or a commit hash).\\nTo do so, use the `revision` parameter:\\n\\n```python\\n# Download from the `v1.0` tag\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"v1.0\")\\n\\n# Download from the `test-branch` branch\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"test-branch\")\\n\\n# Download from Pull Request #3\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"refs/pr/3\")\\n\\n# Download from a specific commit hash\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"877b84a8f93f2d619faa2a6e514a32beef88ab0a\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 2640}, page_content='```\\n\\n**Note:** When using the commit hash, it must be the full-length hash instead of a 7-character commit hash.\\n\\n### Construct a download URL\\n\\nIn case you want to construct the URL used to download a file from a repo, you can use [`hf_hub_url`] which returns a URL.\\nNote that it is used internally by [`hf_hub_download`].\\n\\n## Download an entire repository\\n\\n[`snapshot_download`] downloads an entire repository at a given revision. It uses internally [`hf_hub_download`] which\\nmeans all downloaded files are also cached on your local disk. Downloads are made concurrently to speed-up the process.\\n\\nTo download a whole repository, just pass the `repo_id` and `repo_type`:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\")\\n\\'/home/lysandre/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade\\''),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 3544}, page_content='# Or from a dataset\\n>>> snapshot_download(repo_id=\"google/fleurs\", repo_type=\"dataset\")\\n\\'/home/lysandre/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34\\''),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 3748}, page_content='```\\n\\n[`snapshot_download`] downloads the latest revision by default. If you want a specific repository revision, use the\\n`revision` parameter:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", revision=\"refs/pr/1\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 4026}, page_content='```\\n\\n### Filter files to download\\n\\n[`snapshot_download`] provides an easy way to download a repository. However, you don\\'t always want to download the\\nentire content of a repository. For example, you might want to prevent downloading all `.bin` files if you know you\\'ll\\nonly use the `.safetensors` weights. You can do that using `allow_patterns` and `ignore_patterns` parameters.\\n\\nThese parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing\\npatterns) as documented [here](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm). The pattern matching is\\nbased on [`fnmatch`](https://docs.python.org/3/library/fnmatch.html).\\n\\nFor example, you can use `allow_patterns` to only download JSON configuration files:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", allow_patterns=\"*.json\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 4932}, page_content='```\\n\\nOn the other hand, `ignore_patterns` can exclude certain files from being downloaded. The\\nfollowing example ignores the `.msgpack` and `.h5` file extensions:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", ignore_patterns=[\"*.msgpack\", \"*.h5\"])\\n```\\n\\nFinally, you can combine both to precisely filter your download. Here is an example to download all json and markdown\\nfiles except `vocab.json`.\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"gpt2\", allow_patterns=[\"*.md\", \"*.json\"], ignore_patterns=\"vocab.json\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 5561}, page_content='```\\n\\n## Download file(s) to local folder\\n\\nThe recommended (and default) way to download files from the Hub is to use the [cache-system](./manage-cache).\\nYou can define your cache location by setting `cache_dir` parameter (both in [`hf_hub_download`] and [`snapshot_download`]).'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 5840}, page_content='However, in some cases you want to download files and move them to a specific folder. This is useful to get a workflow\\ncloser to what `git` commands offer. You can do that using the `local_dir` and `local_dir_use_symlinks` parameters:\\n- `local_dir` must be a path to a folder on your system. The downloaded files will keep the same file structure as in the\\nrepo. For example if `filename=\"data/train.csv\"` and `local_dir=\"path/to/folder\"`, then the returned filepath will be\\n`\"path/to/folder/data/train.csv\"`.\\n- `local_dir_use_symlinks` defines how the file must be saved in your local folder.\\n  - The default behavior (`\"auto\"`) is to duplicate small files (<5MB) and use symlinks for bigger files. Symlinks allow\\n    to optimize both bandwidth and disk usage. However manually editing a symlinked file might corrupt the cache, hence\\n    the duplication for small files. The 5MB threshold can be configured with the `HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD`\\n    environment variable.'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 6803}, page_content=\"environment variable.\\n  - If `local_dir_use_symlinks=True` is set, all files are symlinked for an optimal disk space optimization. This is\\n    for example useful when downloading a huge dataset with thousands of small files.\\n  - Finally, if you don't want symlinks at all you can disable them (`local_dir_use_symlinks=False`). The cache directory\\n    will still be used to check wether the file is already cached or not. If already cached, the file is **duplicated**\\n    from the cache (i.e. saves bandwidth but increases disk usage). If the file is not already cached, it will be\\n    downloaded and moved directly to the local dir. This means that if you need to reuse it somewhere else later, it\\n    will be **re-downloaded**.\"),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 7533}, page_content='Here is a table that summarizes the different options to help you choose the parameters that best suit your use case.'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 7652}, page_content='<!-- Generated with https://www.tablesgenerator.com/markdown_tables -->\\n| Parameters | File already cached | Returned path | Can read path? | Can save to path? | Optimized bandwidth | Optimized disk usage |\\n|---|:---:|:---:|:---:|:---:|:---:|:---:|\\n| `local_dir=None` |  | symlink in cache | ✅ | ❌<br>_(save would corrupt the cache)_ | ✅ | ✅ |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=\"auto\"` |  | file or symlink in folder | ✅ | ✅ _(for small files)_ <br> ⚠️ _(for big files do not resolve path before saving)_ | ✅ | ✅ |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=True` |  | symlink in folder | ✅ | ⚠️<br>_(do not resolve path before saving)_ | ✅ | ✅ |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=False` | No | file in folder | ✅ | ✅ | ❌<br>_(if re-run, file is re-downloaded)_ | ⚠️<br>(multiple copies if ran in multiple folders) |'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 8532}, page_content='| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=False` | Yes | file in folder | ✅ | ✅ | ⚠️<br>_(file has to be cached first)_ | ❌<br>_(file is duplicated)_ |'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 8701}, page_content='**Note:** if you are on a Windows machine, you need to enable developer mode or run `huggingface_hub` as admin to enable\\nsymlinks. Check out the [cache limitations](../guides/manage-cache#limitations) section for more details.\\n\\n## Download from the CLI\\n\\nYou can use the `huggingface-cli download` command from the terminal to directly download files from the Hub.\\nInternally, it uses the same [`hf_hub_download`] and [`snapshot_download`] helpers described above and prints the\\nreturned path to the terminal.\\n\\n```bash\\n>>> huggingface-cli download gpt2 config.json\\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 9378}, page_content='```\\n\\nYou can download multiple files at once which displays a progress bar and returns the snapshot path in which the files\\nare located:\\n\\n```bash\\n>>> huggingface-cli download gpt2 config.json model.safetensors\\nFetching 2 files: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 23831.27it/s]\\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 9790}, page_content='```\\n\\nFor more details about the CLI download command, please refer to the [CLI guide](./cli#huggingface-cli-download).\\n\\n## Faster downloads\\n\\nIf you are running on a machine with high bandwidth, you can increase your download speed with [`hf_transfer`](https://github.com/huggingface/hf_transfer), a Rust-based library developed to speed up file transfers with the Hub. To enable it, install the package (`pip install hf_transfer`) and set `HF_HUB_ENABLE_HF_TRANSFER=1` as an environment variable.\\n\\n<Tip>\\n\\nProgress bars are supported in `hf_transfer` starting from version `0.1.4`. Consider upgrading (`pip install -U hf-transfer`) if you plan to enable faster downloads.\\n\\n</Tip>\\n\\n<Tip warning={true}>\\n\\n`hf_transfer` is a power user tool! It is tested and production-ready, but it lacks user-friendly features like advanced error handling or proxies. For more details, please take a look at this [section](https://huggingface.co/docs/huggingface_hub/hf_transfer).\\n\\n</Tip>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 1}, page_content='Two types of value-based methods [[two-types-value-based-methods]]\\n\\nIn value-based methods,\\xa0**we learn a value function**\\xa0that\\xa0**maps a state to the expected value of being at that state.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" alt=\"Value Based Methods\"/>\\n\\nThe value of a state is the\\xa0**expected discounted return**\\xa0the agent can get if it\\xa0**starts at that state and then acts according to our policy.**\\n\\n<Tip>\\nBut what does it mean to act according to our policy? After all, we don\\'t have a policy in value-based methods since we train a value function and not a policy.\\n</Tip>\\n\\nRemember that the goal of an\\xa0**RL agent is to have an optimal policy π\\\\*.**\\n\\nTo find the optimal policy, we learned about two different methods:'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 737}, page_content='To find the optimal policy, we learned about two different methods:\\n\\n- *Policy-based methods:*\\xa0**Directly train the policy**\\xa0to select what action to take given a state (or a probability distribution over actions at that state). In this case, we\\xa0**don\\'t have a value function.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-2.jpg\" alt=\"Two RL approaches\"/>\\n\\nThe policy takes a state as input and outputs what action to take at that state (deterministic policy: a policy that output one action given a state, contrary to stochastic policy that output a probability distribution over actions).\\n\\nAnd consequently,\\xa0**we don\\'t define by hand the behavior of our policy; it\\'s the training that will define it.**\\n\\n- *Value-based methods:*\\xa0**Indirectly, by training a value function**\\xa0that outputs the value of a state or a state-action pair. Given this value function, our policy\\xa0**will take an action.**'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 1712}, page_content='Since the policy is not trained/learned,\\xa0**we need to specify its behavior.**\\xa0For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward,\\xa0**we\\'ll create a Greedy Policy.**\\n\\n<figure>\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-3.jpg\" alt=\"Two RL approaches\"/>\\n  <figcaption>Given a state, our action-value function (that we train) outputs the value of each action at that state. Then, our pre-defined Greedy Policy selects the action that will yield the highest value given a state or a state action pair.</figcaption>\\n</figure>\\n\\nConsequently, whatever method you use to solve your problem,\\xa0**you will have a policy**. In the case of value-based methods, you don\\'t train the policy: your policy\\xa0**is just a simple pre-specified function**\\xa0(for instance, the Greedy Policy) that\\xa0uses the values given by the value-function to select its actions.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 2699}, page_content='So the difference is:\\n\\n- In policy-based training,\\xa0**the optimal policy (denoted π\\\\*) is found by training the policy directly.**\\n- In value-based training,\\xa0**finding an optimal value function (denoted Q\\\\* or V\\\\*, we\\'ll study the difference below) leads to having an optimal policy.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link between value and policy\"/>\\n\\nIn fact, most of the time, in value-based methods, you\\'ll use\\xa0**an Epsilon-Greedy Policy**\\xa0that handles the exploration/exploitation trade-off; we\\'ll talk about this when we talk about Q-Learning in the second part of this unit.\\n\\n\\nAs we mentioned above, we have two types of value-based functions:\\n\\n## The state-value function [[state-value-function]]\\n\\nWe write the state value function under a policy π like this:'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 3505}, page_content='We write the state value function under a policy π like this:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-1.jpg\" alt=\"State value function\"/>\\n\\nFor each state, the state-value function outputs the expected return if the agent **starts at that state** and then follows the policy forever afterward (for all future timesteps, if you prefer).\\n\\n<figure>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-2.jpg\" alt=\"State value function\"/>\\n  <figcaption>If we take the state with value -7: it\\'s the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right.</figcaption>\\n</figure>\\n\\n## The action-value function [[action-value-function]]'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 4331}, page_content='## The action-value function [[action-value-function]]\\n\\nIn the action-value function, for each state and action pair, the action-value function\\xa0**outputs the expected return**\\xa0if the agent starts in that state, takes that action, and then follows the policy forever after.\\n\\nThe value of taking action \\\\\\\\(a\\\\\\\\) in state \\\\\\\\(s\\\\\\\\) under a policy \\\\\\\\(π\\\\\\\\) is:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-1.jpg\" alt=\"Action State value function\"/>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-2.jpg\" alt=\"Action State value function\"/>\\n\\n\\nWe see that the difference is:\\n\\n- For the state-value function, we calculate\\xa0**the value of a state \\\\\\\\(S_t\\\\\\\\)**\\n- For the action-value function, we calculate\\xa0**the value of the state-action pair ( \\\\\\\\(S_t, A_t\\\\\\\\) ) hence the value of taking that action at that state.**'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 5309}, page_content='<figure>\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-types.jpg\" alt=\"Two types of value function\"/>\\n  <figcaption>\\nNote: We didn\\'t fill all the state-action pairs for the example of Action-value function</figcaption>\\n</figure>\\n\\nIn either case, whichever value function we choose (state-value or action-value function),\\xa0**the returned value is the expected return.**\\n\\nHowever, the problem is that\\xa0**to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.**\\n\\nThis can be a computationally expensive process, and that\\'s\\xa0**where the Bellman equation comes in to help us.**'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1}, page_content=\"The Hugging Face Course\\n\\nThis repo contains the content that's used to create the **[Hugging Face course](https://huggingface.co/course/chapter1/1)**. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you'll learn how to use the [Hugging Face](https://huggingface.co/) ecosystem — [🤗 Transformers](https://github.com/huggingface/transformers), [🤗 Datasets](https://github.com/huggingface/datasets), [🤗 Tokenizers](https://github.com/huggingface/tokenizers), and [🤗 Accelerate](https://github.com/huggingface/accelerate) — as well as the [Hugging Face Hub](https://huggingface.co/models). It's completely free and open-source!\\n\\n## 🌎 Languages and translations\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 737}, page_content='| Language                                                                      | Source                                                                             | Authors                                                                                                                                                                                                                                                                                                                                                  |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1251}, page_content='|:------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1765}, page_content='| [English](https://huggingface.co/course/en/chapter1/1)                        | [`chapters/en`](https://github.com/huggingface/course/tree/main/chapters/en)       | [@sgugger](https://github.com/sgugger), [@lewtun](https://github.com/lewtun), [@LysandreJik](https://github.com/LysandreJik), [@Rocketknight1](https://github.com/Rocketknight1), [@sashavor](https://github.com/sashavor), [@osanseviero](https://github.com/osanseviero), [@SaulLu](https://github.com/SaulLu), [@lvwerra](https://github.com/lvwerra) |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 2279}, page_content='| [Bengali](https://huggingface.co/course/bn/chapter1/1) (WIP)                  | [`chapters/bn`](https://github.com/huggingface/course/tree/main/chapters/bn)       | [@avishek-018](https://github.com/avishek-018), [@eNipu](https://github.com/eNipu)                                                                                                                                                                                                                                                                       |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 2793}, page_content='| [German](https://huggingface.co/course/de/chapter1/1) (WIP)                   | [`chapters/de`](https://github.com/huggingface/course/tree/main/chapters/de)       | [@JesperDramsch](https://github.com/JesperDramsch), [@MarcusFra](https://github.com/MarcusFra), [@fabridamicelli](https://github.com/fabridamicelli)                                                                                                                                                                                                                                                          |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 3360}, page_content='| [Spanish](https://huggingface.co/course/es/chapter1/1) (WIP)                  | [`chapters/es`](https://github.com/huggingface/course/tree/main/chapters/es)       | [@camartinezbu](https://github.com/camartinezbu), [@munozariasjm](https://github.com/munozariasjm), [@fordaz](https://github.com/fordaz)                                                                                                                                                                                                                 |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 3874}, page_content='| [Persian](https://huggingface.co/course/fa/chapter1/1) (WIP)                  | [`chapters/fa`](https://github.com/huggingface/course/tree/main/chapters/fa)       | [@jowharshamshiri](https://github.com/jowharshamshiri), [@schoobani](https://github.com/schoobani)                                                                                                                                                                                                                                                       |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 4388}, page_content='| [French](https://huggingface.co/course/fr/chapter1/1)                         | [`chapters/fr`](https://github.com/huggingface/course/tree/main/chapters/fr)       | [@lbourdois](https://github.com/lbourdois), [@ChainYo](https://github.com/ChainYo), [@melaniedrevet](https://github.com/melaniedrevet), [@abdouaziz](https://github.com/abdouaziz)                                                                                                                                                                       |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 4902}, page_content='| [Gujarati](https://huggingface.co/course/gu/chapter1/1) (WIP)                 | [`chapters/gu`](https://github.com/huggingface/course/tree/main/chapters/gu)       | [@pandyaved98](https://github.com/pandyaved98)                                                                                                                                                                                                                                                                                                           |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 5416}, page_content='| [Hebrew](https://huggingface.co/course/he/chapter1/1) (WIP)                   | [`chapters/he`](https://github.com/huggingface/course/tree/main/chapters/he)       | [@omer-dor](https://github.com/omer-dor)                                                                                                                                                                                                                                                                                                                 |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 5930}, page_content='| [Hindi](https://huggingface.co/course/hi/chapter1/1) (WIP)                    | [`chapters/hi`](https://github.com/huggingface/course/tree/main/chapters/hi)       | [@pandyaved98](https://github.com/pandyaved98)                                                                                                                                                                                                                                                                                                           |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 6444}, page_content='| [Bahasa Indonesia](https://huggingface.co/course/id/chapter1/1) (WIP)                   | [`chapters/id`](https://github.com/huggingface/course/tree/main/chapters/id)       | [@gstdl](https://github.com/gstdl)                                                                                                                                                                                                                                                                                                           |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 6956}, page_content='| [Italian](https://huggingface.co/course/it/chapter1/1) (WIP)                  | [`chapters/it`](https://github.com/huggingface/course/tree/main/chapters/it)       | [@CaterinaBi](https://github.com/CaterinaBi), [@ClonedOne](https://github.com/ClonedOne),    [@Nolanogenn](https://github.com/Nolanogenn), [@EdAbati](https://github.com/EdAbati), [@gdacciaro](https://github.com/gdacciaro)                                                                                                                                                                  |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 7508}, page_content='| [Japanese](https://huggingface.co/course/ja/chapter1/1) (WIP)                 | [`chapters/ja`](https://github.com/huggingface/course/tree/main/chapters/ja)       | [@hiromu166](https://github.com/@hiromu166), [@younesbelkada](https://github.com/@younesbelkada), [@HiromuHota](https://github.com/@HiromuHota)                                                                                                                                                                                                       |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 8019}, page_content='| [Korean](https://huggingface.co/course/ko/chapter1/1) (WIP)                   | [`chapters/ko`](https://github.com/huggingface/course/tree/main/chapters/ko)       | [@Doohae](https://github.com/Doohae), [@wonhyeongseo](https://github.com/wonhyeongseo), [@dlfrnaos19](https://github.com/dlfrnaos19), [@nsbg](https://github.com/nsbg)                                                                                                                                                                                                                                                                                                                     |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 8663}, page_content='| [Portuguese](https://huggingface.co/course/pt/chapter1/1) (WIP)               | [`chapters/pt`](https://github.com/huggingface/course/tree/main/chapters/pt)       | [@johnnv1](https://github.com/johnnv1), [@victorescosta](https://github.com/victorescosta), [@LincolnVS](https://github.com/LincolnVS)                                                                                                                                                                                                                   |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 9177}, page_content='| [Russian](https://huggingface.co/course/ru/chapter1/1) (WIP)                  | [`chapters/ru`](https://github.com/huggingface/course/tree/main/chapters/ru)       | [@pdumin](https://github.com/pdumin), [@svv73](https://github.com/svv73)                                                                                                                                                                                                                                                                                 |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 9691}, page_content='| [Thai](https://huggingface.co/course/th/chapter1/1) (WIP)                     | [`chapters/th`](https://github.com/huggingface/course/tree/main/chapters/th)       | [@peeraponw](https://github.com/peeraponw), [@a-krirk](https://github.com/a-krirk), [@jomariya23156](https://github.com/jomariya23156), [@ckingkan](https://github.com/ckingkan)                                                                                                                                                                         |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 10205}, page_content='| [Turkish](https://huggingface.co/course/tr/chapter1/1) (WIP)                  | [`chapters/tr`](https://github.com/huggingface/course/tree/main/chapters/tr)       | [@tanersekmen](https://github.com/tanersekmen), [@mertbozkir](https://github.com/mertbozkir), [@ftarlaci](https://github.com/ftarlaci), [@akkasayaz](https://github.com/akkasayaz)                                                                                                                                                                       |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 10719}, page_content='| [Vietnamese](https://huggingface.co/course/vi/chapter1/1)               | [`chapters/vi`](https://github.com/huggingface/course/tree/main/chapters/vi)       | [@honghanhh](https://github.com/honghanhh)                                                                                                                                                                                                                                                                                                               |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 11227}, page_content='| [Chinese (simplified)](https://huggingface.co/course/zh-CN/chapter1/1)  | [`chapters/zh-CN`](https://github.com/huggingface/course/tree/main/chapters/zh-CN) | [@zhlhyx](https://github.com/zhlhyx), [petrichor1122](https://github.com/petrichor1122), [@1375626371](https://github.com/1375626371)                                                                                                                                                                                                                    |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 11735}, page_content='| [Chinese (traditional)](https://huggingface.co/course/zh-TW/chapter1/1) (WIP) | [`chapters/zh-TW`](https://github.com/huggingface/course/tree/main/chapters/zh-TW) | [@davidpeng86](https://github.com/davidpeng86)                                                                                                                                                                                                                                                                                                           |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 12251}, page_content=\"### Translating the course into your language\\n\\nAs part of our mission to democratise machine learning, we'd love to have the course available in many more languages! Please follow the steps below if you'd like to help translate the course into your language 🙏.\\n\\n**🗞️ Open an issue**\\n\\nTo get started, navigate to the [_Issues_](https://github.com/huggingface/course/issues) page of this repo and check if anyone else has opened an issue for your language. If not, open a new issue by selecting the _Translation template_ from the _New issue_ button.\\n\\nOnce an issue is created, post a comment to indicate which chapters you'd like to work on and we'll add your name to the list.\\n\\n**🗣 Join our Discord**\\n\\nSince it can be difficult to discuss translation details quickly over GitHub issues, we have created dedicated channels for each language on our Discord server. If you'd like to join, follow the instructions at this channel 👉: [https://discord.gg/JfAtkvEtRb](https://discord.gg/JfAtkvEtRb)\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 13244}, page_content=\"**🍴 Fork the repository**\\n\\nNext, you'll need to [fork this repo](https://docs.github.com/en/get-started/quickstart/fork-a-repo). You can do this by clicking on the **Fork** button on the top-right corner of this repo's page.\\n\\nOnce you've forked the repo, you'll want to get the files on your local machine for editing. You can do that by cloning the fork with Git as follows:\\n\\n```bash\\ngit clone https://github.com/YOUR-USERNAME/course\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 13679}, page_content=\"```\\n\\n**📋 Copy-paste the English files with a new language code**\\n\\nThe course files are organised under a main directory:\\n\\n* [`chapters`](https://github.com/huggingface/course/tree/main/chapters): all the text and code snippets associated with the course.\\n\\nYou'll only need to copy the files in the [`chapters/en`](https://github.com/huggingface/course/tree/main/chapters/en) directory, so first navigate to your fork of the repo and run the following:\\n\\n```bash\\ncd ~/path/to/course\\ncp -r chapters/en/CHAPTER-NUMBER chapters/LANG-ID/CHAPTER-NUMBER\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 14225}, page_content=\"```\\n\\nHere, `CHAPTER-NUMBER` refers to the chapter you'd like to work on and `LANG-ID` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](https://www.loc.gov/standards/iso639-2/php/code_list.php) for a handy table.\\n\\n**✍️ Start translating**\\n\\nNow comes the fun part - translating the text! The first thing we recommend is translating the part of the `_toctree.yml` file that corresponds to your chapter. This file is used to render the table of contents on the website and provide the links to the Colab notebooks. The only fields you should change are the `title`, ones -- for example, here are the parts of `_toctree.yml` that we'd translate for [Chapter 0](https://huggingface.co/course/chapter0/1?fw=pt):\\n\\n```yaml\\n- title: 0. Setup # Translate this!\\n  sections:\\n  - local: chapter0/1 # Do not change this!\\n    title: Introduction # Translate this!\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 15097}, page_content=\"```\\n\\n> 🚨 Make sure the `_toctree.yml` file only contains the sections that have been translated! Otherwise you won't be able to build the content on the website or locally (see below how).\\n\\n\\nOnce you have translated the `_toctree.yml` file, you can start translating the [MDX](https://mdxjs.com/) files associated with your chapter.\\n\\n> 🙋 If the `_toctree.yml` file doesn't yet exist for your language, you can simply create one by copy-pasting from the English version and deleting the sections that aren't related to your chapter. Just make sure it exists in the `chapters/LANG-ID/` directory!\\n\\n**👷\\u200d♂️ Build the course locally**\\n\\nOnce you're happy with your changes, you can preview how they'll look by first installing the [`doc-builder`](https://github.com/huggingface/doc-builder) tool that we use for building all documentation at Hugging Face:\\n\\n```\\npip install hf-doc-builder\\n```\\n\\n```\\ndoc-builder preview course ../course/chapters/LANG-ID --not_python_module\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 15979}, page_content='```\\n\\n```\\ndoc-builder preview course ../course/chapters/LANG-ID --not_python_module\\n```\\n\\n**`preview` command does not work with Windows.\\n\\nThis will build and render the course on [http://localhost:3000/](http://localhost:3000/). Although the content looks much nicer on the Hugging Face website, this step will still allow you to check that everything is formatted correctly.\\n\\n**🚀 Submit a pull request**\\n\\nIf the translations look good locally, the final step is to prepare the content for a pull request. Here, the first think to check is that the files are formatted correctly. For that you can run:\\n\\n```\\npip install -r requirements.txt\\nmake style'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 16581}, page_content=\"```\\npip install -r requirements.txt\\nmake style\\n```\\n\\nOnce that's run, commit any changes, open a pull request, and tag [@lewtun](https://github.com/lewtun) for a review. Congratulations, you've now completed your first translation 🥳!\\n\\n> 🚨 To build the course on the website, double-check your language code exists in `languages` field of the `build_documentation.yml` and `build_pr_documentation.yml` files in the `.github` folder. If not, just add them in their alphabetical order.\\n\\n## 📔 Jupyter notebooks\\n\\nThe Jupyter notebooks containing all the code from the course are hosted on the [`huggingface/notebooks`](https://github.com/huggingface/notebooks) repo. If you wish to generate them locally, first install the required dependencies:\\n\\n```bash\\npython -m pip install -r requirements.txt\\n```\\n\\nThen run the following script:\\n\\n```bash\\npython utils/generate_notebooks.py --output_dir nbs\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 17469}, page_content='```\\n\\nThis script extracts all the code snippets from the chapters and stores them as notebooks in the `nbs` folder (which is ignored by Git by default).\\n\\n## ✍️ Contributing a new chapter\\n\\n> Note: we are not currently accepting community contributions for new chapters. These instructions are for the Hugging Face authors.\\n\\nAdding a new chapter to the course is quite simple:'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 17792}, page_content=\"Adding a new chapter to the course is quite simple:\\n\\n1. Create a new directory under `chapters/en/chapterX`, where `chapterX` is the chapter you'd like to add.\\n2. Add numbered MDX files `sectionX.mdx` for each section. If you need to include images, place them in the [huggingface-course/documentation-images](https://huggingface.co/datasets/huggingface-course/documentation-images) repository and use the [HTML Images Syntax](https://www.w3schools.com/html/html_images.asp) with the path `https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/{langY}/{chapterX}/{your-image.png}`.\\n3. Update the `_toctree.yml` file to include your chapter sections -- this information will render the table of contents on the website. If your section involves both the PyTorch and TensorFlow APIs of `transformers`, make sure you include links to both Colabs in the `colab` field.\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 18689}, page_content='If you get stuck, check out one of the existing chapters -- this will often show you the expected syntax.\\n\\nOnce you are happy with the content, open a pull request and tag [@lewtun](https://github.com/lewtun) for a review. We recommend adding the first chapter draft as a single pull request -- the team will then provide feedback internally to iterate on the content 🤗!\\n\\n## 🙌 Acknowledgements\\n\\nThe structure of this repo and README are inspired by the wonderful [Advanced NLP with spaCy](https://github.com/ines/spacy-course) course.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 1}, page_content='The advantages and disadvantages of policy-gradient methods\\n\\nAt this point, you might ask, \"but Deep Q-Learning is excellent! Why use policy-gradient methods?\". To answer this question, let\\'s study the **advantages and disadvantages of policy-gradient methods**.\\n\\n## Advantages\\n\\nThere are multiple advantages over value-based methods. Let\\'s see some of them:\\n\\n### The simplicity of integration\\n\\nWe can estimate the policy directly without storing additional data (action values).\\n\\n### Policy-gradient methods can learn a stochastic policy\\n\\nPolicy-gradient methods can\\xa0**learn a stochastic policy while value functions can\\'t**.\\n\\nThis has two consequences:\\n\\n1. We **don\\'t need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent explores\\xa0**the state space without always taking the same trajectory.**'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 878}, page_content='2. We also get rid of the problem of **perceptual aliasing**. Perceptual aliasing is when two states seem (or are) the same but need different actions.\\n\\nLet\\'s take an example: we have an intelligent vacuum cleaner whose goal is to suck the dust and avoid killing the hamsters.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster1.jpg\" alt=\"Hamster 1\"/>\\n</figure>\\n\\nOur vacuum cleaner can only perceive where the walls are.\\n\\nThe problem is that the **two red (colored) states are aliased states because the agent perceives an upper and lower wall for each**.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster2.jpg\" alt=\"Hamster 1\"/>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 1754}, page_content='Under a deterministic policy, the policy will either always move right when in a red state or always move left. **Either case will cause our agent to get stuck and never suck the dust**.\\n\\nUnder a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic policy** (\"greedy epsilon strategy\"). Consequently, our agent can **spend a lot of time before finding the dust**.\\n\\nOn the other hand, an optimal stochastic policy **will randomly move left or right in red (colored) states**. Consequently, **it will not be stuck and will reach the goal state with a high probability**.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg\" alt=\"Hamster 1\"/>\\n</figure>\\n\\n### Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 2664}, page_content=\"The problem with Deep Q-learning is that their **predictions assign a score (maximum expected future reward) for each possible action**, at each time step, given the current state.\\n\\nBut what if we have an infinite possibility of actions?\\n\\nFor instance, with a self-driving car, at each state, you can have a (near) infinite choice of actions (turning the wheel at 15°, 17.2°, 19,4°, honking, etc.). **We'll need to output a Q-value for each possible action**! And **taking the max action of a continuous output is an optimization problem itself**!\\n\\nInstead, with policy-gradient methods, we output a\\xa0**probability distribution over actions.**\\n\\n### Policy-gradient methods have better convergence properties\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 3308}, page_content=\"### Policy-gradient methods have better convergence properties\\n\\nIn value-based methods, we use an aggressive operator to **change the value function: we take the maximum over Q-estimates**.\\nConsequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.\\n\\nFor instance, if during the training, the best action was left (with a Q-value of 0.22) and the training step after it's right (since the right Q-value becomes 0.23), we dramatically changed the policy since now the policy will take most of the time right instead of left.\\n\\nOn the other hand, in policy-gradient methods, stochastic policy action preferences (probability of taking action) **change smoothly over time**.\\n\\n## Disadvantages\\n\\nNaturally, policy-gradient methods also have some disadvantages:\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 4117}, page_content=\"## Disadvantages\\n\\nNaturally, policy-gradient methods also have some disadvantages:\\n\\n- **Frequently, policy-gradient methods converges to a local maximum instead of a global optimum.**\\n- Policy-gradient goes slower,\\xa0**step by step: it can take longer to train (inefficient).**\\n- Policy-gradient can have high variance. We'll see in the actor-critic unit why, and how we can solve this problem.\\n\\n👉 If you want to go deeper into the advantages and disadvantages of policy-gradient methods, [you can check this video](https://youtu.be/y3oqOjHilio).\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/shap_e.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Shap-E\\n\\nThe Shap-E model was proposed in [Shap-E: Generating Conditional 3D Implicit Functions](https://huggingface.co/papers/2305.02463) by Alex Nichol and Heewoo Jun from [OpenAI](https://github.com/openai).\\n\\nThe abstract from the paper is:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/shap_e.md', 'start_index': 797}, page_content='The abstract from the paper is:\\n\\n*We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space.*'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/shap_e.md', 'start_index': 1719}, page_content='The original codebase can be found at [openai/shap-e](https://github.com/openai/shap-e).\\n\\n<Tip>\\n\\nSee the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## ShapEPipeline\\n[[autodoc]] ShapEPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ShapEImg2ImgPipeline\\n[[autodoc]] ShapEImg2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ShapEPipelineOutput\\n[[autodoc]] pipelines.shap_e.pipeline_shap_e.ShapEPipelineOutput'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/streaming_wav2vec/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: streaming_wav2vec\\n\\n\\n```\\n!pip install -q gradio torch transformers \\n```\\n\\n\\n```\\nfrom transformers import pipeline\\nimport gradio as gr\\nimport time\\n\\np = pipeline(\"automatic-speech-recognition\")\\n\\ndef transcribe(audio, state=\"\"):\\n    time.sleep(2)\\n    text = p(audio)[\"text\"]\\n    state += text + \" \"\\n    return state, state\\n\\ndemo = gr.Interface(\\n    fn=transcribe, \\n    inputs=[\\n        gr.Audio(sources=[\"microphone\"], type=\"filepath\", streaming=True), \\n        \"state\"\\n    ],\\n    outputs=[\\n        \"textbox\",\\n        \"state\"\\n    ],\\n    live=True\\n)\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 1}, page_content=\"Adding a Sign-In with HF button to your Space\\n\\nYou can enable a built-in sign-in flow in your Space by seamlessly creating and associating an [OAuth/OpenID connect](https://developer.okta.com/blog/2019/10/21/illustrated-guide-to-oauth-and-oidc) app so users can log in with their HF account.\\n\\nThis enables new use cases for your Space. For instance, when combined with [Persistent Storage](https://huggingface.co/docs/hub/spaces-storage), a generative AI Space could allow users to log in to access their previous generations, only accessible to them.\\n\\n<Tip>\\n\\nThis guide will take you through the process of integrating a *Sign-In with HF* button into any Space. If you're seeking a fast and simple method to implement this in a **Gradio** Space, take a look at its [built-in integration](https://www.gradio.app/guides/sharing-your-app#o-auth-login-via-hugging-face).\\n\\n</Tip>\\n\\n<Tip>\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 870}, page_content='</Tip>\\n\\n<Tip>\\n\\nYou can also use the HF OAuth flow to create a \"Sign in with HF\" flow in any website or App, outside of Spaces. [Read our general OAuth page](./oauth).\\n\\n</Tip>\\n\\n## Create an OAuth app\\n\\nAll you need to do is add `hf_oauth: true` to your Space\\'s metadata inside your `README.md` file.\\n\\nHere\\'s an example of metadata for a Gradio Space:\\n\\n```yaml\\ntitle: Gradio Oauth Test\\nemoji: 🏆\\ncolorFrom: pink\\ncolorTo: pink\\nsdk: gradio\\nsdk_version: 3.40.0\\npython_version: 3.10.6\\napp_file: app.py\\n\\nhf_oauth: true\\n# optional, see \"Scopes\" below. \"openid profile\" is always included.\\nhf_oauth_scopes:\\n - read-repos\\n - write-repos\\n - manage-repos\\n - inference-api'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 1528}, page_content='```\\n\\nYou can check out the [configuration reference docs](./spaces-config-reference) for more information.\\n\\nThis will add the following [environment variables](https://huggingface.co/docs/hub/spaces-overview#helper-environment-variables) to your space:\\n\\n- `OAUTH_CLIENT_ID`: the client ID of your OAuth app (public)\\n- `OAUTH_CLIENT_SECRET`: the client secret of your OAuth app\\n- `OAUTH_SCOPES`: scopes accessible by your OAuth app.\\n- `OPENID_PROVIDER_URL`: The URL of the OpenID provider. The OpenID metadata will be available at [`{OPENID_PROVIDER_URL}/.well-known/openid-configuration`](https://huggingface.co/.well-known/openid-configuration).\\n\\nAs for any other environment variable, you can use them in your code by using `os.getenv(\"OAUTH_CLIENT_ID\")`, for example.\\n\\n## Redirect URLs \\n\\nYou can use any redirect URL you want, as long as it targets your Space.'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 2300}, page_content=\"## Redirect URLs \\n\\nYou can use any redirect URL you want, as long as it targets your Space.\\n\\nNote that `SPACE_HOST` is [available](https://huggingface.co/docs/hub/spaces-overview#helper-environment-variables) as an environment variable.\\n\\nFor example, you can use `https://{SPACE_HOST}/login/callback` as a redirect URI.\\n\\n## Scopes\\n\\nThe following scopes are always included for Spaces:\\n\\n- `openid`: Get the ID token in addition to the access token.\\n- `profile`: Get the user's profile information (username, avatar, etc.)\\n\\nThose scopes are optional and can be added by setting `hf_oauth_scopes` in your Space's metadata:\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 2822}, page_content=\"Those scopes are optional and can be added by setting `hf_oauth_scopes` in your Space's metadata:\\n\\n- `email`: Get the user's email address.\\n- `read-repos`: Get read access to the user's personal repos.\\n- `write-repos`: Get write access to the user's personal repos. Does not grant read access on its own, you need to include `read-repos` as well.\\n- `manage-repos`: Get access to a repo's settings. Also grants repo creation and deletion.\\n- `inference-api`: Get access to the [Inference API](https://huggingface.co/docs/api-inference/index), you will be able to make inference requests on behalf of the user.\\n\\n## Adding the button to your Space\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 3431}, page_content='## Adding the button to your Space\\n\\nYou now have all the information to add a \"Sign-in with HF\" button to your Space. Some libraries ([Python](https://github.com/lepture/authlib), [NodeJS](https://github.com/panva/node-openid-client)) can help you implement the OpenID/OAuth protocol. Gradio also provides **built-in support**, making implementing the Sign-in with HF button a breeze; you can [check out the associated guide](https://www.gradio.app/guides/sharing-your-app#o-auth-login-via-hugging-face).\\n\\nBasically, you need to:'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 3937}, page_content='Basically, you need to:\\n\\n- Redirect the user to `https://huggingface.co/oauth/authorize?redirect_uri={REDIRECT_URI}&scope=openid%20profile&client_id={CLIENT_ID}&state={STATE}`, where `STATE` is a random string that you will need to verify later.\\n- Handle the callback on `/auth/callback` or `/login/callback` (or your own custom callback URL) and verify the `state` parameter.\\n- Use the `code` query parameter to get an access token and id token from `https://huggingface.co/oauth/token` (POST request with `client_id`, `code`, `grant_type=authorization_code` and `redirect_uri` as form data, and with `Authorization: Basic {base64(client_id:client_secret)}` as a header).\\n\\n<Tip warning={true}>\\n\\nYou should use `target=_blank` on the button to open the sign-in page in a new tab, unless you run the space outside its `iframe`. Otherwise, you might encounter issues with cookies on some browsers.\\n\\n</Tip>\\n\\n## Examples:'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 4834}, page_content='</Tip>\\n\\n## Examples:\\n\\n- [Gradio test app](https://huggingface.co/spaces/Wauplin/gradio-oauth-test)\\n- [Hugging Chat (NodeJS/SvelteKit)](https://huggingface.co/spaces/huggingchat/chat-ui)\\n- [Inference Widgets (Auth.js/SvelteKit)](https://huggingface.co/spaces/huggingfacejs/inference-widgets), uses the `inference-api` scope to make inference requests on behalf of the user.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# 🧨 Diffusers Examples\\n\\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\\nfor a variety of use cases involving training or fine-tuning.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 788}, page_content='**Note**: If you are looking for **official** examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).\\n\\nOur examples aspire to be **self-contained**, **easy-to-tweak**, **beginner-friendly** and for **one-purpose-only**.\\nMore specifically, this means:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 1159}, page_content='- **Self-contained**: An example script shall only depend on \"pip-install-able\" Python packages that can be found in a `requirements.txt` file. Example scripts shall **not** depend on any local files. This means that one can simply download an example script, *e.g.* [train_unconditional.py](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/train_unconditional.py), install the required dependencies, *e.g.* [requirements.txt](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/requirements.txt) and execute the example script.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 1769}, page_content=\"- **Easy-to-tweak**: While we strive to present as many use cases as possible, the example scripts are just that - examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs. To help you with that, most of the examples fully expose the preprocessing of the data and the training loop to allow you to tweak and edit them as required.\\n- **Beginner-friendly**: We do not aim for providing state-of-the-art training scripts for the newest models, but rather examples that can be used as a way to better understand diffusion models and how to use them with the `diffusers` library. We often purposefully leave out certain state-of-the-art methods if we consider them too complex for beginners.\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 2573}, page_content='- **One-purpose-only**: Examples should show one task and one task only. Even if a task is from a modeling point of view very similar, *e.g.* image super-resolution and image modification tend to use the same model and training method, we want examples to showcase only one task to keep them as readable and easy-to-understand as possible.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 2914}, page_content='We provide **official** examples that cover the most popular tasks of diffusion models.\\n*Official* examples are **actively** maintained by the `diffusers` maintainers and we try to rigorously follow our example philosophy as defined above.\\nIf you feel like another important example should exist, we are more than happy to welcome a [Feature Request](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feature_request.md&title=) or directly a [Pull Request](https://github.com/huggingface/diffusers/compare) from you!\\n\\nTraining examples show how to pretrain or fine-tune diffusion models for a variety of tasks. Currently we support:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 3578}, page_content='| Task | 🤗 Accelerate | 🤗 Datasets | Colab\\n|---|---|:---:|:---:|\\n| [**Unconditional Image Generation**](./unconditional_image_generation) | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)\\n| [**Text-to-Image fine-tuning**](./text_to_image) | ✅ | ✅ |\\n| [**Textual Inversion**](./textual_inversion) | ✅ | - | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\\n| [**Dreambooth**](./dreambooth) | ✅ | - | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb)\\n| [**ControlNet**](./controlnet) | ✅ | ✅ | -\\n| [**InstructPix2Pix**](./instruct_pix2pix) | ✅ | ✅ | -'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 4503}, page_content='| [**InstructPix2Pix**](./instruct_pix2pix) | ✅ | ✅ | -\\n| [**Reinforcement Learning for Control**](https://github.com/huggingface/diffusers/blob/main/examples/reinforcement_learning/run_diffusers_locomotion.py)                    | - | - | coming soon.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 4757}, page_content='## Community\\n\\nIn addition, we provide **community** examples, which are examples added and maintained by our community.\\nCommunity examples can consist of both *training* examples or *inference* pipelines.\\nFor such examples, we are more lenient regarding the philosophy defined above and also cannot guarantee to provide maintenance for every issue.\\nExamples that are useful for the community, but are either not yet deemed popular or not yet following our above philosophy should go into the [community examples](https://github.com/huggingface/diffusers/tree/main/examples/community) folder. The community folder therefore includes training examples and inference pipelines.\\n**Note**: Community examples can be a [great first contribution](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) to show to the community how you like to use `diffusers` 🪄.\\n\\n## Research Projects'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 5661}, page_content='## Research Projects\\n\\nWe also provide **research_projects** examples that are maintained by the community as defined in the respective research project folders. These examples are useful and offer the extended capabilities which are complementary to the official examples. You may refer to [research_projects](https://github.com/huggingface/diffusers/tree/main/examples/research_projects) for details.\\n\\n## Important note\\n\\nTo make sure you can successfully run the latest versions of the example scripts, you have to **install the library from source** and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install .'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 6417}, page_content='```\\nThen cd in the example folder of your choice and run\\n```bash\\npip install -r requirements.txt\\n```'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/examples/image_classification/README.md', 'start_index': 1}, page_content='Fine-tuning for image classification using LoRA and 🤗 PEFT\\n\\n## Vision Transformer model from transformers\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/image_classification/image_classification_peft_lora.ipynb) \\n\\nWe provide a notebook (`image_classification_peft_lora.ipynb`) where we learn how to use [LoRA](https://arxiv.org/abs/2106.09685) from 🤗 PEFT to fine-tune an image classification model by ONLY using **0.7%** of the original trainable parameters of the model. \\n\\nLoRA adds low-rank \"update matrices\" to certain blocks in the underlying model (in this case the attention blocks) and ONLY trains those matrices during fine-tuning. During inference, these update matrices are _merged_ with the original model parameters. For more details, check out the [original LoRA paper](https://arxiv.org/abs/2106.09685). \\n\\n## PoolFormer model from timm'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/examples/image_classification/README.md', 'start_index': 930}, page_content='## PoolFormer model from timm\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb) \\n\\nThe notebook `image_classification_timm_peft_lora.ipynb` showcases fine-tuning an image classification model using from the [timm](https://huggingface.co/docs/timm/index) library. Again, LoRA is used to reduce the numberof trainable parameters to a fraction of the total.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 0}, page_content='--\\ntitle: \"Generating Human-level Text with Contrastive Search in Transformers 🤗\"\\nthumbnail: /blog/assets/115_introducing_contrastive_search/thumbnail.png\\nauthors:\\n- user: GMFTBY\\n---\\n\\n# Generating Human-level Text with Contrastive Search in Transformers 🤗\\n\\n\\n****\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n### 1. Introduction:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 510}, page_content='### 1. Introduction:\\n\\nNatural language generation (i.e. text generation) is one of the core tasks in natural language processing (NLP). In this blog, we introduce the current state-of-the-art decoding method, ___Contrastive Search___, for neural text generation. Contrastive search is originally proposed in _\"A Contrastive Framework for Neural Text Generation\"_ <a href=\\'#references\\'>[1]</a> ([[Paper]](https://arxiv.org/abs/2202.06417)[[Official Implementation]](https://github.com/yxuansu/SimCTG)) at NeurIPS 2022. Moreover, in this follow-up work,  _\"Contrastive Search Is What You Need For Neural Text Generation\"_ <a href=\\'#references\\'>[2]</a> ([[Paper]](https://arxiv.org/abs/2210.14140) [[Official Implementation]](https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need)), the authors further demonstrate that contrastive search can generate human-level text using **off-the-shelf** language models across **16** languages.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 1451}, page_content=\"**[Remark]** For users who are not familiar with text generation, please refer more details to [this blog post](https://huggingface.co/blog/how-to-generate).\\n\\n****\\n\\n<span id='demo'/>\\n\\n### 2. Hugging Face 🤗 Demo of Contrastive Search:\\n\\nContrastive Search is now available on 🤗 `transformers`, both on PyTorch and TensorFlow. You can interact with the examples shown in this blog post using your framework of choice in [this Colab notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb), which is linked at the top. We have also built this awesome [demo](https://huggingface.co/spaces/joaogante/contrastive_search_generation) which directly compares contrastive search with other popular decoding methods (e.g. beam search, top-k sampling <a href='#references'>[3]</a>, and nucleus sampling <a href='#references'>[4]</a>).\\n\\n****\\n\\n<span id='installation'/>\\n\\n### 3. Environment Installation:\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 2347}, page_content='****\\n\\n<span id=\\'installation\\'/>\\n\\n### 3. Environment Installation:\\n\\nBefore running the experiments in the following sections, please install the update-to-date version of `transformers` as\\n```yaml\\npip install torch\\npip install \"transformers==4.24.0\"'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 2596}, page_content=\"```\\n\\n****\\n\\n<span id='problems_of_decoding_methods'/>\\n\\n### 4. Problems of Existing Decoding Methods:\\n\\nDecoding methods can be divided into two categories: (i) deterministic methods and (ii) stochastic methods. Let's discuss both!\\n\\n\\n<span id='deterministic_methods'/>\\n\\n#### 4.1. Deterministic Methods:\\n\\nDeterministic methods, e.g. greedy search and beam search, generate text by selecting the text continuation with the highest likelihood measured by the language model. However, as widely discussed in previous studies <a href='#references'>[3]</a><a href='#references'>[4]</a>, deterministic methods often lead to the problem of _model degeneration_, i.e., the generated text is unnatural and contains undesirable repetitions.\\n\\nBelow, let's see an example of generated text from greedy search using GPT-2 model.\\n\\n```python\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 3409}, page_content='```python\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\'gpt2-large\\')\\ninput_ids = tokenizer(\\'DeepMind Company is\\', return_tensors=\\'pt\\').input_ids\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2-large\\')\\n\\noutput = model.generate(input_ids, max_length=128)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 3828}, page_content=\"```\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeepMind Company is a leading AI research company, with a focus on deep learning and deep\\nlearning-based systems.\\n\\nThe company's research is focused on the development of deep learning-based systems that\\ncan learn from large amounts of data, and that can be used to solve real-world problems.\\n\\nDeepMind's research is also used by the UK government to develop new technologies for the\\nUK's National Health Service.\\n\\nDeepMind's research is also used by the UK government to develop new technologies for the\\nUK's National Health Service.\\n\\nDeepMind's research is also used by the UK government to develop new technologies\\n----------------------------------------------------------------------------------------------------\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 4721}, page_content=\"```\\n</details>\\n\\n**[Remark]** From the result generated by greedy search, we can see obvious pattern of repetitions.\\n\\n<span id='stochastic_methods'/>\\n\\n#### 4.2. Stochastic Methods:\\n\\nTo address the issues posed by deterministic methods, stochastic methods generate text by introducing randomness during the decoding process. Two widely-used stochastic methods are (i) top-k sampling <a href='#references'>[3]</a> and (ii) nucleus sampling (also called top-p sampling) <a href='#references'>[4]</a>.\\n\\nBelow, we illustrate an example of generated text by nucleus sampling (p=0.95) using the GPT-2 model.\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('gpt2-large')\\ninput_ids = tokenizer('DeepMind Company is', return_tensors='pt').input_ids\\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-large')\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 5589}, page_content='torch.manual_seed(0.)\\noutput = model.generate(input_ids, do_sample=True, max_length=128, top_p=0.95, top_k=0)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 5813}, page_content='```\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeepMind Company is a leading provider of AI-based research, development, and delivery of\\nAI solutions for security, infrastructure, machine learning, communications, and so on.\"\\n\\n\\'AI is not journalism\\'\\n\\nWorse still was the message its researchers hoped would reach the world\\'s media — that it\\nwas not really research, but rather a get-rich-quick scheme to profit from living forces\\'\\nignorance.\\n\\n\"The thing is, we know that people don\\'t consciously assess the value of the others\\'\\ninformation. They understand they will get the same on their own.\"\\n\\nOne example? Given the details of today\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 6677}, page_content=\"```\\n</details>\\n\\n**[Remark]** While nucleus sampling can generate text free of repetitions, the semantic coherence of the generated text is not well-maintained. For instance, the generated phrase _'AI is not journalism'_ is incoherent with respect to the given prefix, i.e. _'DeepMind Company'_.\\n\\nWe note that this semantic inconsistency problem can partially be remedied by lowering the temperature. However, reducing the temperature brings nucleus sampling closer to greedy search, which can be seen as a trade-off between greedy search and nucleus sampling. Generally, it is challenging to find a prompt and model-independent temperature that avoids both the pitfalls of greedy search and nucleus sampling.\\n\\n****\\n\\n<span id='contrastive_search'/>\\n\\n### 5. Contrastive Search:\\n\\nIn this section, we introduce a new decoding method, ___Contrastive Search___, in details.\\n\\n<span id='contrastive_objective'/>\\n\\n#### 5.1. Decoding Objective:\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 7546}, page_content='<span id=\\'contrastive_objective\\'/>\\n\\n#### 5.1. Decoding Objective:\\n\\nGiven the prefix text \\\\\\\\(x_{< t}\\\\\\\\), the selection of the output token \\\\\\\\(x_{t}\\\\\\\\) follows\\n\\n<center class=\"half\">\\n    <img src=\"assets/115_introducing_contrastive_search/formulation.png\" width=\"750\"/>\\n</center>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 7825}, page_content=\"where \\\\\\\\(V^{(k)}\\\\\\\\) is the set of top-k predictions from the language model's probability distribution \\\\\\\\(p_{\\\\theta}(v|x_{< t})\\\\\\\\). The first term, i.e. _model confidence_, is the probability of the candidate \\\\\\\\(v\\\\\\\\) predicted by the language model. The second term, _degeneration penalty_, measures how discriminative of \\\\\\\\(v\\\\\\\\) with respect to the previous context \\\\\\\\( x_{< t}\\\\\\\\) and the function \\\\\\\\(s(\\\\cdot, \\\\cdot)\\\\\\\\) computes the cosine similarity between the token representations. More specifically, the degeneration penalty is defined as the maximum cosine similarity between the token representation of \\\\\\\\(v\\\\\\\\), i.e. \\\\\\\\(h_{v}\\\\\\\\), and that of all tokens in the context \\\\\\\\(x_{< t}\\\\\\\\). Here, the candidate representation \\\\\\\\(h_{v}\\\\\\\\) is computed by the language model given the concatenation of \\\\\\\\(x_{< t}\\\\\\\\) and \\\\\\\\(v\\\\\\\\). Intuitively, a larger degeneration penalty of \\\\\\\\(v\\\\\\\\) means it is more similar (in the representation space) to the context, therefore more likely leading to the problem of\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 8730}, page_content='(in the representation space) to the context, therefore more likely leading to the problem of model degeneration. The hyperparameter \\\\\\\\(\\\\alpha\\\\\\\\) regulates the importance of these two components. When \\\\\\\\(\\\\alpha=0\\\\\\\\), contrastive search degenerates to the vanilla greedy search.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 9009}, page_content='**[Remark]** When generating output, contrastive search jointly considers (i) the probability predicted by the language model to maintain the semantic coherence between the generated text and the prefix text; and (ii) the similarity with respect to the previous context to avoid model degeneration.\\n\\n\\n<span id=\\'contrastive_generation\\'/>\\n\\n#### 5.2. Generating Text with Contrastive Search:\\n\\nBelow, we use the same prefix text (i.e. _\"DeepMind Company is\"_) as in Section <a href=\\'#deterministic_methods\\'>4.1</a> and <a href=\\'#stochastic_methods\\'>4.2</a>, and generate the text with contrastive search (k=4 and \\\\\\\\(\\\\alpha=0.6\\\\\\\\)). To fully demonstrate the superior capability of contrastive search, we let the language model generate a **long** document with **512** tokens as\\n\\n```python\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 9784}, page_content='```python\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\\n\\nmodel_name = \\'gpt2-large\\'\\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\\nmodel = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\\nmodel.eval()\\n\\n# prepare the prefix\\nprefix_text = r\\'DeepMind Company is\\'\\ninput_ids = tokenizer(prefix_text, return_tensors=\\'pt\\').input_ids\\n\\n# generate the result with contrastive search\\noutput = model.generate(input_ids, penalty_alpha=0.6, top_k=4, max_length=512)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 10398}, page_content='```\\n\\nThe arguments are as follows:\\n* `--top_k`: The hyperparameter \\\\\\\\(k\\\\\\\\) in contrastive search.\\n* `--penalty_alpha`: The hyperparameter \\\\\\\\(\\\\alpha\\\\\\\\) in contrastive search.\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 10629}, page_content=\"```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeepMind Company is a leader in artificial intelligence (AI). We have a long history of working\\nwith companies such as Google, Facebook, Amazon, and Microsoft to build products that improve\\npeople's lives, and today we are excited to announce that DeepMind's AlphaGo program has won the\\ngame of Go, becoming the first program to defeat a professional Go player.\\n\\nThe victory is a testament to the power of deep learning, and to the incredible work of our\\nresearch team, which has been at the forefront of AI research for the past five years. AlphaGo\\nis one of the most advanced Go programs ever created, and its performance is an important step\\ntowards the goal of human-level AI.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 11424}, page_content='\"This is the culmination of a decade of hard work,\" said Andy Ng, co-founder and CTO of DeepMind.\\n\"We are thrilled to have achieved this milestone and look forward to continuing to develop AI that\\ncan be used in a wide range of applications and to help people live better lives.\"\\n\\nDeepMind\\'s work on Go began in 2010, when it began to train a neural network to play Go using\\nmillions of games played by top Go players around the world. Since then, the team has refined the\\nalgorithm, adding more and more layers of reinforcement learning to make it better at recognizing\\npatterns and making decisions based on those patterns. In the past year and a half, the team has\\nmade significant progress in the game, winning a record-tying 13 games in a row to move into the\\ntop four of the world rankings.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 12222}, page_content='\"The game of Go is a complex game in which players have to be very careful not to overextend their\\nterritory, and this is something that we have been able to improve over and over again,\" said\\nDr. Demis Hassabis, co-founder and Chief Scientific Officer of DeepMind. \"We are very proud of our\\nteam\\'s work, and we hope that it will inspire others to take the next step in their research and\\napply the same techniques to other problems.\"'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 12658}, page_content='In addition to the win in Go, DeepMind has also developed an AI system that can learn to play a\\nnumber of different games, including poker, Go, and chess. This AI system, called Tarsier, was\\ndeveloped in partnership with Carnegie Mellon University and the University of California,\\nBerkeley, and is being used to teach computer vision and machine learning to identify objects in\\nimages and recognize speech in natural language. Tarsier has been trained to play the game of Go\\nand other games on a\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 13256}, page_content='```\\n</details>\\n\\n**[Remark]** We see that the generated text is of exceptionally high quality. The entire document is grammatically fluent as well as semantically coherent. Meanwhile, the generated text also well maintains its factually correctness. For instance, in the first paragraph, it elaborates _\"AlphaGo\"_ as the _\"first program to defeat a professional Go player\"_.\\n\\n\\n<span id=\\'contrastive_visual_demonstration\\'/>\\n\\n#### 5.3. Visual Demonstration of Contrastive Search:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 13679}, page_content='#### 5.3. Visual Demonstration of Contrastive Search:\\n\\nTo better understand how contrastive search works, we provide a visual comparison between greedy search (<a href=\\'#deterministic_methods\\'>Section 4.1</a>) and contrastive search. Specifically, we visualize the token similarity matrix of the generated text from greedy search and contrastive search, respectively. The similarity between two tokens is defined as the cosine similarity between their token representations (i.e. the hidden states of the last transformer layer). The results of greedy search (top) and contrastive search (bottom) are shown in the Figure below.\\n\\n<center class=\"half\">\\n    <img src=\"assets/115_introducing_contrastive_search/greedy_search_visualization.png\" width=\"400\"/>\\n    <img src=\"assets/115_introducing_contrastive_search/contrastive_search_visualization.png\" width=\"400\"/>\\n</center>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 14552}, page_content=\"**[Remark]** From the result of greedy search, we see high similarity scores in the off-diagonal entries which clearly indicates the generated repetitions by greedy search. On the contrary, in the result of contrastive search, the high similarity scores mostly appear in the diagonal entries which verifies that the degeneration problem is successfully addressed. This nice property of contrastive search is achieved by the introduction of degeneration penalty (see <a href='#contrastive_objective'>Section 5.1</a>) during the decoding process.\\n\\n\\n****\\n\\n<span id='more_examples'/>\\n\\n### 6. More Generated Examples:\\n\\nIn this section, we provide more generated examples to compare different decoding methods.\\n\\n<span id='gpt2_example_one'/>\\n\\n#### 6.1. Example One - GPT-2:\\n\\nIn this part, we use GPT-2 to generate text with the prefix text from the original [OpenAI blog](https://openai.com/blog/better-language-models/) that announced the release of GPT-2.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 15505}, page_content='> _In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English._\\n\\n\\n<details open>\\n<summary><b> Load the language model and prepare the prefix text:</b></summary>\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\'gpt2-large\\')\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2-large\\')\\n\\nprefix_text = r\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\\ninput_ids = tokenizer(prefix_text, return_tensors=\\'pt\\').input_ids'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 16343}, page_content='```\\n</details>\\n\\n<span id=\\'gpt2_greedy_example_one\\'/>\\n\\n##### 6.1.1. Generating Text with Greedy Search:\\n\\n<details>\\n<summary><b>Code: [click to expand]</b></summary>\\n\\n```python\\noutput = model.generate(input_ids, max_length=512)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n</details>\\n\\n<details>\\n<summary><b>Model Output: [click to expand]</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 16768}, page_content='```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously\\nunexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact\\nthat the unicorns spoke perfect English.\\n\\nThe researchers, led by Dr. David R. Williams of the University of California, Santa Cruz,\\ndiscovered the unicorns in the Andes Mountains of Peru. The area is known for its unique geology\\nand is home to a number of rare species of animals.\\n\\nThe researchers found the unicorns in the Andes Mountains of Peru.\\n\\n\"We were surprised to find that the unicorns were able to communicate with each other,\" Williams\\nsaid. \"We were also surprised to find that they were able to communicate in English.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 17733}, page_content='\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 18656}, page_content='The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago\\n----------------------------------------------------------------------------------------------------'),\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebf312",
   "metadata": {},
   "source": [
    "We also have to keep in mind that when embedding documents, we will use an embedding model that accepts a certain maximum sequence length max_seq_length.\n",
    "\n",
    "So we should make sure that our chunk sizes are below this limit because any longer chunk will be truncated before processing, thus losing relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e41ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's max seq length: 512\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(f\"Model's max seq length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1fc63de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c367950b0f41bdb9b27a16f06b48d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31085 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
    "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86ffdf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the distribution of document lengths, counted as the number of tokens\n",
    "\n",
    "figure = pd.Series(lengths).hist()\n",
    "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "991cea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "\n",
    "def split_documents(\n",
    "  chunk_size: int,\n",
    "  kb_article:List[LangchainDocument],\n",
    "  tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME    \n",
    ") -> List[LangchainDocument] :\n",
    "    \"\"\"\n",
    "    Split document into chunks of maximum size 'chunk size' token and return a list of unique documents\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators = MARKDOWN_SEPARATORS\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in kb_article:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # remove duplicates\n",
    "\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content]=True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f7a7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed = split_documents(\n",
    "    512,     # We choose a chunk size=512 as per our model embedding model's max sent size \n",
    "    RAW_KNOWLEDGE_BASE,\n",
    "    tokenizer_name=EMBEDDING_MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d49eed0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "lengths = [len(tokenizer.encode(doc.page_content))  for doc in docs_processed]\n",
    "\n",
    "figure = pd.Series(lengths).hist()\n",
    "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
    "plt.ylabel(\"Number of tokens\")\n",
    "plt.xlabel(\"Number of documents\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abfdd6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 1}, page_content='Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />'),\n",
       " Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 1580}, page_content='## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1}, page_content=\"Choosing a metric for your task\\n\\n**So you've trained your model and want to see how well it’s doing on a dataset of your choice. Where do you start?**\\n\\nThere is no “one size fits all” approach to choosing an evaluation metric, but some good guidelines to keep in mind are:\\n\\n## Categories of metrics\\n\\nThere are 3 high-level categories of metrics:\\n\\n1. *Generic metrics*, which can be applied to a variety of situations and datasets, such as precision and accuracy.\\n2. *Task-specific metrics*, which are limited to a given task, such as Machine Translation (often evaluated using metrics [BLEU](https://huggingface.co/metrics/bleu) or [ROUGE](https://huggingface.co/metrics/rouge)) or Named Entity Recognition (often evaluated with [seqeval](https://huggingface.co/metrics/seqeval)).\\n3. *Dataset-specific metrics*, which aim to measure model performance on specific benchmarks: for instance, the [GLUE benchmark](https://huggingface.co/datasets/glue) has a dedicated [evaluation metric](https://huggingface.co/metrics/glue).\\n\\nLet's look at each of these three cases:\\n\\n### Generic metrics\\n\\nMany of the metrics used in the Machine Learning community are quite generic and can be applied in a variety of tasks and datasets.\\n\\nThis is the case for metrics like [accuracy](https://huggingface.co/metrics/accuracy) and [precision](https://huggingface.co/metrics/precision), which can be used for evaluating labeled (supervised) datasets, as well as [perplexity](https://huggingface.co/metrics/perplexity), which can be used for evaluating different kinds of (unsupervised) generative tasks.\\n\\nTo see the input structure of a given metric, you can look at its metric card. For example, in the case of [precision](https://huggingface.co/metrics/precision), the format is:\"),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1760}, page_content='```\\n>>> precision_metric = evaluate.load(\"precision\")\\n>>> results = precision_metric.compute(references=[0, 1], predictions=[0, 1])\\n>>> print(results)\\n{\\'precision\\': 1.0}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1930}, page_content='```\\n\\n### Task-specific metrics\\n\\nPopular ML tasks like Machine Translation and Named Entity Recognition have specific metrics that can be used to compare models. For example, a series of different metrics have been proposed for text generation, ranging from [BLEU](https://huggingface.co/metrics/bleu) and its derivatives such as [GoogleBLEU](https://huggingface.co/metrics/google_bleu) and [GLEU](https://huggingface.co/metrics/gleu), but also [ROUGE](https://huggingface.co/metrics/rouge), [MAUVE](https://huggingface.co/metrics/mauve), etc.\\n\\nYou can find the right metric for your task by:\\n\\n- **Looking at the [Task pages](https://huggingface.co/tasks)** to see what metrics can be used for evaluating models for a given task.\\n- **Checking out leaderboards** on sites like [Papers With Code](https://paperswithcode.com/) (you can search by task and by dataset).\\n-  **Reading the metric cards** for the relevant metrics and see which ones are a good fit for your use case. For example, see the [BLEU metric card](https://github.com/huggingface/evaluate/tree/main/metrics/bleu) or [SQuaD metric card](https://github.com/huggingface/evaluate/tree/main/metrics/squad).\\n- **Looking at papers and blog posts** published on the topic and see what metrics they report. This can change over time, so try to pick papers from the last couple of years!\\n\\n### Dataset-specific metrics\\n\\nSome datasets have specific metrics associated with them -- this is especially in the case of popular benchmarks like [GLUE](https://huggingface.co/metrics/glue) and [SQuAD](https://huggingface.co/metrics/squad).'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 3518}, page_content='<Tip warning={true}>\\n💡\\nGLUE is actually a collection of different subsets on different tasks, so first you need to choose the one that corresponds to the NLI task, such as mnli, which is described as “crowdsourced collection of sentence pairs with textual entailment annotations”\\n</Tip>\\n\\n\\nIf you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use its dedicated evaluation metric. Make sure you respect the format that they require. For example, to evaluate your model on the [SQuAD](https://huggingface.co/datasets/squad) dataset, you need to feed the `question` and `context` into your model and return the `prediction_text`, which should be compared with the `references` (based on matching the `id` of the question) :'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 4281}, page_content='```\\n>>> from evaluate import load\\n>>> squad_metric = load(\"squad\")\\n>>> predictions = [{\\'prediction_text\\': \\'1976\\', \\'id\\': \\'56e10a3be3433e1400422b22\\'}]\\n>>> references = [{\\'answers\\': {\\'answer_start\\': [97], \\'text\\': [\\'1976\\']}, \\'id\\': \\'56e10a3be3433e1400422b22\\'}]\\n>>> results = squad_metric.compute(predictions=predictions, references=references)\\n>>> results\\n{\\'exact_match\\': 100.0, \\'f1\\': 100.0}\\n```\\n\\nYou can find examples of dataset structures by consulting the \"Dataset Preview\" function or the dataset card for a given dataset, and you can see how to use its dedicated evaluation function based on the metric card.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 1}, page_content='主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio 的主要特点：\\n\\n1. [添加示例输入](#example-inputs)\\n2. [传递自定义错误消息](#errors)\\n3. [添加描述内容](#descriptive-content)\\n4. [设置旗标](#flagging)\\n5. [预处理和后处理](#preprocessing-and-postprocessing)\\n6. [样式化演示](#styling)\\n7. [排队用户](#queuing)\\n8. [迭代输出](#iterative-outputs)\\n9. [进度条](#progress-bars)\\n10. [批处理函数](#batch-functions)\\n11. [在协作笔记本上运行](#colab-notebooks)\\n\\n## 示例输入\\n\\n您可以提供用户可以轻松加载到 \"Interface\" 中的示例数据。这对于演示模型期望的输入类型以及演示数据集和模型一起探索的方式非常有帮助。要加载示例数据，您可以将嵌套列表提供给 Interface 构造函数的 `examples=` 关键字参数。外部列表中的每个子列表表示一个数据样本，子列表中的每个元素表示每个输入组件的输入。有关每个组件的示例数据格式在[Docs](https://gradio.app/docs#components)中有说明。\\n\\n$code_calculator\\n$demo_calculator\\n\\n您可以将大型数据集加载到示例中，通过 Gradio 浏览和与数据集进行交互。示例将自动分页（可以通过 Interface 的 `examples_per_page` 参数进行配置）。\\n\\n继续了解示例，请参阅[更多示例](https://gradio.app/more-on-examples)指南。\\n\\n## 错误'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': -1}, page_content='继续了解示例，请参阅[更多示例](https://gradio.app/more-on-examples)指南。\\n\\n## 错误\\n\\n您希望向用户传递自定义错误消息。为此，with `gr.Error(\"custom message\")` 来显示错误消息。如果在上面的计算器示例中尝试除以零，将显示自定义错误消息的弹出模态窗口。了解有关错误的更多信息，请参阅[文档](https://gradio.app/docs#error)。\\n\\n## 描述性内容\\n\\n在前面的示例中，您可能已经注意到 Interface 构造函数中的 `title=` 和 `description=` 关键字参数，帮助用户了解您的应用程序。\\n\\nInterface 构造函数中有三个参数用于指定此内容应放置在哪里：\\n\\n- `title`：接受文本，并可以将其显示在界面的顶部，也将成为页面标题。\\n- `description`：接受文本、Markdown 或 HTML，并将其放置在标题正下方。\\n- `article`：也接受文本、Markdown 或 HTML，并将其放置在界面下方。\\n\\n![annotated](/assets/guides/annotated.png)\\n\\n如果您使用的是 `Blocks` API，则可以 with `gr.Markdown(...)` 或 `gr.HTML(...)` 组件在任何位置插入文本、Markdown 或 HTML，其中描述性内容位于 `Component` 构造函数内部。\\n\\n另一个有用的关键字参数是 `label=`，它存在于每个 `Component` 中。这修改了每个 `Component` 顶部的标签文本。还可以为诸如 `Textbox` 或 `Radio` 之类的表单元素添加 `info=` 关键字参数，以提供有关其用法的进一步信息。'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 1520}, page_content=\"```python\\ngr.Number(label='年龄', info='以年为单位，必须大于0')\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 1572}, page_content='```\\n\\n## 旗标\\n\\n默认情况下，\"Interface\" 将有一个 \"Flag\" 按钮。当用户测试您的 `Interface` 时，如果看到有趣的输出，例如错误或意外的模型行为，他们可以将输入标记为您进行查看。在由 `Interface` 构造函数的 `flagging_dir=` 参数提供的目录中，将记录标记的输入到一个 CSV 文件中。如果界面涉及文件数据，例如图像和音频组件，将创建文件夹来存储这些标记的数据。\\n\\n例如，对于上面显示的计算器界面，我们将在下面的旗标目录中存储标记的数据：\\n\\n```directory\\n+-- calculator.py\\n+-- flagged/\\n|   +-- logs.csv\\n```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nnum1,operation,num2,Output\\n5,add,7,12\\n6,subtract,1.5,4.5\\n```\\n\\n与早期显示的冷色界面相对应，我们将在下面的旗标目录中存储标记的数据：\\n\\n```directory\\n+-- sepia.py\\n+-- flagged/\\n|   +-- logs.csv\\n|   +-- im/\\n|   |   +-- 0.png\\n|   |   +-- 1.png\\n|   +-- Output/\\n|   |   +-- 0.png\\n|   |   +-- 1.png\\n```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': -1}, page_content='```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png\\n```\\n\\n如果您希望用户提供旗标原因，可以将字符串列表传递给 Interface 的 `flagging_options` 参数。用户在进行旗标时必须选择其中一个字符串，这将作为附加列保存到 CSV 中。\\n\\n## 预处理和后处理 (Preprocessing and Postprocessing)\\n\\n![annotated](/assets/img/dataflow.svg)\\n\\n如您所见，Gradio 包括可以处理各种不同数据类型的组件，例如图像、音频和视频。大多数组件都可以用作输入或输出。\\n\\n当组件用作输入时，Gradio 自动处理*预处理*，将数据从用户浏览器发送的类型（例如网络摄像头快照的 base64 表示）转换为您的函数可以接受的形式（例如 `numpy` 数组）。\\n\\n同样，当组件用作输出时，Gradio 自动处理*后处理*，将数据从函数返回的形式（例如图像路径列表）转换为可以在用户浏览器中显示的形式（例如以 base64 格式显示图像的 `Gallery`）。\\n\\n您可以使用构建图像组件时的参数控制*预处理*。例如，如果您使用以下参数实例化 `Image` 组件，它将将图像转换为 `PIL` 类型，并将其重塑为`(100, 100)`，而不管提交时的原始大小如何：\\n\\n```py\\nimg = gr.Image(shape=(100, 100), type=\"pil\")'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 2853}, page_content='```\\n\\n相反，这里我们保留图像的原始大小，但在将其转换为 numpy 数组之前反转颜色：\\n\\n```py\\nimg = gr.Image(invert_colors=True, type=\"numpy\")\\n```\\n\\n后处理要容易得多！Gradio 自动识别返回数据的格式（例如 `Image` 是 `numpy` 数组还是 `str` 文件路径？），并将其后处理为可以由浏览器显示的格式。\\n\\n请查看[文档](https://gradio.app/docs)，了解每个组件的所有与预处理相关的参数。\\n\\n## 样式 (Styling)\\n\\nGradio 主题是自定义应用程序外观和感觉的最简单方法。您可以选择多种主题或创建自己的主题。要这样做，请将 `theme=` 参数传递给 `Interface` 构造函数。例如：\\n\\n```python\\ndemo = gr.Interface(..., theme=gr.themes.Monochrome())\\n```\\n\\nGradio 带有一组预先构建的主题，您可以从 `gr.themes.*` 加载。您可以扩展这些主题或从头开始创建自己的主题 - 有关更多详细信息，请参阅[主题指南](https://gradio.app/theming-guide)。\\n\\n要增加额外的样式能力，您可以 with `css=` 关键字将任何 CSS 传递给您的应用程序。\\nGradio 应用程序的基类是 `gradio-container`，因此以下是一个更改 Gradio 应用程序背景颜色的示例：\\n\\n```python\\nwith `gr.Interface(css=\".gradio-container {background-color: red}\") as demo:\\n    ...'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 3611}, page_content='```\\n\\n## 队列 (Queuing)\\n\\n如果您的应用程序预计会有大量流量，请 with `queue()` 方法来控制处理速率。这将排队处理调用，因此一次只处理一定数量的请求。队列使用 Websockets，还可以防止网络超时，因此如果您的函数的推理时间很长（> 1 分钟），应使用队列。\\n\\nwith `Interface`：\\n\\n```python\\ndemo = gr.Interface(...).queue()\\ndemo.launch()\\n```\\n\\nwith `Blocks`：\\n\\n```python\\nwith gr.Blocks() as demo：\\n    #...\\ndemo.queue()\\ndemo.launch()\\n```\\n\\n您可以通过以下方式控制一次处理的请求数量：\\n\\n```python\\ndemo.queue(concurrency_count=3)\\n```\\n\\n查看有关配置其他队列参数的[队列文档](/docs/#queue)。\\n\\n在 Blocks 中指定仅对某些函数进行排队：\\n\\n```python\\nwith gr.Blocks() as demo2：\\n    num1 = gr.Number()\\n    num2 = gr.Number()\\n    output = gr.Number()\\n    gr.Button(\"Add\").click(\\n        lambda a, b: a + b, [num1, num2], output)\\n    gr.Button(\"Multiply\").click(\\n        lambda a, b: a * b, [num1, num2], output, queue=True)\\ndemo2.launch()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 4360}, page_content='```\\n\\n## 迭代输出 (Iterative Outputs)\\n\\n在某些情况下，您可能需要传输一系列输出而不是一次显示单个输出。例如，您可能有一个图像生成模型，希望显示生成的每个步骤的图像，直到最终图像。或者您可能有一个聊天机器人，它逐字逐句地流式传输响应，而不是一次返回全部响应。\\n\\n在这种情况下，您可以将**生成器**函数提供给 Gradio，而不是常规函数。在 Python 中创建生成器非常简单：函数不应该有一个单独的 `return` 值，而是应该 with `yield` 连续返回一系列值。通常，`yield` 语句放置在某种循环中。下面是一个简单示例，生成器只是简单计数到给定数字：\\n\\n```python\\ndef my_generator(x):\\n    for i in range(x):\\n        yield i'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 4732}, page_content='```\\n\\n您以与常规函数相同的方式将生成器提供给 Gradio。例如，这是一个（虚拟的）图像生成模型，它在输出图像之前生成数个步骤的噪音：\\n\\n$code_fake_diffusion\\n$demo_fake_diffusion\\n\\n请注意，我们在迭代器中添加了 `time.sleep(1)`，以创建步骤之间的人工暂停，以便您可以观察迭代器的步骤（在真实的图像生成模型中，这可能是不必要的）。\\n\\n将生成器提供给 Gradio **需要**在底层 Interface 或 Blocks 中启用队列（请参阅上面的队列部分）。\\n\\n## 进度条\\n\\nGradio 支持创建自定义进度条，以便您可以自定义和控制向用户显示的进度更新。要启用此功能，只需为方法添加一个默认值为 `gr.Progress` 实例的参数即可。然后，您可以直接调用此实例并传入 0 到 1 之间的浮点数来更新进度级别，或者 with `Progress` 实例的 `tqdm()` 方法来跟踪可迭代对象上的进度，如下所示。必须启用队列以进行进度更新。\\n\\n$code_progress_simple\\n$demo_progress_simple\\n\\n如果您 with `tqdm` 库，并且希望从函数内部的任何 `tqdm.tqdm` 自动报告进度更新，请将默认参数设置为 `gr.Progress(track_tqdm=True)`！\\n\\n## 批处理函数 (Batch Functions)\\n\\nGradio 支持传递*批处理*函数。批处理函数只是接受输入列表并返回预测列表的函数。'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': -1}, page_content='## 批处理函数 (Batch Functions)\\n\\nGradio 支持传递*批处理*函数。批处理函数只是接受输入列表并返回预测列表的函数。\\n\\n例如，这是一个批处理函数，它接受两个输入列表（一个单词列表和一个整数列表），并返回修剪过的单词列表作为输出：\\n\\n```python\\nimport time\\n\\ndef trim_words(words, lens):\\n    trimmed_words = []\\n    time.sleep(5)\\n    for w, l in zip(words, lens):\\n        trimmed_words.append(w[:int(l)])\\n    return [trimmed_words]\\n    for w, l in zip(words, lens):'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 5686}, page_content='```\\n\\n使用批处理函数的优点是，如果启用了队列，Gradio 服务器可以自动*批处理*传入的请求并并行处理它们，从而可能加快演示速度。以下是 Gradio 代码的示例（请注意 `batch=True` 和 `max_batch_size=16` - 这两个参数都可以传递给事件触发器或 `Interface` 类）\\n\\nwith `Interface`：\\n\\n```python\\ndemo = gr.Interface(trim_words, [\"textbox\", \"number\"], [\"output\"],\\n                    batch=True, max_batch_size=16)\\ndemo.queue()\\ndemo.launch()\\n```\\n\\nwith `Blocks`：\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        word = gr.Textbox(label=\"word\")\\n        leng = gr.Number(label=\"leng\")\\n        output = gr.Textbox(label=\"Output\")\\n    with gr.Row():\\n        run = gr.Button()\\n\\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\\n\\ndemo.queue()\\ndemo.launch()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 6401}, page_content='```\\n\\n在上面的示例中，可以并行处理 16 个请求（总推理时间为 5 秒），而不是分别处理每个请求（总推理时间为 80 秒）。许多 Hugging Face 的 `transformers` 和 `diffusers` 模型在 Gradio 的批处理模式下自然工作：这是[使用批处理生成图像的示例演示](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\\n\\n注意：使用 Gradio 的批处理函数 **requires** 在底层 Interface 或 Blocks 中启用队列（请参阅上面的队列部分）。\\n\\n## Gradio 笔记本 (Colab Notebooks)\\n\\nGradio 可以在任何运行 Python 的地方运行，包括本地 Jupyter 笔记本和协作笔记本，如[Google Colab](https://colab.research.google.com/)。对于本地 Jupyter 笔记本和 Google Colab 笔记本，Gradio 在本地服务器上运行，您可以在浏览器中与之交互。（注意：对于 Google Colab，这是通过[服务工作器隧道](https://github.com/tensorflow/tensorboard/blob/master/docs/design/colab_integration.md)实现的，您的浏览器需要启用 cookies。）对于其他远程笔记本，Gradio 也将在服务器上运行，但您需要使用[SSH 隧道](https://coderwall.com/p/ohk6cg/remote-access-to-ipython-notebooks-via-ssh)在本地浏览器中查看应用程序。通常，更简单的选择是使用 Gradio 内置的公共链接，[在下一篇指南中讨论](/sharing-your-app/#sharing-demos)。'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Training on TPU with TensorFlow\\n\\n<Tip>\\n\\nIf you don\\'t need long explanations and just want TPU code samples to get started with, check out [our TPU example notebook!](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)\\n\\n</Tip>\\n\\n### What is a TPU?\\n\\nA TPU is a **Tensor Processing Unit.** They are hardware designed by Google, which are used to greatly speed up the tensor computations within neural networks, much like GPUs. They can be used for both network training and inference. They are generally accessed through Google’s cloud services, but small TPUs can also be accessed directly for free through Google Colab and Kaggle Kernels.\\n\\nBecause [all TensorFlow models in 🤗 Transformers are Keras models](https://huggingface.co/blog/tensorflow-philosophy), most of the methods in this document are generally applicable to TPU training for any Keras model! However, there are a few points that are specific to the HuggingFace ecosystem (hug-o-system?) of Transformers and Datasets, and we’ll make sure to flag them up when we get to them.\\n\\n### What kinds of TPU are available?\\n\\nNew users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between **TPU Nodes** and **TPU VMs.**'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': -1}, page_content='New users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between **TPU Nodes** and **TPU VMs.**\\n\\nWhen you use a **TPU Node**, you are effectively indirectly accessing a remote TPU. You will need a separate VM, which will initialize your network and data pipeline and then forward them to the remote node. When you use a TPU on Google Colab, you are accessing it in the **TPU Node** style.\\n\\nUsing TPU Nodes can have some quite unexpected behaviour for people who aren’t used to them! In particular, because the TPU is located on a physically different system to the machine you’re running your Python code on, your data cannot be local to your machine - any data pipeline that loads from your machine’s internal storage will totally fail! Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU node.\\n\\n<Tip>\\n\\nIf you can fit all your data in memory as `np.ndarray` or `tf.Tensor`, then you can `fit()` on that data even when using Colab or a TPU Node, without needing to upload it to Google Cloud Storage.\\n\\n</Tip>\\n\\n<Tip>\\n\\n**🤗Specific Hugging Face Tip🤗:** The methods `Dataset.to_tf_dataset()` and its higher-level wrapper `model.prepare_tf_dataset()` , which you will see throughout our TF code examples, will both fail on a TPU Node. The reason for this is that even though they create a `tf.data.Dataset` it is not a “pure” `tf.data` pipeline and uses `tf.numpy_function` or `Dataset.from_generator()` to stream data from the underlying HuggingFace `Dataset`. This HuggingFace `Dataset` is backed by data that is on a local disc and which the remote TPU Node will not be able to read.\\n\\n</Tip>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 3585}, page_content='</Tip>\\n\\nThe second way to access a TPU is via a **TPU VM.** When using a TPU VM, you connect directly to the machine that the TPU is attached to, much like training on a GPU VM. TPU VMs are generally easier to work with, particularly when it comes to your data pipeline. All of the above warnings do not apply to TPU VMs!\\n\\nThis is an opinionated document, so here’s our opinion: **Avoid using TPU Node if possible.** It is more confusing and more difficult to debug than TPU VMs. It is also likely to be unsupported in future - Google’s latest TPU, TPUv4, can only be accessed as a TPU VM, which suggests that TPU Nodes are increasingly going to become a “legacy” access method. However, we understand that the only free TPU access is on Colab and Kaggle Kernels, which uses TPU Node - so we’ll try to explain how to handle it if you have to! Check the [TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) for code samples that explain this in more detail.\\n\\n### What sizes of TPU are available?\\n\\nA single TPU (a v2-8/v3-8/v4-8) runs 8 replicas. TPUs exist in **pods** that can run hundreds or thousands of replicas simultaneously. When you use more than a single TPU but less than a whole pod (for example, a v3-32), your TPU fleet is referred to as a **pod slice.**\\n\\nWhen you access a free TPU via Colab, you generally get a single v2-8 TPU.\\n\\n### I keep hearing about this XLA thing. What’s XLA, and how does it relate to TPUs?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': -1}, page_content='When you access a free TPU via Colab, you generally get a single v2-8 TPU.\\n\\n### I keep hearing about this XLA thing. What’s XLA, and how does it relate to TPUs?\\n\\nXLA is an optimizing compiler, used by both TensorFlow and JAX. In JAX it is the only compiler, whereas in TensorFlow it is optional (but mandatory on TPU!). The easiest way to enable it when training a Keras model is to pass the argument `jit_compile=True` to `model.compile()`. If you don’t get any errors and performance is good, that’s a great sign that you’re ready to move to TPU!\\n\\nDebugging on TPU is generally a bit harder than on CPU/GPU, so we recommend getting your code running on CPU/GPU with XLA first before trying it on TPU. You don’t have to train for long, of course - just for a few steps to make sure that your model and data pipeline are working like you expect them to.\\n\\n<Tip>\\n\\nXLA compiled code is usually faster - so even if you’re not planning to run on TPU, adding `jit_compile=True` can improve your performance. Be sure to note the caveats below about XLA compatibility, though!\\n\\n</Tip>\\n\\n<Tip warning={true}>\\n\\n**Tip born of painful experience:** Although using `jit_compile=True` is a good way to get a speed boost and test if your CPU/GPU code is XLA-compatible, it can actually cause a lot of problems if you leave it in when actually training on TPU. XLA compilation will happen implicitly on TPU, so remember to remove that line before actually running your code on a TPU!\\n\\n</Tip>\\n\\n### How do I make my model XLA compatible?\\n\\nIn many cases, your code is probably XLA-compatible already! However, there are a few things that work in normal TensorFlow that don’t work in XLA. We’ve distilled them into three core rules below:\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 6647}, page_content='<Tip>\\n\\n**🤗Specific HuggingFace Tip🤗:** We’ve put a lot of effort into rewriting our TensorFlow models and loss functions to be XLA-compatible. Our models and loss functions generally obey rule #1 and #2 by default, so you can skip over them if you’re using `transformers` models. Don’t forget about these rules when writing your own models and loss functions, though!\\n\\n</Tip>\\n\\n#### XLA Rule #1: Your code cannot have “data-dependent conditionals”\\n\\nWhat that means is that any `if` statement cannot depend on values inside a `tf.Tensor`. For example, this code block cannot be compiled with XLA!\\n\\n```python\\nif tf.reduce_sum(tensor) > 10:\\n    tensor = tensor / 2.0'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 7310}, page_content='```\\n\\nThis might seem very restrictive at first, but most neural net code doesn’t need to do this. You can often get around this restriction by using `tf.cond` (see the documentation [here](https://www.tensorflow.org/api_docs/python/tf/cond)) or by removing the conditional and finding a clever math trick with indicator variables instead, like so:\\n\\n```python\\nsum_over_10 = tf.cast(tf.reduce_sum(tensor) > 10, tf.float32)\\ntensor = tensor / (1.0 + sum_over_10)\\n```\\n\\nThis code has exactly the same effect as the code above, but by avoiding a conditional, we ensure it will compile with XLA without problems!\\n\\n#### XLA Rule #2: Your code cannot have “data-dependent shapes”\\n\\nWhat this means is that the shape of all of the `tf.Tensor` objects in your code cannot depend on their values. For example, the function `tf.unique` cannot be compiled with XLA, because it returns a `tensor` containing one instance of each unique value in the input. The shape of this output will obviously be different depending on how repetitive the input `Tensor` was, and so XLA refuses to handle it!\\n\\nIn general, most neural network code obeys rule #2 by default. However, there are a few common cases where it becomes a problem. One very common one is when you use **label masking**, setting your labels to a negative value to indicate that those positions should be ignored when computing the loss. If you look at NumPy or PyTorch loss functions that support label masking, you will often see code like this that uses [boolean indexing](https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing):\\n\\n```python\\nlabel_mask = labels >= 0\\nmasked_outputs = outputs[label_mask]\\nmasked_labels = labels[label_mask]\\nloss = compute_loss(masked_outputs, masked_labels)\\nmean_loss = torch.mean(loss)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 9094}, page_content='```\\n\\nThis code is totally fine in NumPy or PyTorch, but it breaks in XLA! Why? Because the shape of `masked_outputs` and `masked_labels` depends on how many positions are masked - that makes it a **data-dependent shape.** However, just like for rule #1, we can often rewrite this code to yield exactly the same output without any data-dependent shapes.\\n\\n```python\\nlabel_mask = tf.cast(labels >= 0, tf.float32)\\nloss = compute_loss(outputs, labels)\\nloss = loss * label_mask  # Set negative label positions to 0\\nmean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 9663}, page_content='```\\n\\nHere, we avoid data-dependent shapes by computing the loss for every position, but zeroing out the masked positions in both the numerator and denominator when we calculate the mean, which yields exactly the same result as the first block while maintaining XLA compatibility. Note that we use the same trick as in rule #1 - converting a `tf.bool` to `tf.float32` and using it as an indicator variable. This is a really useful trick, so remember it if you need to convert your own code to XLA!\\n\\n#### XLA Rule #3: XLA will need to recompile your model for every different input shape it sees\\n\\nThis is the big one. What this means is that if your input shapes are very variable, XLA will have to recompile your model over and over, which will create huge performance problems. This commonly arises in NLP models, where input texts have variable lengths after tokenization. In other modalities, static shapes are more common and this rule is much less of a problem.\\n\\nHow can you get around rule #3? The key is **padding** - if you pad all your inputs to the same length, and then use an `attention_mask`, you can get the same results as you’d get from variable shapes, but without any XLA issues. However, excessive padding can cause severe slowdown too - if you pad all your samples to the maximum length in the whole dataset, you might end up with batches consisting endless padding tokens, which will waste a lot of compute and memory!\\n\\nThere isn’t a perfect solution to this problem. However, you can try some tricks. One very useful trick is to **pad batches of samples up to a multiple of a number like 32 or 64 tokens.** This often only increases the number of tokens by a small amount, but it hugely reduces the number of unique input shapes, because every input shape now has to be a multiple of 32 or 64. Fewer unique input shapes means fewer XLA compilations!\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 11535}, page_content='<Tip>\\n\\n**🤗Specific HuggingFace Tip🤗:** Our tokenizers and data collators have methods that can help you here. You can use `padding=\"max_length\"` or `padding=\"longest\"` when calling tokenizers to get them to output padded data. Our tokenizers and data collators also have a `pad_to_multiple_of` argument that you can use to reduce the number of unique input shapes you see!\\n\\n</Tip>\\n\\n### How do I actually train my model on TPU?\\n\\nOnce your training is XLA-compatible and (if you’re using TPU Node / Colab) your dataset has been prepared appropriately, running on TPU is surprisingly easy! All you really need to change in your code is to add a few lines to initialize your TPU, and to ensure that your model and dataset are created inside a `TPUStrategy` scope. Take a look at [our TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) to see this in action!\\n\\n### Summary\\n\\nThere was a lot in here, so let’s summarize with a quick checklist you can follow when you want to get your model ready for TPU training:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': -1}, page_content='### Summary\\n\\nThere was a lot in here, so let’s summarize with a quick checklist you can follow when you want to get your model ready for TPU training:\\n\\n- Make sure your code follows the three rules of XLA\\n- Compile your model with `jit_compile=True` on CPU/GPU and confirm that you can train it with XLA\\n- Either load your dataset into memory or use a TPU-compatible dataset loading approach (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Migrate your code either to Colab (with accelerator set to “TPU”) or a TPU VM on Google Cloud\\n- Add TPU initializer code (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Create your `TPUStrategy` and make sure dataset loading and model creation are inside the `strategy.scope()` (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Don’t forget to take `jit_compile=True` out again when you move to TPU!\\n- 🙏🙏🙏🥺🥺🥺\\n- Call model.fit()\\n- You did it!'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/blocks_random_slider/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef func(slider_1, slider_2):\\n    return slider_1 * 5 + slider_2\\n\\n\\nwith gr.Blocks() as demo:\\n    slider = gr.Slider(minimum=-10.2, maximum=15, label=\"Random Slider (Static)\", randomize=True)\\n    slider_1 = gr.Slider(minimum=100, maximum=200, label=\"Random Slider (Input 1)\", randomize=True)\\n    slider_2 = gr.Slider(minimum=10, maximum=23.2, label=\"Random Slider (Input 2)\", randomize=True)\\n    slider_3 = gr.Slider(value=3, label=\"Non random slider\")\\n    btn = gr.Button(\"Run\")\\n    btn.click(func, inputs=[slider_1, slider_2], outputs=gr.Number())\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 1}, page_content='Git over SSH\\n\\nYou can access and write data in repositories on huggingface.co using SSH (Secure Shell Protocol). When you connect via SSH, you authenticate using a private key file on your local machine.\\n\\nSome actions, such as pushing changes, or cloning private repositories, will require you to upload your SSH public key to your account on huggingface.co.\\n\\nYou can use a pre-existing SSH key, or generate a new one specifically for huggingface.co.\\n\\n## Checking for existing SSH keys\\n\\nIf you have an existing SSH key, you can use that key to authenticate Git operations over SSH.\\n\\nSSH keys are usually located under `~/.ssh` on Mac & Linux, and under `C:\\\\\\\\Users\\\\\\\\<username>\\\\\\\\.ssh` on Windows. List files under that directory and look for files of the form:\\n\\n- id_rsa.pub\\n- id_ecdsa.pub\\n- id_ed25519.pub\\n\\nThose files contain your SSH public key.\\n\\nIf you don\\'t have such file under `~/.ssh`, you will have to [generate a new key](#generating-a-new-ssh-keypair). Otherwise, you can [add your existing SSH public key(s) to your huggingface.co account](#add-a-ssh-key-to-your-account).\\n\\n## Generating a new SSH keypair\\n\\nIf you don\\'t have any SSH keys on your machine, you can use `ssh-keygen` to generate a new SSH key pair (public + private keys):\\n\\n```\\n$ ssh-keygen -t ed25519 -C \"your.email@example.co\"\\n```\\n\\nWe recommend entering a passphrase when you are prompted to. A passphrase is an extra layer of security: it is a password that will be prompted whenever you use your SSH key.\\n\\nOnce your new key is generated, add it to your SSH agent with `ssh-add`:\\n\\n```\\n$ ssh-add ~/.ssh/id_ed25519'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 1558}, page_content='```\\n$ ssh-add ~/.ssh/id_ed25519\\n```\\n\\nIf you chose a different location than the default to store your SSH key, you would have to replace `~/.ssh/id_ed25519` with the file location you used.\\n\\n## Add a SSH key to your account\\n\\nTo access private repositories with SSH, or to push changes via SSH, you will need to add your SSH public key to your huggingface.co account. You can manage your SSH keys [in your user settings](https://huggingface.co/settings/keys).\\n\\nTo add a SSH key to your account, click on the \"Add SSH key\" button.\\n\\nThen, enter a name for this key (for example, \"Personal computer\"), and copy and paste the content of your **public** SSH key in the area below. The public key is located in the `~/.ssh/id_XXXX.pub` file you found or generated in the previous steps.\\n\\nClick on \"Add key\", and voilà! You have added a SSH key to your huggingface.co account.\\n\\n\\n## Testing your SSH authentication\\n\\nOnce you have added your SSH key to your huggingface.co account, you can test that the connection works as expected.\\n\\nIn a terminal, run:\\n```\\n$ ssh -T git@hf.co\\n```\\n\\nIf you see a message with your username, congrats! Everything went well, you are ready to use git over SSH.\\n\\nOtherwise, if the message states something like the following, make sure your SSH key is actually used by your SSH agent.\\n```\\nHi anonymous, welcome to Hugging Face.\\n```'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# Token classification with LayoutLMv3 (PyTorch version)\\n\\nThis directory contains a script, `run_funsd_cord.py`, that can be used to fine-tune (or evaluate) LayoutLMv3 on form understanding datasets, such as [FUNSD](https://guillaumejaume.github.io/FUNSD/) and [CORD](https://github.com/clovaai/cord).\\n\\nThe script `run_funsd_cord.py` leverages the 🤗 Datasets library and the Trainer API. You can easily customize it to your needs.\\n\\n## Fine-tuning on FUNSD\\n\\nFine-tuning LayoutLMv3 for token classification on [FUNSD](https://guillaumejaume.github.io/FUNSD/) can be done as follows:\\n\\n```bash\\npython run_funsd_cord.py \\\\\\n  --model_name_or_path microsoft/layoutlmv3-base \\\\\\n  --dataset_name funsd \\\\\\n  --output_dir layoutlmv3-test \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_steps 1000 \\\\\\n  --evaluation_strategy steps \\\\\\n  --eval_steps 100 \\\\\\n  --learning_rate 1e-5 \\\\\\n  --load_best_model_at_end \\\\\\n  --metric_for_best_model \"eval_f1\" \\\\\\n  --push_to_hub \\\\\\n  --push_to_hub°model_id layoutlmv3-finetuned-funsd'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 1584}, page_content='```\\n\\n👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd. By specifying the `push_to_hub` flag, the model gets uploaded automatically to the hub (regularly), together with a model card, which includes metrics such as precision, recall and F1. Note that you can easily update the model card, as it\\'s just a README file of the respective repo on the hub.\\n\\nThere\\'s also the \"Training metrics\" [tab](https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd/tensorboard), which shows Tensorboard logs over the course of training. Pretty neat, huh?\\n\\n## Fine-tuning on CORD\\n\\nFine-tuning LayoutLMv3 for token classification on [CORD](https://github.com/clovaai/cord) can be done as follows:\\n\\n```bash\\npython run_funsd_cord.py \\\\\\n  --model_name_or_path microsoft/layoutlmv3-base \\\\\\n  --dataset_name cord \\\\\\n  --output_dir layoutlmv3-test \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_steps 1000 \\\\\\n  --evaluation_strategy steps \\\\\\n  --eval_steps 100 \\\\\\n  --learning_rate 5e-5 \\\\\\n  --load_best_model_at_end \\\\\\n  --metric_for_best_model \"eval_f1\" \\\\\\n  --push_to_hub \\\\\\n  --push_to_hub°model_id layoutlmv3-finetuned-cord\\n```\\n\\n👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-cord. Note that a model card gets generated automatically in case you specify the `push_to_hub` flag.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/03_building-with-blocks/03_state-in-blocks.md', 'start_index': 1}, page_content=\"State in Blocks\\n\\nWe covered [State in Interfaces](https://gradio.app/interface-state), this guide takes a look at state in Blocks, which works mostly the same.\\n\\n## Global State\\n\\nGlobal state in Blocks works the same as in Interface. Any variable created outside a function call is a reference shared between all users.\\n\\n## Session State\\n\\nGradio supports session **state**, where data persists across multiple submits within a page session, in Blocks apps as well. To reiterate, session data is _not_ shared between different users of your model. To store data in a session state, you need to do three things:\\n\\n1. Create a `gr.State()` object. If there is a default value to this stateful object, pass that into the constructor.\\n2. In the event listener, put the `State` object as an input and output.\\n3. In the event listener function, add the variable to the input parameters and the return value.\\n\\nLet's take a look at a game of hangman.\\n\\n$code_hangman\\n$demo_hangman\\n\\nLet's see how we do each of the 3 steps listed above in this game:\\n\\n1. We store the used letters in `used_letters_var`. In the constructor of `State`, we set the initial value of this to `[]`, an empty list.\\n2. In `btn.click()`, we have a reference to `used_letters_var` in both the inputs and outputs.\\n3. In `guess_letter`, we pass the value of this `State` to `used_letters`, and then return an updated value of this `State` in the return statement.\\n\\nWith more complex apps, you will likely have many State variables storing session state in a single Blocks app.\\n\\nLearn more about `State` in the [docs](https://gradio.app/docs#state).\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 1}, page_content='如何使用地图组件绘制图表\\n\\nRelated spaces:\\nTags: PLOTS, MAPS\\n\\n## 简介\\n\\n本指南介绍如何使用 Gradio 的 `Plot` 组件在地图上绘制地理数据。Gradio 的 `Plot` 组件可以与 Matplotlib、Bokeh 和 Plotly 一起使用。在本指南中，我们将使用 Plotly 进行操作。Plotly 可以让开发人员轻松创建各种地图来展示他们的地理数据。点击[这里](https://plotly.com/python/maps/)查看一些示例。\\n\\n## 概述\\n\\n我们将使用纽约市的 Airbnb 数据集，该数据集托管在 kaggle 上，点击[这里](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)。我已经将其上传到 Hugging Face Hub 作为一个数据集，方便使用和下载，点击[这里](https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data)。使用这些数据，我们将在地图上绘制 Airbnb 的位置，并允许基于价格和位置进行筛选。下面是我们将要构建的演示。 ⚡️\\n\\n$demo_map_airbnb\\n\\n## 步骤 1-加载 CSV 数据 💾\\n\\n让我们首先从 Hugging Face Hub 加载纽约市的 Airbnb 数据。\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\\ndf = dataset.to_pandas()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': -1}, page_content='dataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\\ndf = dataset.to_pandas()\\n\\ndef filter_map(min_price, max_price, boroughs):\\n    new_df = df[(df[\\'neighbourhood_group\\'].isin(boroughs)) &\\n            (df[\\'price\\'] > min_price) & (df[\\'price\\'] < max_price)]\\n    names = new_df[\"name\"].tolist()\\n    prices = new_df[\"price\"].tolist()\\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 1092}, page_content='```\\n\\n在上面的代码中，我们先将 CSV 数据加载到一个 pandas dataframe 中。让我们首先定义一个函数，这将作为 gradio 应用程序的预测函数。该函数将接受最低价格、最高价格范围和筛选结果地区的列表作为参数。我们可以使用传入的值 (`min_price`、`max_price` 和地区列表) 来筛选数据框并创建 `new_df`。接下来，我们将创建包含每个 Airbnb 的名称和价格的 `text_list`，以便在地图上使用作为标签。\\n\\n## 步骤 2-地图图表 🌐\\n\\nPlotly 使得处理地图变得很容易。让我们看一下下面的代码，了解如何创建地图图表。\\n\\n```python\\nimport plotly.graph_objects as go\\n\\nfig = go.Figure(go.Scattermapbox(\\n            customdata=text_list,\\n            lat=new_df[\\'latitude\\'].tolist(),\\n            lon=new_df[\\'longitude\\'].tolist(),\\n            mode=\\'markers\\',\\n            marker=go.scattermapbox.Marker(\\n                size=6\\n            ),\\n            hoverinfo=\"text\",\\n            hovertemplate=\\'<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}\\'\\n        ))\\n\\nfig.update_layout(\\n    mapbox_style=\"open-street-map\",\\n    hovermode=\\'closest\\',\\n    mapbox=dict(\\n        bearing=0,\\n        center=go.layout.mapbox.Center(\\n            lat=40.67,\\n            lon=-73.90\\n        ),\\n        pitch=0,\\n        zoom=9\\n    ),\\n)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 2088}, page_content='```\\n\\n上面的代码中，我们通过传入经纬度列表来创建一个散点图。我们还传入了名称和价格的自定义数据，以便在鼠标悬停在每个标记上时显示额外的信息。接下来，我们使用 `update_layout` 来指定其他地图设置，例如缩放和居中。\\n\\n有关使用 Mapbox 和 Plotly 创建散点图的更多信息，请点击[这里](https://plotly.com/python/scattermapbox/)。\\n\\n## 步骤 3-Gradio 应用程序 ⚡️\\n\\n我们将使用两个 `gr.Number` 组件和一个 `gr.CheckboxGroup` 组件，允许用户指定价格范围和地区位置。然后，我们将使用 `gr.Plot` 组件作为我们之前创建的 Plotly + Mapbox 地图的输出。\\n\\n```python\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\\n        btn = gr.Button(value=\"Update Filter\")\\n        map = gr.Plot()\\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\\n    btn.click(filter_map, [min_price, max_price, boroughs], map)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 3014}, page_content='```\\n\\n我们使用 `gr.Column` 和 `gr.Row` 布局这些组件，并为演示加载时和点击 \" 更新筛选 \" 按钮时添加了事件触发器，以触发地图更新新的筛选条件。\\n\\n以下是完整演示代码：\\n\\n$code_map_airbnb\\n\\n## 步骤 4-部署 Deployment 🤗\\n\\n如果你运行上面的代码，你的应用程序将在本地运行。\\n如果要获取临时共享链接，可以将 `share=True` 参数传递给 `launch`。\\n\\n但如果你想要一个永久的部署解决方案呢？\\n让我们将我们的 Gradio 应用程序部署到免费的 HuggingFace Spaces 平台。\\n\\n如果你以前没有使用过 Spaces，请按照之前的指南[这里](/using_hugging_face_integrations)。\\n\\n## 结论 🎉\\n\\n你已经完成了！这是构建地图演示所需的所有代码。\\n\\n链接到演示：[地图演示](https://huggingface.co/spaces/gradio/map_airbnb)和[完整代码](https://huggingface.co/spaces/gradio/map_airbnb/blob/main/run.py)（在 Hugging Face Spaces）'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 1}, page_content='SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) that employs [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block) to enable the network to perform dynamic channel-wise feature recalibration.\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model(\\'seresnet152d\\', pretrained=True)\\n>>> model.eval()\\n```\\n\\nTo load and preprocess the image:\\n\\n```py \\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 1253}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n>>> # Get imagenet class mappings\\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\n>>> urllib.request.urlretrieve(url, filename) \\n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\\n...     categories = [s.strip() for s in f.readlines()]\\n\\n>>> # Print top categories per image\\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\\n>>> for i in range(top5_prob.size(0)):\\n...     print(categories[top5_catid[i]], top5_prob[i].item())\\n>>> # prints class names and probabilities like:\\n>>> # [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 2047}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('seresnet152d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\\n```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{hu2019squeezeandexcitation,\\n      title={Squeeze-and-Excitation Networks}, \\n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\\n      year={2019},\\n      eprint={1709.01507},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 3206}, page_content='```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 3211}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Networks\\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\\nModels:\\n- Name: seresnet152d\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 20161904304\\n    Parameters: 66840000\\n    File Size: 268144497\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet152d\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 152\\n    Dropout: 0.2\\n    Crop Pct: '0.94'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '256'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1206\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet152d_ra2-04464dd2.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83.74%\\n      Top 5 Accuracy: 96.77%\\n- Name: seresnet50\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 5285062320\\n    Parameters: 28090000\\n    File Size: 112621903\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': -1}, page_content=\"- Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet50\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 50\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1180\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet50_ra_224-8efdb4bb.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.26%\\n      Top 5 Accuracy: 95.07%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 0}, page_content='--\\ntitle: poseval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  The poseval metric can be used to evaluate POS taggers. Since seqeval does not work well with POS data \\n  that is not in IOB format the poseval is an alternative. It treats each token in the dataset as independant \\n  observation and computes the precision, recall and F1-score irrespective of sentences. It uses scikit-learns\\'s\\n  classification report to compute the scores.\\n---\\n\\n# Metric Card for peqeval\\n\\n## Metric description\\n\\nThe poseval metric can be used to evaluate POS taggers. Since seqeval does not work well with POS data (see e.g. [here](https://stackoverflow.com/questions/71327693/how-to-disable-seqeval-label-formatting-for-pos-tagging)) that is not in IOB format the poseval is an alternative. It treats each token in the dataset as independant observation and computes the precision, recall and F1-score irrespective of sentences. It uses scikit-learns\\'s [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to compute the scores.\\n\\n\\n## How to use \\n\\nPoseval produces labelling scores along with its sufficient statistics from a source against references.\\n\\nIt takes two mandatory arguments:\\n\\n`predictions`: a list of lists of predicted labels, i.e. estimated targets as returned by a tagger.\\n\\n`references`: a list of lists of reference labels, i.e. the ground truth/target values.\\n\\nIt can also take several optional arguments:\\n\\n`zero_division`: Which value to substitute as a metric value when encountering zero division. Should be one of [`0`,`1`,`\"warn\"`]. `\"warn\"` acts as `0`, but the warning is raised.'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 1754}, page_content='```python\\n>>> predictions = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'NOUN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'VERB\\', \\'SYM\\']]\\n>>> references = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'SYM\\']]\\n>>> poseval = evaluate.load(\"poseval\")\\n>>> results = poseval.compute(predictions=predictions, references=references)\\n>>> print(list(results.keys()))\\n[\\'ADP\\', \\'INTJ\\', \\'NOUN\\', \\'PROPN\\', \\'PUNCT\\', \\'SYM\\', \\'VERB\\', \\'accuracy\\', \\'macro avg\\', \\'weighted avg\\']\\n>>> print(results[\"accuracy\"])\\n0.8\\n>>> print(results[\"PROPN\"][\"recall\"])\\n0.5'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 2291}, page_content='```\\n\\n## Output values\\n\\nThis metric returns a a classification report as a dictionary with a summary of scores for overall and per type:\\n\\nOverall (weighted and macro avg):\\n\\n`accuracy`: the average [accuracy](https://huggingface.co/metrics/accuracy), on a scale between 0.0 and 1.0.\\n    \\n`precision`: the average [precision](https://huggingface.co/metrics/precision), on a scale between 0.0 and 1.0.\\n    \\n`recall`: the average [recall](https://huggingface.co/metrics/recall), on a scale between 0.0 and 1.0.\\n\\n`f1`: the average [F1 score](https://huggingface.co/metrics/f1), which is the harmonic mean of the precision and recall. It also has a scale of 0.0 to 1.0.\\n\\nPer type (e.g. `MISC`, `PER`, `LOC`,...):\\n\\n`precision`: the average [precision](https://huggingface.co/metrics/precision), on a scale between 0.0 and 1.0.\\n\\n`recall`: the average [recall](https://huggingface.co/metrics/recall), on a scale between 0.0 and 1.0.\\n\\n`f1`: the average [F1 score](https://huggingface.co/metrics/f1), on a scale between 0.0 and 1.0.\\n\\n\\n## Examples'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': -1}, page_content='`f1`: the average [F1 score](https://huggingface.co/metrics/f1), on a scale between 0.0 and 1.0.\\n\\n\\n## Examples \\n\\n```python\\n>>> predictions = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'NOUN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'VERB\\', \\'SYM\\']]\\n>>> references = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'SYM\\']]\\n>>> poseval = evaluate.load(\"poseval\")\\n>>> results = poseval.compute(predictions=predictions, references=references)\\n>>> print(list(results.keys()))\\n[\\'ADP\\', \\'INTJ\\', \\'NOUN\\', \\'PROPN\\', \\'PUNCT\\', \\'SYM\\', \\'VERB\\', \\'accuracy\\', \\'macro avg\\', \\'weighted avg\\']\\n>>> print(results[\"accuracy\"])\\n0.8\\n>>> print(results[\"PROPN\"][\"recall\"])\\n0.5'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 3865}, page_content='```\\n\\n## Limitations and bias\\n\\nIn contrast to [seqeval](https://github.com/chakki-works/seqeval), the poseval metric treats each token independently and computes the classification report over all concatenated sequences..\\n\\n\\n## Citation\\n\\n```bibtex\\n@article{scikit-learn,\\n title={Scikit-learn: Machine Learning in {P}ython},\\n author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n journal={Journal of Machine Learning Research},\\n volume={12},\\n pages={2825--2830},\\n year={2011}\\n}\\n```\\n    \\n## Further References \\n- [README for seqeval at GitHub](https://github.com/chakki-works/seqeval)\\n- [Classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) \\n- [Issues with seqeval](https://stackoverflow.com/questions/71327693/how-to-disable-seqeval-label-formatting-for-pos-tagging)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 0}, page_content='--\\ntitle: \"Large Language Models: A New Moore\\'s Law?\"\\nthumbnail: /blog/assets/33_large_language_models/01_model_size.jpg\\nauthors:\\n- user: juliensimon\\n---\\n\\n# Large Language Models: A New Moore\\'s Law?\\n\\n\\n\\nA few days ago, Microsoft and NVIDIA [introduced](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) Megatron-Turing NLG 530B, a Transformer-based model hailed as \"*the world’s largest and most powerful generative language model*.\"\\n \\nThis is an impressive show of Machine Learning engineering, no doubt about it. Yet, should we be excited about this mega-model trend?  I, for one, am not. Here\\'s why.\\n\\n<kbd>\\n  <img src=\"assets/33_large_language_models/01_model_size.jpg\">\\n</kbd>\\n\\n### This is your Brain on Deep Learning\\n\\nResearchers estimate that the human brain contains an average of [86 billion neurons](https://pubmed.ncbi.nlm.nih.gov/19226510/) and 100 trillion synapses. It\\'s safe to assume that not all of them are dedicated to language either. Interestingly, GPT-4 is [expected](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/) to have about 100 trillion parameters... As crude as this analogy is, shouldn\\'t we wonder whether building language models that are about the size of the human brain is the best long-term approach?\\n\\nOf course, our brain is a marvelous device, produced by millions of years of evolution, while Deep Learning models are only a few decades old. Still, our intuition should tell us that something doesn\\'t compute (pun intended).\\n\\n### Deep Learning, Deep Pockets?'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 1609}, page_content='### Deep Learning, Deep Pockets?\\n\\nAs you would expect, training a 530-billion parameter model on humongous text datasets requires a fair bit of infrastructure. In fact, Microsoft and NVIDIA used hundreds of DGX A100 multi-GPU servers. At $199,000 a piece, and factoring in networking equipment, hosting costs, etc., anyone looking to replicate this experiment would have to spend close to $100 million dollars. Want fries with that?\\n\\nSeriously, which organizations have business use cases that would justify spending $100 million on Deep Learning infrastructure? Or even $10 million? Very few. So who are these models for, really?\\n\\n### That Warm Feeling is your GPU Cluster\\n\\nFor all its engineering brilliance, training Deep Learning models on GPUs is a brute force technique. According to the spec sheet, each DGX server can consume up to 6.5 kilowatts. Of course, you\\'ll need at least as much cooling power in your datacenter (or your server closet). Unless you\\'re the Starks and need to keep Winterfell warm in winter, that\\'s another problem you\\'ll have to deal with. \\n\\nIn addition, as public awareness grows on climate and social responsibility issues, organizations need to account for their carbon footprint. According to this 2019 [study](https://arxiv.org/pdf/1906.02243.pdf) from the University of Massachusetts, \"*training BERT on GPU is roughly equivalent to a trans-American flight*\".\\n\\nBERT-Large has 340 million parameters. One can only extrapolate what the footprint of Megatron-Turing could be... People who know me wouldn\\'t call me a bleeding-heart environmentalist. Still, some numbers are hard to ignore.\\n\\n### So?\\n\\nAm I excited by Megatron-Turing NLG 530B and whatever beast is coming next? No. Do I think that the (relatively small) benchmark improvement is worth the added cost, complexity and carbon footprint? No. Do I think that building and promoting these huge models is helping organizations understand and adopt Machine Learning ? No.\\n\\nI\\'m left wondering what\\'s the point of it all. Science for the sake of science? Good old marketing? Technological supremacy? Probably a bit of each. I\\'ll leave them to it, then.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': -1}, page_content=\"I'm left wondering what's the point of it all. Science for the sake of science? Good old marketing? Technological supremacy? Probably a bit of each. I'll leave them to it, then.\\n\\nInstead, let me focus on pragmatic and actionable techniques that you can all use to build high quality Machine Learning solutions.\\n\\n### Use Pretrained Models\\n\\nIn the vast majority of cases, you won't need a custom model architecture. Maybe you'll *want* a custom one (which is a different thing), but there be dragons. Experts only!\\n\\nA good starting point is to look for [models](https://huggingface.co/models) that have been pretrained for the task you're trying to solve (say, [summarizing English text](https://huggingface.co/models?language=en&pipeline_tag=summarization&sort=downloads)).\\n\\nThen, you should quickly try out a few models to predict your own data. If metrics tell you that one works well enough, you're done! If you need a little more accuracy, you should consider fine-tuning the model (more on this in a minute).\\n\\n### Use Smaller Models\\n\\nWhen evaluating models, you should pick the smallest one that can deliver the accuracy you need. It will predict faster and require fewer hardware resources for training and inference. Frugality goes a long way.\\n\\nIt's nothing new either. Computer Vision practitioners will remember when [SqueezeNet](https://arxiv.org/abs/1602.07360) came out in 2017, achieving a 50x reduction in model size compared to [AlexNet](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html), while meeting or exceeding its accuracy. How clever that was!\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 5175}, page_content='Downsizing efforts are also under way in the Natural Language Processing community, using transfer learning techniques such as [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation). [DistilBERT](https://arxiv.org/abs/1910.01108) is perhaps its most widely known achievement. Compared to the original BERT model, it retains 97% of language understanding while being 40% smaller and 60% faster. You can try it [here](https://huggingface.co/distilbert-base-uncased). The same approach has been applied to other models, such as Facebook\\'s [BART](https://arxiv.org/abs/1910.13461), and you can try DistilBART [here](https://huggingface.co/models?search=distilbart).\\n\\nRecent models from the [Big Science](https://bigscience.huggingface.co/) project are also very impressive. As visible in this graph included in the [research paper](https://arxiv.org/abs/2110.08207), their T0 model outperforms GPT-3 on many tasks while being 16x smaller.\\n\\n<kbd>\\n  <img src=\"assets/33_large_language_models/02_t0.png\">\\n</kbd>\\n\\nYou can try T0 [here](https://huggingface.co/bigscience/T0pp). This is the kind of research we need more of!\\n\\n### Fine-Tune Models\\n\\nIf you need to specialize a model, there should be very few reasons to train it from scratch. Instead, you should fine-tune it, that is to say train it only for a few epochs on your own data. If you\\'re short on data, maybe of one these [datasets](https://huggingface.co/datasets) can get you started.\\n\\nYou guessed it, that\\'s another way to do transfer learning, and it\\'ll help you save on everything!\\n \\n* Less data to collect, store, clean and annotate,\\n* Faster experiments and iterations,\\n* Fewer resources required in production.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 6871}, page_content='In other words: save time, save money, save hardware resources, save the world! \\n\\nIf you need a tutorial, the Hugging Face [course](https://huggingface.co/course) will get you started in no time.\\n\\n### Use Cloud-Based Infrastructure\\n\\nLike them or not, cloud companies know how to build efficient infrastructure. Sustainability studies show that cloud-based infrastructure is more energy and carbon efficient than the alternative: see [AWS](https://sustainability.aboutamazon.com/environment/the-cloud), [Azure](https://azure.microsoft.com/en-us/global-infrastructure/sustainability), and [Google](https://cloud.google.com/sustainability). Earth.org [says](https://earth.org/environmental-impact-of-cloud-computing/) that while cloud infrastructure is not perfect, \"[*it\\'s] more energy efficient than the alternative and facilitates environmentally beneficial services and economic growth.*\"\\n\\nCloud certainly has a lot going for it when it comes to ease of use, flexibility and pay as you go. It\\'s also a little greener than you probably thought. If you\\'re short on GPUs, why not try fine-tune your Hugging Face models on [Amazon SageMaker](https://aws.amazon.com/sagemaker/), AWS\\' managed service for Machine Learning? We\\'ve got [plenty of examples](https://huggingface.co/docs/sagemaker/train) for you.\\n\\n### Optimize Your Models\\n\\nFrom compilers to virtual machines, software engineers have long used tools that automatically optimize their code for whatever hardware they\\'re running on. \\n\\nHowever, the Machine Learning community is still struggling with this topic, and for good reason. Optimizing models for size and speed is a devilishly complex task, which involves techniques such as:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': -1}, page_content=\"However, the Machine Learning community is still struggling with this topic, and for good reason. Optimizing models for size and speed is a devilishly complex task, which involves techniques such as:\\n\\n* Specialized hardware that speeds up training ([Graphcore](https://www.graphcore.ai/), [Habana](https://habana.ai/)) and inference ([Google TPU](https://cloud.google.com/tpu), [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)).\\n* Pruning: remove model parameters that have little or no impact on the predicted outcome.\\n* Fusion: merge model layers (say, convolution and activation).\\n* Quantization: storing model parameters in smaller values (say, 8 bits instead of 32 bits)\\n\\nFortunately, automated tools are starting to appear, such as the [Optimum](https://huggingface.co/hardware) open source library, and [Infinity](https://huggingface.co/infinity), a containerized solution that delivers Transformers accuracy at 1-millisecond latency.\\n\\n### Conclusion \\n\\nLarge language model size has been increasing 10x every year for the last few years. This is starting to look like another [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law).  \\n\\nWe've been there before, and we should know that this road leads to diminishing returns, higher cost, more complexity, and new risks. Exponentials tend not to end well. Remember [Meltdown and Spectre](https://meltdownattack.com/)? Do we want to find out what that looks like for AI?\\n\\nInstead of chasing trillion-parameter models (place your bets), wouldn't all be better off if we built practical and efficient solutions that all developers can use to solve real-world problems?\\n\\n*Interested in how Hugging Face can help your organization build and deploy production-grade Machine Learning solutions? Get in touch at [julsimon@huggingface.co](mailto:julsimon@huggingface.co) (no recruiters, no sales pitches, please).*\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md', 'start_index': 0}, page_content='!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# VisionTextDualEncoder\\n\\n## Overview\\n\\nThe [`VisionTextDualEncoderModel`] can be used to initialize a vision-text dual encoder model with\\nany pretrained vision autoencoding model as the vision encoder (*e.g.* [ViT](vit), [BEiT](beit), [DeiT](deit)) and any pretrained text autoencoding model as the text encoder (*e.g.* [RoBERTa](roberta), [BERT](bert)). Two projection layers are added on top of both the vision and text encoder to project the output embeddings\\nto a shared latent space. The projection layers are randomly initialized so the model should be fine-tuned on a\\ndownstream task. This model can be used to align the vision-text embeddings using CLIP like contrastive image-text\\ntraining and then can be used for zero-shot vision tasks such image-classification or retrieval.\\n\\nIn [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how\\nleveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement on\\nnew zero-shot vision tasks such as image classification or retrieval.\\n\\n## VisionTextDualEncoderConfig\\n\\n[[autodoc]] VisionTextDualEncoderConfig\\n\\n## VisionTextDualEncoderProcessor'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md', 'start_index': -1}, page_content='## VisionTextDualEncoderConfig\\n\\n[[autodoc]] VisionTextDualEncoderConfig\\n\\n## VisionTextDualEncoderProcessor\\n\\n[[autodoc]] VisionTextDualEncoderProcessor\\n\\n<frameworkcontent>\\n<pt>\\n\\n## VisionTextDualEncoderModel\\n\\n[[autodoc]] VisionTextDualEncoderModel\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## FlaxVisionTextDualEncoderModel\\n\\n[[autodoc]] FlaxVisionTextDualEncoderModel\\n    - __call__\\n\\n</tf>\\n<jax>\\n\\n## TFVisionTextDualEncoderModel\\n\\n[[autodoc]] TFVisionTextDualEncoderModel\\n    - call\\n\\n</jax>\\n</frameworkcontent>'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02d_dynamic-padding.md', 'start_index': 0}, page_content='hat is dynamic padding? In the \"Batching Inputs together\" video, we have seen that to be able to group inputs of different lengths in the same batch, we need to add padding tokens to all the short inputs until they are all of the same length. Here for instance, the longest sentence is the third one, and we need to add 5, 2 and 7 pad tokens to the other to have four sentences of the same lengths. When dealing with a whole dataset, there are various padding strategies we can apply. The most obvious one is to pad all the elements of the dataset to the same length: the length of the longest sample. This will then give us batches that all have the same shape determined by the maximum sequence length. The downside is that batches composed from short sentences will have a lot of padding tokens which introduce more computations in the model we ultimately don\\'t need. To avoid this, another strategy is to pad the elements when we batch them together, to the longest sentence inside the batch. This way batches composed of short inputs will be smaller than the batch containing the longest sentence in the dataset. This will yield some nice speedup on CPU and GPU. The downside is that all batches will then have different shapes, which slows down training on other accelerators like TPUs. Let\\'s see how to apply both strategies in practice. We have actually seen how to apply fixed padding in the Datasets Overview video, when we preprocessed the MRPC dataset: after loading the dataset and tokenizer, we applied the tokenization to all the dataset with padding and truncation to make all samples of length 128. As a result, if we pass this dataset to a PyTorch DataLoader, we get batches of shape batch size (here 16) by 128. To apply dynamic padding, we must defer the padding to the batch preparation, so we remove that part from our tokenize function. We still leave the truncation part so that inputs that are bigger than the maximum length accepted by the model (usually 512) get truncated to that length. Then we pad our samples dynamically by using a data collator. Those classes in the Transformers library are responsible for applying all the final processing needed before forming a batch, here DataCollatorWithPadding will pad the samples to the maximum length inside the batch of sentences. We pass it to'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02d_dynamic-padding.md', 'start_index': -1}, page_content='a data collator. Those classes in the Transformers library are responsible for applying all the final processing needed before forming a batch, here DataCollatorWithPadding will pad the samples to the maximum length inside the batch of sentences. We pass it to the PyTorch DataLoader as a collate function, then observe that the batches generated have various lenghs, all way below the 128 from before. Dynamic batching will almost always be faster on CPUs and GPUs, so you should apply it if you can. Remember to switch back to fixed padding however if you run your training script on TPU or need batches of fixed shapes.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Kandinsky 3\\n\\nKandinsky 3 is created by [Vladimir Arkhipkin](https://github.com/oriBetelgeuse),[Anastasia Maltseva](https://github.com/NastyaMittseva),[Igor Pavlov](https://github.com/boomb0om),[Andrei Filatov](https://github.com/anvilarth),[Arseniy Shakhmatov](https://github.com/cene555),[Andrey Kuznetsov](https://github.com/kuznetsoffandrey),[Denis Dimitrov](https://github.com/denndimitrov), [Zein Shaheen](https://github.com/zeinsh)\\n\\nThe description from it\\'s Github page: \\n\\n*Kandinsky 3.0 is an open-source text-to-image diffusion model built upon the Kandinsky2-x model family. In comparison to its predecessors, enhancements have been made to the text understanding and visual quality of the model, achieved by increasing the size of the text encoder and Diffusion U-Net models, respectively.*\\n\\nIts architecture includes 3 main components:\\n1. [FLAN-UL2](https://huggingface.co/google/flan-ul2), which is an encoder decoder model based on the T5 architecture. \\n2. New U-Net architecture featuring BigGAN-deep blocks doubles depth while maintaining the same number of parameters.\\n3. Sber-MoVQGAN is a decoder proven to have superior results in image restoration.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 1758}, page_content='The original codebase can be found at [ai-forever/Kandinsky-3](https://github.com/ai-forever/Kandinsky-3).\\n\\n<Tip>\\n\\nCheck out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\\n\\n</Tip>\\n\\n<Tip>\\n\\nMake sure to check out the schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## Kandinsky3Pipeline\\n\\n[[autodoc]] Kandinsky3Pipeline\\n\\t- all\\n\\t- __call__\\n\\n## Kandinsky3Img2ImgPipeline\\n\\n[[autodoc]] Kandinsky3Img2ImgPipeline\\n\\t- all\\n\\t- __call__'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 1}, page_content='Datasets server - worker\\n\\n> Workers that pre-compute and cache the response to /splits, /first-rows, /parquet, /info and /size.\\n\\n## Configuration\\n\\nUse environment variables to configure the workers. The prefix of each environment variable gives its scope.\\n\\n### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`WORKER_UVICORN_` prefix). It is used for the /healthcheck and the /metrics endpoints:\\n\\n- `WORKER_UVICORN_HOSTNAME`: the hostname. Defaults to `\"localhost\"`.\\n- `WORKER_UVICORN_NUM_WORKERS`: the number of uvicorn workers. Defaults to `2`.\\n- `WORKER_UVICORN_PORT`: the port. Defaults to `8000`.\\n\\n### Prometheus\\n\\n- `PROMETHEUS_MULTIPROC_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every uvicorn worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker.\\n\\n## Worker configuration\\n\\nSet environment variables to configure the worker.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 1063}, page_content='- `WORKER_CONTENT_MAX_BYTES`: the maximum size in bytes of the response content computed by a worker (to prevent returning big responses in the REST API). Defaults to `10_000_000`.\\n- `WORKER_DIFFICULTY_MAX`: the maximum difficulty of the jobs to process. Defaults to None.\\n- `WORKER_DIFFICULTY_MIN`: the minimum difficulty of the jobs to process. Defaults to None.\\n- `WORKER_HEARTBEAT_INTERVAL_SECONDS`: the time interval between two heartbeats. Each heartbeat updates the job \"last_heartbeat\" field in the queue. Defaults to `60` (1 minute).\\n- `WORKER_JOB_TYPES_BLOCKED`: comma-separated list of job types that will not be processed, e.g. \"dataset-config-names,dataset-split-names\". If empty, no job type is blocked. Defaults to empty.\\n- `WORKER_JOB_TYPES_ONLY`: comma-separated list of the non-blocked job types to process, e.g. \"dataset-config-names,dataset-split-names\". If empty, the worker processes all the non-blocked jobs. Defaults to empty.\\n- `WORKER_KILL_LONG_JOB_INTERVAL_SECONDS`: the time interval at which the worker looks for long jobs to kill them. Defaults to `60` (1 minute).\\n- `WORKER_KILL_ZOMBIES_INTERVAL_SECONDS`: the time interval at which the worker looks for zombie jobs to kill them. Defaults to `600` (10 minutes).\\n- `WORKER_MAX_DISK_USAGE_PCT`: maximum disk usage of every storage disk in the list (in percentage) to allow a job to start. Set to 0 to disable the test. Defaults to 90.\\n- `WORKER_MAX_JOB_DURATION_SECONDS`: the maximum duration allowed for a job to run. If the job runs longer, it is killed (see `WORKER_KILL_LONG_JOB_INTERVAL_SECONDS`). Defaults to `1200` (20 minutes).'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 2678}, page_content=\"- `WORKER_MAX_LOAD_PCT`: maximum load of the machine (in percentage: the max between the 1m load and the 5m load divided by the number of CPUs \\\\*100) allowed to start a job. Set to 0 to disable the test. Defaults to 70.\\n- `WORKER_MAX_MEMORY_PCT`: maximum memory (RAM + SWAP) usage of the machine (in percentage) allowed to start a job. Set to 0 to disable the test. Defaults to 80.\\n- `WORKER_MAX_MISSING_HEARTBEATS`: the number of hearbeats a job must have missed to be considered a zombie job. Defaults to `5`.\\n- `WORKER_SLEEP_SECONDS`: wait duration in seconds at each loop iteration before checking if resources are available and processing a job if any is available. Note that the loop doesn't wait just after finishing a job: the next job is immediately processed. Defaults to `15`.\\n- `WORKER_STORAGE_PATHS`: comma-separated list of paths to check for disk usage. Defaults to empty.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 3567}, page_content=\"Also, it's possible to force the parent directory in which the temporary files (as the current job state file and its associated lock file) will be created by setting `TMPDIR` to a writable directory. If not set, the worker will use the default temporary directory of the system, as described in https://docs.python.org/3/library/tempfile.html#tempfile.gettempdir.\\n\\n### Datasets based worker\\n\\nSet environment variables to configure the datasets-based worker (`DATASETS_BASED_` prefix):\\n\\n- `DATASETS_BASED_HF_DATASETS_CACHE`: directory where the `datasets` library will store the cached datasets' data. If not set, the datasets library will choose the default location. Defaults to None.\\n\\nAlso, set the modules cache configuration for the datasets-based worker. See [../../libs/libcommon/README.md](../../libs/libcommon/README.md). Note that this variable has no `DATASETS_BASED_` prefix:\\n\\n- `HF_MODULES_CACHE`: directory where the `datasets` library will store the cached dataset scripts. If not set, the datasets library will choose the default location. Defaults to None.\\n\\nNote that both directories will be appended to `WORKER_STORAGE_PATHS` (see [../../libs/libcommon/README.md](../../libs/libcommon/README.md)) to hold the workers when the disk is full.\\n\\n### Numba library\\n\\nNumba requires setting the `NUMBA_CACHE_DIR` environment variable to a writable directory to cache the compiled functions. Required on cloud infrastructure (see https://stackoverflow.com/a/63367171/7351594):\\n\\n- `NUMBA_CACHE_DIR`: directory where the `numba` decorators (used by `librosa`) can write cache.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': -1}, page_content='- `NUMBA_CACHE_DIR`: directory where the `numba` decorators (used by `librosa`) can write cache.\\n\\nNote that this directory will be appended to `WORKER_STORAGE_PATHS` (see [../../libs/libcommon/README.md](../../libs/libcommon/README.md)) to hold the workers when the disk is full.\\n\\n### Huggingface_hub library\\n\\nIf the Hub is not https://huggingface.co (i.e., if you set the `COMMON_HF_ENDPOINT` environment variable), you must set the `HF_ENDPOINT` environment variable to the same value. See https://github.com/huggingface/datasets/pull/5196#issuecomment-1322191411 for more details:\\n\\n- `HF_ENDPOINT`: the URL of the Hub. Defaults to `https://huggingface.co`.\\n\\n### First rows worker\\n\\nSet environment variables to configure the `first-rows` worker (`FIRST_ROWS_` prefix):\\n\\n- `FIRST_ROWS_MAX_BYTES`: the max size of the /first-rows response in bytes. Defaults to `1_000_000` (1 MB).\\n- `FIRST_ROWS_MAX_NUMBER`: the max number of rows fetched by the worker for the split and provided in the /first-rows response. Defaults to `100`.\\n- `FIRST_ROWS_MIN_CELL_BYTES`: the minimum size in bytes of a cell when truncating the content of a row (see `FIRST_ROWS_ROWS_MAX_BYTES`). Below this limit, the cell content will not be truncated. Defaults to `100`.\\n- `FIRST_ROWS_MIN_NUMBER`: the min number of rows fetched by the worker for the split and provided in the /first-rows response. Defaults to `10`.\\n- `FIRST_ROWS_COLUMNS_MAX_NUMBER`: the max number of columns (features) provided in the /first-rows response. If the number of columns is greater than the limit, an error is returned. Defaults to `1_000`.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 6651}, page_content='Also, set the assets-related configuration for the first-rows worker. See [../../libs/libcommon/README.md](../../libs/libcommon/README.md).\\n\\n### Parquet and info worker\\n\\nSet environment variables to configure the `parquet-and-info` worker (`PARQUET_AND_INFO_` prefix):'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 6921}, page_content='- `PARQUET_AND_INFO_COMMIT_MESSAGE`: the git commit message when the worker uploads the parquet files to the Hub. Defaults to `Update parquet files`.\\n- `PARQUET_AND_INFO_COMMITTER_HF_TOKEN`: the HuggingFace token to commit the parquet files to the Hub. The token must be an app token associated with a user that has the right to 1. create the `refs/convert/parquet` branch (see `PARQUET_AND_INFO_TARGET_REVISION`) and 2. push commits to it on any dataset. [Datasets maintainers](https://huggingface.co/datasets-maintainers) members have these rights. The token must have permission to write. If not set, the worker will fail. Defaults to None.\\n- `PARQUET_AND_INFO_MAX_DATASET_SIZE_BYTES`: the maximum size in bytes of the dataset to pre-compute the parquet files. Bigger datasets, or datasets without that information, are partially streamed to get parquet files up to this value. Defaults to `100_000_000`.\\n- `PARQUET_AND_INFO_MAX_EXTERNAL_DATA_FILES`: the maximum number of external files of the datasets. Bigger datasets, or datasets without that information, are partially streamed to get parquet files up to `PARQUET_AND_INFO_MAX_DATASET_SIZE_BYTES` bytes. Defaults to `10_000`.\\n- `PARQUET_AND_INFO_MAX_ROW_GROUP_BYTE_SIZE_FOR_COPY`: the maximum size in bytes of the row groups of parquet datasets that are copied to the target revision. Bigger datasets, or datasets without that information, are partially streamed to get parquet files up to `PARQUET_AND_INFO_MAX_DATASET_SIZE_BYTES` bytes. Defaults to `100_000_000`.\\n- `PARQUET_AND_INFO_SOURCE_REVISION`: the git revision of the dataset to use to prepare the parquet files. Defaults to `main`.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': -1}, page_content='- `PARQUET_AND_INFO_SOURCE_REVISION`: the git revision of the dataset to use to prepare the parquet files. Defaults to `main`.\\n- `PARQUET_AND_INFO_TARGET_REVISION`: the git revision of the dataset where to store the parquet files. Make sure the committer token (`PARQUET_AND_INFO_COMMITTER_HF_TOKEN`) has the permission to write there. Defaults to `refs/convert/parquet`.\\n- `PARQUET_AND_INFO_URL_TEMPLATE`: the URL template to build the parquet file URLs. Defaults to `/datasets/%s/resolve/%s/%s`.'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 8944}, page_content=\"### Duckdb Index worker\\n\\nSet environment variables to configure the `duckdb-index` worker (`DUCKDB_INDEX_` prefix):\\n\\n- `DUCKDB_INDEX_CACHE_DIRECTORY`: directory where the temporal duckdb index files are stored. Defaults to empty.\\n- `DUCKDB_INDEX_COMMIT_MESSAGE`: the git commit message when the worker uploads the duckdb index file to the Hub. Defaults to `Update duckdb index file`.\\n- `DUCKDB_INDEX_COMMITTER_HF_TOKEN`: the HuggingFace token to commit the duckdb index file to the Hub. The token must be an app token associated with a user that has the right to 1. create the `refs/convert/parquet` branch (see `DUCKDB_INDEX_TARGET_REVISION`) and 2. push commits to it on any dataset. [Datasets maintainers](https://huggingface.co/datasets-maintainers) members have these rights. The token must have permission to write. If not set, the worker will fail. Defaults to None.\\n- `DUCKDB_INDEX_MAX_DATASET_SIZE_BYTES`: the maximum size in bytes of the dataset's parquet files to index. Datasets with bigger size are ignored. Defaults to `100_000_000`.\\n- `DUCKDB_INDEX_TARGET_REVISION`: the git revision of the dataset where to store the duckdb index file. Make sure the committer token (`DUCKDB_INDEX_COMMITTER_HF_TOKEN`) has the permission to write there. Defaults to `refs/convert/parquet`.\\n- `DUCKDB_INDEX_URL_TEMPLATE`: the URL template to build the duckdb index file URL. Defaults to `/datasets/%s/resolve/%s/%s`.\\n- `DUCKDB_INDEX_EXTENSIONS_DIRECTORY`: directory where the duckdb extensions will be downloaded. Defaults to empty.\\n\\n### Descriptive statistics worker\\n\\nSet environment variables to configure the `descriptive-statistics` worker (`DESCRIPTIVE_STATISTICS_` prefix):\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': -1}, page_content='### Descriptive statistics worker\\n\\nSet environment variables to configure the `descriptive-statistics` worker (`DESCRIPTIVE_STATISTICS_` prefix):\\n\\n- `DESCRIPTIVE_STATISTICS_CACHE_DIRECTORY`: directory to which a dataset in parquet format is downloaded. Defaults to empty.\\n- `DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS`: number of histogram bins (see examples below for more info).\\n- `DESCRIPTIVE_STATISTICS_MAX_PARQUET_SIZE_BYTES`: maximum size in bytes of the dataset\\'s parquet files to compute statistics. Datasets with bigger size are ignored. Defaults to `100_000_000`.\\n\\n#### How descriptive statistics are computed \\n\\nDescriptive statistics are currently computed for the following data types: strings, floats, and ints (including `ClassLabel` int). \\nResponse has two fields: `num_examples` and `statistics`. `statistics` field is a list of dicts with three keys: `column_name`, `column_type`, and `column_statistics`.\\n\\n`column_type` is one of the following values:\\n* `class_label` - for `datasets.ClassLabel` feature\\n* `float` - for float dtypes (\"float16\", \"float32\", \"float64\")\\n* `int` - for integer dtypes (\"int8\", \"int16\", \"int32\", \"int64\", \"uint8\", \"uint16\", \"uint32\", \"uint64\")\\n* `string_label` - for string dtypes (\"string\", \"large_string\") - if there are less than or equal to `MAX_NUM_STRING_LABELS` unique values (hardcoded in worker\\'s code, for now it\\'s 30)\\n* `string_text` - for string dtypes (\"string\", \"large_string\") - if there are more than `MAX_NUM_STRING_LABELS` unique values\\n* `bool` - for boolean dtype (\"bool\")\\n\\n`column_statistics` content depends on the feature type, see examples below.\\n##### class_label\\n\\n<details><summary>example: </summary>\\n<p>'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 12014}, page_content='`column_statistics` content depends on the feature type, see examples below.\\n##### class_label\\n\\n<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    \"column_name\": \"class_col\",\\n    \"column_type\": \"class_label\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"no_label_count\": 0,  # number of -1 values - special value of the `datasets` lib to encode `no label` \\n        \"no_label_proportion\": 0.0,\\n        \"n_unique\": 5,  # number of unique values (excluding `no label` and nan)\\n        \"frequencies\": {   # mapping value -> its count\\n            \"this\": 19834,\\n            \"are\": 20159,\\n            \"random\": 20109,\\n            \"words\": 20172,\\n            \"test\": 19726\\n        }\\n    }\\n}'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 12752}, page_content='```\\n</p>\\n</details> \\n\\n##### float\\n\\nBin size for histogram is counted as `(max_value - min_value) / DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS`\\n\\n<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    \"column_name\": \"delay\",\\n    \"column_type\": \"float\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": -10.206,\\n        \"max\": 8.48053,\\n        \"mean\": 2.10174,\\n        \"median\": 3.4012,\\n        \"std\": 3.12487,\\n        \"histogram\": {\\n            \"hist\": [\\n                2,\\n                34,\\n                256,\\n                15198,\\n                9037,\\n                2342,\\n                12743,\\n                45114,\\n                14904,\\n                370\\n            ],\\n            \"bin_edges\": [\\n                -10.206,\\n                -8.33734,\\n                -6.46869,\\n                -4.60004,\\n                -2.73139,\\n                -0.86273,\\n                1.00592,\\n                2.87457,\\n                4.74322,\\n                6.61188,\\n                8.48053  # includes maximum value, so len is always len(hist) + 1\\n            ]\\n        }\\n    }\\n}'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 13883}, page_content=\"```\\n</p>\\n</details> \\n\\n##### int\\n\\nAs bin edges for integer values also must be integers, bin size is counted as `np.ceil((max_value - min_value + 1) / DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS)`. Rounding up means that there might be smaller number of bins in response then provided `DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS`. The last bin's size might be smaller than that of the others if the feature's range is not divisible by the rounded bin size. \\n\\n<details><summary>examples: </summary>\\n<p>\"),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 14382}, page_content='```python\\n{\\n    \"column_name\": \"direction\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 0,\\n        \"max\": 1,\\n        \"mean\": 0.49925,\\n        \"median\": 0.0,\\n        \"std\": 0.5,\\n        \"histogram\": {\\n            \"hist\": [\\n                50075,\\n                49925\\n            ],\\n            \"bin_edges\": [\\n                0,\\n                1,\\n                1  # if the last value is equal to the last but one, that means that this bin includes only this value\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"hour\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 0,\\n        \"max\": 23,\\n        \"mean\": 13.44402,\\n        \"median\": 14.0,\\n        \"std\": 5.49455,\\n        \"histogram\": {\\n            \"hist\": [\\n                2694,\\n                2292,\\n                16785,\\n                16326,\\n                16346,\\n                17809,\\n                16546,\\n                11202\\n            ],\\n            \"bin_edges\": [\\n                0,\\n                3,\\n                6,\\n                9,\\n                12,\\n                15,\\n                18,\\n                21,\\n                23\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"humidity\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 54,\\n        \"max\": 99,\\n        \"mean\": 83.89878,\\n        \"median\": 85.0,\\n        \"std\": 8.65174,\\n        \"histogram\": {\\n            \"hist\": [\\n                554,\\n                1662,\\n                3823,\\n                6532,\\n                12512,\\n                17536,\\n                23871,\\n                20355,\\n                12896,\\n                259\\n            ],\\n            \"bin_edges\": [\\n                54,\\n                59,\\n                64,\\n                69,\\n                74,\\n                79,\\n                84,\\n                89,\\n                94,\\n                99,\\n                99\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"weekday\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': -1}, page_content='84,\\n                89,\\n                94,\\n                99,\\n                99\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"weekday\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 0,\\n        \"max\": 6,\\n        \"mean\": 3.08063,\\n        \"median\": 3.0,\\n        \"std\": 1.90347,\\n        \"histogram\": {\\n            \"hist\": [\\n                10282,\\n                15416,\\n                15291,\\n                15201,\\n                15586,\\n                15226,\\n                12998\\n            ],\\n            \"bin_edges\": [\\n                0,\\n                1,\\n                2,\\n                3,\\n                4,\\n                5,\\n                6,\\n                6\\n            ]\\n        }\\n    }\\n}'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 17176}, page_content='```\\n\\n</p>\\n</details>\\n\\n##### string_label\\n\\nIf the number of unique values in a column (within requested split) is <= `MAX_NUM_STRING_LABELS` (currently 30), the column is considered to be a category and the categories counts are computed.\\n\\n<details><summary>examples: </summary>\\n<p>\\n\\n```python\\n{\\n    \\'column_name\\': \\'string_col\\',\\n    \\'column_type\\': \\'string_label\\',\\n    \\'column_statistics\\': \\n        {\\n            \"nan_count\": 0,\\n            \"nan_proportion\": 0.0,\\n            \"n_unique\": 5,  # number of unique values (excluding nan)\\n            \"frequencies\": {   # mapping value -> its count\\n                \"this\": 19834,\\n                \"are\": 20159,\\n                \"random\": 20109,\\n                \"words\": 20172,\\n                \"test\": 19726\\n        }\\n    }\\n}\\n```\\n</p>\\n</details>\\n\\n##### string_text\\n\\nIf the number of unique values in a column (within requested split) is > `MAX_NUM_STRING_LABELS` (currently 30), the column is considered to be text and the distribution of text **lengths** is computed.\\n\\n<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    \\'column_name\\': \\'text_col\\',\\n    \\'column_type\\': \\'string_text\\',\\n    \\'column_statistics\\': {\\n        \\'max\\': 296,\\n        \\'mean\\': 97.46649,\\n        \\'median\\': 88.0,\\n        \\'min\\': 11,\\n        \\'nan_count\\': 0,\\n        \\'nan_proportion\\': 0.0,\\n        \\'std\\': 55.82714,\\n        \\'histogram\\': {\\n            \\'bin_edges\\': [\\n                11,\\n                40,\\n                69,\\n                98,\\n                127,\\n                156,\\n                185,\\n                214,\\n                243,\\n                272,\\n                296\\n            ],\\n            \\'hist\\': [\\n                171,\\n                224,\\n                235,\\n                180,\\n                102,\\n                99,\\n                53,\\n                28,\\n                10,\\n                2\\n               ]\\n             },\\n    }\\n}'),\n",
       " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 19059}, page_content=\"```\\n</p>\\n</details>\\n\\n##### bool\\n\\n<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    'column_name': 'bool__nan_column', \\n    'column_type': 'bool', \\n    'column_statistics': \\n        {\\n            'nan_count': 3, \\n            'nan_proportion': 0.15, \\n            'frequencies': {\\n                'False': 7, \\n                'True': 10\\n            }\\n        }\\n}\\n```\\n</p>\\n</details>\\n\\n\\n\\n### Splits worker\\n\\nThe `splits` worker does not need any additional configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 1}, page_content='Differences between Dataset and IterableDataset\\n\\nThere are two types of dataset objects, a [`Dataset`] and an [`IterableDataset`].\\nWhichever type of dataset you choose to use or create depends on the size of the dataset.\\nIn general, an [`IterableDataset`] is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and speed advantages, while a [`Dataset`] is great for everything else.\\nThis page will compare the differences between a [`Dataset`] and an [`IterableDataset`] to help you pick the right dataset object for you.\\n\\n## Downloading and streaming\\n\\nWhen you have a regular [`Dataset`], you can access it using `my_dataset[0]`. This provides random access to the rows.\\nSuch datasets are also called \"map-style\" datasets.\\nFor example you can download ImageNet-1k like this and access any row:\\n\\n```python\\nfrom datasets import load_dataset\\n\\nimagenet = load_dataset(\"imagenet-1k\", split=\"train\")  # downloads the full dataset\\nprint(imagenet[0])\\n```\\n\\nBut one caveat is that you must have the entire dataset stored on your disk or in memory, which blocks you from accessing datasets bigger than the disk.\\nBecause it can become inconvenient for big datasets, there exists another type of dataset, the [`IterableDataset`].\\nWhen you have an `IterableDataset`, you can access it using a `for` loop to load the data progressively as you iterate over the dataset.\\nThis way, only a small fraction of examples is loaded in memory, and you don\\'t write anything on disk.\\n\\nFor example, you can stream the ImageNet-1k dataset without downloading it on disk:\\n\\n```python\\nfrom datasets import load_dataset\\n\\nimagenet = load_dataset(\"imagenet-1k\", split=\"train\", streaming=True)  # will start loading the data when iterated over\\nfor example in imagenet:\\n    print(example)\\n    break'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 1785}, page_content='```\\n\\nStreaming can read online data without writing any file to disk.\\nFor example, you can stream datasets made out of multiple shards, each of which is hundreds of gigabytes like [C4](https://huggingface.co/datasets/c4), [OSCAR](https://huggingface.co/datasets/oscar) or [LAION-2B](https://huggingface.co/datasets/laion/laion2B-en).\\nLearn more about how to stream a dataset in the [Dataset Streaming Guide](./stream).\\n\\nThis is not the only difference though, because the \"lazy\" behavior of an `IterableDataset` is also present when it comes to dataset creation and processing.\\n\\n## Creating map-style datasets and iterable datasets\\n\\nYou can create a [`Dataset`] using lists or dictionaries, and the data is entirely converted to Arrow so you can easily access any row:\\n```python\\nmy_dataset = Dataset.from_dict({\"col_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]})\\nprint(my_dataset[0])\\n```\\n\\nTo create an `IterableDataset` on the other hand, you must provide a \"lazy\" way to load the data.\\nIn Python, we generally use generator functions. These functions `yield` one example at a time, which means you can\\'t access a row by slicing it like a regular `Dataset`:\\n```python\\ndef my_generator(n):\\n    for i in range(n):\\n        yield {\"col_1\": i}\\n\\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs={\"n\": 10})\\nfor example in my_iterable_dataset:\\n    print(example)\\n    break'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 3169}, page_content='```\\n\\n## Loading local files entirely and progressively\\n\\nIt is possible to convert local or remote data files to an Arrow [`Dataset`] using [`load_dataset`]:\\n```python\\ndata_files = {\"train\": [\"path/to/data.csv\"]}\\nmy_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\")\\nprint(my_dataset[0])\\n```\\n\\nHowever, this requires a conversion step from CSV to Arrow format, which takes time and disk space if your dataset is big.\\n\\nTo save disk space and skip the conversion step, you can define an `IterableDataset` by streaming from the local files directly.\\nThis way, the data is read progressively from the local files as you iterate over the dataset:\\n\\n```python\\ndata_files = {\"train\": [\"path/to/data.csv\"]}\\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\\nfor example in my_iterable_dataset:  # this reads the CSV file progressively as you iterate over the dataset\\n    print(example)\\n    break\\n```\\n\\nMany file formats are supported, like CSV, JSONL, and Parquet, as well as image and audio files.\\nYou can find more information in the corresponding guides for loading [tabular](./tabular_load), [text](./nlp_load), [vision](./image_load), and [audio](./audio_load]) datasets.\\n\\n## Eager data processing and lazy data processing\\n\\nWhen you process a [`Dataset`] object using [`Dataset.map`], the entire dataset is processed immediately and returned.\\nThis is similar to how `pandas` works for example.\\n\\n```python\\nmy_dataset = my_dataset.map(process_fn)  # process_fn is applied on all the examples of the dataset\\nprint(my_dataset[0])'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 4750}, page_content='```\\n\\nOn the other hand, due to the \"lazy\" nature of an `IterableDataset`, calling [`IterableDataset.map`] does not apply your `map` function over the full dataset.\\nInstead, your `map` function is applied on-the-fly.\\n\\nBecause of that, you can chain multiple processing steps and they will all run at once when you start iterating over the dataset:\\n\\n```python\\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_1)\\nmy_iterable_dataset = my_iterable_dataset.filter(filter_fn)\\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_2)\\n\\n# process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset\\nfor example in my_iterable_dataset:  \\n    print(example)\\n    break\\n```\\n\\n## Exact and fast approximate shuffling\\n\\nWhen you shuffle a [`Dataset`] using [`Dataset.shuffle`], you apply an exact shuffling of the dataset.\\nIt works by taking a list of indices `[0, 1, 2, ... len(my_dataset) - 1]` and shuffling this list.\\nThen, accessing `my_dataset[0]` returns the row and index defined by the first element of the indices mapping that has been shuffled:\\n```python\\nmy_dataset = my_dataset.shuffle(seed=42)\\nprint(my_dataset[0])'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 5908}, page_content=\"```\\n\\nSince we don't have random access to the rows in the case of an `IterableDataset`, we can't use a shuffled list of indices and access a row at an arbitrary position.\\nThis prevents the use of exact shuffling.\\nInstead, a fast approximate shuffling is used in [`IterableDataset.shuffle`].\\nIt uses a shuffle buffer to sample random examples iteratively from the dataset.\\nSince the dataset is still read iteratively, it provides excellent speed performance:\\n```python\\nmy_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\\nfor example in my_iterable_dataset:\\n    print(example)\\n    break\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 6517}, page_content='```\\n\\nBut using a shuffle buffer is not enough to provide a satisfactory shuffling for machine learning model training. So [`IterableDataset.shuffle`] also shuffles the dataset shards if your dataset is made of multiple files or sources:\\n\\n```python\\n# Stream from the internet\\nmy_iterable_dataset = load_dataset(\"deepmind/code_contests\", split=\"train\", streaming=True)\\nmy_iterable_dataset.n_shards  # 39\\n\\n# Stream from local files\\ndata_files = {\"train\": [f\"path/to/data_{i}.csv\" for i in range(1024)]}\\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\\nmy_iterable_dataset.n_shards  # 1024\\n\\n# From a generator function\\ndef my_generator(n, sources):\\n    for source in sources:\\n        for example_id_for_current_source in range(n):\\n            yield {\"example_id\": f\"{source}_{example_id_for_current_source}\"}\\n\\ngen_kwargs = {\"n\": 10, \"sources\": [f\"path/to/data_{i}\" for i in range(1024)]}\\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs=gen_kwargs)\\nmy_iterable_dataset.n_shards  # 1024'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 7575}, page_content=\"```\\n\\n## Speed differences\\n\\nRegular [`Dataset`] objects are based on Arrow which provides fast random access to the rows.\\nThanks to memory mapping and the fact that Arrow is an in-memory format, reading data from disk doesn't do expensive system calls and deserialization.\\nIt provides even faster data loading when iterating using a `for` loop by iterating on contiguous Arrow record batches.\\n\\nHowever as soon as your [`Dataset`] has an indices mapping (via [`Dataset.shuffle`] for example), the speed can become 10x slower.\\nThis is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you aren't reading contiguous chunks of data anymore.\\nTo restore the speed, you'd need to rewrite the entire dataset on your disk again using [`Dataset.flatten_indices`], which removes the indices mapping.\\nThis may take a lot of time depending of the size of your dataset though:\\n\\n```python\\nmy_dataset[0]  # fast\\nmy_dataset = my_dataset.shuffle(seed=42)\\nmy_dataset[0]  # up to 10x slower\\nmy_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\\nmy_dataset[0]  # fast again\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 8743}, page_content='```\\n\\n\\nIn this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approximate shuffling method [`IterableDataset.shuffle`].\\nIt only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal.\\nYou can also reshuffle the dataset easily:\\n\\n```python\\nfor example in enumerate(my_iterable_dataset):  # fast\\n    pass\\n\\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\\n\\nfor example in enumerate(shuffled_iterable_dataset):  # as fast as before\\n    pass\\n\\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=1337, buffer_size=100)  # reshuffling using another seed is instantaneous\\n\\nfor example in enumerate(shuffled_iterable_dataset):  # still as fast as before\\n    pass\\n```\\n\\nIf you\\'re using your dataset on multiple epochs, the effective seed to shuffle the shards order in the shuffle buffer is `seed + epoch`.\\nIt makes it easy to reshuffle a dataset between epochs:\\n```python\\nfor epoch in range(n_epochs):\\n    my_iterable_dataset.set_epoch(epoch)\\n    for example in my_iterable_dataset:  # fast + reshuffled at each epoch using `effective_seed = seed + epoch`\\n        pass\\n```\\n\\n## Switch from map-style to iterable\\n\\nIf you want to benefit from the \"lazy\" behavior of an [`IterableDataset`] or their speed advantages, you can switch your map-style [`Dataset`] to an [`IterableDataset`]:\\n```python\\nmy_iterable_dataset = my_dataset.to_iterable_dataset()'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 10212}, page_content='```\\n\\nIf you want to shuffle your dataset or [use it with a PyTorch DataLoader](./use_with_pytorch#stream-data), we recommend generating a sharded [`IterableDataset`]:\\n```python\\nmy_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=1024)\\nmy_iterable_dataset.n_shards  # 1024\\n```'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/q-learning-recap.mdx', 'start_index': 1}, page_content='Q-Learning Recap [[q-learning-recap]]\\n\\n\\n*Q-Learning* **is the RL algorithm that** :\\n\\n- Trains a *Q-function*, an **action-value function** encoded, in internal memory, by a *Q-table* **containing all the state-action pair values.**\\n\\n- Given a state and action, our Q-function **will search its Q-table for the corresponding value.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\\n\\n- When the training is done, **we have an optimal Q-function, or, equivalently, an optimal Q-table.**\\n\\n- And if we **have an optimal Q-function**, we\\nhave an optimal policy, since we **know, for each state, the best action to take.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>\\n\\nBut, in the beginning,\\xa0our **Q-table is useless since it gives arbitrary values for each state-action pair\\xa0(most of the time we initialize the Q-table to 0 values)**. But, as we\\xa0explore the environment and update our Q-table it will give us a better and better approximation.\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\\n\\nThis is the Q-Learning pseudocode:'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/q-learning-recap.mdx', 'start_index': 1343}, page_content='This is the Q-Learning pseudocode:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Zero-shot object detection\\n\\n[[open-in-colab]]\\n\\nTraditionally, models used for [object detection](object_detection) require labeled image datasets for training,\\nand are limited to detecting the set of classes from the training data.\\n\\nZero-shot object detection is supported by the [OWL-ViT](../model_doc/owlvit) model which uses a different approach. OWL-ViT\\nis an open-vocabulary object detector. It means that it can detect objects in images based on free-text queries without\\nthe need to fine-tune the model on labeled datasets.\\n\\nOWL-ViT leverages multi-modal representations to perform open-vocabulary detection. It combines [CLIP](../model_doc/clip) with\\nlightweight object classification and localization heads. Open-vocabulary detection is achieved by embedding free-text queries with the text encoder of CLIP and using them as input to the object classification and localization heads.\\nassociate images and their corresponding textual descriptions, and ViT processes image patches as inputs. The authors\\nof OWL-ViT first trained CLIP from scratch and then fine-tuned OWL-ViT end to end on standard object detection datasets using\\na bipartite matching loss.\\n\\nWith this approach, the model can detect objects based on textual descriptions without prior training on labeled datasets.\\n\\nIn this guide, you will learn how to use OWL-ViT:\\n- to detect objects based on text prompts\\n- for batch object detection\\n- for image-guided object detection\\n\\nBefore you begin, make sure you have all the necessary libraries installed:\\n\\n```bash\\npip install -q transformers'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 2313}, page_content='```\\n\\n## Zero-shot object detection pipeline\\n\\nThe simplest way to try out inference with OWL-ViT is to use it in a [`pipeline`]. Instantiate a pipeline\\nfor zero-shot object detection from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit):\\n\\n```python\\n>>> from transformers import pipeline\\n\\n>>> checkpoint = \"google/owlvit-base-patch32\"\\n>>> detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\\n```\\n\\nNext, choose an image you\\'d like to detect objects in. Here we\\'ll use the image of astronaut Eileen Collins that is\\na part of the [NASA](https://www.nasa.gov/multimedia/imagegallery/index.html) Great Images dataset.\\n\\n```py\\n>>> import skimage\\n>>> import numpy as np\\n>>> from PIL import Image\\n\\n>>> image = skimage.data.astronaut()\\n>>> image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\\n\\n>>> image'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 3162}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\" alt=\"Astronaut Eileen Collins\"/>\\n</div>\\n\\nPass the image and the candidate object labels to look for to the pipeline.\\nHere we pass the image directly; other suitable options include a local path to an image or an image url. We also pass text descriptions for all items we want to query the image for.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 3644}, page_content='```py\\n>>> predictions = detector(\\n...     image,\\n...     candidate_labels=[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\\n... )\\n>>> predictions\\n[{\\'score\\': 0.3571370542049408,\\n  \\'label\\': \\'human face\\',\\n  \\'box\\': {\\'xmin\\': 180, \\'ymin\\': 71, \\'xmax\\': 271, \\'ymax\\': 178}},\\n {\\'score\\': 0.28099656105041504,\\n  \\'label\\': \\'nasa badge\\',\\n  \\'box\\': {\\'xmin\\': 129, \\'ymin\\': 348, \\'xmax\\': 206, \\'ymax\\': 427}},\\n {\\'score\\': 0.2110239565372467,\\n  \\'label\\': \\'rocket\\',\\n  \\'box\\': {\\'xmin\\': 350, \\'ymin\\': -1, \\'xmax\\': 468, \\'ymax\\': 288}},\\n {\\'score\\': 0.13790413737297058,\\n  \\'label\\': \\'star-spangled banner\\',\\n  \\'box\\': {\\'xmin\\': 1, \\'ymin\\': 1, \\'xmax\\': 105, \\'ymax\\': 509}},\\n {\\'score\\': 0.11950037628412247,\\n  \\'label\\': \\'nasa badge\\',\\n  \\'box\\': {\\'xmin\\': 277, \\'ymin\\': 338, \\'xmax\\': 327, \\'ymax\\': 380}},\\n {\\'score\\': 0.10649408400058746,\\n  \\'label\\': \\'rocket\\',\\n  \\'box\\': {\\'xmin\\': 358, \\'ymin\\': 64, \\'xmax\\': 424, \\'ymax\\': 280}}]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 4523}, page_content='```\\n\\nLet\\'s visualize the predictions:\\n\\n```py\\n>>> from PIL import ImageDraw\\n\\n>>> draw = ImageDraw.Draw(image)\\n\\n>>> for prediction in predictions:\\n...     box = prediction[\"box\"]\\n...     label = prediction[\"label\"]\\n...     score = prediction[\"score\"]\\n\\n...     xmin, ymin, xmax, ymax = box.values()\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\\n\\n>>> image\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_2.png\" alt=\"Visualized predictions on NASA image\"/>\\n</div>\\n\\n## Text-prompted zero-shot object detection by hand\\n\\nNow that you\\'ve seen how to use the zero-shot object detection pipeline, let\\'s replicate the same\\nresult manually.\\n\\nStart by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit).\\nHere we\\'ll use the same checkpoint as before:\\n\\n```py\\n>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\\n\\n>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\\n>>> processor = AutoProcessor.from_pretrained(checkpoint)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 5790}, page_content='```\\n\\nLet\\'s take a different image to switch things up.\\n\\n```py\\n>>> import requests\\n\\n>>> url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\\n>>> im = Image.open(requests.get(url, stream=True).raw)\\n>>> im\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/>\\n</div>\\n\\nUse the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\\nimage for the model by resizing and normalizing it, and a [`CLIPTokenizer`] that takes care of the text inputs.\\n\\n```py\\n>>> text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\\n>>> inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 6658}, page_content='```\\n\\nPass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\\nfeeding them to the model, you need to use the [`~OwlViTImageProcessor.post_process_object_detection`] method to make sure the predicted bounding\\nboxes have the correct coordinates relative to the original image:\\n\\n```py\\n>>> import torch\\n\\n>>> with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = torch.tensor([im.size[::-1]])\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\\n\\n>>> draw = ImageDraw.Draw(im)\\n\\n>>> scores = results[\"scores\"].tolist()\\n>>> labels = results[\"labels\"].tolist()\\n>>> boxes = results[\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{text_queries[label]}: {round(score,2)}\", fill=\"white\")\\n\\n>>> im'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 7666}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\\n</div>\\n\\n## Batch processing\\n\\nYou can pass multiple sets of images and text queries to search for different (or same) objects in several images.\\nLet\\'s use both an astronaut image and the beach image together.\\nFor batch processing, you should pass text queries as a nested list to the processor and images as lists of PIL images,\\nPyTorch tensors, or NumPy arrays.\\n\\n```py\\n>>> images = [image, im]\\n>>> text_queries = [\\n...     [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\\n...     [\"hat\", \"book\", \"sunglasses\", \"camera\"],\\n... ]\\n>>> inputs = processor(text=text_queries, images=images, return_tensors=\"pt\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 8508}, page_content='```\\n\\nPreviously for post-processing you passed the single image\\'s size as a tensor, but you can also pass a tuple, or, in case\\nof several images, a list of tuples. Let\\'s create predictions for the two examples, and visualize the second one (`image_idx = 1`).\\n\\n```py\\n>>> with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = [x.size[::-1] for x in images]\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\\n\\n>>> image_idx = 1\\n>>> draw = ImageDraw.Draw(images[image_idx])\\n\\n>>> scores = results[image_idx][\"scores\"].tolist()\\n>>> labels = results[image_idx][\"labels\"].tolist()\\n>>> boxes = results[image_idx][\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{text_queries[image_idx][label]}: {round(score,2)}\", fill=\"white\")\\n\\n>>> images[image_idx]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 9505}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\\n</div>\\n\\n## Image-guided object detection\\n\\nIn addition to zero-shot object detection with text queries, OWL-ViT offers image-guided object detection. This means\\nyou can use an image query to find similar objects in the target image.\\nUnlike text queries, only a single example image is allowed.\\n\\nLet\\'s take an image with two cats on a couch as a target image, and an image of a single cat\\nas a query:\\n\\n```py\\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\n>>> image_target = Image.open(requests.get(url, stream=True).raw)\\n\\n>>> query_url = \"http://images.cocodataset.org/val2017/000000524280.jpg\"\\n>>> query_image = Image.open(requests.get(query_url, stream=True).raw)\\n```\\n\\nLet\\'s take a quick look at the images:\\n\\n```py\\n>>> import matplotlib.pyplot as plt\\n\\n>>> fig, ax = plt.subplots(1, 2)\\n>>> ax[0].imshow(image_target)\\n>>> ax[1].imshow(query_image)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 10593}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_5.png\" alt=\"Cats\"/>\\n</div>\\n\\nIn the preprocessing step, instead of text queries, you now need to use `query_images`:\\n\\n```py\\n>>> inputs = processor(images=image_target, query_images=query_image, return_tensors=\"pt\")\\n```\\n\\nFor predictions, instead of passing the inputs to the model, pass them to [`~OwlViTForObjectDetection.image_guided_detection`]. Draw the predictions\\nas before except now there are no labels.\\n\\n```py\\n>>> with torch.no_grad():\\n...     outputs = model.image_guided_detection(**inputs)\\n...     target_sizes = torch.tensor([image_target.size[::-1]])\\n...     results = processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]\\n\\n>>> draw = ImageDraw.Draw(image_target)\\n\\n>>> scores = results[\"scores\"].tolist()\\n>>> boxes = results[\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"white\", width=4)\\n\\n>>> image_target'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 11748}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_6.png\" alt=\"Cats with bounding boxes\"/>\\n</div>\\n\\nIf you\\'d like to interactively try out inference with OWL-ViT, check out this demo:\\n\\n<iframe\\n\\tsrc=\"https://adirik-owl-vit.hf.space\"\\n\\tframeborder=\"0\"\\n\\twidth=\"850\"\\n\\theight=\"450\"\\n></iframe>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 1}, page_content='Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.\\n\\n\\n### Q1: Which of the following interpretations of bias-variance tradeoff is the most accurate in the field of Reinforcement Learning?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"The bias-variance tradeoff reflects how my model is able to generalize the knowledge to previously tagged data we give to the model during training time.\",\\n\\t\\t\\texplain: \"This is the traditional bias-variance tradeoff in Machine Learning. In our specific case of Reinforcement Learning, we don\\'t have previously tagged data, but only a reward signal.\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n   \\t\\t{\\n\\t\\t\\ttext: \"The bias-variance tradeoff reflects how well the reinforcement signal reflects the true reward the agent should get from the enviromment\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\t\\t\\n\\t]}\\n/>\\n\\n### Q2: Which of the following statements are true, when talking about models with bias and/or variance in RL?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"An unbiased reward signal returns rewards similar to the real / expected ones from the environment\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"A biased reward signal returns rewards similar to the real / expected ones from the environment\",\\n\\t\\t\\texplain: \"If a reward signal is biased, it means the reward signal we get differs from the real reward we should be getting from an environment\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"A reward signal with high variance has much noise in it and gets affected by, for example, stochastic (non constant) elements in the environment\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\t\\t\\n    \\t\\t{\\n\\t\\t\\ttext: \"A reward signal with low variance has much noise in it and gets affected by, for example, stochastic (non constant) elements in the environment\",\\n\\t\\t\\texplain: \"If a reward signal has low variance, then it\\'s less affected by the noise of the environment and produce similar values regardless the random elements in the environment\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n\\t]}\\n/>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 2211}, page_content='### Q3: Which of the following statements are true about Monte Carlo method?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"It\\'s a sampling mechanism, which means we don\\'t analyze all the possible states, but a sample of those\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"It\\'s very resistant to stochasticity (random elements in the trajectory)\",\\n\\t\\t\\texplain: \"Monte Carlo randomly estimates everytime a sample of trajectories. However, even same trajectories can have different reward values if they contain stochastic elements\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"To reduce the impact of stochastic elements in Monte Carlo, we take `n` strategies and average them, reducing their individual impact\",\\n\\t\\t\\texplain: \"\",\\n\\t\\t\\tcorrect: true,\\n\\t\\t},\\t\\t    \\n\\t]}\\n/>\\n\\n### Q4: How would you describe, with your own words, the Actor-Critic Method (A2C)?\\n\\n<details>\\n<summary>Solution</summary>\\n\\nThe idea behind Actor-Critic is that we learn two function approximations:\\n1. A `policy` that controls how our agent acts (π)\\n2. A `value` function to assist the policy update by measuring how good the action taken is (q)\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/step2.jpg\" alt=\"Actor-Critic, step 2\"/>\\n\\n</details>\\n\\n### Q5: Which of the following statements are true about the Actor-Critic Method?\\n\\n<Question\\n\\tchoices={[\\n   \\t\\t {\\n\\t\\t\\ttext: \"The Critic does not learn any function during the training process\",\\n\\t\\t\\texplain: \"Both the Actor and the Critic function parameters are updated during training time\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"The Actor learns a policy function, while the Critic learns a value function\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"It adds resistance to stochasticity and reduces high variance\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\t    \\n\\t]}\\n/>\\n\\n\\n\\n### Q6: What is `Advantage` in the A2C method?\\n\\n<details>\\n<summary>Solution</summary>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': -1}, page_content='### Q6: What is `Advantage` in the A2C method?\\n\\n<details>\\n<summary>Solution</summary>\\n\\nInstead of using directly the Action-Value function of the Critic as it is, we could use an `Advantage` function. The idea behind an `Advantage` function is that we calculate the relative advantage of an action compared to the others possible at a state, averaging them.\\n\\nIn other words: how taking that action at a state is better compared to the average value of the state\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/advantage1.jpg\" alt=\"Advantage in A2C\"/>\\n\\n</details>\\n\\nCongrats on finishing this Quiz 🥳, if you missed some elements, take time to read the chapter again to reinforce (😏) your knowledge.'),\n",
       " Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx', 'start_index': 1}, page_content='Access and read Logs\\n\\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your Endpoint. \\n\\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\\n\\nThe Container Logs are only available when your Endpoint is in the “Running” state. \\n\\n_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\\n\\n**Build Logs:**\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\\n\\n**Container Logs:**\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/examples_component/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: examples_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\nimport os\\nos.mkdir(\\'images\\')\\n!wget -q -O images/cheetah1.jpg https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/cheetah1.jpg\\n!wget -q -O images/lion.jpg https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/lion.jpg\\n!wget -q -O images/lion.webp https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/lion.webp\\n!wget -q -O images/logo.png https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/logo.png\\n```\\n\\n\\n```\\nimport gradio as gr\\nimport os\\n\\n\\ndef flip(i):\\n    return i.rotate(180)\\n\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        with gr.Column():\\n            img_i = gr.Image(label=\"Input Image\", type=\"pil\")\\n        with gr.Column():\\n            img_o = gr.Image(label=\"Output Image\")\\n    with gr.Row():\\n        btn = gr.Button(value=\"Flip Image\")\\n    btn.click(flip, inputs=[img_i], outputs=[img_o])\\n\\n    gr.Examples(\\n        [\\n            os.path.join(os.path.abspath(\\'\\'), \"images/cheetah1.jpg\"),\\n            os.path.join(os.path.abspath(\\'\\'), \"images/lion.jpg\"),\\n        ],\\n        img_i,\\n        img_o,\\n        flip,\\n    )\\n\\ndemo.launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/additional-readings.mdx', 'start_index': 1}, page_content='Additional Readings\\n\\nThese are **optional readings** if you want to go deeper.\\n\\n\\n## Introduction to Policy Optimization\\n\\n- [Part 3: Intro to Policy Optimization - Spinning Up documentation](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)\\n\\n\\n## Policy Gradient\\n\\n- [https://johnwlambert.github.io/policy-gradients/](https://johnwlambert.github.io/policy-gradients/)\\n- [RL - Policy Gradient Explained](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146)\\n- [Chapter 13, Policy Gradient Methods;  Reinforcement Learning, an introduction by Richard Sutton and Andrew G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)\\n\\n## Implementation\\n\\n- [PyTorch Reinforce implementation](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\\n- [Implementations from DDPG to PPO](https://github.com/MrSyee/pg-is-all-you-need)'),\n",
       " Document(metadata={'source': 'huggingface/optimum/blob/main/docs/source/onnxruntime/package_reference/quantization.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Quantization\\n\\n## ORTQuantizer\\n\\n[[autodoc]] onnxruntime.quantization.ORTQuantizer\\n    - all'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/number_component/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: number_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Blocks() as demo:\\n    gr.Number()\\n\\ndemo.launch()\\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: map_airbnb\\n### Display an interactive map of AirBnB locations with Plotly. Data is hosted on HuggingFace Datasets. \\n        \\n\\n\\n```\\n!pip install -q gradio plotly\\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 181}, page_content='```\\nimport gradio as gr\\nimport plotly.graph_objects as go\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\\ndf = dataset.to_pandas()\\n\\ndef filter_map(min_price, max_price, boroughs):\\n\\n    filtered_df = df[(df[\\'neighbourhood_group\\'].isin(boroughs)) & \\n          (df[\\'price\\'] > min_price) & (df[\\'price\\'] < max_price)]\\n    names = filtered_df[\"name\"].tolist()\\n    prices = filtered_df[\"price\"].tolist()\\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\\n    fig = go.Figure(go.Scattermapbox(\\n            customdata=text_list,\\n            lat=filtered_df[\\'latitude\\'].tolist(),\\n            lon=filtered_df[\\'longitude\\'].tolist(),\\n            mode=\\'markers\\',\\n            marker=go.scattermapbox.Marker(\\n                size=6\\n            ),\\n            hoverinfo=\"text\",\\n            hovertemplate=\\'<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}\\'\\n        ))\\n\\n    fig.update_layout(\\n        mapbox_style=\"open-street-map\",\\n        hovermode=\\'closest\\',\\n        mapbox=dict(\\n            bearing=0,\\n            center=go.layout.mapbox.Center(\\n                lat=40.67,\\n                lon=-73.90\\n            ),\\n            pitch=0,\\n            zoom=9\\n        ),\\n    )\\n\\n    return fig'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 1433}, page_content='return fig\\n\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\\n        btn = gr.Button(value=\"Update Filter\")\\n        map = gr.Plot()\\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\\n\\nif __name__ == \"__main__\":\\n    demo.launch()'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 1}, page_content='Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2Net Blocks](https://paperswithcode.com/method/res2net-block). The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model(\\'res2net101_26w_4s\\', pretrained=True)\\n>>> model.eval()\\n```\\n\\nTo load and preprocess the image:\\n\\n```py \\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 2272}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('res2net101_26w_4s', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\\n```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@article{Gao_2021,\\n   title={Res2Net: A New Multi-Scale Backbone Architecture},\\n   volume={43},\\n   ISSN={1939-3539},\\n   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},\\n   DOI={10.1109/tpami.2019.2938758},\\n   number={2},\\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\\n   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\\n   year={2021},\\n   month={Feb},\\n   pages={652–662}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 3726}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: Res2Net\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale Backbone Architecture'\\n    URL: https://paperswithcode.com/paper/res2net-a-new-multi-scale-backbone\\nModels:\\n- Name: res2net101_26w_4s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 10415881200\\n    Parameters: 45210000\\n    File Size: 181456059\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net101_26w_4s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L152\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net101_26w_4s-02a759a1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.19%\\n      Top 5 Accuracy: 94.43%\\n- Name: res2net50_14w_8s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 5403546768\\n    Parameters: 25060000\\n    File Size: 100638543\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_14w_8s\\n    LR: 0.1\\n    Epochs: 100\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': -1}, page_content=\"- Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_14w_8s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L196\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_14w_8s-6527dddc.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.14%\\n      Top 5 Accuracy: 93.86%\\n- Name: res2net50_26w_4s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 5499974064\\n    Parameters: 25700000\\n    File Size: 103110087\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_26w_4s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L141\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 6725}, page_content=\"Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_4s-06e79181.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.99%\\n      Top 5 Accuracy: 93.85%\\n- Name: res2net50_26w_6s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 8130156528\\n    Parameters: 37050000\\n    File Size: 148603239\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_26w_6s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L163\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_6s-19041792.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.57%\\n      Top 5 Accuracy: 94.12%\\n- Name: res2net50_26w_8s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 10760338992\\n    Parameters: 48400000\\n    File Size: 194085165\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': -1}, page_content=\"- Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_26w_8s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L174\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_8s-2c7c9f12.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.19%\\n      Top 5 Accuracy: 94.37%\\n- Name: res2net50_48w_2s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 5375291520\\n    Parameters: 25290000\\n    File Size: 101421406\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_48w_2s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L185\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 9768}, page_content='Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_48w_2s-afed724a.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.53%\\n      Top 5 Accuracy: 93.56%\\n-->'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': 0}, page_content='ow to slice and dice a dataset. Most of the time, the data you work with won’t be perfectly prepared for training models. In this video we’ll explore various features that Datasets provides to clean up your datasets. The Datasets library provides several built-in methods that allow you to wrangle your data. In this video we\\'ll see how you can shuffle and split your data, select the rows you\\'re interested in, tweak the columns, and apply processing functions with the map() method. Let\\'s start with shuffling. It is generally a good idea to apply shuffling to the training set so that your model doesn\\'t learn any artificial ordering in the data. If you want to shuffle the whole dataset, you can apply the appropriately named shuffle() method to your dataset. You can see an example of this method in action here, where we\\'ve downloaded the training split of the SQUAD dataset and shuffled all the rows randomly.Another way to shuffle the data is to create random train and test splits. This can be useful if you have to create your own test splits from raw data. To do this, you just apply the train_test_split method and specify how large the test split should be. In this example, we\\'ve specified that the test set should be 10% of the total dataset size. You can see that the output of train_test_split is a DatasetDict object, whose keys correspond to the new splits. Now that we know how to shuffle a dataset, let\\'s take a look at returning the rows we\\'re interested in. The most common way to do this is with the select method. This method expects a list or generator of the dataset\\'s indices, and will then return a new Dataset object containing just those rows. If you want to create a random sample of rows, you can do this by chaining the shuffle and select methods together. In this example, we\\'ve created a sample of 5 elements from the SQuAD dataset. The last way to pick out specific rows in a dataset is by applying the filter method. This method checks whether each rows fulfills some condition or not. For example, here we\\'ve created a small lambda function that checks whether the title starts with the letter \"L\". Once we apply this function with the filter method, we get a subset of the data consisting of just these titles. So far we\\'ve been talking'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': -1}, page_content='we\\'ve created a small lambda function that checks whether the title starts with the letter \"L\". Once we apply this function with the filter method, we get a subset of the data consisting of just these titles. So far we\\'ve been talking about the rows of a dataset, but what about the columns? The Datasets library has two main methods for transforming columns: a rename_column method to change the name of a column, and a remove_columns method to delete them. You can see examples of both these method here. Some datasets have nested columns and you can expand these by applying the flatten method. For example in the SQUAD dataset, the answers column contains a text and answer_start field. If we want to promote them to their own separate columns, we can apply flatten as shown here. Of course, no discussion of the Datasets library would be complete without mentioning the famous map method. This method applies a custom processing function to each row in the dataset. For example,here we first define a lowercase_title function that simply lowercases the text in the title column and then we feed that to the map method and voila! we now have lowercase titles. The map method can also be used to feed batches of rows to the processing function. This is especially useful for tokenization, where the tokenizers are backed by the Tokenizers library can use fast multithreading to process batches in parallel.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: question-answering\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\n\\nmodel_name = \"deepset/roberta-base-squad2\"\\n\\nnlp = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\\n\\ncontext = \"The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\"\\nquestion = \"Which continent is the Amazon rainforest in?\"\\n\\n\\ndef predict(context, question):\\n    res = nlp({\"question\": question, \"context\": context})\\n    return res[\"answer\"], res[\"score\"]\\n\\n\\ngr.Interface(\\n    predict,\\n    inputs=[\\n        gr.Textbox(lines=7, value=context, label=\"Context Paragraph\"),\\n        gr.Textbox(lines=2, value=question, label=\"Question\"),\\n    ],\\n    outputs=[gr.Textbox(label=\"Answer\"), gr.Textbox(label=\"Score\")],\\n).launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/loaders/ip_adapter.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# IP-Adapter\\n\\n[IP-Adapter](https://hf.co/papers/2308.06721) is a lightweight adapter that enables prompting a diffusion model with an image. This method decouples the cross-attention layers of the image and text features. The image features are generated from an image encoder. Files generated from IP-Adapter are only ~100MBs.\\n\\n<Tip>\\n\\nLearn how to load an IP-Adapter checkpoint and image in the [IP-Adapter](../../using-diffusers/loading_adapters#ip-adapter) loading guide.\\n\\n</Tip>\\n\\n## IPAdapterMixin\\n\\n[[autodoc]] loaders.ip_adapter.IPAdapterMixin'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/source/package_reference/config.md', 'start_index': 0}, page_content='!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Configuration\\n\\n[`PeftConfigMixin`] is the base configuration class for storing the adapter configuration of a [`PeftModel`], and [`PromptLearningConfig`] is the base configuration class for soft prompt methods (p-tuning, prefix tuning, and prompt tuning). These base classes contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads.\\n\\n## PeftConfigMixin\\n\\n[[autodoc]] config.PeftConfigMixin\\n    - all\\n\\n## PeftConfig\\n\\n[[autodoc]] PeftConfig\\n    - all\\n\\n## PromptLearningConfig\\n\\n[[autodoc]] PromptLearningConfig\\n    - all'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n<p align=\"center\">\\n  <picture>\\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\\n  </picture>\\n  <br/>\\n  <br/>\\n</p>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 1188}, page_content='<p align=\"center\">\\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\">\\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://huggingface.co/docs/transformers/index\">\\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/releases\">\\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">\\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\\n    </a>\\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\\n</p>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 2365}, page_content='<h4 align=\"center\">\\n    <p>\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README.md\">English</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hans.md\">简体中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">繁體中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">한국어</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_es.md\">Español</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">日本語</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">हिन्दी</a> |\\n        <b>Русский</b>\\n        <a href=\"https://github.com/huggingface/transformers//blob/main/README_te.md\">తెలుగు</a> |\\n    <p>\\n</h4>\\n\\n<h3 align=\"center\">\\n    <p>Современное машинное обучение для JAX, PyTorch и TensorFlow</p>\\n</h3>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 3313}, page_content='<h3 align=\"center\">\\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\\n</h3>\\n\\n🤗 Transformers предоставляет тысячи предварительно обученных моделей для выполнения различных задач, таких как текст, зрение и аудио.\\n\\nЭти модели могут быть применены к:\\n\\n* 📝 Тексту для таких задач, как классификация текстов, извлечение информации, ответы на вопросы, обобщение, перевод, генерация текстов на более чем 100 языках.\\n* 🖼️ Изображениям для задач классификации изображений, обнаружения объектов и сегментации.\\n* 🗣️ Аудио для задач распознавания речи и классификации аудио.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 3973}, page_content='Модели transformers также могут выполнять несколько задач, такие как ответы на табличные вопросы, распознавание оптических символов, извлечение информации из отсканированных документов, классификация видео и ответы на визуальные вопросы.\\n\\n🤗 Transformers предоставляет API для быстрой загрузки и использования предварительно обученных моделей, их тонкой настройки на собственных датасетах и последующего взаимодействия ими с сообществом на нашем [сайте](https://huggingface.co/models). В то же время каждый python модуль, определяющий архитектуру, полностью автономен и может быть модифицирован для проведения быстрых исследовательских экспериментов.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 4624}, page_content='🤗 Transformers опирается на три самые популярные библиотеки глубокого обучения - [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) и [TensorFlow](https://www.tensorflow.org/) - и легко интегрируется между ними. Это позволяет легко обучать модели с помощью одной из них, а затем загружать их для выводов с помощью другой.\\n\\n## Онлайн демонстрация\\n\\nБольшинство наших моделей можно протестировать непосредственно на их страницах с [сайта](https://huggingface.co/models). Мы также предлагаем [привтаный хостинг моделей, контроль версий и API для выводов](https://huggingface.co/pricing) для публичных и частных моделей.\\n\\nВот несколько примеров:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 5293}, page_content='В области NLP ( Обработка текстов на естественном языке ):\\n- [Маскированное заполнение слов с помощью BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\\n- [Распознавание сущностей с помощью Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\\n- [Генерация текста с помощью GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)\\n- [Выводы на естественном языке с помощью RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 5891}, page_content='- [Обобщение с помощью BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 6744}, page_content='- [Ответы на вопросы с помощью'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 6775}, page_content='DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+Sou'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 7236}, page_content='+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyan'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 7697}, page_content='amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 8102}, page_content='- [Перевод с помощью T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 8206}, page_content='В области компьютерного зрения:\\n- [Классификация изображений с помощью ViT](https://huggingface.co/google/vit-base-patch16-224)\\n- [Обнаружение объектов с помощью DETR](https://huggingface.co/facebook/detr-resnet-50)\\n- [Семантическая сегментация с помощью SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\\n- [Сегментация паноптикума с помощью MaskFormer](https://huggingface.co/facebook/maskformer-swin-small-coco)\\n- [Оценка глубины с помощью DPT](https://huggingface.co/docs/transformers/model_doc/dpt)\\n- [Классификация видео с помощью VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)\\n- [Универсальная сегментация с помощью OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 8953}, page_content='В области звука:\\n- [Автоматическое распознавание речи с помощью Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)\\n- [Поиск ключевых слов с помощью Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\\n- [Классификация аудиоданных с помощью траснформера аудиоспектрограмм](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 9313}, page_content='В мультимодальных задачах:\\n- [Ответы на вопросы по таблице с помощью TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)\\n- [Визуальные ответы на вопросы с помощью ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)\\n- [Zero-shot классификация изображений с помощью CLIP](https://huggingface.co/openai/clip-vit-large-patch14)\\n- [Ответы на вопросы по документам с помощью LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)\\n- [Zero-shot классификация видео с помощью X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)\\n\\n\\n## 100 проектов, использующих Transformers'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 9876}, page_content='## 100 проектов, использующих Transformers\\n\\nTransformers - это не просто набор инструментов для использования предварительно обученных моделей: это сообщество проектов, созданное на его основе, и\\nHugging Face Hub. Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, инженерам и всем желающим\\nсоздавать проекты своей мечты.\\n\\nЧтобы отпраздновать 100 тысяч звезд Transformers, мы решили сделать акцент на сообществе, и создали страницу [awesome-transformers](./awesome-transformers.md), на которой перечислены 100\\nневероятных проектов, созданных с помощью transformers.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 10485}, page_content='Если вы являетесь владельцем или пользователем проекта, который, по вашему мнению, должен быть включен в этот список, пожалуйста, откройте PR для его добавления!\\n\\n## Если вы хотите получить индивидуальную поддержку от команды Hugging Face\\n\\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\\n</a><br>\\n\\n## Быстрый гайд'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 11051}, page_content=\"## Быстрый гайд\\n\\nДля использования модели на заданном входе (текст, изображение, звук, ...) мы предоставляем API `pipeline`. Конвейеры объединяют предварительно обученную модель с препроцессингом, который использовался при ее обучении. Вот как можно быстро использовать конвейер для классификации положительных и отрицательных текстов:\\n\\n```python\\n>>> from transformers import pipeline\\n\\n# Выделение конвейера для анализа настроений\\n>>> classifier = pipeline('sentiment-analysis')\\n>>> classifier('Мы очень рады представить конвейер в transformers.')\\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 11652}, page_content='```\\n\\nВторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейером, а третья оценивает ее на заданном тексте. Здесь ответ \"POSITIVE\" с уверенностью 99,97%.\\n\\nВо многих задачах, как в НЛП, так и в компьютерном зрении и речи, уже есть готовый `pipeline`. Например, мы можем легко извлечь обнаруженные объекты на изображении:\\n\\n``` python\\n>>> import requests\\n>>> from PIL import Image\\n>>> from transformers import pipeline\\n\\n# Скачиваем изображение с милыми котиками\\n>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\\n>>> image_data = requests.get(url, stream=True).raw\\n>>> image = Image.open(image_data)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 12342}, page_content=\"# Выделение конвейера для обнаружения объектов\\n>>> object_detector = pipeline('object-detection')\\n>>> object_detector(image)\\n[{'score': 0.9982201457023621,\\n  'label': 'remote',\\n  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\\n {'score': 0.9960021376609802,\\n  'label': 'remote',\\n  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\\n {'score': 0.9954745173454285,\\n  'label': 'couch',\\n  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\\n {'score': 0.9988006353378296,\\n  'label': 'cat',\\n  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\\n {'score': 0.9986783862113953,\\n  'label': 'cat',\\n  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 13030}, page_content='```\\n\\nЗдесь мы получаем список объектов, обнаруженных на изображении, с рамкой вокруг объекта и оценкой достоверности. Слева - исходное изображение, справа прогнозы:\\n\\n<h3 align=\"center\">\\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\" width=\"400\"></a>\\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png\" width=\"400\"></a>\\n</h3>\\n\\nПодробнее о задачах, поддерживаемых API `pipeline`, можно узнать в [этом учебном пособии](https://huggingface.co/docs/transformers/task_sum)\\n\\nВ дополнение к `pipeline`, для загрузки и использования любой из предварительно обученных моделей в заданной задаче достаточно трех строк кода. Вот версия для PyTorch:\\n```python\\n>>> from transformers import AutoTokenizer, AutoModel'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 13871}, page_content='>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = AutoModel.from_pretrained(\"bert-base-uncased\")\\n\\n>>> inputs = tokenizer(\"Привет мир!\", return_tensors=\"pt\")\\n>>> outputs = model(**inputs)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 14087}, page_content='```\\n\\nА вот эквивалентный код для TensorFlow:\\n```python\\n>>> from transformers import AutoTokenizer, TFAutoModel\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\\n\\n>>> inputs = tokenizer(\"Привет мир!\", return_tensors=\"tf\")\\n>>> outputs = model(**inputs)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 14417}, page_content='```\\n\\nТокенизатор отвечает за всю предварительную обработку, которую ожидает предварительно обученная модель, и может быть вызван непосредственно с помощью одной строки (как в приведенных выше примерах) или на списке. В результате будет получен словарь, который можно использовать в последующем коде или просто напрямую передать в модель с помощью оператора распаковки аргументов **.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 14801}, page_content='Сама модель представляет собой обычный [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) или [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (в зависимости от используемого бэкенда), который можно использовать как обычно. [В этом руководстве](https://huggingface.co/docs/transformers/training) рассказывается, как интегрировать такую модель в классический цикл обучения PyTorch или TensorFlow, или как использовать наш API `Trainer` для быстрой тонкой настройки на новом датасете.\\n\\n## Почему необходимо использовать transformers?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 15353}, page_content='## Почему необходимо использовать transformers?\\n\\n1. Простые в использовании современные модели:\\n    - Высокая производительность в задачах понимания и генерации естественного языка, компьютерного зрения и аудио.\\n    - Низкий входной барьер для преподавателей и практиков.\\n    - Небольшое количество абстракций для пользователя и всего три класса для изучения.\\n    - Единый API для использования всех наших предварительно обученных моделей.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 15794}, page_content='1. Более низкие вычислительные затраты, меньший \"углеродный след\":\\n    - Исследователи могут обмениваться обученными моделями вместо того, чтобы постоянно их переобучать.\\n    - Практики могут сократить время вычислений и производственные затраты.\\n    - Десятки архитектур с более чем 60 000 предварительно обученных моделей для всех модальностей.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 16142}, page_content='1. Выбор подходящего фреймворка для каждого этапа жизни модели:\\n    - Обучение самых современных моделей за 3 строки кода.\\n    - Перемещайте одну модель между фреймворками TF2.0/PyTorch/JAX по своему усмотрению.\\n    - Беспрепятственный выбор подходящего фреймворка для обучения, оценки и производства.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 16445}, page_content='1. Легко настроить модель или пример под свои нужды:\\n    - Мы предоставляем примеры для каждой архитектуры, чтобы воспроизвести результаты, опубликованные их авторами.\\n    - Внутренние компоненты модели раскрываются максимально последовательно.\\n    - Файлы моделей можно использовать независимо от библиотеки для проведения быстрых экспериментов.\\n\\n## Почему я не должен использовать transformers?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 16843}, page_content='- Данная библиотека не является модульным набором строительных блоков для нейронных сетей. Код в файлах моделей специально не рефакторится дополнительными абстракциями, чтобы исследователи могли быстро итеративно работать с каждой из моделей, не погружаясь в дополнительные абстракции/файлы.\\n- API обучения не предназначен для работы с любой моделью, а оптимизирован для работы с моделями, предоставляемыми библиотекой. Для работы с общими циклами машинного обучения следует использовать другую библиотеку (возможно, [Accelerate](https://huggingface.co/docs/accelerate)).'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 17415}, page_content='- Несмотря на то, что мы стремимся представить как можно больше примеров использования, скрипты в нашей папке [примеров](https://github.com/huggingface/transformers/tree/main/examples) являются именно примерами. Предполагается, что они не будут работать \"из коробки\" для решения вашей конкретной задачи, и вам придется изменить несколько строк кода, чтобы адаптировать их под свои нужды.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 17804}, page_content='## Установка\\n\\n### С помощью pip\\n\\nДанный репозиторий протестирован на Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ и TensorFlow 2.6+.\\n\\nУстанавливать 🤗 Transformers следует в [виртуальной среде](https://docs.python.org/3/library/venv.html). Если вы не знакомы с виртуальными средами Python, ознакомьтесь с [руководством пользователя](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\\n\\nСначала создайте виртуальную среду с той версией Python, которую вы собираетесь использовать, и активируйте ее.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 18330}, page_content='Затем необходимо установить хотя бы один бекенд из Flax, PyTorch или TensorFlow.\\nПожалуйста, обратитесь к страницам [TensorFlow установочная страница](https://www.tensorflow.org/install/), [PyTorch установочная страница](https://pytorch.org/get-started/locally/#start-locally) и/или [Flax](https://github.com/google/flax#quick-install) и [Jax](https://github.com/google/jax#installation), где описаны команды установки для вашей платформы.\\n\\nПосле установки одного из этих бэкендов 🤗 Transformers может быть установлен с помощью pip следующим образом:\\n\\n```bash\\npip install transformers'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 18915}, page_content='```\\n\\nЕсли вы хотите поиграть с примерами или вам нужен самый современный код и вы не можете ждать нового релиза, вы должны [установить библиотеку из исходного кода](https://huggingface.co/docs/transformers/installation#installing-from-source).\\n\\n### С помощью conda\\n\\nНачиная с версии Transformers v4.0.0, у нас появилсась поддержка conda: `huggingface`.\\n\\nУстановить Transformers с помощью conda можно следующим образом:\\n\\n```bash\\nconda install -c huggingface transformers'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 19385}, page_content='```\\n\\nО том, как установить Flax, PyTorch или TensorFlow с помощью conda, читайте на страницах, посвященных их установке.\\n\\n> **_ЗАМЕТКА:_** В операционной системе Windows вам может быть предложено активировать режим разработчика, чтобы воспользоваться преимуществами кэширования. Если для вас это невозможно, сообщите нам об этом [здесь](https://github.com/huggingface/huggingface_hub/issues/1062).\\n\\n## Модельные архитектуры\\n\\n**[Все контрольные точки моделей](https://huggingface.co/models)**, предоставляемые 🤗 Transformers, беспрепятственно интегрируются с huggingface.co [model hub](https://huggingface.co/models), куда они загружаются непосредственно [пользователями](https://huggingface.co/users) и [организациями](https://huggingface.co/organizations).'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 20144}, page_content='Текущее количество контрольных точек: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\\n\\n🤗 В настоящее время Transformers предоставляет следующие архитектуры (подробное описание каждой из них см. [здесь](https://huggingface.co/docs/transformers/model_summary)):'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 20458}, page_content='1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\\n1. **[ALIGN](https://huggingface.co/docs/transformers/model_doc/align)** (from Google Research) released with the paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.\\n1. **[AltCLIP](https://huggingface.co/docs/transformers/model_doc/altclip)** (from BAAI) released with the paper [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679) by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.\\n1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 21804}, page_content='1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua University) released with the paper [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.\\n1. **[Bark](https://huggingface.co/docs/transformers/model_doc/bark)** (from Suno) released in the repository [suno-ai/bark](https://github.com/suno-ai/bark) by Suno AI team.\\n1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\\n1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** (from École polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\\n1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 23264}, page_content='1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\\n1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\\n1. **[BERT For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\\n1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (from VinAI Research) released with the paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.\\n1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 24743}, page_content='1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\\n1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)** (from Microsoft Research AI4Science) released with the paper [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.\\n1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (from Google AI) released with the paper [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 25876}, page_content='1. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\\n1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\\n1. **[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)** (from Salesforce) released with the paper [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\\n1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** (from Salesforce) released with the paper [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 27225}, page_content='1. **[BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).\\n1. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) by Adrian de Wynter and Daniel J. Perry.\\n1. **[BridgeTower](https://huggingface.co/docs/transformers/model_doc/bridgetower)** (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.\\n1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (from NAVER CLOVA) released with the paper [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.\\n1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 28704}, page_content='1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.\\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.\\n1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 30040}, page_content='1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\\n1. **[CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)** (from University of Göttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker.\\n1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 31005}, page_content='1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (from MetaAI) released with the paper [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.\\n1. **[Conditional DETR](https://huggingface.co/docs/transformers/model_doc/conditional_detr)** (from Microsoft Research Asia) released with the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.\\n1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 32279}, page_content='1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\\n1. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\\n1. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\\n1. **[CPM-Ant](https://huggingface.co/docs/transformers/model_doc/cpmant)** (from OpenBMB) released by the [OpenBMB](https://www.openbmb.org/).'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 33554}, page_content='1. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\\n1. **[CvT](https://huggingface.co/docs/transformers/model_doc/cvt)** (from Microsoft) released with the paper [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\\n1. **[Data2Vec](https://huggingface.co/docs/transformers/model_doc/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\\n1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 34755}, page_content='1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\\n1. **[Decision Transformer](https://huggingface.co/docs/transformers/model_doc/decision_transformer)** (from Berkeley/Facebook/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\\n1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)** (from SenseTime Research) released with the paper [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.\\n1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 36077}, page_content='1. **[DePlot](https://huggingface.co/docs/transformers/model_doc/deplot)** (from Google AI) released with the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.\\n1. **[DETA](https://huggingface.co/docs/transformers/model_doc/deta)** (from The University of Texas at Austin) released with the paper [NMS Strikes Back](https://arxiv.org/abs/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.\\n1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\\n1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 37398}, page_content='1. **[DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat)** (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.\\n1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (from Meta AI) released with the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 38232}, page_content='1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.\\n1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\\n1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 39621}, page_content='1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\\n1. **[DPT](https://huggingface.co/docs/transformers/master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\\n1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.\\n1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google Brain) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) by Mingxing Tan, Quoc V. Le.\\n1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 41117}, page_content='1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)** (from Meta AI) released with the paper [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\\n1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\\n1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)** (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.\\n1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)** (from Baidu) released with the paper [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674) by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 42321}, page_content='1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (from Meta AI) are transformer protein language models.  **ESM-1b** was released with the paper [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. **ESM-1v** was released with the paper [Language models enable zero-shot prediction of the effects of mutations on protein function](https://doi.org/10.1101/2021.07.09.450648) by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. **ESM-2 and ESMFold** were released with the paper [Language models of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.\\n1. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)** (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 43820}, page_content='1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\\n1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 45133}, page_content='1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.\\n1. **[FLAVA](https://huggingface.co/docs/transformers/model_doc/flava)** (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.\\n1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\\n1. **[FocalNet](https://huggingface.co/docs/transformers/model_doc/focalnet)** (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 46358}, page_content='1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\\n1. **[Fuyu](https://huggingface.co/docs/transformers/model_doc/fuyu)** (from ADEPT) Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar. Released with the paper [blog post](https://www.adept.ai/blog/fuyu-8b)\\n1. **[GIT](https://huggingface.co/docs/transformers/model_doc/git)** (from Microsoft Research) released with the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.\\n1. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 47573}, page_content='1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\\n1. **[GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.\\n1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\\n1. **[GPT NeoX Japanese](https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese)** (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 48771}, page_content='1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\\n1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.\\n1. **[GPT-Sw3](https://huggingface.co/docs/transformers/model_doc/gpt-sw3)** (from AI-Sweden) released with the paper [Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf) by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 49746}, page_content=\"1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) released with the paper [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.\\n1. **[GPTSAN-japanese](https://huggingface.co/docs/transformers/model_doc/gptsan-japanese)** released in the repository [tanreinama/GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md) by Toshiyuki Sakamoto(tanreinama).\\n1. **[Graphormer](https://huggingface.co/docs/transformers/model_doc/graphormer)** (from Microsoft) released with the paper [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 51167}, page_content='1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\\n1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.\\n1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\\n1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 52458}, page_content='1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) released with the paper [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527) by Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh.\\n1. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.\\n1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)** (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\\n1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)** (from Salesforce) released with the paper [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 53944}, page_content='1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)** (from OpenAI) released with the paper [Jukebox: A Generative Model for Music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.\\n1. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\\n1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\\n1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 55256}, page_content=\"1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\\n1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\\n1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.\\n1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 56496}, page_content='1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 56945}, page_content='1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (from The FAIR team of Meta AI) released with the paper [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX) by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 58257}, page_content='1. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\\n1. **[LongT5](https://huggingface.co/docs/transformers/model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.\\n1. **[LUKE](https://huggingface.co/docs/transformers/model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\\n1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) by Hao Tan and Mohit Bansal.\\n1. **[M-CTC-T](https://huggingface.co/docs/transformers/model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 59718}, page_content='1. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\\n1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) released with the paper [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](https://arxiv.org/abs/2309.04662) by Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat.\\n1. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)** Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by Jörg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.\\n1. **[MarkupLM](https://huggingface.co/docs/transformers/model_doc/markuplm)** (from Microsoft Research Asia) released with the paper [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 61174}, page_content='1. **[Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former)** (from FAIR and UIUC) released with the paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.\\n1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\\n1. **[MatCha](https://huggingface.co/docs/transformers/model_doc/matcha)** (from Google AI) released with the paper [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662) by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.\\n1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 62505}, page_content='1. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\\n1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (from Meta/USC/CMU/SJTU) released with the paper [Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\\n1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 63865}, page_content='1. **[MGP-STR](https://huggingface.co/docs/transformers/model_doc/mgp-str)** (from Alibaba Research) released with the paper [Multi-Granularity Prediction for Scene Text Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang, Cheng Da, and Cong Yao.\\n1. **[mLUKE](https://huggingface.co/docs/transformers/model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\\n1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (from Facebook) released with the paper [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516) by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.\\n1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 65156}, page_content='1. **[MobileNetV1](https://huggingface.co/docs/transformers/model_doc/mobilenet_v1)** (from Google Inc.) released with the paper [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\\n1. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)** (from Google Inc.) released with the paper [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.\\n1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari.\\n1. **[MobileViTV2](https://huggingface.co/docs/transformers/model_doc/mobilevitv2)** (from Apple) released with the paper [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) by Sachin Mehta and Mohammad Rastegari.\\n1. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 66644}, page_content='1. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)** (from MosaiML) released with the repository [llm-foundry](https://github.com/mosaicml/llm-foundry/) by the MosaicML NLP Team.\\n1. **[MRA](https://huggingface.co/docs/transformers/model_doc/mra)** (from the University of Wisconsin - Madison) released with the paper [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.\\n1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\\n1. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)** (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.\\n1. **[MVP](https://huggingface.co/docs/transformers/model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 68059}, page_content='1. **[NAT](https://huggingface.co/docs/transformers/model_doc/nat)** (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\\n1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)** (from Huawei Noah’s Ark Lab) released with the paper [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.\\n1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.\\n1. **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.\\n1. **[Nyströmformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 69512}, page_content='1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\\n1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (from [s-JoL](https://huggingface.co/s-JoL)) released on GitHub (now removed).\\n1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\\n1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 70716}, page_content='1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.\\n1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (from Google) released with the paper [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) by Jason Phang, Yao Zhao, and Peter J. Liu.\\n1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.\\n1. **[Persimmon](https://huggingface.co/docs/transformers/main/model_doc/persimmon)** (from ADEPT) released in a [blog post](https://www.adept.ai/blog/persimmon-8b) by Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 72052}, page_content='1. **[Phi](https://huggingface.co/docs/main/transformers/model_doc/phi)** (from Microsoft Research) released with the papers - [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.\\n1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.\\n1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (from Google) released with the paper [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 73404}, page_content='1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\\n1. **[PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.\\n1. **[Pop2Piano](https://huggingface.co/docs/transformers/model_doc/pop2piano)** released with the paper [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi and Kyogu Lee.\\n1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 74595}, page_content='1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)** (from Nanjing University, The University of Hong Kong etc.) released with the paper [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/pdf/2102.12122.pdf) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.\\n1. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.\\n1. **[RAG](https://huggingface.co/docs/transformers/model_doc/rag)** (from Facebook) released with the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.\\n1. **[REALM](https://huggingface.co/docs/transformers/model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 75993}, page_content='1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.\\n1. **[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)** (from META Platforms) released with the paper [Designing Network Design Space](https://arxiv.org/abs/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.\\n1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder.\\n1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\\n1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 77426}, page_content='1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)** (from Facebook) released with the paper [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.\\n1. **[RoCBert](https://huggingface.co/docs/transformers/model_doc/roc_bert)** (from WeChatAI) released with the paper [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.\\n1. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\\n1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (from Bo Peng), released on [this repo](https://github.com/BlinkDL/RWKV-LM) by Bo Peng.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 78542}, page_content='1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\\n1. **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)** (from Meta AI) released with the paper [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\\n1. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\\n1. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 79840}, page_content='1. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\\n1. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\\n1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\\n1. **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 81199}, page_content='1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.\\n1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\\n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 82557}, page_content='1. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)** (from University of Würzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.\\n1. **[SwitchTransformers](https://huggingface.co/docs/transformers/model_doc/switch_transformers)** (from Google) released with the paper [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\\n1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\\n1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 83984}, page_content='1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.\\n1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos.\\n1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\\n1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)** (from HuggingFace).\\n1. **[TimeSformer](https://huggingface.co/docs/transformers/model_doc/timesformer)** (from Facebook) released with the paper [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Gedas Bertasius, Heng Wang, Lorenzo Torresani.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 85321}, page_content='1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine\\n1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\\n1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\\n1. **[TVLT](https://huggingface.co/docs/transformers/model_doc/tvlt)** (from UNC Chapel Hill) released with the paper [TVLT: Textless Vision-Language Transformer](https://arxiv.org/abs/2209.14156) by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 86567}, page_content='1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler\\n1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (from Google Research) released with the paper [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\\n1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\\n1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 87995}, page_content='1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)** (from Peking University) released with the paper [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221) by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.\\n1. **[VAN](https://huggingface.co/docs/transformers/model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\\n1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\\n1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 89189}, page_content='1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\\n1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\\n1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)** (from Meta AI) released with the paper [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 90654}, page_content='1. **[ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.\\n1. **[ViTMatte](https://huggingface.co/docs/transformers/main/model_doc/vitmatte)** (from HUST-VL) rreleased with the paper [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.\\n1. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)** (from Meta AI) released with the paper [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.\\n1. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)** (from Kakao Enterprise) released with the paper [Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) by Jaehyeon Kim, Jungil Kong, Juhee Son.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 91869}, page_content='1. **[ViViT](https://huggingface.co/docs/transformers/model_doc/vivit)** (from Google Research) released with the paper [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.\\n1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\\n1. **[Wav2Vec2-Conformer](https://huggingface.co/docs/transformers/model_doc/wav2vec2-conformer)** (from Facebook AI) released with the paper [FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.\\n1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/transformers/model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 93059}, page_content='1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\\n1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\\n1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (from Microsoft Research) released with the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\\n1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)** (from Meta AI) released with the paper [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 94503}, page_content=\"1. **[XGLM](https://huggingface.co/docs/transformers/model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\\n1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\\n1. **[XLM-ProphetNet](https://huggingface.co/docs/transformers/model_doc/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 95609}, page_content='1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.\\n1. **[XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\\n1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (from Meta AI) released with the paper [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472) by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.\\n1. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)** (from Google/CMU) released with the paper [\\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 97005}, page_content='1. **[XLS-R](https://huggingface.co/docs/transformers/model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\\n1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\\n1. **[YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\\n1. **[YOSO](https://huggingface.co/docs/transformers/model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 98482}, page_content='1. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in the process of adding a new model. You can find them in the [`templates`](./templates) folder of the repository. Be sure to check the [contributing guidelines](./CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 98853}, page_content='Чтобы проверить, есть ли у каждой модели реализация на Flax, PyTorch или TensorFlow, или связанный с ней токенизатор, поддерживаемый библиотекой 🤗 Tokenizers, обратитесь к [этой таблице](https://huggingface.co/docs/transformers/index#supported-frameworks).\\n\\nЭти реализации были протестированы на нескольких наборах данных (см. примеры скриптов) и должны соответствовать производительности оригинальных реализаций. Более подробную информацию о производительности можно найти в разделе \"Примеры\" [документации](https://github.com/huggingface/transformers/tree/main/examples).\\n\\n\\n## Изучи больше'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 99446}, page_content='| Секция | Описание |\\n|-|-|\\n| [Документация](https://huggingface.co/docs/transformers/) | Полная документация по API и гайды |\\n| [Краткие описания задач](https://huggingface.co/docs/transformers/task_summary) | Задачи поддерживаются 🤗 Transformers |\\n| [Пособие по предварительной обработке](https://huggingface.co/docs/transformers/preprocessing) | Использование класса `Tokenizer` для подготовки данных для моделей |\\n| [Обучение и доработка](https://huggingface.co/docs/transformers/training) | Использование моделей, предоставляемых 🤗 Transformers, в цикле обучения PyTorch/TensorFlow и API `Trainer`. |\\n| [Быстрый тур: Тонкая настройка/скрипты использования](https://github.com/huggingface/transformers/tree/main/examples) | Примеры скриптов для тонкой настройки моделей на широком спектре задач |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 100247}, page_content='| [Совместное использование и загрузка моделей](https://huggingface.co/docs/transformers/model_sharing) | Загружайте и делитесь с сообществом своими доработанными моделями |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 100422}, page_content='## Цитирование\\n\\nТеперь у нас есть [статья](https://www.aclweb.org/anthology/2020.emnlp-demos.6/), которую можно цитировать для библиотеки 🤗 Transformers:\\n```bibtex\\n@inproceedings{wolf-etal-2020-transformers,\\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\\n    month = oct,\\n    year = \"2020\",\\n    address = \"Online\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\\n    pages = \"38--45\"\\n}'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Share a model\\n\\nThe last two tutorials showed how you can fine-tune a model with PyTorch, Keras, and 🤗 Accelerate for distributed setups. The next step is to share your model with the community! At Hugging Face, we believe in openly sharing knowledge and resources to democratize artificial intelligence for everyone. We encourage you to consider sharing your model with the community to help others save time and resources.\\n\\nIn this tutorial, you will learn two methods for sharing a trained or fine-tuned model on the [Model Hub](https://huggingface.co/models):\\n\\n- Programmatically push your files to the Hub.\\n- Drag-and-drop your files to the Hub with the web interface.\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XvSGPZFEjDY\" title=\"YouTube video player\"\\nframeborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;\\npicture-in-picture\" allowfullscreen></iframe>\\n\\n<Tip>\\n\\nTo share a model with the community, you need an account on [huggingface.co](https://huggingface.co/join). You can also join an existing organization or create a new one.\\n\\n</Tip>\\n\\n## Repository features\\n\\nEach repository on the Model Hub behaves like a typical GitHub repository. Our repositories offer versioning, commit history, and the ability to visualize differences.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': -1}, page_content='</Tip>\\n\\n## Repository features\\n\\nEach repository on the Model Hub behaves like a typical GitHub repository. Our repositories offer versioning, commit history, and the ability to visualize differences.\\n\\nThe Model Hub\\'s built-in versioning is based on git and [git-lfs](https://git-lfs.github.com/). In other words, you can treat one model as one repository, enabling greater access control and scalability. Version control allows *revisions*, a method for pinning a specific version of a model with a commit hash, tag or branch.\\n\\nAs a result, you can load a specific model version with the `revision` parameter:\\n\\n```py\\n>>> model = AutoModel.from_pretrained(\\n...     \"julien-c/EsperBERTo-small\", revision=\"v2.0.1\"  # tag name, or branch name, or commit hash\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 2617}, page_content='```\\n\\nFiles are also easily edited in a repository, and you can view the commit history as well as the difference:\\n\\n![vis_diff](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis_diff.png)\\n\\n## Setup\\n\\nBefore sharing a model to the Hub, you will need your Hugging Face credentials. If you have access to a terminal, run the following command in the virtual environment where 🤗 Transformers is installed. This will store your access token in your Hugging Face cache folder (`~/.cache/` by default):\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nIf you are using a notebook like Jupyter or Colaboratory, make sure you have the [`huggingface_hub`](https://huggingface.co/docs/hub/adding-a-library) library installed. This library allows you to programmatically interact with the Hub.\\n\\n```bash\\npip install huggingface_hub\\n```\\n\\nThen use `notebook_login` to sign-in to the Hub, and follow the link [here](https://huggingface.co/settings/token) to generate a token to login with:\\n\\n```py\\n>>> from huggingface_hub import notebook_login\\n\\n>>> notebook_login()'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 3684}, page_content='```\\n\\n## Convert a model for all frameworks\\n\\nTo ensure your model can be used by someone working with a different framework, we recommend you convert and upload your model with both PyTorch and TensorFlow checkpoints. While users are still able to load your model from a different framework if you skip this step, it will be slower because 🤗 Transformers will need to convert the checkpoint on-the-fly.\\n\\nConverting a checkpoint for another framework is easy. Make sure you have PyTorch and TensorFlow installed (see [here](installation) for installation instructions), and then find the specific model for your task in the other framework. \\n\\n<frameworkcontent>\\n<pt>\\nSpecify `from_tf=True` to convert a checkpoint from TensorFlow to PyTorch:\\n\\n```py\\n>>> pt_model = DistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_tf=True)\\n>>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\\n```\\n</pt>\\n<tf>\\nSpecify `from_pt=True` to convert a checkpoint from PyTorch to TensorFlow:\\n\\n```py\\n>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\\n```\\n\\nThen you can save your new TensorFlow model with its new checkpoint:\\n\\n```py\\n>>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\\n```\\n</tf>\\n<jax>\\nIf a model is available in Flax, you can also convert a checkpoint from PyTorch to Flax:\\n\\n```py\\n>>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\\n...     \"path/to/awesome-name-you-picked\", from_pt=True\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 5219}, page_content='```\\n</jax>\\n</frameworkcontent>\\n\\n## Push a model during training\\n\\n<frameworkcontent>\\n<pt>\\n<Youtube id=\"Z1-XMy-GNLQ\"/>\\n\\nSharing a model to the Hub is as simple as adding an extra parameter or callback. Remember from the [fine-tuning tutorial](training), the [`TrainingArguments`] class is where you specify hyperparameters and additional training options. One of these training options includes the ability to push a model directly to the Hub. Set `push_to_hub=True` in your [`TrainingArguments`]:\\n\\n```py\\n>>> training_args = TrainingArguments(output_dir=\"my-awesome-model\", push_to_hub=True)\\n```\\n\\nPass your training arguments as usual to [`Trainer`]:\\n\\n```py\\n>>> trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     train_dataset=small_train_dataset,\\n...     eval_dataset=small_eval_dataset,\\n...     compute_metrics=compute_metrics,\\n... )\\n```\\n\\nAfter you fine-tune your model, call [`~transformers.Trainer.push_to_hub`] on [`Trainer`] to push the trained model to the Hub. 🤗 Transformers will even automatically add training hyperparameters, training results and framework versions to your model card!\\n\\n```py\\n>>> trainer.push_to_hub()'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 6373}, page_content='```\\n</pt>\\n<tf>\\nShare a model to the Hub with [`PushToHubCallback`]. In the [`PushToHubCallback`] function, add:\\n\\n- An output directory for your model.\\n- A tokenizer.\\n- The `hub_model_id`, which is your Hub username and model name.\\n\\n```py\\n>>> from transformers import PushToHubCallback\\n\\n>>> push_to_hub_callback = PushToHubCallback(\\n...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\\n... )\\n```\\n\\nAdd the callback to [`fit`](https://keras.io/api/models/model_training_apis/), and 🤗 Transformers will push the trained model to the Hub:\\n\\n```py\\n>>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\\n```\\n</tf>\\n</frameworkcontent>\\n\\n## Use the `push_to_hub` function\\n\\nYou can also call `push_to_hub` directly on your model to upload it to the Hub.\\n\\nSpecify your model name in `push_to_hub`:\\n\\n```py\\n>>> pt_model.push_to_hub(\"my-awesome-model\")\\n```\\n\\nThis creates a repository under your username with the model name `my-awesome-model`. Users can now load your model with the `from_pretrained` function:\\n\\n```py\\n>>> from transformers import AutoModel\\n\\n>>> model = AutoModel.from_pretrained(\"your_username/my-awesome-model\")\\n```\\n\\nIf you belong to an organization and want to push your model under the organization name instead, just add it to the `repo_id`:\\n\\n```py\\n>>> pt_model.push_to_hub(\"my-awesome-org/my-awesome-model\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 7804}, page_content='```\\n\\nThe `push_to_hub` function can also be used to add other files to a model repository. For example, add a tokenizer to a model repository:\\n\\n```py\\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\\n```\\n\\nOr perhaps you\\'d like to add the TensorFlow version of your fine-tuned PyTorch model:\\n\\n```py\\n>>> tf_model.push_to_hub(\"my-awesome-model\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 8143}, page_content=\"```\\n\\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\\n\\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\\n\\n## Upload with the web interface\\n\\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\\n\\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\\n\\nFrom here, add some information about your model:\\n\\n- Select the **owner** of the repository. This can be yourself or any of the organizations you belong to.\\n- Pick a name for your model, which will also be the repository name.\\n- Choose whether your model is public or private.\\n- Specify the license usage for your model.\\n\\nNow click on the **Files** tab and click on the **Add file** button to upload a new file to your repository. Then drag-and-drop a file to upload and add a commit message.\\n\\n![upload_file](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/upload_file.png)\\n\\n## Add a model card\\n\\nTo make sure users understand your model's capabilities, limitations, potential biases and ethical considerations, please add a model card to your repository. The model card is defined in the `README.md` file. You can add a model card by:\\n\\n* Manually creating and uploading a `README.md` file.\\n* Clicking on the **Edit model card** button in your model repository.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': -1}, page_content=\"* Manually creating and uploading a `README.md` file.\\n* Clicking on the **Edit model card** button in your model repository.\\n\\nTake a look at the DistilBert [model card](https://huggingface.co/distilbert-base-uncased) for a good example of the type of information a model card should include. For more details about other options you can control in the `README.md` file such as a model's carbon footprint or widget examples, refer to the documentation [here](https://huggingface.co/docs/hub/models-cards).\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# LoRA\\n\\n<Tip warning={true}>\\n\\nThis is experimental and the API may change in the future.\\n\\n</Tip>\\n\\n[LoRA (Low-Rank Adaptation of Large Language Models)](https://hf.co/papers/2106.09685) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained. This makes training with LoRA much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), which are easier to store and share. LoRA can also be combined with other training techniques like DreamBooth to speedup training.\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 1229}, page_content='<Tip>\\n\\nLoRA is very versatile and supported for [DreamBooth](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py), [Kandinsky 2.2](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_lora_decoder.py), [Stable Diffusion XL](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py), [text-to-image](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py), and [Wuerstchen](https://github.com/huggingface/diffusers/blob/main/examples/wuerstchen/text_to_image/train_text_to_image_lora_prior.py).\\n\\n</Tip>\\n\\nThis guide will explore the [train_text_to_image_lora.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py) script to help you become more familiar with it, and how you can adapt it for your own use-case.\\n\\nBefore running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install .'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 2347}, page_content='```\\n\\nNavigate to the example folder with the training script and install the required dependencies for the script you\\'re using:\\n\\n<hfoptions id=\"installation\">\\n<hfoption id=\"PyTorch\">\\n\\n```bash\\ncd examples/text_to_image\\npip install -r requirements.txt\\n```\\n\\n</hfoption>\\n<hfoption id=\"Flax\">\\n\\n```bash\\ncd examples/text_to_image\\npip install -r requirements_flax.txt\\n```\\n\\n</hfoption>\\n</hfoptions>\\n\\n<Tip>\\n\\n🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It\\'ll automatically configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\\n\\n</Tip>\\n\\nInitialize an 🤗 Accelerate environment:\\n\\n```bash\\naccelerate config\\n```\\n\\nTo setup a default 🤗 Accelerate environment without choosing any configurations:\\n\\n```bash\\naccelerate config default\\n```\\n\\nOr if your environment doesn\\'t support an interactive shell, like a notebook, you can use:\\n\\n```bash\\nfrom accelerate.utils import write_basic_config\\n\\nwrite_basic_config()'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': -1}, page_content=\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```bash\\nfrom accelerate.utils import write_basic_config\\n\\nwrite_basic_config()\\n```\\n\\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\\n\\n<Tip>\\n\\nThe following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py) and let us know if you have any questions or concerns.\\n\\n</Tip>\\n\\n## Script parameters\\n\\nThe training script has many parameters to help you customize your training run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L85) function. Default values are provided for most parameters that work pretty well, but you can also set your own values in the training command if you'd like.\\n\\nFor example, to increase the number of epochs to train:\\n\\n```bash\\naccelerate launch train_text_to_image_lora.py \\\\\\n  --num_train_epochs=150 \\\\\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 4664}, page_content=\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters) training guide, so this guide just focuses on the LoRA relevant parameters:\\n\\n- `--rank`: the number of low-rank matrices to train\\n- `--learning_rate`: the default learning rate is 1e-4, but with LoRA, you can use a higher learning rate\\n\\n## Training script\\n\\nThe dataset preprocessing code and training loop are found in the [`main()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L371) function, and if you need to adapt the training script, this is where you'll make your changes.\\n\\nAs with the script parameters, a walkthrough of the training script is provided in the [Text-to-image](text2image#training-script) training guide. Instead, this guide takes a look at the LoRA relevant parts of the script.\\n\\nThe script begins by adding the [new LoRA weights](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L447) to the attention layers. This involves correctly configuring the weight size for each block in the UNet. You'll see the `rank` parameter is used to create the [`~models.attention_processor.LoRAAttnProcessor`]:\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 5983}, page_content='```py\\nlora_attn_procs = {}\\nfor name in unet.attn_processors.keys():\\n    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\\n    if name.startswith(\"mid_block\"):\\n        hidden_size = unet.config.block_out_channels[-1]\\n    elif name.startswith(\"up_blocks\"):\\n        block_id = int(name[len(\"up_blocks.\")])\\n        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\\n    elif name.startswith(\"down_blocks\"):\\n        block_id = int(name[len(\"down_blocks.\")])\\n        hidden_size = unet.config.block_out_channels[block_id]\\n\\n    lora_attn_procs[name] = LoRAAttnProcessor(\\n        hidden_size=hidden_size,\\n        cross_attention_dim=cross_attention_dim,\\n        rank=args.rank,\\n    )\\n\\nunet.set_attn_processor(lora_attn_procs)\\nlora_layers = AttnProcsLayers(unet.attn_processors)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 6823}, page_content=\"```\\n\\nThe [optimizer](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L519) is initialized with the `lora_layers` because these are the only weights that'll be optimized:\\n\\n```py\\noptimizer = optimizer_cls(\\n    lora_layers.parameters(),\\n    lr=args.learning_rate,\\n    betas=(args.adam_beta1, args.adam_beta2),\\n    weight_decay=args.adam_weight_decay,\\n    eps=args.adam_epsilon,\\n)\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 7290}, page_content='```\\n\\nAside from setting up the LoRA layers, the training script is more or less the same as train_text_to_image.py!\\n\\n## Launch the script\\n\\nOnce you\\'ve made all your changes or you\\'re okay with the default configuration, you\\'re ready to launch the training script! 🚀\\n\\nLet\\'s train on the [Pokémon BLIP captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions) dataset to generate our yown Pokémon. Set the environment variables `MODEL_NAME` and `DATASET_NAME` to the model and dataset respectively. You should also specify where to save the model in `OUTPUT_DIR`, and the name of the model to save to on the Hub with `HUB_MODEL_ID`. The script creates and saves the following files to your repository:\\n\\n- saved model checkpoints\\n- `pytorch_lora_weights.safetensors` (the trained LoRA weights)\\n\\nIf you\\'re training on more than one GPU, add the `--multi_gpu` parameter to the `accelerate launch` command.\\n\\n<Tip warning={true}>\\n\\nA full training run takes ~5 hours on a 2080 Ti GPU with 11GB of VRAM.\\n\\n</Tip>\\n\\n```bash\\nexport MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\\nexport OUTPUT_DIR=\"/sddata/finetune/lora/pokemon\"\\nexport HUB_MODEL_ID=\"pokemon-lora\"\\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 8512}, page_content='accelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\\\\n  --dataset_name=$DATASET_NAME \\\\\\n  --dataloader_num_workers=8 \\\\\\n  --resolution=512 \\\\\\n  --center_crop \\\\\\n  --random_flip \\\\\\n  --train_batch_size=1 \\\\\\n  --gradient_accumulation_steps=4 \\\\\\n  --max_train_steps=15000 \\\\\\n  --learning_rate=1e-04 \\\\\\n  --max_grad_norm=1 \\\\\\n  --lr_scheduler=\"cosine\" \\\\\\n  --lr_warmup_steps=0 \\\\\\n  --output_dir=${OUTPUT_DIR} \\\\\\n  --push_to_hub \\\\\\n  --hub_model_id=${HUB_MODEL_ID} \\\\\\n  --report_to=wandb \\\\\\n  --checkpointing_steps=500 \\\\\\n  --validation_prompt=\"A pokemon with blue eyes.\" \\\\\\n  --seed=1337'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 9146}, page_content='```\\n\\nOnce training has been completed, you can use your model for inference:\\n\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\\npipeline.load_lora_weights(\"path/to/lora/model\", weight_name=\"pytorch_lora_weights.safetensors\")\\nimage = pipeline(\"A pokemon with blue eyes\").images[0]\\n```\\n\\n## Next steps\\n\\nCongratulations on training a new model with LoRA! To learn more about how to use your new model, the following guides may be helpful:\\n\\n- Learn how to [load different LoRA formats](../using-diffusers/loading_adapters#LoRA) trained using community trainers like Kohya and TheLastBen.\\n- Learn how to use and [combine multiple LoRA\\'s](../tutorials/using_peft_for_inference) with PEFT for inference.'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 0}, page_content='--\\ntitle: MAPE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  Mean Absolute Percentage Error (MAPE) is the mean percentage error difference between the predicted and actual\\n  values.\\n---\\n\\n# Metric Card for MAPE\\n\\n\\n## Metric Description\\n\\nMean Absolute Error (MAPE) is the mean of the percentage error of difference between the predicted $x_i$ and actual $y_i$ numeric values:\\n![image](https://user-images.githubusercontent.com/8100/200005316-c3975d32-8978-40f3-b541-c2ef57ec7c5b.png)\\n\\n## How to Use\\n\\nAt minimum, this metric requires predictions and references as inputs.\\n\\n```python\\n>>> mape_metric = evaluate.load(\"mape\")\\n>>> predictions = [2.5, 0.0, 2, 8]\\n>>> references = [3, -0.5, 2, 7]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 869}, page_content='```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (`n_samples`, `n_outputs`), representing the estimated target values.\\n- `references`: numeric array-like of shape (`n_samples,`) or (`n_samples`, `n_outputs`), representing the ground truth (correct) target values.\\n\\nOptional arguments:\\n- `sample_weight`: numeric array-like of shape (`n_samples,`) representing sample weights. The default is `None`.\\n- `multioutput`: `raw_values`, `uniform_average` or numeric array-like of shape (`n_outputs,`), which defines the aggregation of multiple output values. The default value is `uniform_average`.\\n  - `raw_values` returns a full set of errors in case of multioutput input.\\n  - `uniform_average` means that the errors of all outputs are averaged with uniform weight. \\n  - the array-like value defines weights used to average errors.\\n\\n### Output Values\\nThis metric outputs a dictionary, containing the mean absolute error score, which is of type:\\n- `float`: if multioutput is `uniform_average` or an ndarray of weights, then the weighted average of all output errors is returned.\\n- numeric array-like of shape (`n_outputs,`): if multioutput is `raw_values`, then the score is returned for each output separately. \\n\\nEach MAPE `float` value is postive with the best value being 0.0.\\n\\nOutput Example(s):\\n```python\\n{\\'mape\\': 0.5}\\n```\\n\\nIf `multioutput=\"raw_values\"`:\\n```python\\n{\\'mape\\': array([0.5, 1. ])}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': -1}, page_content='```\\n\\nIf `multioutput=\"raw_values\"`:\\n```python\\n{\\'mape\\': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\n\\nExample with the `uniform_average` config:\\n```python\\n>>> mape_metric = evaluate.load(\"mape\")\\n>>> predictions = [2.5, 0.0, 2, 8]\\n>>> references = [3, -0.5, 2, 7]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)\\n>>> print(results)\\n{\\'mape\\': 0.3273...}\\n```\\n\\nExample with multi-dimensional lists, and the `raw_values` config:\\n```python\\n>>> mape_metric = evaluate.load(\"mape\", \"multilist\")\\n>>> predictions = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> references = [[0.1, 2], [-1, 2], [8, -5]]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)\\n>>> print(results)\\n{\\'mape\\': 0.8874...}\\n>>> results = mape_metric.compute(predictions=predictions, references=references, multioutput=\\'raw_values\\')\\n>>> print(results)\\n{\\'mape\\': array([1.3749..., 0.4])}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 3156}, page_content='```\\n\\n## Limitations and Bias\\nOne limitation of MAPE is that it cannot be used if the ground truth is zero or close to zero. This metric is also asymmetric in that it puts a heavier penalty on predictions less than the ground truth and a smaller penalty on predictions bigger than the ground truth and thus can lead to a bias of methods being select which under-predict if selected via this metric.\\n\\n## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n  title={Scikit-learn: Machine Learning in {P}ython},\\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n  journal={Journal of Machine Learning Research},\\n  volume={12},\\n  pages={2825--2830},\\n  year={2011}\\n}\\n```\\n\\n```bibtex\\n@article{DEMYTTENAERE201638,\\n    title = {Mean Absolute Percentage Error for regression models},\\n    journal = {Neurocomputing},\\n    volume = {192},\\n    pages = {38--48},\\n    year = {2016},\\n    note = {Advances in artificial neural networks, machine learning and computational intelligence},\\n    issn = {0925-2312},\\n    doi = {https://doi.org/10.1016/j.neucom.2015.12.114},\\n    url = {https://www.sciencedirect.com/science/article/pii/S0925231216003325},\\n    author = {Arnaud {de Myttenaere} and Boris Golden and Bénédicte {Le Grand} and Fabrice Rossi},\\n}\\n```\\n\\n## Further References\\n- [Mean absolute percentage error - Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 1}, page_content='# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on the Inception family of architectures but incorporates [residual connections](https://paperswithcode.com/method/residual-connection) (replacing the filter concatenation stage of the Inception architecture).\\n\\nThis particular model was trained for study of adversarial examples (adversarial training).\\n\\nThe weights from this model were ported from [Tensorflow/Models](https://github.com/tensorflow/models).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model(\\'ens_adv_inception_resnet_v2\\', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 1444}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 2197}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 2825}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@article{DBLP:journals/corr/abs-1804-00097,\\n  author    = {Alexey Kurakin and\\n               Ian J. Goodfellow and\\n               Samy Bengio and\\n               Yinpeng Dong and\\n               Fangzhou Liao and\\n               Ming Liang and\\n               Tianyu Pang and\\n               Jun Zhu and\\n               Xiaolin Hu and\\n               Cihang Xie and\\n               Jianyu Wang and\\n               Zhishuai Zhang and\\n               Zhou Ren and\\n               Alan L. Yuille and\\n               Sangxia Huang and\\n               Yao Zhao and\\n               Yuzhe Zhao and\\n               Zhonglin Han and\\n               Junjiajia Long and\\n               Yerkebulan Berdibekov and\\n               Takuya Akiba and\\n               Seiya Tokui and\\n               Motoki Abe},\\n  title     = {Adversarial Attacks and Defences Competition},\\n  journal   = {CoRR},\\n  volume    = {abs/1804.00097},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1804.00097},\\n  archivePrefix = {arXiv},\\n  eprint    = {1804.00097},\\n  timestamp = {Thu, 31 Oct 2019 16:31:22 +0100},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 4420}, page_content=\"```\\n\\n<!--\\nType: model-index\\nCollections:\\n- Name: Ensemble Adversarial\\n  Paper:\\n    Title: Adversarial Attacks and Defences Competition\\n    URL: https://paperswithcode.com/paper/adversarial-attacks-and-defences-competition\\nModels:\\n- Name: ens_adv_inception_resnet_v2\\n  In Collection: Ensemble Adversarial\\n  Metadata:\\n    FLOPs: 16959133120\\n    Parameters: 55850000\\n    File Size: 223774238\\n    Architecture:\\n    - 1x1 Convolution\\n    - Auxiliary Classifier\\n    - Average Pooling\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inception-v3 Module\\n    - Max Pooling\\n    - ReLU\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: ens_adv_inception_resnet_v2\\n    Crop Pct: '0.897'\\n    Image Size: '299'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/inception_resnet_v2.py#L351\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ens_adv_inception_resnet_v2-2592a550.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 1.0%\\n      Top 5 Accuracy: 17.32%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': 1}, page_content='Using Flair at Hugging Face\\n\\n[Flair](https://github.com/flairNLP/flair) is a very simple framework for state-of-the-art NLP.\\nDeveloped by [Humboldt University of Berlin](https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en/) and friends.\\n\\n## Exploring Flair in the Hub\\n\\nYou can find `flair` models by filtering at the left of the [models page](https://huggingface.co/models?library=flair).\\n\\nAll models on the Hub come with these useful features:\\n\\n1. An automatically generated model card with a brief description.\\n2. An interactive widget you can use to play with the model directly in the browser.\\n3. An Inference API that allows you to make inference requests.\\n\\n## Installation\\n\\nTo get started, you can follow the [Flair installation guide](https://github.com/flairNLP/flair?tab=readme-ov-file#requirements-and-installation).\\nYou can also use the following one-line install through pip:\\n\\n```\\n$ pip install -U flair\\n```\\n\\n## Using existing models\\n\\nAll `flair` models can easily be loaded from the Hub:\\n\\n```py\\nfrom flair.data import Sentence\\nfrom flair.models import SequenceTagger\\n\\n# load tagger\\ntagger = SequenceTagger.load(\"flair/ner-multi\")\\n```\\n\\nOnce loaded, you can use `predict()` to perform inference:\\n\\n```py\\nsentence = Sentence(\"George Washington ging nach Washington.\")\\ntagger.predict(sentence)\\n\\n# print sentence\\nprint(sentence)\\n```\\n\\nIt outputs the following:\\n\\n```text\\nSentence[6]: \"George Washington ging nach Washington.\" → [\"George Washington\"/PER, \"Washington\"/LOC]'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': -1}, page_content='```\\n\\nIt outputs the following:\\n\\n```text\\nSentence[6]: \"George Washington ging nach Washington.\" → [\"George Washington\"/PER, \"Washington\"/LOC]\\n```\\n\\nIf you want to load a specific Flair model, you can click `Use in Flair` in the model card and you will be given a working snippet!\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet1.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet1-dark.png\"/>\\n</div>\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet2.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet2-dark.png\"/>\\n</div>\\n\\n## Additional resources\\n\\n* Flair [repository](https://github.com/flairNLP/flair)\\n* Flair [docs](https://flairnlp.github.io/docs/intro)\\n* Official Flair [models](https://huggingface.co/flair) on the Hub (mainly trained by [@alanakbik](https://huggingface.co/alanakbik) and [@stefan-it](https://huggingface.co/stefan-it))'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/accordion/README.md', 'start_index': 1}, page_content='`@gradio/button`\\n\\n```html\\n<script>\\n\\timport { Button } from \"@gradio/button\";\\n</script>\\n\\n<button type=\"primary|secondary\" href=\"string\" on:click=\"{e.detail === href}\">\\n\\tcontent\\n</button>\\n```'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': 0}, page_content='he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library that provides an API to quickly download many public datasets and preprocess them. In this video we will explore how to do that. The downloading part is easy: with the load_dataset function, you can directly download and cache a dataset from its identifier on the Dataset hub. Here we fetch the MRPC dataset from the GLUE benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which is a sort of dictionary containing each split of our dataset. We can access each split by indexing with its name. This split is then an instance of the Dataset class, with columns (here sentence1, sentence2. label and idx) and rows. We can access a given element by its index. The amazing thing about the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which means that even if your dataset is huge you won\\'t get out of RAM: only the elements you request are loaded in memory. Accessing a slice of your dataset is as easy as one element. The result is then a dictionary with list of values for each keys (here the list of labels, the list of first sentences and the list of second sentences). The features attribute of a Dataset gives us more information about its columns. In particular, we can see here it gives us the correspondence between the integers and names for the labels. 0 stands for not equivalent and 1 for equivalent. To preprocess all the elements of our dataset, we need to tokenize them. Have a look at the video \"Preprocess sentence pairs\" for a refresher, but you just have to send the two sentences to the tokenizer with some additional keyword arguments. Here we indicate a maximum length of 128 and pad inputs shorter than this length, truncate inputs that are longer. We put all of this in a tokenize_function that we can directly apply to all the splits in our dataset with the map method. As long as the function returns a dictionary-like object, the map method will add new columns as needed or update existing ones. To speed up preprocessing and take advantage of the fact our tokenizer is backed by Rust thanks to the Hugging'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': -1}, page_content=\"As long as the function returns a dictionary-like object, the map method will add new columns as needed or update existing ones. To speed up preprocessing and take advantage of the fact our tokenizer is backed by Rust thanks to the Hugging Face Tokenizers library, we can process several elements at the same time to our tokenize function, using the batched=True argument. Since the tokenizer can handle list of first/second sentences, the tokenize_function does not need to change for this. You can also use multiprocessing with the map method, check out its documentation! Once this is done, we are almost ready for training: we just remove the columns we don't need anymore with the remove_columns method, rename label to labels (since the models from Hugging Face Transformers expect that) and set the output format to our desired backend: torch, tensorflow or numpy. If needed, we can also generate a short sample of a dataset using the select method.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 0}, page_content='--\\ntitle: \"Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too\"\\nthumbnail: /blog/assets/78_ml_director_insights/mantis1.png\\nauthors:\\n- user: mattupson\\n  guest: true\\n---\\n\\n# Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too\\n\\n\\n\\nHugging Face recently launched [Inference Endpoints](https://huggingface.co/inference-endpoints); which as they put it: solves transformers in production. Inference Endpoints is a managed service that allows you to:\\n\\n- Deploy (almost) any model on Hugging Face Hub\\n- To any cloud (AWS, and Azure, GCP on the way)\\n- On a range of instance types (including GPU)\\n- We’re switching some of our Machine Learning (ML) models that do inference on a CPU to this new service. This blog is about why, and why you might also want to consider it.\\n\\n## What were we doing?\\n\\nThe models that we have switched over to Inference Endpoints were previously managed internally and were running on AWS [Elastic Container Service](https://aws.amazon.com/ecs/) (ECS) backed by [AWS Fargate](https://aws.amazon.com/fargate/). This gives you a serverless cluster which can run container based tasks. Our process was as follows:\\n\\n- Train model on a GPU instance (provisioned by [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))\\n- Upload to [Hugging Face Hub](https://huggingface.co/models)\\n- Build API to serve model [(FastAPI)](https://fastapi.tiangolo.com/)\\n- Wrap API in container [(Docker)](https://www.docker.com/)\\n- Upload container to AWS [Elastic Container Repository](https://aws.amazon.com/ecr/) (ECR)\\n- Deploy model to ECS Cluster'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 1655}, page_content='Now, you can reasonably argue that ECS was not the best approach to serving ML models, but it served us up until now, and also allowed ML models to sit alongside other container based services, so it reduced cognitive load.\\n\\n## What do we do now?\\n\\nWith Inference Endpoints, our flow looks like this:\\n\\n- Train model on a GPU instance (provisioned by  [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))\\n- Upload to [Hugging Face Hub](https://huggingface.co/models)\\n- Deploy using Hugging Face Inference Endpoints.\\n\\nSo this is significantly easier. We could also use another managed service such as [SageMaker](https://aws.amazon.com/es/sagemaker/), [Seldon](https://www.seldon.io/), or [Bento ML](https://www.bentoml.com/), etc., but since we are already uploading our model to Hugging Face hub to act as a model registry, and we’re pretty invested in Hugging Face’s other tools (like transformers, and [AutoTrain](https://huggingface.co/autotrain)) using Inference Endpoints makes a lot of sense for us.\\n\\n\\n## What about Latency and Stability?\\n\\nBefore switching to Inference Endpoints we tested different CPU endpoints types using [ab](https://httpd.apache.org/docs/2.4/programs/ab.html).\\n\\nFor ECS we didn’t test so extensively, but we know that a large container had a latency of about ~200ms from an instance in the same region. The tests we did for Inference Endpoints we based on text classification model fine tuned on [RoBERTa](https://huggingface.co/roberta-base) with the following test parameters:\\n\\n- Requester region: eu-east-1\\n- Requester instance size: t3-medium\\n- Inference endpoint region: eu-east-1\\n- Endpoint Replicas: 1\\n- Concurrent connections: 1\\n- Requests: 1000 (1000 requests in 1–2 minutes even from a single connection would represent very heavy use for this particular application)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 3511}, page_content='The following table shows latency (ms ± standard deviation and time to complete test in seconds) for four Intel Ice Lake equipped CPU endpoints.\\n\\n```bash\\nsize   |  vCPU (cores) |   Memory (GB)  |  ECS (ms) |  🤗 (ms)\\n----------------------------------------------------------------------\\nsmall  |  1            |  2             |   _       | ~ 296   \\nmedium |  2            |  4             |   _       | 156 ± 51 (158s)  \\nlarge  |  4            |   8            |   ~200    | 80 ± 30 (80s)   \\nxlarge |  8            | 16             |  _        | 43 ± 31 (43s)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 4076}, page_content='```\\nWhat we see from these results is pretty encouraging. The application that will consume these endpoints serves requests in real time, so we need as low latency as possible. We can see that the vanilla Hugging Face container was more than twice as fast as our bespoke container run on ECS — the slowest response we received from the large Inference Endpoint was just 108ms.\\n\\n## What about the cost?\\n\\nSo how much does this all cost? The table below shows a price comparison for what we were doing previously (ECS + Fargate) and using Inference Endpoints.\\n\\n```bash\\nsize   |  vCPU         |   Memory (GB)  |  ECS      |  🤗       |  % diff\\n----------------------------------------------------------------------\\nsmall  |  1            |  2             |  $ 33.18  | $ 43.80   |  0.24\\nmedium |  2            |  4             |  $ 60.38  | $ 87.61   |  0.31 \\nlarge  |  4            |  8             |  $ 114.78 | $ 175.22  |  0.34\\nxlarge |  8            | 16             |  $ 223.59 | $ 350.44  | 0.5'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 5074}, page_content='```\\n\\nWe can say a couple of things about this. Firstly, we want a managed solution to deployment, we don’t have a dedicated MLOPs team (yet), so we’re looking for a solution that helps us minimize the time we spend on deploying models, even if it costs a little more than handling the deployments ourselves.\\n\\nInference Endpoints are more expensive that what we were doing before, there’s an increased cost of between 24% and 50%. At the scale we’re currently operating, this additional cost, a difference of ~$60 a month for a large CPU instance is nothing compared to the time and cognitive load we are saving by not having to worry about APIs, and containers. If we were deploying 100s of ML microservices we would probably want to think again, but that is probably true of many approaches to hosting.\\n\\n## Some notes and caveats:\\n\\n- You can find pricing for Inference Endpoints [here](https://huggingface.co/pricing#endpoints), but a different number is displayed when you deploy a new endpoint from the [GUI](https://ui.endpoints.huggingface.co/new). I’ve used the latter, which is higher.\\n- The values that I present in the table for ECS + Fargate are an underestimate, but probably not by much. I extracted them from the [fargate pricing page](https://aws.amazon.com/fargate/pricing/) and it includes just the cost of hosting the instance. I’m not including the data ingress/egress (probably the biggest thing is downloading the model from Hugging Face hub), nor have I included the costs related to ECR.\\n\\n## Other considerations\\n\\n### Deployment Options\\n\\nCurrently you can deploy an Inference Endpoint from the [GUI](https://ui.endpoints.huggingface.co/new) or using a [RESTful API](https://huggingface.co/docs/inference-endpoints/api_reference). You can also make use of our command line tool [hugie](https://github.com/MantisAI/hfie) (which will be the subject of a future blog) to launch Inference Endpoints in one line of code by passing a configuration, it’s really this simple:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 7064}, page_content='```bash\\nhugie endpoint create example/development.json'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 7119}, page_content='```\\n\\nFor me, what’s lacking is a [custom terraform provider](https://www.hashicorp.com/blog/writing-custom-terraform-providers). It’s all well and good deploying an inference endpoint from a [GitHub action](https://github.com/features/actions) using hugie, as we do, but it would be better if we could use the awesome state machine that is terraform to keep track of these. I’m pretty sure that someone (if not Hugging Face) will write one soon enough — if not, we will.\\n\\n### Hosting multiple models on a single endpoint\\n\\nPhilipp Schmid posted a really nice blog about how to write a custom [Endpoint Handler](https://www.philschmid.de/multi-model-inference-endpoints) class to allow you to host multiple models on a single endpoint, potentially saving you quite a bit of money. His blog was about GPU inference, and the only real limitation is how many models you can fit into the GPU memory. I assume this will also work for CPU instances, though I’ve not tried yet.\\n\\n## To conclude…\\n\\nWe find Hugging Face Inference Endpoints to be a very simple and convenient way to deploy transformer (and [sklearn](https://huggingface.co/scikit-learn)) models into an endpoint so they can be consumed by an application. Whilst they cost a little more than the ECS approach we were using before, it’s well worth it because it saves us time on thinking about deployment, we can concentrate on the thing we want to: building NLP solutions for our clients to help solve their problems.\\n\\n_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!_\\n\\n_This article was originally published on February 15, 2023 [in Medium](https://medium.com/mantisnlp/why-were-switching-to-hugging-face-inference-endpoints-and-maybe-you-should-too-829371dcd330)._'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 0}, page_content='--\\ntitle: ROUGE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for\\n  evaluating automatic summarization and machine translation software in natural language processing.\\n  The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\\n  \\n  Note that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\\n  \\n  This metrics is a wrapper around Google Research reimplementation of ROUGE:\\n  https://github.com/google-research/google-research/tree/master/rouge\\n---\\n\\n# Metric Card for ROUGE\\n\\n## Metric Description\\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\\n\\nNote that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\\n\\nThis metrics is a wrapper around the [Google Research reimplementation of ROUGE](https://github.com/google-research/google-research/tree/master/rouge)\\n\\n## How to Use\\nAt minimum, this metric takes as input a list of predictions and a list of references:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello there\", \"general kenobi\"]\\n>>> references = [\"hello there\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references)\\n>>> print(results)\\n{\\'rouge1\\': 1.0, \\'rouge2\\': 1.0, \\'rougeL\\': 1.0, \\'rougeLsum\\': 1.0}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 1931}, page_content='```\\n\\nOne can also pass a custom tokenizer which is especially useful for non-latin languages.\\n```python\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n                            tokenizer=lambda x: x.split())\\n>>> print(results)\\n{\\'rouge1\\': 1.0, \\'rouge2\\': 1.0, \\'rougeL\\': 1.0, \\'rougeLsum\\': 1.0}\\n```\\n\\nIt can also deal with lists of references for each predictions:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello there\", \"general kenobi\"]\\n>>> references = [[\"hello\", \"there\"], [\"general kenobi\", \"general yoda\"]]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references)\\n>>> print(results)\\n{\\'rouge1\\': 0.8333, \\'rouge2\\': 0.5, \\'rougeL\\': 0.8333, \\'rougeLsum\\': 0.8333}```'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 2716}, page_content='```\\n```\\n\\n### Inputs\\n- **predictions** (`list`): list of predictions to score. Each prediction\\n        should be a string with tokens separated by spaces.\\n- **references** (`list` or `list[list]`): list of reference for each prediction or a list of several references per prediction. Each\\n        reference should be a string with tokens separated by spaces.\\n- **rouge_types** (`list`): A list of rouge types to calculate. Defaults to `[\\'rouge1\\', \\'rouge2\\', \\'rougeL\\', \\'rougeLsum\\']`.\\n    - Valid rouge types:\\n        - `\"rouge1\"`: unigram (1-gram) based scoring\\n        - `\"rouge2\"`: bigram (2-gram) based scoring\\n        - `\"rougeL\"`: Longest common subsequence based scoring.\\n        - `\"rougeLSum\"`: splits text using `\"\\\\n\"`\\n        - See [here](https://github.com/huggingface/datasets/issues/617) for more information\\n- **use_aggregator** (`boolean`): If True, returns aggregates. Defaults to `True`.\\n- **use_stemmer** (`boolean`): If `True`, uses Porter stemmer to strip word suffixes. Defaults to `False`.\\n\\n### Output Values\\nThe output is a dictionary with one entry for each rouge type in the input list `rouge_types`. If `use_aggregator=False`, each dictionary entry is a list of scores, with one score for each sentence. E.g. if `rouge_types=[\\'rouge1\\', \\'rouge2\\']` and `use_aggregator=False`, the output is:\\n\\n```python\\n{\\'rouge1\\': [0.6666666666666666, 1.0], \\'rouge2\\': [0.0, 1.0]}\\n```\\n\\nIf `rouge_types=[\\'rouge1\\', \\'rouge2\\']` and `use_aggregator=True`, the output is of the following format:\\n```python\\n{\\'rouge1\\': 1.0, \\'rouge2\\': 1.0}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 4250}, page_content='```\\n\\nThe ROUGE values are in the range of 0 to 1.\\n\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nAn example without aggregation:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello goodbye\", \"ankh morpork\"]\\n>>> references = [\"goodbye\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n...                         use_aggregator=False)\\n>>> print(list(results.keys()))\\n[\\'rouge1\\', \\'rouge2\\', \\'rougeL\\', \\'rougeLsum\\']\\n>>> print(results[\"rouge1\"])\\n[0.5, 0.0]\\n```\\n\\nThe same example, but with aggregation:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello goodbye\", \"ankh morpork\"]\\n>>> references = [\"goodbye\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n...                         use_aggregator=True)\\n>>> print(list(results.keys()))\\n[\\'rouge1\\', \\'rouge2\\', \\'rougeL\\', \\'rougeLsum\\']\\n>>> print(results[\"rouge1\"])\\n0.25\\n```\\n\\nThe same example, but only calculating `rouge_1`:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello goodbye\", \"ankh morpork\"]\\n>>> references = [\"goodbye\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n...                         rouge_types=[\\'rouge_1\\'],\\n...                         use_aggregator=True)\\n>>> print(list(results.keys()))\\n[\\'rouge1\\']\\n>>> print(results[\"rouge1\"])\\n0.25'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 5729}, page_content='```\\n\\n## Limitations and Bias\\nSee [Schluter (2017)](https://aclanthology.org/E17-2007/) for an in-depth discussion of many of ROUGE\\'s limits.\\n\\n## Citation\\n```bibtex\\n@inproceedings{lin-2004-rouge,\\n    title = \"{ROUGE}: A Package for Automatic Evaluation of Summaries\",\\n    author = \"Lin, Chin-Yew\",\\n    booktitle = \"Text Summarization Branches Out\",\\n    month = jul,\\n    year = \"2004\",\\n    address = \"Barcelona, Spain\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://www.aclweb.org/anthology/W04-1013\",\\n    pages = \"74--81\",\\n}\\n```\\n\\n## Further References\\n- This metrics is a wrapper around the [Google Research reimplementation of ROUGE](https://github.com/google-research/google-research/tree/master/rouge)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# AudioLDM\\n\\nAudioLDM was proposed in [AudioLDM: Text-to-Audio Generation with Latent Diffusion Models](https://huggingface.co/papers/2301.12503) by Haohe Liu et al. Inspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview), AudioLDM\\nis a text-to-audio _latent diffusion model (LDM)_ that learns continuous audio representations from [CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)\\nlatents. AudioLDM takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional\\nsound effects, human speech and music.\\n\\nThe abstract from the paper is:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 1199}, page_content='The abstract from the paper is:\\n\\n*Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at [this https URL](https://audioldm.github.io/).*\\n\\nThe original codebase can be found at [haoheliu/AudioLDM](https://github.com/haoheliu/AudioLDM).\\n\\n## Tips\\n\\nWhen constructing a prompt, keep in mind:\\n\\n* Descriptive prompt inputs work best; you can use adjectives to describe the sound (for example, \"high quality\" or \"clear\") and make the prompt context specific (for example, \"water stream in a forest\" instead of \"stream\").\\n* It\\'s best to use general terms like \"cat\" or \"dog\" instead of specific names or abstract objects the model may not be familiar with.\\n\\nDuring inference:\\n\\n* The _quality_ of the predicted audio sample can be controlled by the `num_inference_steps` argument; higher steps give higher quality audio at the expense of slower inference.\\n* The _length_ of the predicted audio sample can be controlled by varying the `audio_length_in_s` argument.\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 3210}, page_content='<Tip>\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## AudioLDMPipeline\\n[[autodoc]] AudioLDMPipeline\\n\\t- all\\n\\t- __call__\\n\\n## AudioPipelineOutput\\n[[autodoc]] pipelines.AudioPipelineOutput'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter2/04c_character-based-tokenizers.md', 'start_index': 0}, page_content='efore diving in character-based tokenization, understanding why this kind of tokenization is interesting requires understanding the flaws of word-based tokenization. If you haven\\'t seen the first video on word-based tokenization we recommend you check it out before looking at this video. Let\\'s take a look at character-based tokenization. We now split our text into individual characters, rather than words. There are generally a lot of different words in languages, while the number of characters stays low. Here for example, for the English language that has an estimated 170,000 different words, we would need a very large vocabulary to encompass all words. With a character-based vocabulary, we can get by with only 256 characters! Even languages with a lot of different characters like the Chinese languages have dictionaries with ~20,000 different characters but more than 375,000 different words. Character-based vocabularies let us fewer different tokens than the word-based tokenization dictionaries we would otherwise use. These vocabularies are also more complete than their word-based vocabularies counterparts. As our vocabulary contains all characters used in a language, even words unseen during the tokenizer training can still be tokenized, so out-of-vocabulary tokens will be less frequent. This includes the ability to correctly tokenize misspelled words, rather than discarding them as unknown straight away. However, this algorithm isn\\'t perfect either! Intuitively, characters do not hold as much information individually as a word would hold. For example, \"Let\\'s\" holds more information than \"l\". Of course, this is not true for all languages, as some languages like ideogram-based languages have a lot of information held in single characters, but for others like roman-based languages, the model will have to make sense of multiple tokens at a time to get the information held in a single word. This leads to another issue with character-based tokenizers: their sequences are translated into very large amount of tokens to be processed by the model. This can have an impact on the size of the context the model will carry around, and will reduce the size of the text we can use as input for our model. This tokenization, while it has some issues, has seen some very good results in the past and should be considered when approaching a new problem as it solves some issues encountered in the word-based algorithm.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 1}, page_content=\"Hands-on\\n\\nNow that you learned the basics of multi-agents, you're ready to train your first agents in a multi-agent system: **a 2vs2 soccer team that needs to beat the opponent team**.\\n\\nAnd you’re going to participate in AI vs. AI challenges where your trained agent will compete against other classmates’ **agents every day and be ranked on a new leaderboard.**\\n\\nTo validate this hands-on for the certification process, you just need to push a trained model. There **are no minimal results to attain to validate it.**\\n\\nFor more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)\\n\\nThis hands-on will be different since to get correct results **you need to train your agents from 4 hours to 8 hours**. And given the risk of timeout in Colab, we advise you to train on your computer. You don’t need a supercomputer: a simple laptop is good enough for this exercise.\\n\\nLet's get started! 🔥\\n\\n## What is AI vs. AI?\\n\\nAI vs. AI is an open-source tool we developed at Hugging Face to compete agents on the Hub against one another in a multi-agent setting. These models are then ranked in a leaderboard.\\n\\nThe idea of this tool is to have a robust evaluation tool: **by evaluating your agent with a lot of others, you’ll get a good idea of the quality of your policy.**\\n\\nMore precisely, AI vs. AI is three tools:\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 1443}, page_content=\"More precisely, AI vs. AI is three tools:\\n\\n- A *matchmaking process* defining the matches (which model against which) and running the model fights using a background task in the Space.\\n- A *leaderboard* getting the match history results and displaying the models’ ELO ratings: [https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos](https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos)\\n- A *Space demo* to visualize your agents playing against others: [https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos)\\n\\nIn addition to these three tools, your classmate cyllum created a 🤗 SoccerTwos Challenge Analytics where you can check the detailed match results of a model: [https://huggingface.co/spaces/cyllum/soccertwos-analytics](https://huggingface.co/spaces/cyllum/soccertwos-analytics)\\n\\nWe're [wrote a blog post to explain this AI vs. AI tool in detail](https://huggingface.co/blog/aivsai), but to give you the big picture it works this way:\\n\\n- Every four hours, our algorithm **fetches all the available models for a given environment (in our case ML-Agents-SoccerTwos).**\\n- It creates a **queue of matches with the matchmaking algorithm.**\\n- We simulate the match in a Unity headless process and **gather the match result** (1 if the first model won, 0.5 if it’s a draw, 0 if the second model won) in a Dataset.\\n- Then, when all matches from the matches queue are done, **we update the ELO score for each model and update the leaderboard.**\\n\\n### Competition Rules\\n\\nThis first AI vs. AI competition **is an experiment**: the goal is to improve the tool in the future with your feedback. So some **breakups can happen during the challenge**. But don't worry\\n**all the results are saved in a dataset so we can always restart the calculation correctly without losing information**.\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 3317}, page_content=\"In order for your model to get correctly evaluated against others you need to follow these rules:\\n\\n1. **You can't change the observation space or action space of the agent.** By doing that your model will not work during evaluation.\\n2. You **can't use a custom trainer for now,** you need to use the Unity MLAgents ones.\\n3. We provide executables to train your agents. You can also use the Unity Editor if you prefer **, but to avoid bugs, we advise that you use our executables**.\\n\\nWhat will make the difference during this challenge are **the hyperparameters you choose**.\\n\\nWe're constantly trying to improve our tutorials, so\\xa0**if you find some issues in this notebook**, please\\xa0[open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).\\n\\n### Chat with your classmates, share advice and ask questions on Discord\\n\\n- We created a new channel called `ai-vs-ai-challenge` to exchange advice and ask questions.\\n- If you didn’t join the discord server yet, you can [join here](https://discord.gg/ydHrjt3WP5)\\n\\n## Step 0: Install MLAgents and download the correct executable\\n\\nWe advise you to use [conda](https://docs.conda.io/en/latest/) as a package manager and create a new environment.\\n\\nWith conda, we create a new environment called rl with **Python 3.10.12**:\\n\\n```bash\\nconda create --name rl python=3.10.12\\nconda activate rl\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 4673}, page_content='```\\n\\nTo be able to train our agents correctly and push to the Hub, we need to install ML-Agents\\n\\n```bash\\ngit clone https://github.com/Unity-Technologies/ml-agents\\n```\\n\\nWhen the cloning is done (it takes 2.63 GB), we go inside the repository and install the package\\n\\n```bash\\ncd ml-agents\\npip install -e ./ml-agents-envs\\npip install -e ./ml-agents'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 5019}, page_content='```\\n\\nFinally, you need to install git-lfs: https://git-lfs.com/\\n\\nNow that it’s installed, we need to add the environment training executable. Based on your operating system you need to download one of them, unzip it and place it in a new folder inside `ml-agents` that you call `training-envs-executables`\\n\\nAt the end your executable should be in `ml-agents/training-envs-executables/SoccerTwos`\\n\\nWindows: Download [this executable](https://drive.google.com/file/d/1sqFxbEdTMubjVktnV4C6ICjp89wLhUcP/view?usp=sharing)\\n\\nLinux (Ubuntu): Download [this executable](https://drive.google.com/file/d/1KuqBKYiXiIcU4kNMqEzhgypuFP5_45CL/view?usp=sharing)\\n\\nMac: Download [this executable](https://drive.google.com/drive/folders/1h7YB0qwjoxxghApQdEUQmk95ZwIDxrPG?usp=share_link)\\n⚠ For Mac you need also to call this `xattr -cr training-envs-executables/SoccerTwos/SoccerTwos.app` to be able to run SoccerTwos\\n\\n## Step 1: Understand the environment\\n\\nThe environment is called `SoccerTwos`. The Unity MLAgents Team made it. You can find its documentation [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md#soccer-twos)\\n\\nThe goal in this environment **is to get the ball into the opponent\\'s goal while preventing the ball from entering your own goal.**\\n\\n<figure>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif\" alt=\"SoccerTwos\"/>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 6464}, page_content='<figcaption>This environment was made by the <a href=\"https://github.com/Unity-Technologies/ml-agents\"> Unity MLAgents Team</a></figcaption>\\n\\n</figure>\\n\\n### The reward function\\n\\nThe reward function is:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccerreward.png\" alt=\"SoccerTwos Reward\"/>\\n\\n### The observation space\\n\\nThe observation space is composed of vectors of size 336:\\n\\n- 11 ray-casts forward distributed over 120 degrees (264 state dimensions)\\n- 3 ray-casts backward distributed over 90 degrees (72 state dimensions)\\n- Both of these ray-casts can detect 6 objects:\\n    - Ball\\n    - Blue Goal\\n    - Purple Goal\\n    - Wall\\n    - Blue Agent\\n    - Purple Agent\\n\\n### The action space\\n\\nThe action space is three discrete branches:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/socceraction.png\" alt=\"SoccerTwos Action\"/>\\n\\n## Step 2: Understand MA-POCA\\n\\nWe know how to train agents to play against others: **we can use self-play.** This is a perfect technique for a 1vs1.\\n\\nBut in our case we’re 2vs2, and each team has 2 agents. How then can we **train cooperative behavior for groups of agents?**\\n\\nAs explained in the [Unity Blog](https://blog.unity.com/technology/ml-agents-v20-release-now-supports-training-complex-cooperative-behaviors), agents typically receive a reward as a group (+1 - penalty) when the team scores a goal. This implies that **every agent on the team is rewarded even if each agent didn’t contribute the same to the win**, which makes it difficult to learn what to do independently.\\n\\nThe Unity MLAgents team developed the solution in a new multi-agent trainer called *MA-POCA (Multi-Agent POsthumous Credit Assignment)*.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': -1}, page_content='The Unity MLAgents team developed the solution in a new multi-agent trainer called *MA-POCA (Multi-Agent POsthumous Credit Assignment)*.\\n\\nThe idea is simple but powerful: a centralized critic **processes the states of all agents in the team to estimate how well each agent is doing**. Think of this critic as a coach.\\n\\nThis allows each agent to **make decisions based only on what it perceives locally**, and **simultaneously evaluate how good its behavior is in the context of the whole group**.\\n\\n\\n<figure>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/mapoca.png\" alt=\"MA POCA\"/>\\n\\n<figcaption>This illustrates MA-POCA’s centralized learning and decentralized execution. Source: <a href=\"https://blog.unity.com/technology/ml-agents-plays-dodgeball\">MLAgents Plays Dodgeball</a>\\n</figcaption>\\n\\n</figure>\\n\\nThe solution then is to use Self-Play with an MA-POCA trainer (called poca). The poca trainer will help us to train cooperative behavior and self-play to win against an opponent team.\\n\\nIf you want to dive deeper into this MA-POCA algorithm, you need to read the paper they published [here](https://arxiv.org/pdf/2111.05992.pdf) and the sources we put on the additional readings section.\\n\\n## Step 3: Define the config file\\n\\nWe already learned in [Unit 5](https://huggingface.co/deep-rl-course/unit5/introduction) that in ML-Agents, you define **the training hyperparameters in `config.yaml` files.**\\n\\nThere are multiple hyperparameters. To understand them better, you should read the explanations for each of them in\\xa0**[the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)**'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 9809}, page_content='The config file we’re going to use here is in  `./config/poca/SoccerTwos.yaml`. It looks like this:\\n\\n```csharp\\nbehaviors:\\n  SoccerTwos:\\n    trainer_type: poca\\n    hyperparameters:\\n      batch_size: 2048\\n      buffer_size: 20480\\n      learning_rate: 0.0003\\n      beta: 0.005\\n      epsilon: 0.2\\n      lambd: 0.95\\n      num_epoch: 3\\n      learning_rate_schedule: constant\\n    network_settings:\\n      normalize: false\\n      hidden_units: 512\\n      num_layers: 2\\n      vis_encode_type: simple\\n    reward_signals:\\n      extrinsic:\\n        gamma: 0.99\\n        strength: 1.0\\n    keep_checkpoints: 5\\n    max_steps: 5000000\\n    time_horizon: 1000\\n    summary_freq: 10000\\n    self_play:\\n      save_steps: 50000\\n      team_change: 200000\\n      swap_steps: 2000\\n      window: 10\\n      play_against_latest_model_ratio: 0.5\\n      initial_elo: 1200.0'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 10644}, page_content='```\\n\\nCompared to Pyramids or SnowballTarget, we have new hyperparameters with a self-play part. How you modify them can be critical in getting good results.\\n\\nThe advice I can give you here is to check the explanation and recommended value for each parameters (especially self-play ones) against\\xa0**[the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md).**\\n\\nNow that you’ve modified our config file, you’re ready to train your agents.\\n\\n## Step 4: Start the training\\n\\nTo train the agents, we need to\\xa0**launch mlagents-learn and select the executable containing the environment.**\\n\\nWe define four parameters:\\n\\n1. `mlagents-learn <config>`: the path where the hyperparameter config file is.\\n2. `-env`: where the environment executable is.\\n3. `-run_id`: the name you want to give to your training run id.\\n4. `-no-graphics`: to not launch the visualization during the training.\\n\\nDepending on your hardware, 5M timesteps (the recommended value, but you can also try 10M) will take 5 to 8 hours of training. You can continue using your computer in the meantime, but I advise deactivating the computer standby mode to prevent the training from being stopped.\\n\\nDepending on the executable you use (windows, ubuntu, mac) the training command will look like this (your executable path can be different so don’t hesitate to check before running).\\n\\n```bash\\nmlagents-learn ./config/poca/SoccerTwos.yaml --env=./training-envs-executables/SoccerTwos.exe --run-id=\"SoccerTwos\" --no-graphics'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 12187}, page_content='```\\n\\nThe executable contains 8 copies of SoccerTwos.\\n\\n⚠️ It’s normal if you don’t see a big increase of ELO score (and even a decrease below 1200) before 2M timesteps, since your agents will spend most of their time moving randomly on the field before being able to goal.\\n\\n⚠️ You can stop the training with Ctrl + C but beware of typing this command only once to stop the training since MLAgents needs to generate a final .onnx file before closing the run.\\n\\n## Step 5: **Push the agent to the Hugging Face Hub**\\n\\nNow that we trained our agents, we’re\\xa0**ready to push them to the Hub to be able to participate in the AI vs. AI challenge and visualize them playing on your browser🔥.**\\n\\nTo be able to share your model with the community, there are three more steps to follow:\\n\\n1️⃣ (If it’s not already done) create an account to HF ➡\\xa0[https://huggingface.co/join](https://huggingface.co/join)\\n\\n2️⃣ Sign in and store your authentication token from the Hugging Face website.\\n\\nCreate a new token (https://huggingface.co/settings/tokens)\\xa0**with write role**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\\n\\nCopy the token, run this, and paste the token\\n\\n```bash\\nhuggingface-cli login'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 13467}, page_content='```\\n\\nThen, we need to run `mlagents-push-to-hf`.\\n\\nAnd we define four parameters:\\n\\n1. `-run-id`: the name of the training run id.\\n2. `-local-dir`: where the agent was saved, it’s results/<run_id name>, so in my case results/First Training.\\n3. `-repo-id`: the name of the Hugging Face repo you want to create or update. It’s always <your huggingface username>/<the repo name>\\nIf the repo does not exist **it will be created automatically**\\n4. `--commit-message`: since HF repos are git repositories you need to give a commit message.\\n\\nIn my case\\n\\n```bash\\nmlagents-push-to-hf  --run-id=\"SoccerTwos\" --local-dir=\"./results/SoccerTwos\" --repo-id=\"ThomasSimonini/poca-SoccerTwos\" --commit-message=\"First Push\"`\\n```\\n\\n```bash\\nmlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id --commit-message=\"First Push\"'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 14322}, page_content='```\\n\\nIf everything worked you should see this at the end of the process (but with a different url 😆) :\\n\\nYour model is pushed to the Hub. You can view your model here: https://huggingface.co/ThomasSimonini/poca-SoccerTwos\\n\\nIt\\'s the link to your model. It contains a model card that explains how to use it, your Tensorboard, and your config file. **What\\'s awesome is that it\\'s a git repository, which means you can have different commits, update your repository with a new push, etc.**\\n\\n## Step 6: Verify that your model is ready for AI vs AI Challenge\\n\\nNow that your model is pushed to the Hub, **it’s going to be added automatically to the AI vs AI Challenge model pool.** It can take a little bit of time before your model is added to the leaderboard given we do a run of matches every 4h.\\n\\nBut to ensure that everything works perfectly you need to check:\\n\\n1. That you have this tag in your model: ML-Agents-SoccerTwos. This is the tag we use to select models to be added to the challenge pool. To do that go to your model and check the tags\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify1.png\" alt=\"Verify\"/>\\n\\n\\nIf it’s not the case you just need to modify the readme and add it\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify2.png\" alt=\"Verify\"/>\\n\\n2. That you have a `SoccerTwos.onnx` file\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify3.png\" alt=\"Verify\"/>\\n\\nWe strongly suggest that you create a new model when you push to the Hub if you want to train it again or train a new version.\\n\\n## Step 7: Visualize some match in our demo'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': -1}, page_content=\"We strongly suggest that you create a new model when you push to the Hub if you want to train it again or train a new version.\\n\\n## Step 7: Visualize some match in our demo\\n\\nNow that your model is part of AI vs AI Challenge, **you can visualize how good it is compared to others**: https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos\\n\\nIn order to do that, you just need to go to this demo:\\n\\n- Select your model as team blue (or team purple if you prefer) and another model to compete against. The best opponents to compare your model to are either whoever is on top of the leaderboard or the [baseline model](https://huggingface.co/unity/MLAgents-SoccerTwos)\\n\\nThe matches you see live are not used in the calculation of your result **but they are a good way to visualize how good your agent is**.\\n\\nAnd don't hesitate to share the best score your agent gets on discord in the #rl-i-made-this channel 🔥\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/sales_projections/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: sales_projections\\n\\n\\n```\\n!pip install -q gradio pandas numpy matplotlib\\n```\\n\\n\\n```\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nimport gradio as gr\\n\\n\\ndef sales_projections(employee_data):\\n    sales_data = employee_data.iloc[:, 1:4].astype(\"int\").to_numpy()\\n    regression_values = np.apply_along_axis(\\n        lambda row: np.array(np.poly1d(np.polyfit([0, 1, 2], row, 2))), 0, sales_data\\n    )\\n    projected_months = np.repeat(\\n        np.expand_dims(np.arange(3, 12), 0), len(sales_data), axis=0\\n    )\\n    projected_values = np.array(\\n        [\\n            month * month * regression[0] + month * regression[1] + regression[2]\\n            for month, regression in zip(projected_months, regression_values)\\n        ]\\n    )\\n    plt.plot(projected_values.T)\\n    plt.legend(employee_data[\"Name\"])\\n    return employee_data, plt.gcf(), regression_values\\n\\n\\ndemo = gr.Interface(\\n    sales_projections,\\n    gr.Dataframe(\\n        headers=[\"Name\", \"Jan Sales\", \"Feb Sales\", \"Mar Sales\"],\\n        value=[[\"Jon\", 12, 14, 18], [\"Alice\", 14, 17, 2], [\"Sana\", 8, 9.5, 12]],\\n    ),\\n    [\"dataframe\", \"plot\", \"numpy\"],\\n    description=\"Enter sales figures for employees to predict sales trajectory over year.\",\\n)\\nif __name__ == \"__main__\":\\n    demo.launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 1}, page_content='Metric Card for F1\\n\\n\\n## Metric Description\\n\\nThe F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\\nF1 = 2 * (precision * recall) / (precision + recall)\\n\\n\\n## How to Use\\n\\nAt minimum, this metric requires predictions and references as input\\n\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(predictions=[0, 1], references=[0, 1])\\n>>> print(results)\\n[\"{\\'f1\\': 1.0}\"]'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 445}, page_content=\"```\\n\\n\\n### Inputs\\n- **predictions** (`list` of `int`): Predicted labels.\\n- **references** (`list` of `int`): Ground truth labels.\\n- **labels** (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`, and the order of the labels if `average` is `None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\\n- **pos_label** (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\\n- **average** (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\\n    - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\\n    - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\\n    - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\\n    - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. This option can result in an F-score that is not between precision and recall.\\n    - 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\\n- **sample_weight** (`list` of `float`): Sample weights Defaults to None.\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 2353}, page_content=\"### Output Values\\n- **f1**(`float` or `array` of `float`): F1 score or list of f1 scores, depending on the value passed to `average`. Minimum possible value is 0. Maximum possible value is 1. Higher f1 scores are better.\\n\\nOutput Example(s):\\n```python\\n{'f1': 0.26666666666666666}\"),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 2632}, page_content='```\\n```python\\n{\\'f1\\': array([0.8, 0.0, 0.0])}\\n```\\n\\nThis metric outputs a dictionary, with either a single f1 score, of type `float`, or an array of f1 scores, with entries of type `float`.\\n\\n\\n#### Values from Popular Papers\\n\\n\\n\\n\\n### Examples\\n\\nExample 1-A simple binary example\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])\\n>>> print(results)\\n{\\'f1\\': 0.5}\\n```\\n\\nExample 2-The same simple binary example as in Example 1, but with `pos_label` set to `0`.\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)\\n>>> print(round(results[\\'f1\\'], 2))\\n0.67\\n```\\n\\nExample 3-The same simple binary example as in Example 1, but with `sample_weight` included.\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])\\n>>> print(round(results[\\'f1\\'], 2))\\n0.35'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 3691}, page_content='```\\n\\nExample 4-A multiclass example, with different values for the `average` input.\\n```python\\n>>> predictions = [0, 2, 1, 0, 0, 1]\\n>>> references = [0, 1, 2, 0, 1, 2]\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=\"macro\")\\n>>> print(round(results[\\'f1\\'], 2))\\n0.27\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=\"micro\")\\n>>> print(round(results[\\'f1\\'], 2))\\n0.33\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\\n>>> print(round(results[\\'f1\\'], 2))\\n0.27\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=None)\\n>>> print(results)\\n{\\'f1\\': array([0.8, 0. , 0. ])}\\n```\\n\\n\\n## Limitations and Bias\\n\\n\\n\\n## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n    title={Scikit-learn: Machine Learning in {P}ython},\\n    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n           and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n           and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n           Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n    journal={Journal of Machine Learning Research},\\n    volume={12},\\n    pages={2825--2830},\\n    year={2011}\\n}\\n```\\n\\n\\n## Further References'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# TimeSformer\\n\\n## Overview\\n\\nThe TimeSformer model was proposed in [TimeSformer: Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Facebook Research.\\nThis work is a milestone in action-recognition field being the first video transformer. It inspired many transformer based video understanding and classification papers.\\n\\nThe abstract from the paper is the following:\\n\\n*We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named \"TimeSformer,\" adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that \"divided attention,\" where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: [this https URL](https://github.com/facebookresearch/TimeSformer).*'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 2281}, page_content='This model was contributed by [fcakyon](https://huggingface.co/fcakyon).\\nThe original code can be found [here](https://github.com/facebookresearch/TimeSformer).\\n\\n## Usage tips\\n\\nThere are many pretrained variants. Select your pretrained model based on the dataset it is trained on. Moreover,\\nthe number of input frames per clip changes based on the model size so you should consider this parameter while selecting your pretrained model.\\n\\n## Resources\\n\\n- [Video classification task guide](../tasks/video_classification)\\n\\n## TimesformerConfig\\n\\n[[autodoc]] TimesformerConfig\\n\\n## TimesformerModel\\n\\n[[autodoc]] TimesformerModel\\n    - forward\\n\\n## TimesformerForVideoClassification\\n\\n[[autodoc]] TimesformerForVideoClassification\\n    - forward'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Swin Transformer V2\\n\\n## Overview\\n\\nThe Swin Transformer V2 model was proposed in [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\\n\\nThe abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 1059}, page_content='The abstract from the paper is the following:\\n\\n*Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google\\'s billion-level visual models, which consumes 40 times less labelled data and 40 times less training time.*\\n\\nThis model was contributed by [nandwalritik](https://huggingface.co/nandwalritik).\\nThe original code can be found [here](https://github.com/microsoft/Swin-Transformer).\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Swin Transformer v2.\\n\\n<PipelineTag pipeline=\"image-classification\"/>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': -1}, page_content='## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Swin Transformer v2.\\n\\n<PipelineTag pipeline=\"image-classification\"/>\\n\\n- [`Swinv2ForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\\n- See also: [Image classification task guide](../tasks/image_classification)\\n\\nBesides that:\\n\\n- [`Swinv2ForMaskedImageModeling`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).\\n\\nIf you\\'re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\\'ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n## Swinv2Config\\n\\n[[autodoc]] Swinv2Config\\n\\n## Swinv2Model\\n\\n[[autodoc]] Swinv2Model\\n    - forward\\n\\n## Swinv2ForMaskedImageModeling\\n\\n[[autodoc]] Swinv2ForMaskedImageModeling\\n    - forward\\n\\n## Swinv2ForImageClassification\\n\\n[[autodoc]] transformers.Swinv2ForImageClassification\\n    - forward'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# RemBERT\\n\\n## Overview\\n\\nThe RemBERT model was proposed in [Rethinking Embedding Coupling in Pre-trained Language Models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, Sebastian Ruder.\\n\\nThe abstract from the paper is the following:\\n\\n*We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art\\npre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to\\nsignificantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By\\nreallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on\\nstandard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that\\nallocating additional capacity to the output embedding provides benefits to the model that persist through the\\nfine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger\\noutput embeddings prevent the model\\'s last layers from overspecializing to the pre-training task and encourage\\nTransformer representations to be more general and more transferable to other tasks and languages. Harnessing these\\nfindings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\\nnumber of parameters at the fine-tuning stage.*\\n\\n## Usage tips'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 2232}, page_content='## Usage tips\\n\\nFor fine-tuning, RemBERT can be thought of as a bigger version of mBERT with an ALBERT-like factorization of the\\nembedding layer. The embeddings are not tied in pre-training, in contrast with BERT, which enables smaller input\\nembeddings (preserved during fine-tuning) and bigger output embeddings (discarded at fine-tuning). The tokenizer is\\nalso similar to the Albert one rather than the BERT one.\\n\\n## Resources\\n\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\\n- [Multiple choice task guide](../tasks/multiple_choice)\\n\\n## RemBertConfig\\n\\n[[autodoc]] RemBertConfig\\n\\n## RemBertTokenizer\\n\\n[[autodoc]] RemBertTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## RemBertTokenizerFast\\n\\n[[autodoc]] RemBertTokenizerFast\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n<frameworkcontent>\\n<pt>\\n\\n## RemBertModel\\n\\n[[autodoc]] RemBertModel\\n    - forward\\n\\n## RemBertForCausalLM\\n\\n[[autodoc]] RemBertForCausalLM\\n    - forward\\n\\n## RemBertForMaskedLM\\n\\n[[autodoc]] RemBertForMaskedLM\\n    - forward\\n\\n## RemBertForSequenceClassification\\n\\n[[autodoc]] RemBertForSequenceClassification\\n    - forward\\n\\n## RemBertForMultipleChoice\\n\\n[[autodoc]] RemBertForMultipleChoice\\n    - forward\\n\\n## RemBertForTokenClassification'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': -1}, page_content='## RemBertForMultipleChoice\\n\\n[[autodoc]] RemBertForMultipleChoice\\n    - forward\\n\\n## RemBertForTokenClassification\\n\\n[[autodoc]] RemBertForTokenClassification\\n    - forward\\n\\n## RemBertForQuestionAnswering\\n\\n[[autodoc]] RemBertForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFRemBertModel\\n\\n[[autodoc]] TFRemBertModel\\n    - call\\n\\n## TFRemBertForMaskedLM\\n\\n[[autodoc]] TFRemBertForMaskedLM\\n    - call\\n\\n## TFRemBertForCausalLM\\n\\n[[autodoc]] TFRemBertForCausalLM\\n    - call\\n\\n## TFRemBertForSequenceClassification\\n\\n[[autodoc]] TFRemBertForSequenceClassification\\n    - call\\n\\n## TFRemBertForMultipleChoice\\n\\n[[autodoc]] TFRemBertForMultipleChoice\\n    - call\\n\\n## TFRemBertForTokenClassification\\n\\n[[autodoc]] TFRemBertForTokenClassification\\n    - call\\n\\n## TFRemBertForQuestionAnswering\\n\\n[[autodoc]] TFRemBertForQuestionAnswering\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n[[open-in-colab]]\\n\\n# Performing inference with LCM-LoRA\\n\\nLatent Consistency Models (LCM) enable quality image generation in typically 2-4 steps making it possible to use diffusion models in almost real-time settings. \\n\\nFrom the [official website](https://latent-consistency-models.github.io/):\\n\\n> LCMs can be distilled from any pre-trained Stable Diffusion (SD) in only 4,000 training steps (~32 A100 GPU Hours) for generating high quality 768 x 768 resolution images in 2~4 steps or even one step, significantly accelerating text-to-image generation. We employ LCM to distill the Dreamshaper-V7 version of SD in just 4,000 training iterations.\\n\\nFor a more technical overview of LCMs, refer to [the paper](https://huggingface.co/papers/2310.04378).\\n\\nHowever, each model needs to be distilled separately for latent consistency distillation. The core idea with LCM-LoRA is to train just a few adapter layers, the adapter being LoRA in this case. \\nThis way, we don\\'t have to train the full model and keep the number of trainable parameters manageable. The resulting LoRAs can then be applied to any fine-tuned version of the model without distilling them separately.\\nAdditionally, the LoRAs can be applied to image-to-image, ControlNet/T2I-Adapter, inpainting, AnimateDiff etc. \\nThe LCM-LoRA can also be combined with other LoRAs to generate styled images in very few steps (4-8).'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 1966}, page_content=\"LCM-LoRAs are available for [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5), [stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), and the [SSD-1B](https://huggingface.co/segmind/SSD-1B) model. All the checkpoints can be found in this [collection](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6).\\n\\nFor more details about LCM-LoRA, refer to [the technical report](https://huggingface.co/papers/2311.05556).\\n\\nThis guide shows how to perform inference with LCM-LoRAs for \\n- text-to-image\\n- image-to-image\\n- combined with styled LoRAs\\n- ControlNet/T2I-Adapter\\n- inpainting\\n- AnimateDiff\\n\\nBefore going through this guide, we'll take a look at the general workflow for performing inference with LCM-LoRAs.\\nLCM-LoRAs are similar to other Stable Diffusion LoRAs so they can be used with any [`DiffusionPipeline`] that supports LoRAs.\\n\\n- Load the task specific pipeline and model.\\n- Set the scheduler to [`LCMScheduler`].\\n- Load the LCM-LoRA weights for the model.\\n- Reduce the `guidance_scale` between `[1.0, 2.0]` and set the `num_inference_steps` between [4, 8].\\n- Perform inference with the pipeline with the usual parameters.\\n\\nLet's look at how we can perform inference with LCM-LoRAs for different tasks.\\n\\nFirst, make sure you have [peft](https://github.com/huggingface/peft) installed, for better LoRA support.\\n\\n```bash\\npip install -U peft\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 3442}, page_content='```\\n\\n## Text-to-image\\n\\nYou\\'ll use the [`StableDiffusionXLPipeline`] with the scheduler: [`LCMScheduler`] and then load the LCM-LoRA. Together with the LCM-LoRA and the scheduler, the pipeline enables a fast inference workflow overcoming the slow iterative nature of diffusion models.\\n\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\\n    variant=\"fp16\",\\n    torch_dtype=torch.float16\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\\n\\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\\n\\ngenerator = torch.manual_seed(42)\\nimage = pipe(\\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\\n).images[0]'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 4339}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2i.png)\\n\\nNotice that we use only 4 steps for generation which is way less than what\\'s typically used for standard SDXL.\\n\\n<Tip>\\n\\nYou may have noticed that we set `guidance_scale=1.0`, which disables classifer-free-guidance. This is because the LCM-LoRA is trained with guidance, so the batch size does not have to be doubled in this case. This leads to a faster inference time, with the drawback that negative prompts don\\'t have any effect on the denoising process.\\n\\nYou can also use guidance with LCM-LoRA, but due to the nature of training the model is very sensitve to the `guidance_scale` values, high values can lead to artifacts in the generated images. In our experiments, we found that the best values are in the range of [1.0, 2.0].\\n\\n</Tip>\\n\\n### Inference with a fine-tuned model\\n\\nAs mentioned above, the LCM-LoRA can be applied to any fine-tuned version of the model without having to distill them separately. Let\\'s look at how we can perform inference with a fine-tuned model. In this example, we\\'ll use the [animagine-xl](https://huggingface.co/Linaqruf/animagine-xl) model, which is a fine-tuned version of the SDXL model for generating anime.\\n\\n```python\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretrained(\\n    \"Linaqruf/animagine-xl\",\\n    variant=\"fp16\",\\n    torch_dtype=torch.float16\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': -1}, page_content='# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\\n\\nprompt = \"face focus, cute, masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, night, turtleneck\"\\n\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\\n).images[0]'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 6258}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2i_finetuned.png)\\n\\n\\n## Image-to-image\\n\\nLCM-LoRA can be applied to image-to-image tasks too. Let\\'s look at how we can perform image-to-image generation with LCMs. For this example we\\'ll use the [dreamshaper-7](https://huggingface.co/Lykon/dreamshaper-7) model and the LCM-LoRA for `stable-diffusion-v1-5 `.\\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image, LCMScheduler\\nfrom diffusers.utils import make_image_grid, load_image\\n\\npipe = AutoPipelineForImage2Image.from_pretrained(\\n    \"Lykon/dreamshaper-7\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\\n\\n# prepare image\\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\\ninit_image = load_image(url)\\nprompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\\n\\n# pass prompt and image to pipeline\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt,\\n    image=init_image,\\n    num_inference_steps=4,\\n    guidance_scale=1,\\n    strength=0.6,\\n    generator=generator\\n).images[0]\\nmake_image_grid([init_image, image], rows=1, cols=2)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 7636}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_i2i.png)\\n\\n\\n<Tip>\\n\\nYou can get different results based on your prompt and the image you provide. To get the best results, we recommend trying different values for `num_inference_steps`, `strength`, and `guidance_scale` parameters and choose the best one.\\n\\n</Tip>\\n\\n\\n## Combine with styled LoRAs\\n\\nLCM-LoRA can be combined with other LoRAs to generate styled-images in very few steps (4-8). In the following example, we\\'ll use the LCM-LoRA with the [papercut LoRA](TheLastBen/Papercut_SDXL). \\nTo learn more about how to combine LoRAs, refer to [this guide](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#combine-multiple-adapters).\\n\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\\n    variant=\"fp16\",\\n    torch_dtype=torch.float16\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LoRAs\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\", adapter_name=\"lcm\")\\npipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\\n\\n# Combine LoRAs\\npipe.set_adapters([\"lcm\", \"papercut\"], adapter_weights=[1.0, 0.8])'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': -1}, page_content='# Combine LoRAs\\npipe.set_adapters([\"lcm\", \"papercut\"], adapter_weights=[1.0, 0.8])\\n\\nprompt = \"papercut, a cute fox\"\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]\\nimage'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 9170}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdx_lora_mix.png)\\n\\n\\n## ControlNet/T2I-Adapter\\n\\nLet\\'s look at how we can perform inference with ControlNet/T2I-Adapter and LCM-LoRA. \\n\\n### ControlNet\\nFor this example, we\\'ll use the SD-v1-5 model and the LCM-LoRA for SD-v1-5 with canny ControlNet.\\n\\n```python\\nimport torch\\nimport cv2\\nimport numpy as np\\nfrom PIL import Image\\n\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\\nfrom diffusers.utils import load_image\\n\\nimage = load_image(\\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\\n).resize((512, 512))\\n\\nimage = np.array(image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image)\\n\\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    controlnet=controlnet,\\n    torch_dtype=torch.float16,\\n    safety_checker=None,\\n    variant=\"fp16\"\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': -1}, page_content='# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\\n\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    \"the mona lisa\",\\n    image=canny_image,\\n    num_inference_steps=4,\\n    guidance_scale=1.5,\\n    controlnet_conditioning_scale=0.8,\\n    cross_attention_kwargs={\"scale\": 1},\\n    generator=generator,\\n).images[0]\\nmake_image_grid([canny_image, image], rows=1, cols=2)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 10909}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_controlnet.png)\\n\\n\\n<Tip>\\nThe inference parameters in this example might not work for all examples, so we recommend you to try different values for `num_inference_steps`, `guidance_scale`, `controlnet_conditioning_scale` and `cross_attention_kwargs` parameters and choose the best one. \\n</Tip>\\n\\n### T2I-Adapter\\n\\nThis example shows how to use the LCM-LoRA with the [Canny T2I-Adapter](TencentARC/t2i-adapter-canny-sdxl-1.0) and SDXL.\\n\\n```python\\nimport torch\\nimport cv2\\nimport numpy as np\\nfrom PIL import Image\\n\\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, LCMScheduler\\nfrom diffusers.utils import load_image, make_image_grid\\n\\n# Prepare image\\n# Detect the canny map in low resolution to avoid high-frequency details\\nimage = load_image(\\n    \"https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg\"\\n).resize((384, 384))\\n\\nimage = np.array(image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image).resize((1024, 1024))\\n\\n# load adapter\\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 12297}, page_content='pipe = StableDiffusionXLAdapterPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\", \\n    adapter=adapter,\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\", \\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\\n\\nprompt = \"Mystical fairy in real, magic, 4k picture, high quality\"\\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"\\n\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt,\\n    negative_prompt=negative_prompt,\\n    image=canny_image,\\n    num_inference_steps=4,\\n    guidance_scale=1.5, \\n    adapter_conditioning_scale=0.8, \\n    adapter_conditioning_factor=1,\\n    generator=generator,\\n).images[0]\\nmake_image_grid([canny_image, image], rows=1, cols=2)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 13185}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2iadapter.png)\\n\\n\\n## Inpainting\\n\\nLCM-LoRA can be used for inpainting as well. \\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\\nfrom diffusers.utils import load_image, make_image_grid\\n\\npipe = AutoPipelineForInpainting.from_pretrained(\\n    \"runwayml/stable-diffusion-inpainting\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\\n\\n# load base and mask image\\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")\\n\\n# generator = torch.Generator(\"cuda\").manual_seed(92)\\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt,\\n    image=init_image,\\n    mask_image=mask_image,\\n    generator=generator,\\n    num_inference_steps=4,\\n    guidance_scale=4, \\n).images[0]\\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 14555}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_inpainting.png)\\n\\n\\n## AnimateDiff\\n\\n[`AnimateDiff`] allows you to animate images using Stable Diffusion models. To get good results, we need to generate multiple frames (16-24), and doing this with standard SD models can be very slow. \\nLCM-LoRA can be used to speed up the process significantly, as you just need to do 4-8 steps for each frame. Let\\'s look at how we can perform animation with LCM-LoRA and AnimateDiff.\\n\\n```python\\nimport torch\\nfrom diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler\\nfrom diffusers.utils import export_to_gif\\n\\nadapter = MotionAdapter.from_pretrained(\"diffusers/animatediff-motion-adapter-v1-5\")\\npipe = AnimateDiffPipeline.from_pretrained(\\n    \"frankjoshua/toonyou_beta6\",\\n    motion_adapter=adapter,\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\", adapter_name=\"lcm\")\\npipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", weight_name=\"diffusion_pytorch_model.safetensors\", adapter_name=\"motion-lora\")\\n\\npipe.set_adapters([\"lcm\", \"motion-lora\"], adapter_weights=[0.55, 1.2])'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': -1}, page_content='pipe.set_adapters([\"lcm\", \"motion-lora\"], adapter_weights=[0.55, 1.2])\\n\\nprompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\\ngenerator = torch.manual_seed(0)\\nframes = pipe(\\n    prompt=prompt,\\n    num_inference_steps=5,\\n    guidance_scale=1.25,\\n    cross_attention_kwargs={\"scale\": 1},\\n    num_frames=24,\\n    generator=generator\\n).frames[0]\\nexport_to_gif(frames, \"animation.gif\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 16206}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_animatediff.gif)'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 1}, page_content=\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https://arxiv.org/abs/2310.04378) is a method to distill a latent diffusion model to enable swift inference with minimal steps. This example demonstrates how to use latent consistency distillation to distill SDXL for inference with few timesteps.\\n\\n## Full model distillation\\n\\n### Running locally with PyTorch\\n\\n#### Installing the dependencies\\n\\nBefore running the scripts, make sure to install the library's training dependencies:\\n\\n**Important**\\n\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install -e .\\n```\\n\\nThen cd in the example folder and run\\n```bash\\npip install -r requirements.txt\\n```\\n\\nAnd initialize an [🤗 Accelerate](https://github.com/huggingface/accelerate/) environment with:\\n\\n```bash\\naccelerate config\\n```\\n\\nOr for a default accelerate configuration without answering questions about your environment\\n\\n```bash\\naccelerate config default\\n```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom accelerate.utils import write_basic_config\\nwrite_basic_config()\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 1443}, page_content='```\\n\\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramatic speedups.\\n\\n\\n#### Example\\n\\nThe following uses the [Conceptual Captions 12M (CC12M) dataset](https://github.com/google-research-datasets/conceptual-12m) as an example, and for illustrative purposes only. For best results you may consider large and high-quality text-image datasets such as [LAION](https://laion.ai/blog/laion-400-open-dataset/). You may also need to search the hyperparameter space according to the dataset you use.\\n\\n```bash\\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\\nexport OUTPUT_DIR=\"path/to/saved/model\"'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': -1}, page_content='```bash\\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\\nexport OUTPUT_DIR=\"path/to/saved/model\"\\n\\naccelerate launch train_lcm_distill_sdxl_wds.py \\\\\\n    --pretrained_teacher_model=$MODEL_NAME \\\\\\n    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\\\\n    --output_dir=$OUTPUT_DIR \\\\\\n    --mixed_precision=fp16 \\\\\\n    --resolution=1024 \\\\\\n    --learning_rate=1e-6 --loss_type=\"huber\" --use_fix_crop_and_size --ema_decay=0.95 --adam_weight_decay=0.0 \\\\\\n    --max_train_steps=1000 \\\\\\n    --max_train_samples=4000000 \\\\\\n    --dataloader_num_workers=8 \\\\\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\\\\n    --validation_steps=200 \\\\\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\\\\n    --train_batch_size=12 \\\\\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\\\\n    --gradient_accumulation_steps=1 \\\\\\n    --use_8bit_adam \\\\\\n    --resume_from_checkpoint=latest \\\\\\n    --report_to=wandb \\\\\\n    --seed=453645634 \\\\\\n    --push_to_hub \\\\'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 3079}, page_content='```\\n\\n## LCM-LoRA\\n\\nInstead of fine-tuning the full model, we can also just train a LoRA that can be injected into any SDXL model.\\n\\n### Example\\n\\nThe following uses the [Conceptual Captions 12M (CC12M) dataset](https://github.com/google-research-datasets/conceptual-12m) as an example. For best results you may consider large and high-quality text-image datasets such as [LAION](https://laion.ai/blog/laion-400-open-dataset/).\\n\\n```bash\\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\\nexport OUTPUT_DIR=\"path/to/saved/model\"'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': -1}, page_content='```bash\\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\\nexport OUTPUT_DIR=\"path/to/saved/model\"\\n\\naccelerate launch train_lcm_distill_lora_sdxl_wds.py \\\\\\n    --pretrained_teacher_model=$MODEL_DIR \\\\\\n    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\\\\n    --output_dir=$OUTPUT_DIR \\\\\\n    --mixed_precision=fp16 \\\\\\n    --resolution=1024 \\\\\\n    --lora_rank=64 \\\\\\n    --learning_rate=1e-6 --loss_type=\"huber\" --use_fix_crop_and_size --adam_weight_decay=0.0 \\\\\\n    --max_train_steps=1000 \\\\\\n    --max_train_samples=4000000 \\\\\\n    --dataloader_num_workers=8 \\\\\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\\\\n    --validation_steps=200 \\\\\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\\\\n    --train_batch_size=12 \\\\\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\\\\n    --gradient_accumulation_steps=1 \\\\\\n    --use_8bit_adam \\\\\\n    --resume_from_checkpoint=latest \\\\\\n    --report_to=wandb \\\\\\n    --seed=453645634 \\\\\\n    --push_to_hub \\\\\\n```'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Autoformer\\n\\n## Overview\\n\\nThe Autoformer model was proposed in [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.\\n\\nThis model augments the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.\\n\\nThe abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': -1}, page_content=\"This model augments the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.\\n\\nThe abstract from the paper is the following:\\n\\n*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.*\\n\\nThis model was contributed by [elisim](https://huggingface.co/elisim) and [kashif](https://huggingface.co/kashif).\\nThe original code can be found [here](https://github.com/thuml/Autoformer).\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)](https://huggingface.co/blog/autoformer)\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 3228}, page_content='- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)](https://huggingface.co/blog/autoformer)\\n\\n## AutoformerConfig\\n\\n[[autodoc]] AutoformerConfig\\n\\n## AutoformerModel\\n\\n[[autodoc]] AutoformerModel\\n    - forward\\n\\n## AutoformerForPrediction\\n\\n[[autodoc]] AutoformerForPrediction\\n    - forward'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 0}, page_content='--\\ntitle: \"DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub\" \\nthumbnail: /blog/assets/hub_duckdb/hub_duckdb.png\\nauthors:\\n- user: stevhliu\\n- user: lhoestq\\n- user: severo\\n---\\n\\n# DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub\\n\\n\\nThe Hugging Face Hub is dedicated to providing open access to datasets for everyone and giving users the tools to explore and understand them. You can find many of the datasets used to train popular large language models (LLMs) like [Falcon](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [MPT](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf), and [StarCoder](https://huggingface.co/datasets/bigcode/the-stack). There are tools for addressing fairness and bias in datasets like [Disaggregators](https://huggingface.co/spaces/society-ethics/disaggregators), and tools for previewing examples inside a dataset like the Dataset Viewer.\\n\\n<div class=\"flex justify-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets-server/oasst1_light.png\"/>\\n</div>\\n<small>A preview of the OpenAssistant dataset with the Dataset Viewer.</small>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 1235}, page_content='We are happy to share that we recently added another feature to help you analyze datasets on the Hub; you can run SQL queries with DuckDB on any dataset stored on the Hub! According to the 2022 [StackOverflow Developer Survey](https://survey.stackoverflow.co/2022/#section-most-popular-technologies-programming-scripting-and-markup-languages), SQL is the 3rd most popular programming language. We also wanted a fast database management system (DBMS) designed for running analytical queries, which is why we’re excited about integrating with [DuckDB](https://duckdb.org/). We hope this allows even more users to access and analyze datasets on the Hub!\\n\\n## TLDR\\n\\n[Datasets Server](https://huggingface.co/docs/datasets-server/index) **automatically converts all public datasets on the Hub to Parquet files**, that you can see by clicking on the \"Auto-converted to Parquet\" button at the top of a dataset page. You can also access the list of the Parquet files URLs with a simple HTTP call.\\n\\n```py\\nr = requests.get(\"https://datasets-server.huggingface.co/parquet?dataset=blog_authorship_corpus\")\\nj = r.json()\\nurls = [f[\\'url\\'] for f in j[\\'parquet_files\\'] if f[\\'split\\'] == \\'train\\']\\nurls\\n[\\'https://huggingface.co/datasets/blog_authorship_corpus/resolve/refs%2Fconvert%2Fparquet/blog_authorship_corpus/blog_authorship_corpus-train-00000-of-00002.parquet\\',\\n \\'https://huggingface.co/datasets/blog_authorship_corpus/resolve/refs%2Fconvert%2Fparquet/blog_authorship_corpus/blog_authorship_corpus-train-00001-of-00002.parquet\\']'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 2750}, page_content='```\\n\\nCreate a connection to DuckDB and install and load the `httpfs` extension to allow reading and writing remote files:\\n\\n```py\\nimport duckdb\\n\\nurl = \"https://huggingface.co/datasets/blog_authorship_corpus/resolve/refs%2Fconvert%2Fparquet/blog_authorship_corpus/blog_authorship_corpus-train-00000-of-00002.parquet\"\\n\\ncon = duckdb.connect()\\ncon.execute(\"INSTALL httpfs;\")\\ncon.execute(\"LOAD httpfs;\")\\n```\\n\\nOnce you’re connected, you can start writing SQL queries!\\n\\n```sql\\ncon.sql(f\"\"\"SELECT horoscope, \\n\\tcount(*), \\n\\tAVG(LENGTH(text)) AS avg_blog_length \\n\\tFROM \\'{url}\\' \\n\\tGROUP BY horoscope \\n\\tORDER BY avg_blog_length \\n\\tDESC LIMIT(5)\"\"\"\\n)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 3384}, page_content=\"```\\n\\nTo learn more, check out the [documentation](https://huggingface.co/docs/datasets-server/parquet_process).\\n\\n## From dataset to Parquet\\n\\n[Parquet](https://parquet.apache.org/docs/) files are columnar, making them more efficient to store, load and analyze. This is especially important when you're working with large datasets, which we’re seeing more and more of in the LLM era. To support this, Datasets Server automatically converts and publishes any public dataset on the Hub as Parquet files. The URL to the Parquet files can be retrieved with the [`/parquet`](https://huggingface.co/docs/datasets-server/quick_start#access-parquet-files) endpoint.\\n\\n## Analyze with DuckDB\\n\\nDuckDB offers super impressive performance for running complex analytical queries. It is able to execute a SQL query directly on a remote Parquet file without any overhead. With the [`httpfs`](https://duckdb.org/docs/extensions/httpfs) extension, DuckDB is able to query remote files such as datasets stored on the Hub using the URL provided from the `/parquet` endpoint. DuckDB also supports querying multiple Parquet files which is really convenient because Datasets Server shards big datasets into smaller 500MB chunks.\\n\\n## Looking forward\\n\\nKnowing what’s inside a dataset is important for developing models because it can impact model quality in all sorts of ways! By allowing users to write and execute any SQL query on Hub datasets, this is another way for us to enable open access to datasets and help users be more aware of the datasets contents. We are excited for you to try this out, and we’re looking forward to what kind of insights your analysis uncovers!\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 1}, page_content='Gradio and W&B Integration\\n\\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\\nTags: WANDB, SPACES\\nContributed by Gradio team\\n\\n## Introduction\\n\\nIn this Guide, we\\'ll walk you through:\\n\\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\\n\\n\\n## What is Wandb?\\n\\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\\n\\n<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\\n\\n## What are Hugging Face Spaces & Gradio?\\n\\n### Gradio\\n\\nGradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model\\'s inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\\n\\nGet started [here](https://gradio.app/getting_started)\\n\\n### Hugging Face Spaces\\n\\nHugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\\n\\n## Setting up a Gradio Demo for JoJoGAN'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 1760}, page_content=\"## Setting up a Gradio Demo for JoJoGAN\\n\\nNow, let's walk you through how to do this on your own. We'll make the assumption that you're new to W&B and Gradio for the purposes of this tutorial.\\n\\nLet's get started!\\n\\n1. Create a W&B account\\n\\n   Follow [these quick instructions](https://app.wandb.ai/login) to create your free account if you don’t have one already. It shouldn't take more than a couple minutes. Once you're done (or if you've already got an account), next, we'll run a quick colab.\\n\\n2. Open Colab Install Gradio and W&B\\n\\n   We'll be following along with the colab provided in the JoJoGAN repo with some minor modifications to use Wandb and Gradio more effectively.\\n\\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb)\\n\\n   Install Gradio and Wandb at the top:\\n\\n```sh\\n\\npip install gradio wandb\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 2675}, page_content='```\\n\\n3. Finetune StyleGAN and W&B experiment tracking\\n\\n   This next step will open a W&B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here\\'s the code you need for that:\\n\\n   ```python\\n\\n   alpha =  1.0\\n   alpha = 1-alpha\\n\\n   preserve_color = True\\n   num_iter = 100\\n   log_interval = 50\\n\\n\\n   samples = []\\n   column_names = [\"Reference (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\\n\\n   wandb.init(project=\"JoJoGAN\")\\n   config = wandb.config\\n   config.num_iter = num_iter\\n   config.preserve_color = preserve_color\\n   wandb.log(\\n   {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\\n   step=0)\\n\\n   # load discriminator for perceptual loss\\n   discriminator = Discriminator(1024, 2).eval().to(device)\\n   ckpt = torch.load(\\'models/stylegan2-ffhq-config-f.pt\\', map_location=lambda storage, loc: storage)\\n   discriminator.load_state_dict(ckpt[\"d\"], strict=False)\\n\\n   # reset generator\\n   del generator\\n   generator = deepcopy(original_generator)\\n\\n   g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\\n\\n   # Which layers to swap for generating a family of plausible real images -> fake image\\n   if preserve_color:\\n       id_swap = [9,11,15,16,17]\\n   else:\\n       id_swap = list(range(7, generator.n_latent))'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 4032}, page_content='for idx in tqdm(range(num_iter)):\\n       mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\\n       in_latent = latents.clone()\\n       in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\\n\\n       img = generator(in_latent, input_is_latent=True)\\n\\n       with torch.no_grad():\\n           real_feat = discriminator(targets)\\n       fake_feat = discriminator(img)\\n\\n       loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\\n\\n\\n       wandb.log({\"loss\": loss}, step=idx)\\n       if idx % log_interval == 0:\\n           generator.eval()\\n           my_sample = generator(my_w, input_is_latent=True)\\n           generator.train()\\n           my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\\n           wandb.log(\\n           {\"Current stylization\": [wandb.Image(my_sample)]},\\n           step=idx)\\n       table_data = [\\n               wandb.Image(transforms.ToPILImage()(target_im)),\\n               wandb.Image(img),\\n               wandb.Image(my_sample),\\n           ]\\n       samples.append(table_data)\\n\\n       g_optim.zero_grad()\\n       loss.backward()\\n       g_optim.step()\\n\\n   out_table = wandb.Table(data=samples, columns=column_names)\\n   wandb.log({\"Current Samples\": out_table})'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 5388}, page_content='```\\n\\nalpha = 1.0\\nalpha = 1-alpha\\n\\npreserve_color = True\\nnum_iter = 100\\nlog_interval = 50\\n\\nsamples = []\\ncolumn_names = [\"Referece (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\\n\\nwandb.init(project=\"JoJoGAN\")\\nconfig = wandb.config\\nconfig.num_iter = num_iter\\nconfig.preserve_color = preserve_color\\nwandb.log(\\n{\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\\nstep=0)\\n\\n# load discriminator for perceptual loss\\n\\ndiscriminator = Discriminator(1024, 2).eval().to(device)\\nckpt = torch.load(\\'models/stylegan2-ffhq-config-f.pt\\', map_location=lambda storage, loc: storage)\\ndiscriminator.load_state_dict(ckpt[\"d\"], strict=False)\\n\\n# reset generator\\n\\ndel generator\\ngenerator = deepcopy(original_generator)\\n\\ng_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\\n\\n# Which layers to swap for generating a family of plausible real images -> fake image\\n\\nif preserve_color:\\nid_swap = [9,11,15,16,17]\\nelse:\\nid_swap = list(range(7, generator.n_latent))\\n\\nfor idx in tqdm(range(num_iter)):\\nmean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\\nin_latent = latents.clone()\\nin_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\\n\\n    img = generator(in_latent, input_is_latent=True)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 6636}, page_content='img = generator(in_latent, input_is_latent=True)\\n\\n    with torch.no_grad():\\n        real_feat = discriminator(targets)\\n    fake_feat = discriminator(img)\\n\\n    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\\n\\n\\n    wandb.log({\"loss\": loss}, step=idx)\\n    if idx % log_interval == 0:\\n        generator.eval()\\n        my_sample = generator(my_w, input_is_latent=True)\\n        generator.train()\\n        my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\\n        wandb.log(\\n        {\"Current stylization\": [wandb.Image(my_sample)]},\\n        step=idx)\\n    table_data = [\\n            wandb.Image(transforms.ToPILImage()(target_im)),\\n            wandb.Image(img),\\n            wandb.Image(my_sample),\\n        ]\\n    samples.append(table_data)\\n\\n    g_optim.zero_grad()\\n    loss.backward()\\n    g_optim.step()\\n\\nout_table = wandb.Table(data=samples, columns=column_names)\\nwandb.log({\"Current Samples\": out_table})\\n\\n`'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 7616}, page_content='```\\n\\n4. Save, Download, and Load Model\\n\\n    Here\\'s how to save and download your model.\\n\\n```python\\n\\nfrom PIL import Image\\nimport torch\\ntorch.backends.cudnn.benchmark = True\\nfrom torchvision import transforms, utils\\nfrom util import *\\nimport math\\nimport random\\nimport numpy as np\\nfrom torch import nn, autograd, optim\\nfrom torch.nn import functional as F\\nfrom tqdm import tqdm\\nimport lpips\\nfrom model import *\\nfrom e4e_projection import projection as e4e_projection\\n\\nfrom copy import deepcopy\\nimport imageio\\n\\nimport os\\nimport sys\\nimport torchvision.transforms as transforms\\nfrom argparse import Namespace\\nfrom e4e.models.psp import pSp\\nfrom util import *\\nfrom huggingface_hub import hf_hub_download\\nfrom google.colab import files\\n\\ntorch.save({\"g\": generator.state_dict()}, \"your-model-name.pt\")\\n\\nfiles.download(\\'your-model-name.pt\\')\\n\\nlatent_dim = 512\\ndevice=\"cuda\"\\nmodel_path_s = hf_hub_download(repo_id=\"akhaliq/jojogan-stylegan2-ffhq-config-f\", filename=\"stylegan2-ffhq-config-f.pt\")\\noriginal_generator = Generator(1024, latent_dim, 8, 2).to(device)\\nckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)\\noriginal_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\\nmean_latent = original_generator.mean_latent(10000)\\n\\ngenerator = deepcopy(original_generator)\\n\\nckpt = torch.load(\"/content/JoJoGAN/your-model-name.pt\", map_location=lambda storage, loc: storage)\\ngenerator.load_state_dict(ckpt[\"g\"], strict=False)\\ngenerator.eval()\\n\\nplt.rcParams[\\'figure.dpi\\'] = 150'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 9070}, page_content='plt.rcParams[\\'figure.dpi\\'] = 150\\n\\n\\n\\ntransform = transforms.Compose(\\n    [\\n        transforms.Resize((1024, 1024)),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\\n    ]\\n)\\n\\n\\ndef inference(img):\\n    img.save(\\'out.jpg\\')\\n    aligned_face = align_face(\\'out.jpg\\')\\n\\n    my_w = e4e_projection(aligned_face, \"out.pt\", device).unsqueeze(0)\\n    with torch.no_grad():\\n        my_sample = generator(my_w, input_is_latent=True)\\n\\n\\n    npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()\\n    imageio.imwrite(\\'filename.jpeg\\', npimage)\\n    return \\'filename.jpeg\\'\\n`'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 9674}, page_content='```\\n\\n5. Build a Gradio Demo\\n\\n```python\\n\\nimport gradio as gr\\n\\ntitle = \"JoJoGAN\"\\ndescription = \"Gradio Demo for JoJoGAN: One Shot Face Stylization. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\\n\\ndemo = gr.Interface(\\n    inference,\\n    gr.Image(type=\"pil\"),\\n    gr.Image(type=\"file\"),\\n    title=title,\\n    description=description\\n)\\n\\ndemo.launch(share=True)\\n```\\n\\n6. Integrate Gradio into your W&B Dashboard\\n\\n   The last step—integrating your Gradio demo with your W&B dashboard—is just one extra line:\\n\\n```python\\n\\ndemo.integrate(wandb=wandb)\\n```\\n\\n    Once you call integrate, a demo will be created and you can integrate it into your dashboard or report\\n\\n    Outside of W&B with Web components, using the gradio-app tags allows anyone can embed Gradio demos on HF spaces directly into their blogs, websites, documentation, etc.:\\n\\n```html\\n<gradio-app space=\"akhaliq/JoJoGAN\"> </gradio-app>'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 10624}, page_content='```\\n\\n7. (Optional) Embed W&B plots in your Gradio App\\n\\n   It\\'s also possible to embed W&B plots within Gradio apps. To do so, you can create a W&B Report of your plots and\\n   embed them within your Gradio app within a `gr.HTML` block.\\n\\n   The Report will need to be public and you will need to wrap the URL within an iFrame like this:\\n\\n```python\\n\\nimport gradio as gr\\n\\ndef wandb_report(url):\\n    iframe = f\\'<iframe src={url} style=\"border:none;height:1024px;width:100%\">\\'\\n    return gr.HTML(iframe)\\n\\nwith gr.Blocks() as demo:\\n    report_url = \\'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx\\'\\n    report = wandb_report(report_url)\\n\\ndemo.launch(share=True)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 11325}, page_content='```\\n\\n## Conclusion\\n\\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! Thanks for making it to the end. To recap:\\n\\n- Only one single reference image is needed for fine-tuning JoJoGAN which usually takes about 1 minute on a GPU in colab. After training, style can be applied to any input image. Read more in the paper.\\n\\n- W&B tracks experiments with just a few lines of code added to a colab and you can visualize, sort, and understand your experiments in a single, centralized dashboard.\\n\\n- Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web.\\n\\n## How to contribute Gradio demos on HF spaces on the Wandb organization\\n\\n- Create an account on Hugging Face [here](https://huggingface.co/join).\\n- Add Gradio Demo under your username, see this [course](https://huggingface.co/course/chapter9/4?fw=pt) for setting up Gradio Demo on Hugging Face.\\n- Request to join wandb organization [here](https://huggingface.co/wandb).\\n- Once approved transfer model from your username to Wandb organization'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/duplicatebutton_component/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: duplicatebutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Blocks() as demo:\\n    gr.DuplicateButton()\\n\\ndemo.launch()\\n```'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 1}, page_content='Widget Examples\\n\\nNote that each widget example can also optionally describe the corresponding model output, directly in the `output` property. See [the spec](./models-widgets#example-outputs) for more details.\\n\\n## Natural Language Processing\\n\\n### Fill-Mask\\n\\n```yaml\\nwidget:\\n- text: \"Paris is the <mask> of France.\"\\n  example_title: \"Capital\"\\n- text: \"The goal of life is <mask>.\"\\n  example_title: \"Philosophy\"\\n```\\n\\n### Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"What\\'s my name?\"\\n  context: \"My name is Clara and I live in Berkeley.\"\\n  example_title: \"Name\"\\n- text: \"Where do I live?\"\\n  context: \"My name is Sarah and I live in London\"\\n  example_title: \"Location\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 666}, page_content='```\\n\\n### Summarization\\n\\n```yaml\\nwidget:\\n- text: \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\\n  example_title: \"Eiffel Tower\"\\n- text: \"Laika, a dog that was the first living creature to be launched into Earth orbit, on board the Soviet artificial satellite Sputnik 2, on November 3, 1957. It was always understood that Laika would not survive the mission, but her actual fate was misrepresented for decades. Laika was a small (13 pounds [6 kg]), even-tempered, mixed-breed dog about two years of age. She was one of a number of stray dogs that were taken into the Soviet spaceflight program after being rescued from the streets. Only female dogs were used because they were considered to be anatomically better suited than males for close confinement.\"\\n  example_title: \"First in Space\"\\n```\\n\\n### Table Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"How many stars does the transformers repository have?\"\\n  table:\\n    Repository:\\n      - \"Transformers\"\\n      - \"Datasets\"\\n      - \"Tokenizers\"\\n    Stars:\\n      - 36542\\n      - 4512\\n      - 3934\\n    Contributors:\\n      - 651\\n      - 77\\n      - 34\\n    Programming language:\\n      - \"Python\"\\n      - \"Python\"\\n      - \"Rust, Python and NodeJS\"\\n  example_title: \"Github stars\"\\n```\\n\\n### Text Classification\\n\\n```yaml\\nwidget:\\n- text: \"I love football so much\"\\n  example_title: \"Positive\"\\n- text: \"I don\\'t really like this type of food\"\\n  example_title: \"Negative\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 2769}, page_content='```\\n\\n### Text Generation\\n\\n```yaml\\nwidget:\\n- text: \"My name is Julien and I like to\"\\n  example_title: \"Julien\"\\n- text: \"My name is Merve and my favorite\"\\n  example_title: \"Merve\"\\n```\\n\\n### Text2Text Generation\\n\\n```yaml\\nwidget:\\n- text: \"My name is Julien and I like to\"\\n  example_title: \"Julien\"\\n- text: \"My name is Merve and my favorite\"\\n  example_title: \"Merve\"\\n```\\n\\n### Token Classification\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n### Translation\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n### Zero-Shot Classification\\n\\n```yaml\\nwidget:\\n- text: \"I have a problem with my car that needs to be resolved asap!!\"\\n  candidate_labels: \"urgent, not urgent, phone, tablet, computer\"\\n  multi_class: true\\n  example_title: \"Car problem\"\\n- text: \"Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\"\\n  candidate_labels: \"mobile, website, billing, account access\"\\n  multi_class: false\\n  example_title: \"Phone issue\"\\n```\\n### Sentence Similarity\\n\\n```yaml\\nwidget:\\n- source_sentence: \"That is a happy person\"\\n  sentences:\\n    - \"That is a happy dog\"\\n    - \"That is a very happy person\"\\n    - \"Today is a sunny day\"\\n  example_title: \"Happy\"\\n```\\n\\n### Conversational\\n\\n```yaml\\nwidget:\\n- text: \"Hey my name is Julien! How are you?\"\\n  example_title: \"Julien\"\\n- text: \"Hey my name is Clara! How are you?\"\\n  example_title: \"Clara\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 4400}, page_content='```\\n\\n### Feature Extraction\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n## Audio\\n\\n### Text-to-Speech\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n### Automatic Speech Recognition\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2\\n```\\n\\n### Audio-to-Audio\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2\\n```\\n\\n### Audio Classification\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2\\n```\\n\\n### Voice Activity Detection\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 5844}, page_content='```\\n\\n## Computer Vision\\n\\n### Image Classification\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\\n  example_title: Tiger\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\\n  example_title: Teapot\\n```\\n\\n### Object Detection\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\\n  example_title: Football Match\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\\n  example_title: Airport\\n```\\n\\n### Image Segmentation\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\\n  example_title: Football Match\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\\n  example_title: Airport\\n```\\n\\n### Image-to-Image\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/canny-edge.jpg\\n  prompt: Girl with Pearl Earring # `prompt` field is optional in case the underlying model supports text guidance\\n```\\n\\n### Text-to-Image\\n\\n```yaml\\nwidget:\\n- text: \"A cat playing with a ball\"\\n  example_title: \"Cat\"\\n- text: \"A dog jumping over a fence\"\\n  example_title: \"Dog\"'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 7084}, page_content='```\\n\\n### Document Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"What is the invoice number?\"\\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\\n- text: \"What is the purchase amount?\"\\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg\"\\n```\\n\\n### Visual Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"What animal is it?\"\\n  src: \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\"\\n- text: \"Where is it?\"\\n  src: \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\"\\n```\\n\\n### Zero-Shot Image Classification\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\\n  candidate_labels: playing music, playing sports\\n  example_title: Cat & Dog'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 7949}, page_content='```\\n\\n## Other\\n\\n### Structured Data Classification\\n\\n```yaml\\nwidget:\\n- structured_data:\\n    fixed_acidity:\\n      - 7.4\\n      - 7.8\\n      - 10.3\\n    volatile_acidity:\\n      - 0.7\\n      - 0.88\\n      - 0.32\\n    citric_acid:\\n      - 0\\n      - 0\\n      - 0.45\\n    residual_sugar:\\n      - 1.9\\n      - 2.6\\n      - 6.4\\n    chlorides:\\n      - 0.076\\n      - 0.098\\n      - 0.073\\n    free_sulfur_dioxide:\\n      - 11\\n      - 25\\n      - 5\\n    total_sulfur_dioxide:\\n      - 34\\n      - 67\\n      - 13\\n    density:\\n      - 0.9978\\n      - 0.9968\\n      - 0.9976\\n    pH:\\n      - 3.51\\n      - 3.2\\n      - 3.23\\n    sulphates:\\n      - 0.56\\n      - 0.68\\n      - 0.82\\n    alcohol:\\n      - 9.4\\n      - 9.8\\n      - 12.6\\n  example_title: \"Wine\"\\n```'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# UNet1DModel\\n\\nThe [UNet](https://huggingface.co/papers/1505.04597) model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in 🤗 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in 🤗 Diffusers, depending on it\\'s number of dimensions and whether it is a conditional model or not. This is a 1D UNet model.\\n\\nThe abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.*'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 2268}, page_content='## UNet1DModel\\n[[autodoc]] UNet1DModel\\n\\n## UNet1DOutput\\n[[autodoc]] models.unet_1d.UNet1DOutput'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 0}, page_content='--\\ntitle: \"Building an AI WebTV\"\\nthumbnail: /blog/assets/156_ai_webtv/thumbnail.gif\\nauthors:\\n- user: jbilcke-hf\\n---\\n\\n# Building an AI WebTV\\n\\n\\nThe AI WebTV is an experimental demo to showcase the latest advancements in automatic video and music synthesis.\\n\\n👉 Watch the stream now by going to the [AI WebTV Space](https://huggingface.co/spaces/jbilcke-hf/AI-WebTV).\\n\\nIf you are using a mobile device, you can view the stream from the [Twitch mirror](https://www.twitch.tv/ai_webtv).\\n\\n![thumbnail.gif](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/thumbnail.gif)\\n\\n# Concept\\n\\nThe motivation for the AI WebTV is to demo videos generated with open-source [text-to-video models](https://huggingface.co/tasks/text-to-video) such as Zeroscope and MusicGen, in an entertaining and accessible way.\\n\\nYou can find those open-source models on the Hugging Face hub:\\n\\n- For video: [zeroscope_v2_576](https://huggingface.co/cerspense/zeroscope_v2_576w) and [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL)\\n- For music: [musicgen-melody](https://huggingface.co/facebook/musicgen-melody)\\n\\nThe individual video sequences are purposely made to be short, meaning the WebTV should be seen as a tech demo/showreel rather than an actual show (with an art direction or programming).\\n\\n# Architecture\\n\\nThe AI WebTV works by taking a sequence of [video shot](https://en.wikipedia.org/wiki/Shot_(filmmaking)) prompts and passing them to a [text-to-video model](https://huggingface.co/tasks/text-to-video) to generate a sequence of [takes](https://en.wikipedia.org/wiki/Take).'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 1617}, page_content=\"Additionally, a base theme and idea (written by a human) are passed through a LLM (in this case, ChatGPT), in order to generate a variety of individual prompts for each video clip.\\n\\nHere's a diagram of the current architecture of the AI WebTV:\\n\\n![diagram.jpg](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/diagram.jpg)\\n\\n# Implementing the pipeline\\n\\nThe WebTV is implemented in NodeJS and TypeScript, and uses various services hosted on Hugging Face.\\n\\n## The text-to-video model\\n\\nThe central video model is Zeroscope V2, a model based on [ModelScope](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis).\\n\\nZeroscope is comprised of two parts that can be chained together:\\n\\n- A first pass with [zeroscope_v2_576](https://huggingface.co/cerspense/zeroscope_v2_576w), to generate a 576x320 video clip\\n- An optional second pass with [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) to upscale the video to 1024x576\\n\\n👉\\xa0 You will need to use the same prompt for both the generation and upscaling.\\n\\n## Calling the video chain\\n\\nTo make a quick prototype, the WebTV runs Zeroscope from two duplicated Hugging Face Spaces running [Gradio](https://github.com/gradio-app/gradio/), which are called using the [@gradio/client](https://www.npmjs.com/package/@gradio/client) NPM package. You can find the original spaces here:\\n\\n- [zeroscope-v2](https://huggingface.co/spaces/hysts/zeroscope-v2/tree/main) by @hysts\\n- [Zeroscope XL](https://huggingface.co/spaces/fffiloni/zeroscope-XL) by @fffiloni\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 3190}, page_content='Other spaces deployed by the community can also be found if you [search for Zeroscope on the Hub](https://huggingface.co/spaces?search=zeroscope).\\n\\n👉\\xa0 Public Spaces may become overcrowded and paused at any time. If you intend to deploy your own system, please duplicate those Spaces and run them under your own account.\\n\\n## Using a model hosted on a Space\\n\\nSpaces using Gradio have the ability to [expose a REST API](https://www.gradio.app/guides/sharing-your-app#api-page), which can then be called from Node using the [@gradio/client](https://www.npmjs.com/package/@gradio/client) module.\\n\\nHere is an example:\\n\\n```typescript\\nimport { client } from \"@gradio/client\"\\n\\nexport const generateVideo = async (prompt: string) => {\\n  const api = await client(\"*** URL OF THE SPACE ***\")\\n\\n  // call the \"run()\" function with an array of parameters\\n  const { data } = await api.predict(\"/run\", [\\t\\t\\n    prompt,\\n    42,\\t// seed\\t\\n    24, // nbFrames\\n    35 // nbSteps\\n  ])\\n  \\n  const { orig_name } = data[0][0]\\n\\n  const remoteUrl = `${instance}/file=${orig_name}`\\n\\n  // the file can then be downloaded and stored locally\\n}'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 4301}, page_content='```\\n\\n\\n## Post-processing\\n\\nOnce an individual take (a video clip) is upscaled, it is then passed to FILM (Frame Interpolation for Large Motion), a frame interpolation algorithm:\\n\\n- Original links: [website](https://film-net.github.io/), [source code](https://github.com/google-research/frame-interpolation)\\n- Model on Hugging Face: [/frame-interpolation-film-style](https://huggingface.co/akhaliq/frame-interpolation-film-style)\\n- A Hugging Face Space you can duplicate: [video_frame_interpolation](https://huggingface.co/spaces/fffiloni/video_frame_interpolation/blob/main/app.py) by @fffiloni\\n\\nDuring post-processing, we also add music generated with MusicGen:\\n\\n- Original links: [website](https://ai.honu.io/papers/musicgen/), [source code](https://github.com/facebookresearch/audiocraft)\\n- Hugging Face Space you can duplicate: [MusicGen](https://huggingface.co/spaces/facebook/MusicGen)\\n\\n\\n## Broadcasting the stream\\n\\nNote: there are multiple tools you can use to create a video stream. The AI WebTV currently uses [FFmpeg](https://ffmpeg.org/documentation.html) to read a playlist made of mp4 videos files and m4a audio files.\\n\\nHere is an example of creating such a playlist:\\n\\n```typescript\\nimport { promises as fs } from \"fs\"\\nimport path from \"path\"\\n\\nconst allFiles = await fs.readdir(\"** PATH TO VIDEO FOLDER **\")\\nconst allVideos = allFiles\\n  .map(file => path.join(dir, file))\\n  .filter(filePath => filePath.endsWith(\\'.mp4\\'))'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 5735}, page_content='let playlist = \\'ffconcat version 1.0\\\\n\\'\\nallFilePaths.forEach(filePath => {\\n  playlist += `file \\'${filePath}\\'\\\\n`\\n})\\nawait fs.promises.writeFile(\"playlist.txt\", playlist)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 5904}, page_content=\"```\\n\\nThis will generate the following playlist content:\\n\\n```bash\\nffconcat version 1.0\\nfile 'video1.mp4'\\nfile 'video2.mp4'\\n...\\n```\\n\\nFFmpeg is then used again to read this playlist and send a [FLV stream](https://en.wikipedia.org/wiki/Flash_Video) to a [RTMP server](https://en.wikipedia.org/wiki/Real-Time_Messaging_Protocol). FLV is an old format but still popular in the world of real-time streaming due to its low latency.\\n\\n```bash\\nffmpeg -y -nostdin \\\\\\n  -re \\\\\\n  -f concat \\\\\\n  -safe 0 -i channel_random.txt -stream_loop -1 \\\\\\n  -loglevel error \\\\\\n  -c:v libx264 -preset veryfast -tune zerolatency \\\\\\n  -shortest \\\\\\n  -f flv rtmp://<SERVER>\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 6542}, page_content='```\\n\\nThere are many different configuration options for FFmpeg, for more information in the [official documentation](http://trac.ffmpeg.org/wiki/StreamingGuide).\\n\\nFor the RTMP server, you can find [open-source implementations on GitHub](https://github.com/topics/rtmp-server), such as the [NGINX-RTMP module](https://github.com/arut/nginx-rtmp-module).\\n\\nThe AI WebTV itself uses [node-media-server](https://github.com/illuspas/Node-Media-Server).\\n\\n💡 You can also directly stream to [one of the Twitch RTMP entrypoints](https://help.twitch.tv/s/twitch-ingest-recommendation?language=en_US). Check out the Twitch documentation for more details.\\n\\n# Observations and examples\\n\\nHere are some examples of the generated content.\\n\\nThe first thing we notice is that applying the second pass of Zeroscope XL significantly improves the quality of the image. The impact of frame interpolation is also clearly visible.\\n\\n## Characters and scene composition\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo4.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo4.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Photorealistic movie of a <strong>llama acting as a programmer, wearing glasses and a hoodie</strong>, intensely <strong>staring at a screen</strong> with lines of code, in a cozy, <strong>dimly lit room</strong>, Canon EOS, ambient lighting, high details, cinematic, trending on artstation</i></figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 8143}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo5.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo5.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>3D rendered animation showing a group of food characters <strong>forming a pyramid</strong>, with a <strong>banana</strong> standing triumphantly on top. In a city with <strong>cotton candy clouds</strong> and <strong>chocolate road</strong>, Pixar\\'s style, CGI, ambient lighting, direct sunlight, rich color scheme, ultra realistic, cinematic, photorealistic.</i></figcaption>\\n</figure>\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo7.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo7.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Intimate <strong>close-up of a red fox, gazing into the camera with sharp eyes</strong>, ambient lighting creating a high contrast silhouette, IMAX camera, <strong>high detail</strong>, <strong>cinematic effect</strong>, golden hour, film grain.</i></figcaption>\\n</figure>\\n\\n## Simulation of dynamic scenes\\n\\nSomething truly fascinating about text-to-video models is their ability to emulate real-life phenomena they have been trained on.\\n\\nWe\\'ve seen it with large language models and their ability to synthesize convincing content that mimics human responses, but this takes things to a whole new dimension when applied to video.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': -1}, page_content='We\\'ve seen it with large language models and their ability to synthesize convincing content that mimics human responses, but this takes things to a whole new dimension when applied to video.\\n\\nA video model predicts the next frames of a scene, which might include objects in motion such as fluids, people, animals, or vehicles. Today, this emulation isn\\'t perfect, but it will be interesting to evaluate future models (trained on larger or specialized datasets, such as animal locomotion) for their accuracy when reproducing physical phenomena, and also their ability to simulate the behavior of agents.\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo17.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo17.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Cinematic movie shot of <strong>bees energetically buzzing around a flower</strong>, sun rays illuminating the scene, captured in 4k IMAX with a soft bokeh background.</i></figcaption>\\n</figure>\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo8.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo8.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i><strong>Dynamic footage of a grizzly bear catching a salmon in a rushing river</strong>, ambient lighting highlighting the splashing water, low angle, IMAX camera, 4K movie quality, golden hour, film grain.</i></figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 11359}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo18.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo18.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Aerial footage of a quiet morning at the coast of California, with <strong>waves gently crashing against the rocky shore</strong>. A startling sunrise illuminates the coast with vibrant colors, captured beautifully with a DJI Phantom 4 Pro. Colors and textures of the landscape come alive under the soft morning light. Film grain, cinematic, imax, movie</i></figcaption>\\n</figure>\\n\\n💡 It will be interesting to see these capabilities explored more in the future, for instance by training video models on larger video datasets covering more phenomena.\\n\\n## Styling and effects\\n\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo0.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo0.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>\\n<strong>3D rendered video</strong> of a friendly broccoli character wearing a hat, walking in a candy-filled city street with gingerbread houses, under a <strong>bright sun and blue skies</strong>, <strong>Pixar\\'s style</strong>, cinematic, photorealistic, movie, <strong>ambient lighting</strong>, natural lighting, <strong>CGI</strong>, wide-angle view, daytime, ultra realistic.</i>\\n</figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 13025}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo2.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo2.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i><strong>Cinematic movie</strong>, shot of an astronaut and a llama at dawn, the mountain landscape bathed in <strong>soft muted colors</strong>, early morning fog, dew glistening on fur, craggy peaks, vintage NASA suit, Canon EOS, high detailed skin, epic composition, high quality, 4K, trending on artstation, beautiful</i>\\n</figcaption>\\n</figure>\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo1.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo1.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Panda and black cat <strong>navigating down the flowing river</strong> in a small boat, Studio Ghibli style &gt; Cinematic, beautiful composition &gt; IMAX <strong>camera panning following the boat</strong> &gt; High quality, cinematic, movie, mist effect, film grain, trending on Artstation</i>\\n</figcaption>\\n</figure>\\n\\n\\n\\n## Failure cases\\n\\n**Wrong direction:** the model sometimes has trouble with movement and direction. For instance, here the clip seems to be played in reverse. Also the modifier keyword ***green*** was not taken into account.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': -1}, page_content='**Wrong direction:** the model sometimes has trouble with movement and direction. For instance, here the clip seems to be played in reverse. Also the modifier keyword ***green*** was not taken into account.\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"fail1.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/fail1.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Movie showing a <strong>green pumpkin</strong> falling into a bed of nails, slow-mo explosion with chunks flying all over, ambient fog adding to the dramatic lighting, filmed with IMAX camera, 8k ultra high definition, high quality, trending on artstation.</i>\\n</figcaption>\\n</figure>\\n\\n\\n**Rendering errors on realistic scenes:** sometimes we can see artifacts such as moving vertical lines or waves. It is unclear what causes this, but it may be due to the combination of keywords used.\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"fail2.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/fail2.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Film shot of a captivating flight above the Grand Canyon, ledges and plateaus etched in orange and red. <strong>Deep shadows contrast</strong> with the fiery landscape under the midday sun, shot with DJI Phantom 4 Pro. The camera rotates to capture the vastness, <strong>textures</strong> and colors, in imax quality. Film <strong>grain</strong>, cinematic, movie.</i>\\n</figcaption>\\n</figure>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 16159}, page_content='**Text or objects inserted into the image:** the model sometimes injects words from the prompt into the scene, such as \"IMAX\". Mentioning \"Canon EOS\" or \"Drone footage\" in the prompt can also make those objects appear in the video.\\n\\nIn the following example, we notice the word \"llama\" inserts a llama but also two occurrences of the word llama in flames.\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"fail3.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/fail3.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Movie scene of a <strong>llama</strong> acting as a firefighter, in firefighter uniform, dramatically spraying water at <strong>roaring flames</strong>, amidst a chaotic urban scene, Canon EOS, ambient lighting, high quality, award winning, highly detailed fur, cinematic, trending on artstation.</i>\\n</figcaption>\\n</figure>\\n\\n# Recommendations\\n\\nHere are some early recommendations that can be made from the previous observations:\\n\\n## Using video-specific prompt keywords\\n\\nYou may already know that if you don’t prompt a specific aspect of the image with Stable Diffusion, things like the color of clothes or the time of the day might become random, or be assigned a generic value such as a neutral mid-day light.\\n\\nThe same is true for video models: you will want to be specific about things. Examples include camera and character movement, their orientation, speed and direction. You can leave it unspecified for creative purposes (idea generation), but this might not always give you the results you want (e.g., entities animated in reverse).\\n\\n## Maintaining consistency between scenes\\n\\nIf you plan to create sequences of multiple videos, you will want to make sure you add as many details as possible in each prompt, otherwise you may lose important details from one sequence to another, such as the color.\\n\\n💡 This will also improve the quality of the image since the prompt is used for the upscaling part with Zeroscope XL.\\n\\n## Leverage frame interpolation'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': -1}, page_content='💡 This will also improve the quality of the image since the prompt is used for the upscaling part with Zeroscope XL.\\n\\n## Leverage frame interpolation\\n\\nFrame interpolation is a powerful tool which can repair small rendering errors and turn many defects into features, especially in scenes with a lot of animation, or where a cartoon effect is acceptable. The [FILM algorithm](https://film-net.github.io/) will smoothen out elements of a frame with previous and following events in the video clip.\\n\\nThis works great to displace the background when the camera is panning or rotating, and will also give you creative freedom, such as control over the number of frames after the generation, to make slow-motion effects.\\n\\n# Future work\\n\\nWe hope you enjoyed watching the AI WebTV stream and that it will inspire you to build more in this space.\\n\\nAs this was a first trial, a lot of things were not the focus of the tech demo: generating longer and more varied sequences, adding audio (sound effects, dialogue), generating and orchestrating complex scenarios, or letting a language model agent have more control over the pipeline.\\n\\nSome of these ideas may make their way into future updates to the AI WebTV, but we also can’t wait to see what the community of researchers, engineers and builders will come up with!'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 1}, page_content=\"Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:\\n\\n*Stable Diffusion is a text-to-image model that will empower billions of people to create stunning art within seconds. It is a breakthrough in speed and quality meaning that it can run on consumer GPUs. You can see some of the amazing output that has been created by this model without pre or post-processing on this page. The model itself builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all.*\\n\\n## Tips:\\n\\n- Stable Diffusion has the same architecture as [Latent Diffusion](https://arxiv.org/abs/2112.10752) but uses a frozen CLIP Text Encoder instead of training the text encoder jointly with the diffusion model.\\n- An in-detail explanation of the Stable Diffusion model can be found under [Stable Diffusion with 🧨 Diffusers](https://huggingface.co/blog/stable_diffusion).\\n- If you don't want to rely on the Hugging Face Hub and having to pass a authentication token, you can\\ndownload the weights with `git lfs install; git clone https://huggingface.co/runwayml/stable-diffusion-v1-5` and instead pass the local path to the cloned folder to `from_pretrained` as shown below.\\n- Stable Diffusion can work with a variety of different samplers as is shown below.\\n\\n## Available Pipelines:\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 1782}, page_content='## Available Pipelines:\\n\\n| Pipeline | Tasks | Colab\\n|---|---|:---:|\\n| [pipeline_stable_diffusion.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py) | *Text-to-Image Generation* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)\\n| [pipeline_stable_diffusion_img2img](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py) | *Image-to-Image Text-Guided Generation* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/image_2_image_using_diffusers.ipynb)\\n| [pipeline_stable_diffusion_inpaint](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py) | *Text-Guided Image Inpainting* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/in_painting_with_stable_diffusion_using_diffusers.ipynb)\\n\\n## Examples:\\n\\n### Using Stable Diffusion without being logged into the Hub.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': -1}, page_content='## Examples:\\n\\n### Using Stable Diffusion without being logged into the Hub.\\n\\nIf you want to download the model weights using a single Python line, you need to be logged in via `huggingface-cli login`.\\n\\n```python\\nfrom diffusers import DiffusionPipeline\\n\\npipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 3382}, page_content='```\\n\\nThis however can make it difficult to build applications on top of `diffusers` as you will always have to pass the token around. A potential way to solve this issue is by downloading the weights to a local path `\"./stable-diffusion-v1-5\"`:\\n\\n```\\ngit lfs install\\ngit clone https://huggingface.co/runwayml/stable-diffusion-v1-5\\n```\\n\\nand simply passing the local path to `from_pretrained`:\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\n\\npipe = StableDiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\")\\n```\\n\\n### Text-to-Image with default PLMS scheduler\\n\\n```python\\n# make sure you\\'re logged in with `huggingface-cli login`\\nfrom diffusers import StableDiffusionPipeline\\n\\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\\npipe = pipe.to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\"\\nimage = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\")\\n```\\n\\n### Text-to-Image with DDIM scheduler\\n\\n```python\\n# make sure you\\'re logged in with `huggingface-cli login`\\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\\n\\nscheduler =  DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\\n\\npipe = StableDiffusionPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    scheduler=scheduler,\\n).to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\"\\nimage = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 4836}, page_content='```\\n\\n### Text-to-Image with K-LMS scheduler\\n\\n```python\\n# make sure you\\'re logged in with `huggingface-cli login`\\nfrom diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\\n\\nlms = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\\n\\npipe = StableDiffusionPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    scheduler=lms,\\n).to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\"\\nimage = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 5367}, page_content='```\\n\\n### CycleDiffusion using Stable Diffusion and DDIM scheduler\\n\\n```python\\nimport requests\\nimport torch\\nfrom PIL import Image\\nfrom io import BytesIO\\n\\nfrom diffusers import CycleDiffusionPipeline, DDIMScheduler\\n\\n\\n# load the scheduler. CycleDiffusion only supports stochastic schedulers.\\n\\n# load the pipeline\\n# make sure you\\'re logged in with `huggingface-cli login`\\nmodel_id_or_path = \"CompVis/stable-diffusion-v1-4\"\\nscheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\\npipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to(\"cuda\")\\n\\n# let\\'s download an initial image\\nurl = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/An%20astronaut%20riding%20a%20horse.png\"\\nresponse = requests.get(url)\\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\\ninit_image = init_image.resize((512, 512))\\ninit_image.save(\"horse.png\")\\n\\n# let\\'s specify a prompt\\nsource_prompt = \"An astronaut riding a horse\"\\nprompt = \"An astronaut riding an elephant\"\\n\\n# call the pipeline\\nimage = pipe(\\n    prompt=prompt,\\n    source_prompt=source_prompt,\\n    image=init_image,\\n    num_inference_steps=100,\\n    eta=0.1,\\n    strength=0.8,\\n    guidance_scale=2,\\n    source_guidance_scale=1,\\n).images[0]\\n\\nimage.save(\"horse_to_elephant.png\")'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 6638}, page_content='image.save(\"horse_to_elephant.png\")\\n\\n# let\\'s try another example\\n# See more samples at the original repo: https://github.com/ChenWu98/cycle-diffusion\\nurl = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/A%20black%20colored%20car.png\"\\nresponse = requests.get(url)\\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\\ninit_image = init_image.resize((512, 512))\\ninit_image.save(\"black.png\")\\n\\nsource_prompt = \"A black colored car\"\\nprompt = \"A blue colored car\"\\n\\n# call the pipeline\\ntorch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt,\\n    source_prompt=source_prompt,\\n    image=init_image,\\n    num_inference_steps=100,\\n    eta=0.1,\\n    strength=0.85,\\n    guidance_scale=3,\\n    source_guidance_scale=1,\\n).images[0]\\n\\nimage.save(\"black_to_blue.png\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 0}, page_content='p align=\"center\">\\n  <br/>\\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg\" width=\"376\" height=\"59\" style=\"max-width: 100%;\">\\n  <br/>\\n</p>\\n\\n<p align=\"center\">\\n    <i>Huggingface Hub के लिए आधिकारिक पायथन क्लाइंट।</i>\\n</p>'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': -1}, page_content='<p align=\"center\">\\n    <i>Huggingface Hub के लिए आधिकारिक पायथन क्लाइंट।</i>\\n</p>\\n\\n<p align=\"center\">\\n    <a href=\"https://huggingface.co/docs/huggingface_hub/ko/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc\"></a>\\n    <a href=\"https://github.com/huggingface/huggingface_hub/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/huggingface_hub.svg\"></a>\\n    <a href=\"https://github.com/huggingface/huggingface_hub\"><img alt=\"PyPi version\" src=\"https://img.shields.io/pypi/pyversions/huggingface_hub.svg\"></a>\\n    <a href=\"https://pypi.org/project/huggingface-hub\"><img alt=\"downloads\" src=\"https://static.pepy.tech/badge/huggingface_hub/month\"></a>\\n    <a href=\"https://codecov.io/gh/huggingface/huggingface_hub\"><img alt=\"Code coverage\" src=\"https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL\"></a>\\n</p>'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 1258}, page_content='<h4 align=\"center\">\\n    <p>\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README.md\">English</a> |\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_de.md\">Deutsch</a> |\\n        <b>हिंदी</b>  |\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md\">한국어</a> |\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md\">中文（简体）</a> \\n    <p>\\n</h4>\\n\\n---\\n\\n**दस्तावेज़ीकरण**: <a href=\"https://hf.co/docs/huggingface_hub\" target=\"_blank\">https://hf.co/docs/huggingface_hub</a>\\n\\n**सोर्स कोड**: <a href=\"https://github.com/huggingface/huggingface_hub\" target=\"_blank\">https://github.com/huggingface/huggingface_hub</a>\\n\\n---\\n\\n## huggingface_hub लाइब्रेरी में आपका स्वागत है'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': -1}, page_content='---\\n\\n## huggingface_hub लाइब्रेरी में आपका स्वागत है\\n\\n`huggingface_hub` लाइब्रेरी आपको [हगिंग फेस हब](https://huggingface.co/) के साथ बातचीत करने की अनुमति देती है, जो रचनाकारों और सहयोगियों के लिए ओपन-सोर्स मशीन लर्निंग का लोकतंत्रीकरण करने वाला एक मंच है। अपनी परियोजनाओं के लिए पूर्व-प्रशिक्षित मॉडल और डेटासेट खोजें या हब पर होस्ट किए गए हजारों मशीन लर्निंग ऐप्स के साथ खेलें। आप समुदाय के साथ अपने स्वयं के मॉडल, डेटासेट और डेमो भी बना और साझा कर सकते हैं। `huggingface_hub` लाइब्रेरी पायथन के साथ इन सभी चीजों को करने का एक आसान तरीका प्रदान करती है।\\n\\n## प्रमुख विशेषताऐं'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 2547}, page_content='## प्रमुख विशेषताऐं\\n\\n- [फ़ाइलें डाउनलोड करें](https://huggingface.co/docs/huggingface_hub/en/guides/download) हब से।\\n- [फ़ाइलें अपलोड करें](https://huggingface.co/docs/huggingface_hub/en/guides/upload) हब पर।\\n- [अपनी रिपॉजिटरी प्रबंधित करें](https://huggingface.co/docs/huggingface_hub/en/guides/repository)।\\n- तैनात मॉडलों पर [अनुमान चलाएँ](https://huggingface.co/docs/huggingface_hub/en/guides/inference)।\\n- मॉडल, डेटासेट और स्पेस के लिए [खोज](https://huggingface.co/docs/huggingface_hub/en/guides/search)।\\n- [मॉडल कार्ड साझा करें](https://huggingface.co/docs/huggingface_hub/en/guides/model-cards) अपने मॉडलों का दस्तावेजीकरण करने के लिए।\\n- [समुदाय के साथ जुड़ें](https://huggingface.co/docs/huggingface_hub/en/guides/community) पीआर और टिप्पणियों के माध्यम से।\\n\\n## स्थापना\\n\\n[pip](https://pypi.org/project/huggingface-hub/) के साथ `huggingface_hub` पैकेज इंस्टॉल करें:\\n\\n```bash\\npip install huggingface_hub'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 3456}, page_content='```\\n\\nयदि आप चाहें, तो आप इसे [conda](https://huggingface.co/docs/huggingface_hub/en/installation#install-with-conda) से भी इंस्टॉल कर सकते हैं।\\n\\nपैकेज को डिफ़ॉल्ट रूप से न्यूनतम रखने के लिए, `huggingface_hub` कुछ उपयोग मामलों के लिए उपयोगी वैकल्पिक निर्भरता के साथ आता है। उदाहरण के लिए, यदि आप अनुमान के लिए संपूर्ण अनुभव चाहते हैं, तो चलाएँ:\\n\\n```bash\\npip install huggingface_hub[inference]\\n```\\n\\nअधिक इंस्टॉलेशन और वैकल्पिक निर्भरता जानने के लिए, [इंस्टॉलेशन गाइड](https://huggingface.co/docs/huggingface_hub/en/installation) देखें।\\n\\n## जल्दी शुरू\\n\\n### फ़ाइलें डाउनलोड करें\\n\\nएकल फ़ाइल डाउनलोड करें\\n\\n```py\\nfrom huggingface_hub import hf_hub_download\\n\\nhf_hub_download(repo_id=\"tiiuae/falcon-7b-instruct\", filename=\"config.json\")\\n```\\n\\nया एक संपूर्ण भंडार\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_download(\"stabilityai/stable-diffusion-2-1\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': -1}, page_content='```\\n\\nया एक संपूर्ण भंडार\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_download(\"stabilityai/stable-diffusion-2-1\")\\n```\\n\\nफ़ाइलें स्थानीय कैश फ़ोल्डर में डाउनलोड की जाएंगी. [this_guide] में अधिक विवरण (https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache)।\\n\\n### लॉग इन करें\\n\\nHugging Face Hub एप्लिकेशन को प्रमाणित करने के लिए टोकन का उपयोग करता है (देखें [docs](https://huggingface.co/docs/hub/security-tokens))। अपनी मशीन में लॉगिन करने के लिए, निम्नलिखित सीएलआई चलाएँ:\\n\\n```bash\\nhuggingface-cli login\\n# या कृपया इसे एक पर्यावरण चर के रूप में निर्दिष्ट करें।\\nhuggingface-cli login --token $HUGGINGFACE_TOKEN\\n```\\n\\n### एक रिपॉजिटरी बनाएं\\n\\n```py\\nfrom huggingface_hub import create_repo\\n\\ncreate_repo(repo_id=\"super-cool-model\")\\n```\\n\\n### फाइलें अपलोड करें\\n\\nएकल फ़ाइल अपलोड करें\\n\\n```py\\nfrom huggingface_hub import upload_file\\n\\nupload_file(\\n    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\",\\n    path_in_repo=\"README.md\",\\n    repo_id=\"lysandre/test-model\",\\n)\\n```\\n\\nया एक संपूर्ण फ़ोल्डर\\n\\n```py\\nfrom huggingface_hub import upload_folder\\n\\nupload_folder(\\n    folder_path=\"/path/to/local/space\",\\n    repo_id=\"username/my-cool-space\",\\n    repo_type=\"space\",\\n)'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 5369}, page_content='```\\n\\n[अपलोड गाइड](https://huggingface.co/docs/huggingface_hub/en/guides/upload) में विवरण के लिए।\\n\\n## हब से एकीकरण।\\n\\nहम मुफ्त मॉडल होस्टिंग और वर्जनिंग प्रदान करने के लिए शानदार ओपन सोर्स एमएल लाइब्रेरीज़ के साथ साझेदारी कर रहे हैं। आप मौजूदा एकीकरण [यहां](https://huggingface.co/docs/hub/libraries) पा सकते हैं।\\n\\nफायदे ये हैं:\\n\\n- पुस्तकालयों और उनके उपयोगकर्ताओं के लिए निःशुल्क मॉडल या डेटासेट होस्टिंग।\\n- गिट-आधारित दृष्टिकोण के कारण, बहुत बड़ी फ़ाइलों के साथ भी अंतर्निहित फ़ाइल संस्करणिंग।\\n- सभी मॉडलों के लिए होस्टेड अनुमान एपीआई सार्वजनिक रूप से उपलब्ध है।\\n- अपलोड किए गए मॉडलों के साथ खेलने के लिए इन-ब्राउज़र विजेट।\\n- कोई भी आपकी लाइब्रेरी के लिए एक नया मॉडल अपलोड कर सकता है, उन्हें मॉडल को खोजने योग्य बनाने के लिए बस संबंधित टैग जोड़ना होगा।\\n- तेज़ डाउनलोड! हम डाउनलोड को जियो-रेप्लिकेट करने के लिए क्लाउडफ्रंट (एक सीडीएन) का उपयोग करते हैं ताकि वे दुनिया में कहीं से भी तेजी से चमक सकें।\\n- उपयोग आँकड़े और अधिक सुविधाएँ आने वाली हैं।'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 6317}, page_content='यदि आप अपनी लाइब्रेरी को एकीकृत करना चाहते हैं, तो चर्चा शुरू करने के लिए बेझिझक एक मुद्दा खोलें। हमने ❤️ के साथ एक [चरण-दर-चरण मार्गदर्शिका](https://huggingface.co/docs/hub/adding-a-library) लिखी, जिसमें दिखाया गया कि यह एकीकरण कैसे करना है।\\n\\n## योगदान (सुविधा अनुरोध, बग, आदि) का अति स्वागत है 💙💚💛💜🧡❤️\\n\\nयोगदान के लिए हर किसी का स्वागत है और हम हर किसी के योगदान को महत्व देते हैं। कोड समुदाय की मदद करने का एकमात्र तरीका नहीं है।\\nप्रश्नों का उत्तर देना, दूसरों की मदद करना, उन तक पहुंचना और दस्तावेज़ों में सुधार करना समुदाय के लिए बेहद मूल्यवान है।\\nहमने संक्षेप में बताने के लिए एक [योगदान मार्गदर्शिका](https://github.com/huggingface/huggingface_hub/blob/main/CONTRIBUTING.md) लिखी है\\nइस भंडार में योगदान करने की शुरुआत कैसे करें।'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# LXMERT\\n\\n## Overview\\n\\nThe LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders\\n(one for the vision modality, one for the language modality, and then one to fuse both modalities) pretrained using a\\ncombination of masked language modeling, visual-language text alignment, ROI-feature regression, masked\\nvisual-attribute modeling, masked visual-object modeling, and visual-question answering objectives. The pretraining\\nconsists of multiple multi-modal datasets: MSCOCO, Visual-Genome + Visual-Genome Question Answering, VQA 2.0, and GQA.\\n\\nThe abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 1455}, page_content='The abstract from the paper is the following:\\n\\n*Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly,\\nthe alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality\\nEncoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we\\nbuild a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language\\nencoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language\\nsemantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative\\npretraining tasks: masked language modeling, masked object prediction (feature regression and label classification),\\ncross-modality matching, and image question answering. These tasks help in learning both intra-modality and\\ncross-modality relationships. After fine-tuning from our pretrained parameters, our model achieves the state-of-the-art\\nresults on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our\\npretrained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR, and improve the previous\\nbest result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel\\nmodel components and pretraining strategies significantly contribute to our strong results; and also present several\\nattention visualizations for the different encoders*\\n\\nThis model was contributed by [eltoto1219](https://huggingface.co/eltoto1219). The original code can be found [here](https://github.com/airsplay/lxmert).\\n\\n## Usage tips'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 3218}, page_content='## Usage tips\\n\\n- Bounding boxes are not necessary to be used in the visual feature embeddings, any kind of visual-spacial features\\n  will work.\\n- Both the language hidden states and the visual hidden states that LXMERT outputs are passed through the\\n  cross-modality layer, so they contain information from both modalities. To access a modality that only attends to\\n  itself, select the vision/language hidden states from the first input in the tuple.\\n- The bidirectional cross-modality encoder attention only returns attention values when the language modality is used\\n  as the input and the vision modality is used as the context vector. Further, while the cross-modality encoder\\n  contains self-attention for each respective modality and cross-attention, only the cross attention is returned and\\n  both self attention outputs are disregarded.\\n\\n## Resources\\n\\n- [Question answering task guide](../tasks/question_answering)\\n\\n## LxmertConfig\\n\\n[[autodoc]] LxmertConfig\\n\\n## LxmertTokenizer\\n\\n[[autodoc]] LxmertTokenizer\\n\\n## LxmertTokenizerFast\\n\\n[[autodoc]] LxmertTokenizerFast\\n\\n## Lxmert specific outputs\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertModelOutput\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput\\n\\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput\\n\\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\\n\\n<frameworkcontent>\\n<pt>\\n\\n## LxmertModel\\n\\n[[autodoc]] LxmertModel\\n    - forward\\n\\n## LxmertForPreTraining\\n\\n[[autodoc]] LxmertForPreTraining\\n    - forward\\n\\n## LxmertForQuestionAnswering'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': -1}, page_content='## LxmertForPreTraining\\n\\n[[autodoc]] LxmertForPreTraining\\n    - forward\\n\\n## LxmertForQuestionAnswering\\n\\n[[autodoc]] LxmertForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFLxmertModel\\n\\n[[autodoc]] TFLxmertModel\\n    - call\\n\\n## TFLxmertForPreTraining\\n\\n[[autodoc]] TFLxmertForPreTraining\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 1}, page_content='The tokenization pipeline\\n\\nWhen calling `Tokenizer.encode` or\\n`Tokenizer.encode_batch`, the input\\ntext(s) go through the following pipeline:\\n\\n-   `normalization`\\n-   `pre-tokenization`\\n-   `model`\\n-   `post-processing`\\n\\nWe\\'ll see in details what happens during each of those steps in detail,\\nas well as when you want to `decode <decoding>` some token ids, and how the 🤗 Tokenizers library allows you\\nto customize each of those steps to your needs. If you\\'re already\\nfamiliar with those steps and want to learn by seeing some code, jump to\\n`our BERT from scratch example <example>`.\\n\\nFor the examples that require a `Tokenizer` we will use the tokenizer we trained in the\\n`quicktour`, which you can load with:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START reload_tokenizer\",\\n\"end-before\": \"END reload_tokenizer\",\\n\"dedent\": 12}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_reload_tokenizer\",\\n\"end-before\": \"END pipeline_reload_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START reload_tokenizer\",\\n\"end-before\": \"END reload_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\n## Normalization'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 1465}, page_content='## Normalization\\n\\nNormalization is, in a nutshell, a set of operations you apply to a raw\\nstring to make it less random or \"cleaner\". Common operations include\\nstripping whitespace, removing accented characters or lowercasing all\\ntext. If you\\'re familiar with [Unicode\\nnormalization](https://unicode.org/reports/tr15), it is also a very\\ncommon normalization operation applied in most tokenizers.\\n\\nEach normalization operation is represented in the 🤗 Tokenizers library\\nby a `Normalizer`, and you can combine\\nseveral of those by using a `normalizers.Sequence`. Here is a normalizer applying NFD Unicode normalization\\nand removing accents as an example:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START setup_normalizer\",\\n\"end-before\": \"END setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_setup_normalizer\",\\n\"end-before\": \"END pipeline_setup_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START setup_normalizer\",\\n\"end-before\": \"END setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nYou can manually test that normalizer by applying it to any string:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': -1}, page_content='You can manually test that normalizer by applying it to any string:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START test_normalizer\",\\n\"end-before\": \"END test_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_test_normalizer\",\\n\"end-before\": \"END pipeline_test_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START test_normalizer\",\\n\"end-before\": \"END test_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nWhen building a `Tokenizer`, you can\\ncustomize its normalizer by just changing the corresponding attribute:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 3687}, page_content='When building a `Tokenizer`, you can\\ncustomize its normalizer by just changing the corresponding attribute:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START replace_normalizer\",\\n\"end-before\": \"END replace_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_replace_normalizer\",\\n\"end-before\": \"END pipeline_replace_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START replace_normalizer\",\\n\"end-before\": \"END replace_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nOf course, if you change the way a tokenizer applies normalization, you\\nshould probably retrain it from scratch afterward.\\n\\n## Pre-Tokenization\\n\\nPre-tokenization is the act of splitting a text into smaller objects\\nthat give an upper bound to what your tokens will be at the end of\\ntraining. A good way to think of this is that the pre-tokenizer will\\nsplit your text into \"words\" and then, your final tokens will be parts\\nof those words.\\n\\nAn easy way to pre-tokenize inputs is to split on spaces and\\npunctuations, which is done by the\\n`pre_tokenizers.Whitespace`\\npre-tokenizer:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': -1}, page_content='An easy way to pre-tokenize inputs is to split on spaces and\\npunctuations, which is done by the\\n`pre_tokenizers.Whitespace`\\npre-tokenizer:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START setup_pre_tokenizer\",\\n\"end-before\": \"END setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_setup_pre_tokenizer\",\\n\"end-before\": \"END pipeline_setup_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START setup_pre_tokenizer\",\\n\"end-before\": \"END setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThe output is a list of tuples, with each tuple containing one word and\\nits span in the original sentence (which is used to determine the final\\n`offsets` of our `Encoding`). Note that splitting on\\npunctuation will split contractions like `\"I\\'m\"` in this example.\\n\\nYou can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that will\\nsplit on space, punctuation and digits, separating numbers in their\\nindividual digits:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 6174}, page_content='You can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that will\\nsplit on space, punctuation and digits, separating numbers in their\\nindividual digits:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START combine_pre_tokenizer\",\\n\"end-before\": \"END combine_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_combine_pre_tokenizer\",\\n\"end-before\": \"END pipeline_combine_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START combine_pre_tokenizer\",\\n\"end-before\": \"END combine_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nAs we saw in the `quicktour`, you can\\ncustomize the pre-tokenizer of a `Tokenizer` by just changing the corresponding attribute:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': -1}, page_content='As we saw in the `quicktour`, you can\\ncustomize the pre-tokenizer of a `Tokenizer` by just changing the corresponding attribute:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START replace_pre_tokenizer\",\\n\"end-before\": \"END replace_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_replace_pre_tokenizer\",\\n\"end-before\": \"END pipeline_replace_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START replace_pre_tokenizer\",\\n\"end-before\": \"END replace_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nOf course, if you change the way the pre-tokenizer, you should probably\\nretrain your tokenizer from scratch afterward.\\n\\n## Model\\n\\nOnce the input texts are normalized and pre-tokenized, the\\n`Tokenizer` applies the model on the\\npre-tokens. This is the part of the pipeline that needs training on your\\ncorpus (or that has been trained if you are using a pretrained\\ntokenizer).\\n\\nThe role of the model is to split your \"words\" into tokens, using the\\nrules it has learned. It\\'s also responsible for mapping those tokens to\\ntheir corresponding IDs in the vocabulary of the model.\\n\\nThis model is passed along when intializing the\\n`Tokenizer` so you already know how to\\ncustomize this part. Currently, the 🤗 Tokenizers library supports:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 8634}, page_content='This model is passed along when intializing the\\n`Tokenizer` so you already know how to\\ncustomize this part. Currently, the 🤗 Tokenizers library supports:\\n\\n-   `models.BPE`\\n-   `models.Unigram`\\n-   `models.WordLevel`\\n-   `models.WordPiece`\\n\\nFor more details about each model and its behavior, you can check\\n[here](components#models)\\n\\n## Post-Processing\\n\\nPost-processing is the last step of the tokenization pipeline, to\\nperform any additional transformation to the\\n`Encoding` before it\\'s returned, like\\nadding potential special tokens.\\n\\nAs we saw in the quick tour, we can customize the post processor of a\\n`Tokenizer` by setting the\\ncorresponding attribute. For instance, here is how we can post-process\\nto make the inputs suitable for the BERT model:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START setup_processor\",\\n\"end-before\": \"END setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_setup_processor\",\\n\"end-before\": \"END pipeline_setup_processor\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START setup_processor\",\\n\"end-before\": \"END setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nNote that contrarily to the pre-tokenizer or the normalizer, you don\\'t\\nneed to retrain a tokenizer after changing its post-processor.\\n\\n## All together: a BERT tokenizer from scratch'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': -1}, page_content='Note that contrarily to the pre-tokenizer or the normalizer, you don\\'t\\nneed to retrain a tokenizer after changing its post-processor.\\n\\n## All together: a BERT tokenizer from scratch\\n\\nLet\\'s put all those pieces together to build a BERT tokenizer. First,\\nBERT relies on WordPiece, so we instantiate a new\\n`Tokenizer` with this model:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_tokenizer\",\\n\"end-before\": \"END bert_setup_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_tokenizer\",\\n\"end-before\": \"END bert_setup_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_tokenizer\",\\n\"end-before\": \"END bert_setup_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThen we know that BERT preprocesses texts by removing accents and\\nlowercasing. We also use a unicode normalizer:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 11226}, page_content='Then we know that BERT preprocesses texts by removing accents and\\nlowercasing. We also use a unicode normalizer:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_normalizer\",\\n\"end-before\": \"END bert_setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_normalizer\",\\n\"end-before\": \"END bert_setup_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_normalizer\",\\n\"end-before\": \"END bert_setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThe pre-tokenizer is just splitting on whitespace and punctuation:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': -1}, page_content='The pre-tokenizer is just splitting on whitespace and punctuation:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_pre_tokenizer\",\\n\"end-before\": \"END bert_setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_pre_tokenizer\",\\n\"end-before\": \"END bert_setup_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_pre_tokenizer\",\\n\"end-before\": \"END bert_setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nAnd the post-processing uses the template we saw in the previous\\nsection:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 12956}, page_content='And the post-processing uses the template we saw in the previous\\nsection:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_processor\",\\n\"end-before\": \"END bert_setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_processor\",\\n\"end-before\": \"END bert_setup_processor\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_processor\",\\n\"end-before\": \"END bert_setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nWe can use this tokenizer and train on it on wikitext like in the\\n`quicktour`:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': -1}, page_content='We can use this tokenizer and train on it on wikitext like in the\\n`quicktour`:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_train_tokenizer\",\\n\"end-before\": \"END bert_train_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_train_tokenizer\",\\n\"end-before\": \"END bert_train_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_train_tokenizer\",\\n\"end-before\": \"END bert_train_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\n## Decoding\\n\\nOn top of encoding the input texts, a `Tokenizer` also has an API for decoding, that is converting IDs\\ngenerated by your model back to a text. This is done by the methods\\n`Tokenizer.decode` (for one predicted text) and `Tokenizer.decode_batch` (for a batch of predictions).\\n\\nThe `decoder` will first convert the IDs back to tokens\\n(using the tokenizer\\'s vocabulary) and remove all special tokens, then\\njoin those tokens with spaces:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 14917}, page_content='The `decoder` will first convert the IDs back to tokens\\n(using the tokenizer\\'s vocabulary) and remove all special tokens, then\\njoin those tokens with spaces:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START test_decoding\",\\n\"end-before\": \"END test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_test_decoding\",\\n\"end-before\": \"END pipeline_test_decoding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START test_decoding\",\\n\"end-before\": \"END test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nIf you used a model that added special characters to represent subtokens\\nof a given \"word\" (like the `\"##\"` in\\nWordPiece) you will need to customize the `decoder` to treat\\nthem properly. If we take our previous `bert_tokenizer` for instance the\\ndefault decoding will give:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 16085}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_test_decoding\",\\n\"end-before\": \"END bert_test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_test_decoding\",\\n\"end-before\": \"END bert_test_decoding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_test_decoding\",\\n\"end-before\": \"END bert_test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nBut by changing it to a proper decoder, we get:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 16832}, page_content='But by changing it to a proper decoder, we get:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_proper_decoding\",\\n\"end-before\": \"END bert_proper_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_proper_decoding\",\\n\"end-before\": \"END bert_proper_decoding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_proper_decoding\",\\n\"end-before\": \"END bert_proper_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 1}, page_content='TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https://paperswithcode.com/method/resnet) that aim to boost accuracy while maintaining GPU training and inference efficiency.  They contain several design tricks including a SpaceToDepth stem, [Anti-Alias downsampling](https://paperswithcode.com/method/anti-alias-downsampling), In-Place Activated BatchNorm, Blocks selection and [squeeze-and-excitation layers](https://paperswithcode.com/method/squeeze-and-excitation-block).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model(\\'tresnet_l\\', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 1376}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]\\n```\\n\\nReplace the model name with the variant you want to use, e.g. `tresnet_l`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model(\\'tresnet_l\\', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 2721}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{ridnik2020tresnet,\\n      title={TResNet: High Performance GPU-Dedicated Architecture}, \\n      author={Tal Ridnik and Hussam Lawen and Asaf Noy and Emanuel Ben Baruch and Gilad Sharir and Itamar Friedman},\\n      year={2020},\\n      eprint={2003.13630},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 3428}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: TResNet\\n  Paper:\\n    Title: 'TResNet: High Performance GPU-Dedicated Architecture'\\n    URL: https://paperswithcode.com/paper/tresnet-high-performance-gpu-dedicated\\nModels:\\n- Name: tresnet_l\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 10873416792\\n    Parameters: 53456696\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_l\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L267\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_81_5-235b486c.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81.49%\\n      Top 5 Accuracy: 95.62%\\n- Name: tresnet_l_448\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 43488238584\\n    Parameters: 53456696\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': -1}, page_content=\"- Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_l_448\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L285\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_448-940d0cd1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82.26%\\n      Top 5 Accuracy: 95.98%\\n- Name: tresnet_m\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 5733048064\\n    Parameters: 41282200\\n    File Size: 125861314\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    Training Time: < 24 hours\\n    ID: tresnet_m\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 6546}, page_content=\"ID: tresnet_m\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L261\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_80_8-dbc13962.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.8%\\n      Top 5 Accuracy: 94.86%\\n- Name: tresnet_m_448\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 22929743104\\n    Parameters: 29278464\\n    File Size: 125861314\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_m_448\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L279\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 7982}, page_content=\"Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_448-bc359d10.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81.72%\\n      Top 5 Accuracy: 95.57%\\n- Name: tresnet_xl\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 15162534034\\n    Parameters: 75646610\\n    File Size: 314378965\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_xl\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L273\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_xl_82_0-a2d51b00.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82.05%\\n      Top 5 Accuracy: 95.93%\\n- Name: tresnet_xl_448\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 60641712730\\n    Parameters: 75646610\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': -1}, page_content=\"File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_xl_448\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L291\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_448-940d0cd1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83.06%\\n      Top 5 Accuracy: 96.19%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-configure.md', 'start_index': 1}, page_content=\"Configure the Dataset Viewer\\n\\nThe Dataset Viewer supports many [data files formats](./datasets-adding#file-formats), from text to tabular and from image to audio formats.\\nIt also separates the train/validation/test splits based on file and folder names.\\n\\nTo configure the Dataset Viewer for your dataset, first make sure your dataset is in a [supported data format](./datasets-adding#files-formats).\\n\\n## Configure dropdowns for splits or subsets\\n\\nIn the Dataset Viewer you can view the [train/validation/test](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) splits of datasets, and sometimes additionally choose between multiple subsets (e.g. one per language).\\n\\nTo define those dropdowns, you can name the data files or their folder after their split names (train/validation/test).\\nIt is also possible to customize your splits manually using YAML.\\n\\nFor more information, feel free to check out the documentation on [Data files Configuration](./datasets-data-files-configuration).\\n\\n## Disable the viewer\\n\\nThe dataset viewer can be disabled. To do this, add a YAML section to the dataset's `README.md` file (create one if it does not already exist) and add a `viewer` property with the value `false`.\\n\\n```\\n---\\nviewer: false\\n---\\n```\\n\\nNote that the viewer is always disabled on the private datasets.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 0}, page_content=\"Author: [@vasudevgupta7](https://github.com/thevasudevgupta/)\\n\\n## Intro\\n\\nIn this project, we fine-tuned [**BigBird**](https://arxiv.org/abs/2007.14062) on [**natural-questions**](https://huggingface.co/datasets/natural_questions) dataset for **question-answering** task on long documents. **BigBird**, is a **sparse-attention based transformer** which extends Transformer based models, such as BERT to much **longer sequences**.\\n\\nRead more about BigBird at https://huggingface.co/blog/big-bird\\n\\n## Fine-tuning\\n\\n**Setup**\\n\\nYou need to install jax yourself by following the official docs ([refer this](https://github.com/google/jax#installation)). Other requirements for this project can be installed by running following command:\\n\\n```shell\\npip3 install -qr requirements.txt\\n```\\n\\n**Download & prepare dataset**\\n\\nThe Natural Questions corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. This corpus takes ~100 GB on disk. We have used HuggingFace datasets to download & process the dataset.\\n\\n```shell\\n# just run following CMD\\npython3 prepare_natural_questions.py\\n\\n# this will download the whole dataset from HuggingFace Hub & will make it ready for training\\n# this script takes ~3 hours to process the dataset\\n```\\n\\n**Launch Training**\\n\\nWe have trained on Cloud's TPU v3-8. Each epoch took around 4.5 hours and the model got converged in just 2 epochs. You can see complete training args in [this script](bigbird_flax.py).\\n\\n```shell\\n# just run following CMD\\npython3 train.py\\n\\n# In case, you want to try hparams tuning, you can run wandb sweep\\nwandb sweep --project=bigbird sweep_flax.yaml\\nwandb agent <agent-id-obtained-by-above-CMD>\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 1755}, page_content='```\\n\\n## Evaluation\\n\\nOur evaluation script is different from the original script and we are evaluating sequences with length up to 4096 for simplicity. We managed to get the **EM score of ~55.2** using our evaluation script.\\n\\n```shell\\n# download validation-dataset first\\nmkdir natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/natural_questions-validation.arrow -P natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/dataset_info.json -P natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/state.json -P natural-questions-validation\\n\\n# simply run following command\\npython3 evaluate.py\\n```\\n\\nYou can find our checkpoint on HuggingFace Hub ([see this](https://huggingface.co/vasudevgupta/flax-bigbird-natural-questions)). In case you are interested in PyTorch BigBird fine-tuning, you can refer to [this repositary](https://github.com/thevasudevgupta/bigbird).'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 0}, page_content='--\\ntitle: Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\nthumbnail: /blog/assets/30_clip_rsicd/clip_schematic.png\\nauthors:\\n- user: arampacha\\n  guest: true\\n- user: devv\\n  guest: true\\n- user: goutham794\\n  guest: true\\n- user: cataluna84\\n  guest: true\\n- user: ghosh-r\\n  guest: true\\n- user: sujitpal\\n  guest: true\\n---\\n\\n# Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\n\\n\\n\\n## Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\n\\n<img src=\"/blog/assets/30_clip_rsicd/clip-rsicd-header-image.png\"/>\\n\\nIn July this year, [Hugging Face](https://huggingface.co/) organized a [Flax/JAX Community Week](https://github.com/huggingface/transformers/blob/master/examples/research_projects/jax-projects/README.md), and invited the community to submit projects to train Hugging Face [transformers](https://github.com/huggingface/transformers) models in the areas of Natural Language Processing (NLP) and Computer Vision (CV).\\n\\nParticipants used Tensor Processing Units (TPUs) with [Flax](https://github.com/google/flax) and [JAX](https://github.com/google/jax). JAX is a linear algebra library (like `numpy`) that can do automatic differentiation ([Autograd](https://github.com/hips/autograd)) and compile down to [XLA](https://www.tensorflow.org/xla), and Flax is a neural network library and ecosystem for JAX. TPU compute time was provided free by [Google Cloud](https://cloud.google.com/), who co-sponsored the event.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 1464}, page_content='Over the next two weeks, teams participated in lectures from Hugging Face and Google, trained one or more models using JAX/Flax, shared them with the community, and provided a  [Hugging Face Spaces](https://huggingface.co/spaces) demo showcasing the capabilities of their model. Approximately 100 teams participated in the event, and it resulted in 170 models and 36 demos.\\n\\nOur team, like probably many others, is a distributed one, spanning 12 time zones. Our common thread is that we all belong to the [TWIML Slack Channel](https://twimlai.slack.com/), where we came together based on a shared interest in Artificial Intelligence (AI) and Machine Learning (ML) topics. \\n\\nWe fine-tuned the [CLIP Network from OpenAI](https://openai.comclip/) with satellite images and captions from the [RSICD dataset](https://github.com/201528014227051/RSICD_optimal). The CLIP network learns visual concepts by being trained with image and caption pairs in a self-supervised manner, by using text paired with images found across the Internet. During inference, the model can predict the most relevant image given a text description or the most relevant text description given an image. CLIP is powerful enough to be used in zero-shot manner on everyday images. However, we felt that satellite images were sufficiently different from everyday images that it would be useful to fine-tune CLIP with them. Our intuition turned out to be correct, as the evaluation results (described below) shows. In this post, we describe details of our training and evaluation process, and our plans for future work on this project.\\n\\nThe goal of our project was to provide a useful service and demonstrate how to use CLIP for practical use cases. Our model can be used by applications to search through large collections of satellite images using textual queries. Such queries could describe the image in totality (for example, beach, mountain, airport, baseball field, etc) or search or mention specific geographic or man-made features within these images. CLIP can similarly be fine-tuned for other domains as well, as shown by the [medclip-demo team](https://huggingface.co/spaces/flax-community/medclip-demo) for medical images.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 3666}, page_content='The ability to search through large collections of images using text queries is an immensely powerful feature, and can be used as much for social good as for malign purposes. Possible applications include national defense and anti-terrorism activities, the ability to spot and address effects of climate change before they become unmanageable, etc. Unfortunately, this power can also be misused, such as for military and police surveillance by authoritarian nation-states, so it does raise some ethical questions as well.\\n\\nYou can read about the project on our [project page](https://github.com/arampacha/CLIP-rsicd), download our [trained model](https://huggingface.co/flax-community/clip-rsicd-v2) to use for inference on your own data, or see it in action on our [demo](https://huggingface.co/spaces/sujitpal/clip-rsicd-demo).\\n\\n\\n### Training\\n\\n#### Dataset\\n\\nWe fine-tuned the CLIP model primarily with the [RSICD dataset](https://github.com/201528014227051/RSICD_optimal). This dataset consists of about 10,000 images collected from Google Earth, Baidu Map, MapABC, and Tianditu. It is provided freely to the research community to advance remote sensing captioning via [Exploring Models and Data for Remote Sensing Image Caption Generation](https://arxiv.org/abs/1712.0783) (Lu et al, 2017). The images are (224, 224) RGB images at various resolutions, and each image has up to 5 captions associated with it.\\n\\n<img src=\"/blog/assets/30_clip_rsicd/rsicd-images-sampling.png\"/>\\n<center><i>Some examples of images from the RSICD dataset</i></center>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 5216}, page_content='In addition, we used the [UCM Dataset](https://mega.nz/folder/wCpSzSoS#RXzIlrv--TDt3ENZdKN8JA) and the [Sydney dataset](https://mega.nz/folder/pG4yTYYA#4c4buNFLibryZnlujsrwEQ) for training, The UCM dataset is based on the UC Merced Land Use dataset. It consists of 2100 images belonging to 21 classes (100 images per class), and each image has 5 captions. The Sydney dataset contains images of Sydney, Australia from Google Earth. It contains 613 images belonging to 7 classes. Images are (500, 500) RGB and provides 5 captions for each image. We used these additional datasets because we were not sure if the RSICD dataset would be large enough to fine-tune CLIP.\\n\\n\\n#### Model\\n\\nOur model is just the fine-tuned version of the original CLIP model shown below. Inputs to the model are a batch of captions and a batch of images passed through the CLIP text encoder and image encoder respectively. The training process uses [contrastive learning](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607) to learn a joint embedding representation of image and captions. In this embedding space, images and their respective captions are pushed close together, as are similar images and similar captions. Conversely, images and captions for different images, or dissimilar images and captions, are likely to be pushed further apart.\\n\\n<img src=\"/blog/assets/30_clip_rsicd/clip_schematic.png\"/>\\n<center><i>CLIP Training and Inference (Image Credit: CLIP: Connecting Text and Images (https://openai.comclip/))</i></center>\\n\\n\\n#### Data Augmentation\\n\\nIn order to regularize our dataset and prevent overfitting due to the size of the dataset, we used both image and text augmentation.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': -1}, page_content='#### Data Augmentation\\n\\nIn order to regularize our dataset and prevent overfitting due to the size of the dataset, we used both image and text augmentation.\\n\\nImage augmentation was done inline using built-in transforms from Pytorch\\'s [Torchvision](https://pytorch.org/vision/stable/index.html) package. The transformations used were Random Cropping, Random Resizing and Cropping, Color Jitter, and Random Horizontal and Vertical flipping.\\n\\nWe augmented the text with backtranslation to generate captions for images with less than 5 unique captions per image. The [Marian MT]((https://huggingface.co/transformers/model_doc/marian.html)) family of models from Hugging Face was used to translate the existing captions into French, Spanish, Italian, and Portuguese and back to English to fill out the captions for these images.\\n\\nAs shown in these loss plots below, image augmentation reduced overfitting significantly, and text and image augmentation reduced overfitting even further.\\n\\n<img src=\"/blog/assets/30_clip_rsicd/image-augment-loss.png\"/>\\n<img src=\"/blog/assets/30_clip_rsicd/image-text-aug-loss.png\"/>\\n<center><i>Evaluation and Training loss plots comparing (top) no augmentation vs image augmentation, and (bottom) image augmentation vs text+image augmentation</i></center>\\n\\n\\n### Evaluation\\n\\n#### Metrics\\n\\nA subset of the RSICD test set was used for evaluation. We found 30 categories of images in this subset. The evaluation was done by comparing each image with a set of 30 caption sentences of the form `\"An aerial photograph of {category}\"`. The model produced a ranked list of the 30 captions, from most relevant to least relevant. Categories corresponding to captions with the top k scores (for k=1, 3, 5, and 10) were compared with the category provided via the image file name. The scores are averaged over the entire set of images used for evaluation and reported for various values of k, as shown below.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 8678}, page_content='The `baseline` model represents the pre-trained `openai/clip-vit-base-path32` CLIP model. This model was fine-tuned with captions and images from the RSICD dataset, which resulted in a significant performance boost, as shown below.\\n\\nOur best model was trained with image and text augmentation, with batch size 1024 (128 on each of the 8 TPU cores), and the Adam optimizer with learning rate 5e-6. We trained our second base model with the same hyperparameters, except that we used the Adafactor optimizer with learning rate 1e-4. You can download either model from their model repos linked to in the table below.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 9292}, page_content='| Model-name                               | k=1   | k=3   | k=5   | k=10  |\\n| ---------------------------------------- | ----- | ----- | ----- | ----- |\\n| baseline                                 | 0.572 | 0.745 | 0.837 | 0.939 |\\n| bs128x8-lr1e-4-augs/ckpt-2               | 0.819 | 0.950 | 0.974 | 0.994 |\\n| bs128x8-lr1e-4-imgaugs/ckpt-2            | 0.812 | 0.942 | 0.970 | 0.991 |\\n| [bs128x8-lr1e-4-imgaugs-textaugs/ckpt-4](https://huggingface.co/flax-community/clip-rsicd)<sup>2</sup>   | 0.843 | 0.958 | 0.977 | 0.993 |\\n| bs128x8-lr5e-5-imgaugs-textaugs/ckpt-8   | 0.831 | 0.959 | 0.977 | 0.994 |\\n| bs128x8-lr5e-5-imgaugs/ckpt-4            | 0.746 | 0.906 | 0.956 | 0.989 |\\n| bs128x8-lr5e-5-imgaugs-textaugs-2/ckpt-4 | 0.811 | 0.945 | 0.972 | 0.993 |\\n| bs128x8-lr5e-5-imgaugs-textaugs-3/ckpt-5 | 0.823 | 0.946 | 0.971 | 0.992 |\\n| bs128x8-lr5e-5-wd02/ckpt-4               | 0.820 | 0.946 | 0.965 | 0.990 |'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': -1}, page_content='| bs128x8-lr5e-5-wd02/ckpt-4               | 0.820 | 0.946 | 0.965 | 0.990 |\\n| [bs128x8-lr5e-6-adam/ckpt-1](https://huggingface.co/flax-community/clip-rsicd-v2)<sup>1</sup> | **0.883** | **0.968** | **0.982** | **0.998** |'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 10351}, page_content='_1 - our best model, 2 - our second best model_\\n\\n\\n#### Demo\\n\\nYou can access the [CLIP-RSICD Demo](https://huggingface.co/spaces/sujitpal/clip-rsicd-demo) here. It uses our fine-tuned CLIP model to provide the following functionality:\\n\\n* Text to Image search\\n* Image to Image search\\n* Find text feature in image\\n\\nThe first two functionalities use the RSICD test set as its image corpus. They are encoded using our best fine-tuned CLIP model and stored in a [NMSLib](https://github.com/nmslib/nmslib) index which allows Approximate Nearest Neighbor based retrieval. For text-to-image and image-to-image search respectively, the query text or image are encoded with our model and matched against the image vectors in the corpus. For the third functionality, we divide the incoming image into patches and encode them, encode the queried text feature, match the text vector with each image patch vector, and return the probability of finding the feature in each patch.\\n\\n### Future Work\\n\\nWe are grateful that we have been given an opportunity to further refine our model. Some ideas we have for future work are as follows:\\n\\n1. Construct a sequence to sequence model using a CLIP encoder and a GPT-3 decoder and train it for image captioning.\\n2. Fine-tune the model on more image caption pairs from other datasets and investigate if we can improve its performance.\\n3. Investigate how fine-tuning affects the performance of model on non-RSICD image caption pairs.\\n4. Investigate the capability of the fine-tuned model to classify outside the categories it has been fine-tuned on.\\n5. Evaluate the model using other criteria such as image classification.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# 🤗 Transformers Notebooks\\n\\nYou can find here a list of the official notebooks provided by Hugging Face.\\n\\nAlso, we would like to list here interesting content created by the community.\\nIf you wrote some notebook(s) leveraging 🤗 Transformers and would like to be listed here, please open a\\nPull Request so it can be included under the Community notebooks.\\n\\n\\n## Hugging Face\\'s notebooks 🤗\\n\\n### Documentation notebooks\\n\\nYou can open any page of the documentation as a notebook in Colab (there is a button directly on said pages) but they are also listed here if you need them:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 1168}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [Quicktour of the library](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)  | A presentation of the various APIs in Transformers |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/en/transformers_doc/quicktour.ipynb)|\\n| [Summary of the tasks](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)  | How to run the models of the Transformers library task by task |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 2380}, page_content='| [Preprocessing data](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)  | How to use a tokenizer to preprocess your data |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)|\\n| [Fine-tuning a pretrained model](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)  | How to use the Trainer to fine-tune a pretrained model |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 3483}, page_content='| [Summary of the tokenizers](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)  | The differences between the tokenizers algorithm |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)|\\n| [Multilingual models](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)  | How to use the multilingual models of the library |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 4605}, page_content='### PyTorch Examples\\n\\n#### Natural Language Processing[[pytorch-nlp]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 4676}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|\\n| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 5877}, page_content='| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)|\\n| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 7078}, page_content='| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/main/examples/token_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)|\\n| [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SQUAD. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 8291}, page_content='| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SWAG. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)|\\n| [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/main/examples/translation.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on WMT. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/translation.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 9420}, page_content='| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on XSUM. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)|\\n| [How to train a language model from scratch](https://github.com/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)| Highlight all the steps to effectively train Transformer model on custom data | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 10549}, page_content='| [How to generate text](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)| How to use different decoding methods for language generation with transformers | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)|\\n| [How to generate text (with constraints)](https://github.com/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)| How to guide language generation with user-provided constraints | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 11682}, page_content='| [Reformer](https://github.com/huggingface/blog/blob/main/notebooks/03_reformer.ipynb)| How Reformer pushes the limits of language modeling | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/blog/blob/main/notebooks/03_reformer.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/patrickvonplaten/blog/blob/main/notebooks/03_reformer.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 12185}, page_content='#### Computer Vision[[pytorch-cv]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 12221}, page_content='| Notebook                                                                                                                                                                   | Description                                                                                                            |                                                                                                                                                                                                            |   |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 12726}, page_content='|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------:|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 13235}, page_content='| [How to fine-tune a model on image classification (Torchvision)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                   | Show how to preprocess the data using Torchvision and fine-tune any pretrained Vision model on Image Classification    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                 | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)|\\n| [How to fine-tune a model on image classification (Albumentations)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) | Show how to preprocess the data using Albumentations and fine-tune any pretrained Vision model on Image Classification | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)  | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 14626}, page_content='| [How to fine-tune a model on image classification (Kornia)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)                 | Show how to preprocess the data using Kornia and fine-tune any pretrained Vision model on Image Classification         | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)          | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)|\\n| [How to perform zero-shot object detection with OWL-ViT](https://github.com/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)          | Show how to perform zero-shot object detection on images with text queries                                             | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 16026}, page_content='| [How to fine-tune an image captioning model](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)                                      | Show how to fine-tune BLIP for image captioning on a custom dataset                                                    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)|\\n| [How to build an image similarity system with Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)                            | Show how to build an image similarity system                                                                           | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)                     | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 17399}, page_content='| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)                     | Show how to preprocess the data and fine-tune a pretrained SegFormer model on Semantic Segmentation                    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)|\\n| [How to fine-tune a VideoMAE model on video classification](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb)          | Show how to preprocess the data and fine-tune a pretrained VideoMAE model on Video Classification                      | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/video_classification.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/video_classification.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 18762}, page_content='#### Audio[[pytorch-audio]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 18791}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to fine-tune a speech recognition model in English](https://github.com/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)| Show how to preprocess the data and fine-tune a pretrained Speech model on TIMIT | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 19492}, page_content='| [How to fine-tune a speech recognition model in any language](https://github.com/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)| Show how to preprocess the data and fine-tune a multi-lingually pretrained speech model on Common Voice | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)|\\n| [How to fine-tune a model on audio classification](https://github.com/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained Speech model on Keyword Spotting | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 20774}, page_content='#### Biological Sequences[[pytorch-bio]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 20816}, page_content='| Notebook     | Description                                                                             |   |   |\\n|:----------|:----------------------------------------------------------------------------------------|:-------------|------:|\\n| [How to fine-tune a pre-trained protein model](https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) | See how to tokenize proteins and fine-tune a large pre-trained protein \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 21679}, page_content='| [How to generate protein folds](https://github.com/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) | See how to go from protein sequence to a full protein model and PDB file                | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) |\\n| [How to fine-tune a Nucleotide Transformer model](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) | See how to tokenize DNA and fine-tune a large pre-trained DNA \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 22930}, page_content='| [Fine-tune a Nucleotide Transformer model with LoRA](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) | Train even larger DNA models in a memory-efficient way | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) |'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 23616}, page_content='#### Other modalities[[pytorch-other]]\\n\\n| Notebook     | Description                                                                             |   |   |\\n|:----------|:----------------------------------------------------------------------------------------|:-------------|------:|\\n| [Probabilistic Time Series Forecasting](https://github.com/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) | See how to train Time Series Transformer on a custom dataset                            | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) |\\n\\n#### Utility notebooks[[pytorch-utility]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 24510}, page_content='#### Utility notebooks[[pytorch-utility]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to export model to ONNX](https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb)| Highlight how to export and run inference workloads through ONNX |\\n| [How to use Benchmarks](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| How to benchmark models with transformers | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)|\\n\\n### TensorFlow Examples\\n\\n#### Natural Language Processing[[tensorflow-nlp]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 25412}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|\\n| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 26622}, page_content='| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)|\\n| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 27841}, page_content='| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)|\\n| [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SQUAD. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 29072}, page_content='| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SWAG. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)|\\n| [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on WMT. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 30219}, page_content='| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on XSUM. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 30794}, page_content='#### Computer Vision[[tensorflow-cv]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 30833}, page_content='| Notebook                                                                                                                                                 | Description                                                                                         |   |   |\\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------|:-------------|------:|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 31382}, page_content='| [How to fine-tune a model on image classification](https://github.com/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)            | Show how to preprocess the data and fine-tune any pretrained Vision model on Image Classification   | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)|\\n| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb) | Show how to preprocess the data and fine-tune a pretrained SegFormer model on Semantic Segmentation | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 32666}, page_content='#### Biological Sequences[[tensorflow-bio]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to fine-tune a pre-trained protein model](https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | See how to tokenize proteins and fine-tune a large pre-trained protein \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) |\\n\\n#### Utility notebooks[[tensorflow-utility]]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 33443}, page_content=\"#### Utility notebooks[[tensorflow-utility]]\\n\\n| Notebook     |      Description      |   |                                                                                                                                                                                      |\\n|:----------|:-------------|:-------------|------:|\\n| [How to train TF/Keras models on TPU](https://github.com/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | See how to train at high speed on Google's TPU hardware | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) |\\n\\n### Optimum notebooks\\n\\n🤗  [Optimum](https://github.com/huggingface/optimum) is an extension of 🤗 Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardwares.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 34557}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 35385}, page_content='| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| Show how to apply static, dynamic and aware training quantization on a model using [Intel Neural Compressor (INC)](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 36158}, page_content='| [How to fine-tune a model on text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)| Show how to preprocess the data and fine-tune a model on any GLUE task using [ONNX Runtime](https://github.com/microsoft/onnxruntime). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)|\\n| [How to fine-tune a model on summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)| Show how to preprocess the data and fine-tune a model on XSUM using [ONNX Runtime](https://github.com/microsoft/onnxruntime). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)|'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 37486}, page_content='## Community notebooks:\\n\\nMore notebooks developed by the community are available [here](https://hf.co/docs/transformers/community#community-notebooks).'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 0}, page_content='--\\ntitle: \"Multivariate Probabilistic Time Series Forecasting with Informer\" \\nthumbnail: /blog/assets/134_informer/thumbnail.png\\nauthors:\\n- user: elisim\\n  guest: true\\n- user: nielsr\\n- user: kashif\\n---\\n\\n# Multivariate Probabilistic Time Series Forecasting with Informer\\n\\n\\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multivariate_informer.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n## Introduction\\n\\nA few months ago we introduced the [Time Series Transformer](https://huggingface.co/blog/time-series-transformers), which is the vanilla Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)) applied to forecasting, and showed an example for the **univariate** probabilistic forecasting task (i.e. predicting each time series\\' 1-d distribution individually). In this post we introduce the _Informer_ model ([Zhou, Haoyi, et al., 2021](https://arxiv.org/abs/2012.07436)), AAAI21 best paper which is [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in 🤗 Transformers. We will show how to use the Informer model for the **multivariate** probabilistic forecasting task, i.e., predicting the distribution of a future **vector** of time-series target values. Note that this will also work for the vanilla Time Series Transformer model.\\n\\n##  Multivariate Probabilistic Time Series Forecasting'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': -1}, page_content=\"##  Multivariate Probabilistic Time Series Forecasting\\n\\nAs far as the modeling aspect of probabilistic forecasting is concerned, the Transformer/Informer will require no change when dealing with multivariate time series. In both the univariate and multivariate setting, the model will receive a sequence of vectors and thus the only change is on the output or emission side.\\n\\nModeling the full joint conditional distribution of high dimensional data can get computationally expensive and thus methods resort to some approximation of the distribution, the easiest being to model the data as an independent distribution from the same family, or some low-rank approximation to the full covariance, etc. Here we will just resort to the independent (or diagonal) emissions which are supported for the families of distributions we have implemented [here](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils).\\n\\n## Informer - Under The Hood\\n\\nBased on the vanilla Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), Informer employs two major improvements. To understand these improvements, let's recall the drawbacks of the vanilla Transformer:\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 2699}, page_content=\"1. **Quadratic computation of canonical self-attention:** The vanilla Transformer has a computational complexity of \\\\\\\\(O(T^2 D)\\\\\\\\) where \\\\\\\\(T\\\\\\\\) is the time series length and \\\\\\\\(D\\\\\\\\) is the dimension of the hidden states. For long sequence time-series forecasting (also known as the _LSTF problem_), this might be really computationally expensive. To solve this problem, Informer employs a new self-attention mechanism called _ProbSparse_ attention, which has \\\\\\\\(O(T \\\\log T)\\\\\\\\) time and space complexity.\\n1. **Memory bottleneck when stacking layers:** When stacking \\\\\\\\(N\\\\\\\\) encoder/decoder layers, the vanilla Transformer has a memory usage of \\\\\\\\(O(N T^2)\\\\\\\\), which limits the model's capacity for long sequences. Informer uses a _Distilling_ operation, for reducing the input size between layers into its half slice. By doing so, it reduces the whole memory usage to be \\\\\\\\(O(N\\\\cdot T \\\\log T)\\\\\\\\).\\n\\nAs you can see, the motivation for the Informer model is similar to Longformer ([Beltagy et el., 2020](https://arxiv.org/abs/2004.05150)), Sparse Transformer ([Child et al., 2019](https://arxiv.org/abs/1904.10509)) and other NLP papers for reducing the quadratic complexity of the self-attention mechanism **when the input sequence is long**. Now, let's dive into _ProbSparse_ attention and the _Distilling_ operation with code examples. \\n\\n### ProbSparse Attention\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 4037}, page_content='### ProbSparse Attention\\n\\nThe main idea of ProbSparse is that the canonical self-attention scores form a long-tail distribution, where the \"active\" queries lie in the \"head\" scores and \"lazy\" queries lie in the \"tail\" area. By \"active\" query we mean a query \\\\\\\\(q_i\\\\\\\\) such that the dot-product \\\\\\\\(\\\\langle q_i,k_i \\\\rangle\\\\\\\\) **contributes** to the major attention, whereas a \"lazy\" query forms a dot-product which generates **trivial** attention. Here, \\\\\\\\(q_i\\\\\\\\) and \\\\\\\\(k_i\\\\\\\\) are the \\\\\\\\(i\\\\\\\\)-th rows in \\\\\\\\(Q\\\\\\\\) and \\\\\\\\(K\\\\\\\\) attention matrices respectively. \\n\\n| ![informer_full_vs_sparse_attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/informer_full_vs_sparse_attention.png) |\\n|:--:|\\n| Vanilla self attention vs ProbSparse attention from [Autoformer (Wu, Haixu, et al., 2021)](https://wuhaixu2016.github.io/pdf/NeurIPS2021_Autoformer.pdf) |\\n\\nGiven the idea of \"active\" and \"lazy\" queries, the ProbSparse attention selects the \"active\" queries, and creates a reduced query matrix \\\\\\\\(Q_{reduced}\\\\\\\\) which is used to calculate the attention weights in \\\\\\\\(O(T \\\\log T)\\\\\\\\). Let\\'s see this more in detail with a code example. \\n    \\nRecall the canonical self-attention formula:\\n\\n$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': -1}, page_content='$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nWhere \\\\\\\\(Q\\\\in \\\\mathbb{R}^{L_Q \\\\times d}\\\\\\\\), \\\\\\\\(K\\\\in \\\\mathbb{R}^{L_K \\\\times d}\\\\\\\\) and \\\\\\\\(V\\\\in \\\\mathbb{R}^{L_V \\\\times d}\\\\\\\\). Note that in practice, the input length of queries and keys are typically equivalent in the self-attention computation, i.e. \\\\\\\\(L_Q = L_K = T\\\\\\\\) where \\\\\\\\(T\\\\\\\\) is the time series length. Therefore, the \\\\\\\\(QK^T\\\\\\\\) multiplication takes \\\\\\\\(O(T^2 \\\\cdot d)\\\\\\\\) computational complexity. In ProbSparse attention, our goal is to create a new \\\\\\\\(Q_{reduce}\\\\\\\\) matrix and define:\\n\\n$$\\n\\\\textrm{ProbSparseAttention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{Q_{reduce}K^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nwhere the \\\\\\\\(Q_{reduce}\\\\\\\\) matrix only selects the Top  \\\\\\\\(u\\\\\\\\) \"active\" queries. Here, \\\\\\\\(u = c \\\\cdot \\\\log L_Q\\\\\\\\) and \\\\\\\\(c\\\\\\\\) called the _sampling factor_ hyperparameter for the ProbSparse attention. Since \\\\\\\\(Q_{reduce}\\\\\\\\) selects only the Top \\\\\\\\(u\\\\\\\\) queries, its size is \\\\\\\\(c\\\\cdot \\\\log L_Q \\\\times d\\\\\\\\), so the multiplication \\\\\\\\(Q_{reduce}K^T\\\\\\\\) takes only \\\\\\\\(O(L_K \\\\log L_Q) = O(T \\\\log T)\\\\\\\\).'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 6332}, page_content='This is good! But how can we select the \\\\\\\\(u\\\\\\\\) \"active\" queries to create \\\\\\\\(Q_{reduce}\\\\\\\\)? Let\\'s define the _Query Sparsity Measurement_.\\n\\n#### Query Sparsity Measurement\\nQuery Sparsity Measurement \\\\\\\\(M(q_i, K)\\\\\\\\) is used for selecting the \\\\\\\\(u\\\\\\\\) \"active\" queries \\\\\\\\(q_i\\\\\\\\) in \\\\\\\\(Q\\\\\\\\) to create \\\\\\\\(Q_{reduce}\\\\\\\\). In theory, the dominant \\\\\\\\(\\\\langle q_i,k_i \\\\rangle\\\\\\\\) pairs encourage the \"active\" \\\\\\\\(q_i\\\\\\\\)\\'s probability distribution **away** from the uniform distribution as can be seen in the figure below. Hence, the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the actual queries distribution and the uniform distribution is used to define the sparsity measurement. \\n\\n| ![informer_probsparse](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/informer_probsparse.png) | \\n|:--:|\\n| The illustration of ProbSparse Attention from official [repository](https://github.com/zhouhaoyi/Informer2020)|\\n\\n\\nIn practice, the measurement is defined as:\\n\\n$$\\nM(q_i, K) = \\\\max_j \\\\frac{q_ik_j^T}{\\\\sqrt{d}}-\\\\frac{1}{L_k} \\\\sum_{j=1}^{L_k}\\\\frac{q_ik_j^T}{\\\\sqrt{d}}\\n$$\\n\\n\\nThe important thing to understand here is when \\\\\\\\(M(q_i, K)\\\\\\\\) is larger, the query \\\\\\\\(q_i\\\\\\\\) should be in \\\\\\\\(Q_{reduce}\\\\\\\\) and vice versa.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 7615}, page_content='But how can we calculate the term \\\\\\\\(q_ik_j^T\\\\\\\\) in non-quadratic time? Recall that most of the dot-product \\\\\\\\(\\\\langle q_i,k_i \\\\rangle\\\\\\\\) generate either way the trivial attention (i.e. long-tail distribution property), so it is enough to randomly sample a subset of keys from \\\\\\\\(K\\\\\\\\), which will be called `K_sample` in the code.\\n\\nNow, we are ready to see the code of `probsparse_attention`:\\n    \\n```python\\nfrom torch import nn\\nimport math\\n\\n\\ndef probsparse_attention(query_states, key_states, value_states, sampling_factor=5):\\n    \"\"\"\\n    Compute the probsparse self-attention.\\n    Input shape: Batch x Time x Channel\\n\\n    Note the additional `sampling_factor` input.\\n    \"\"\"\\n    # get input sizes with logs\\n    L_K = key_states.size(1)\\n    L_Q = query_states.size(1)\\n    log_L_K = np.ceil(np.log1p(L_K)).astype(\"int\").item()\\n    log_L_Q = np.ceil(np.log1p(L_Q)).astype(\"int\").item()\\n\\n    # calculate a subset of samples to slice from K and create Q_K_sample\\n    U_part = min(sampling_factor * L_Q * log_L_K, L_K)\\n\\n    # create Q_K_sample (the q_i * k_j^T term in the sparsity measurement)\\n    index_sample = torch.randint(0, L_K, (U_part,))\\n    K_sample = key_states[:, index_sample, :]\\n    Q_K_sample = torch.bmm(query_states, K_sample.transpose(1, 2))\\n\\n    # calculate the query sparsity measurement with Q_K_sample\\n    M = Q_K_sample.max(dim=-1)[0] - torch.div(Q_K_sample.sum(dim=-1), L_K)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 9015}, page_content='# calculate u to find the Top-u queries under the sparsity measurement\\n    u = min(sampling_factor * log_L_Q, L_Q)\\n    M_top = M.topk(u, sorted=False)[1]\\n\\n    # calculate Q_reduce as query_states[:, M_top]\\n    dim_for_slice = torch.arange(query_states.size(0)).unsqueeze(-1)\\n    Q_reduce = query_states[dim_for_slice, M_top]  # size: c*log_L_Q x channel\\n\\n    # and now, same as the canonical\\n    d_k = query_states.size(-1)\\n    attn_scores = torch.bmm(Q_reduce, key_states.transpose(-2, -1))  # Q_reduce x K^T\\n    attn_scores = attn_scores / math.sqrt(d_k)\\n    attn_probs = nn.functional.softmax(attn_scores, dim=-1)\\n    attn_output = torch.bmm(attn_probs, value_states)\\n\\n    return attn_output, attn_scores'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 9723}, page_content='```\\nNote that in the implementation, \\\\\\\\(U_{part}\\\\\\\\) contain \\\\\\\\(L_Q\\\\\\\\) in the calculation, for stability issues (see [this disccusion](https://discuss.huggingface.co/t/probsparse-attention-in-informer/34428) for more information).\\n\\nWe did it! Please be aware that this is only a partial implementation of the `probsparse_attention`, and the full implementation can be found in 🤗 Transformers.\\n\\n### Distilling\\n\\nBecause of the ProbSparse self-attention, the encoder’s feature map has some redundancy that can be removed. Therefore,\\nthe distilling operation is used to reduce the input size between encoder layers into its half slice, thus in theory removing this redundancy. In practice, Informer\\'s \"distilling\" operation just adds 1D convolution layers with max pooling between each of the encoder layers. Let \\\\\\\\(X_n\\\\\\\\) be the output of the \\\\\\\\(n\\\\\\\\)-th encoder layer, the distilling operation is then defined as:\\n\\n\\n$$\\nX_{n+1} = \\\\textrm{MaxPool} ( \\\\textrm{ELU}(\\\\textrm{Conv1d}(X_n))\\n$$\\n\\n\\nLet\\'s see this in code:\\n    \\n```python\\nfrom torch import nn'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': -1}, page_content=\"Let's see this in code:\\n    \\n```python\\nfrom torch import nn\\n\\n# ConvLayer is a class with forward pass applying ELU and MaxPool1d\\ndef informer_encoder_forward(x_input, num_encoder_layers=3, distil=True):\\n    # Initialize the convolution layers\\n    if distil:\\n        conv_layers = nn.ModuleList([ConvLayer() for _ in range(num_encoder_layers - 1)])\\n        conv_layers.append(None)\\n    else:\\n        conv_layers = [None] * num_encoder_layers\\n    \\n    # Apply conv_layer between each encoder_layer\\n    for encoder_layer, conv_layer in zip(encoder_layers, conv_layers):\\n        output = encoder_layer(x_input)\\n        if conv_layer is not None:\\n            output = conv_layer(loutput)\\n    \\n    return output\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 11413}, page_content=\"```\\n    \\nBy reducing the input of each layer by two, we get a memory usage of \\\\\\\\(O(N\\\\cdot T \\\\log T)\\\\\\\\) instead of \\\\\\\\(O(N\\\\cdot T^2)\\\\\\\\) where \\\\\\\\(N\\\\\\\\) is the number of encoder/decoder layers. This is what we wanted!\\n    \\nThe Informer model in [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in the 🤗 Transformers library, and simply called `InformerModel`. In the sections below, we will show how to train this model on a custom multivariate time-series dataset.\\n\\n\\n## Set-up Environment\\n\\nFirst, let's install the necessary libraries: 🤗 Transformers, 🤗 Datasets, 🤗 Evaluate, 🤗 Accelerate and [GluonTS](https://github.com/awslabs/gluonts).\\n\\nAs we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches.\\n\\n\\n```python\\n!pip install -q transformers datasets evaluate accelerate gluonts ujson\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 12328}, page_content='```\\n\\n## Load Dataset\\n\\nIn this blog post, we\\'ll use the `traffic_hourly` dataset, which is available on the [Hugging Face Hub](https://huggingface.co/datasets/monash_tsf). This dataset contains the San Francisco Traffic dataset used by [Lai et al. (2017)](https://arxiv.org/abs/1703.07015). It contains 862 hourly time series showing the road occupancy rates in the range \\\\\\\\([0, 1]\\\\\\\\) on the San Francisco Bay area freeways from 2015 to 2016.\\n\\nThis dataset is part of the [Monash Time Series Forecasting](https://forecastingdata.org/) repository, a collection of time series datasets from a number of domains. It can be viewed as the [GLUE benchmark](https://gluebenchmark.com/) of time series forecasting.\\n\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"monash_tsf\", \"traffic_hourly\")\\n```\\n\\nAs can be seen, the dataset contains 3 splits: train, validation and test.\\n\\n\\n```python\\ndataset\\n\\n>>> DatasetDict({\\n        train: Dataset({\\n            features: [\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'],\\n            num_rows: 862\\n        })\\n        test: Dataset({\\n            features: [\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'],\\n            num_rows: 862\\n        })\\n        validation: Dataset({\\n            features: [\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'],\\n            num_rows: 862\\n        })\\n    })'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 13734}, page_content='```\\n\\nEach example contains a few keys, of which `start` and `target` are the most important ones. Let us have a look at the first time series in the dataset:\\n\\n\\n```python\\ntrain_example = dataset[\"train\"][0]\\ntrain_example.keys()\\n\\n>>> dict_keys([\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'])\\n```\\n\\nThe `start` simply indicates the start of the time series (as a datetime), and the `target` contains the actual values of the time series.\\n\\nThe `start` will be useful to add time related features to the time series values, as extra input to the model (such as \"month of year\"). Since we know the frequency of the data is `hourly`, we know for instance that the second value has the timestamp `2015-01-01 01:00:01`, `2015-01-01 02:00:01`, etc.\\n\\n\\n```python\\nprint(train_example[\"start\"])\\nprint(len(train_example[\"target\"]))\\n\\n>>> 2015-01-01 00:00:01\\n    17448\\n```\\n\\nThe validation set contains the same data as the training set, just for a `prediction_length` longer amount of time. This allows us to validate the model\\'s predictions against the ground truth.\\n\\nThe test set is again one `prediction_length` longer data compared to the validation set (or some multiple of `prediction_length` longer data compared to the training set for testing on multiple rolling windows).\\n\\n\\n```python\\nvalidation_example = dataset[\"validation\"][0]\\nvalidation_example.keys()\\n\\n>>> dict_keys([\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'])'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 15193}, page_content='```\\n\\nThe initial values are exactly the same as the corresponding training example. However, this example has `prediction_length=48` (48 hours, or 2 days) additional values compared to the training example. Let us verify it.\\n\\n\\n```python\\nfreq = \"1H\"\\nprediction_length = 48\\n\\nassert len(train_example[\"target\"]) + prediction_length == len(\\n    dataset[\"validation\"][0][\"target\"]\\n)\\n```\\n\\nLet\\'s visualize this:\\n\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\nnum_of_samples = 150\\n\\nfigure, axes = plt.subplots()\\naxes.plot(train_example[\"target\"][-num_of_samples:], color=\"blue\")\\naxes.plot(\\n    validation_example[\"target\"][-num_of_samples - prediction_length :],\\n    color=\"red\",\\n    alpha=0.5,\\n)\\n\\nplt.show()\\n```\\n    \\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_22_0.png)\\n    \\n\\nLet\\'s split up the data:\\n\\n\\n```python\\ntrain_dataset = dataset[\"train\"]\\ntest_dataset = dataset[\"test\"]\\n```\\n\\n## Update `start` to `pd.Period`\\n\\nThe first thing we\\'ll do is convert the `start` feature of each time series to a pandas `Period` index using the data\\'s `freq`:\\n\\n\\n```python\\nfrom functools import lru_cache\\n\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n@lru_cache(10_000)\\ndef convert_to_pandas_period(date, freq):\\n    return pd.Period(date, freq)\\n\\n\\ndef transform_start_field(batch, freq):\\n    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\\n    return batch'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 16617}, page_content='```\\n\\nWe now use `datasets`\\' [`set_transform`](https://huggingface.co/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.set_transform) functionality to do this on-the-fly in place:\\n\\n\\n```python\\nfrom functools import partial\\n\\ntrain_dataset.set_transform(partial(transform_start_field, freq=freq))\\ntest_dataset.set_transform(partial(transform_start_field, freq=freq))\\n```\\n\\nNow, let\\'s convert the dataset into a multivariate time series using the `MultivariateGrouper` from GluonTS. This grouper will convert the individual 1-dimensional time series into a single 2D matrix.\\n\\n\\n```python\\nfrom gluonts.dataset.multivariate_grouper import MultivariateGrouper\\n\\nnum_of_variates = len(train_dataset)\\n\\ntrain_grouper = MultivariateGrouper(max_target_dim=num_of_variates)\\ntest_grouper = MultivariateGrouper(\\n    max_target_dim=num_of_variates,\\n    num_test_dates=len(test_dataset) // num_of_variates, # number of rolling test windows\\n)\\n\\nmulti_variate_train_dataset = train_grouper(train_dataset)\\nmulti_variate_test_dataset = test_grouper(test_dataset)\\n```\\n\\nNote that the target is now 2-dimensional, where the first dimension is the number of variates (number of time series) and the second is the time series values (time dimension): \\n\\n\\n```python\\nmulti_variate_train_example = multi_variate_train_dataset[0]\\nprint(\"multi_variate_train_example[\"target\"].shape =\", multi_variate_train_example[\"target\"].shape)\\n\\n>>> multi_variate_train_example[\"target\"].shape = (862, 17448)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 18098}, page_content='```\\n\\n## Define the Model\\n\\nNext, let\\'s instantiate a model. The model will be trained from scratch, hence we won\\'t use the `from_pretrained` method here, but rather randomly initialize the model from a [`config`](https://huggingface.co/docs/transformers/main/en/model_doc/informer#transformers.InformerConfig).\\n\\nWe specify a couple of additional parameters to the model:\\n- `prediction_length` (in our case, `48` hours): this is the horizon that the decoder of the Informer will learn to predict for;\\n- `context_length`: the model will set the `context_length` (input of the encoder) equal to the `prediction_length`, if no `context_length` is specified;\\n- `lags` for a given frequency: these specify an efficient \"look back\" mechanism, where we concatenate values from the past to the current values as additional features, e.g. for a `Daily` frequency we might consider a look back of `[1, 7, 30, ...]` or for `Minute` data we might consider `[1, 30, 60, 60*24, ...]` etc.;\\n- the number of time features: in our case, this will be `5` as we\\'ll add `HourOfDay`, `DayOfWeek`, ..., and `Age` features (see below).\\n\\nLet us check the default lags provided by GluonTS for the given frequency (\"hourly\"):\\n\\n\\n```python\\nfrom gluonts.time_feature import get_lags_for_frequency\\n\\nlags_sequence = get_lags_for_frequency(freq)\\nprint(lags_sequence)\\n\\n>>> [1, 2, 3, 4, 5, 6, 7, 23, 24, 25, 47, 48, 49, 71, 72, 73, 95, 96, 97, 119, 120, \\n     121, 143, 144, 145, 167, 168, 169, 335, 336, 337, 503, 504, 505, 671, 672, 673, 719, 720, 721]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 19617}, page_content='```\\n\\nThis means that this would look back up to 721 hours (~30 days) for each time step, as additional features. However, the resulting feature vector would end up being of size `len(lags_sequence)*num_of_variates` which for our case will be 34480! This is not going to work so we will use our own sensible lags.\\n\\nLet us also check the default time features which GluonTS provides us:\\n\\n\\n```python\\nfrom gluonts.time_feature import time_features_from_frequency_str\\n\\ntime_features = time_features_from_frequency_str(freq)\\nprint(time_features)\\n\\n>>> [<function hour_of_day at 0x7f3809539240>, <function day_of_week at 0x7f3809539360>, <function day_of_month at 0x7f3809539480>, <function day_of_year at 0x7f38095395a0>]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 20332}, page_content='```\\n\\nIn this case, there are four additional features, namely \"hour of day\", \"day of week\", \"day of month\" and \"day of year\". This means that for each time step, we\\'ll add these features as a scalar values. For example, consider the timestamp `2015-01-01 01:00:01`. The four additional features will be:\\n\\n\\n```python\\nfrom pandas.core.arrays.period import period_array\\n\\ntimestamp = pd.Period(\"2015-01-01 01:00:01\", freq=freq)\\ntimestamp_as_index = pd.PeriodIndex(data=period_array([timestamp]))\\nadditional_features = [\\n    (time_feature.__name__, time_feature(timestamp_as_index))\\n    for time_feature in time_features\\n]\\nprint(dict(additional_features))\\n\\n>>> {\\'hour_of_day\\': array([-0.45652174]), \\'day_of_week\\': array([0.]), \\'day_of_month\\': array([-0.5]), \\'day_of_year\\': array([-0.5])}'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 21115}, page_content='```\\n\\nNote that hours and days are encoded as values between `[-0.5, 0.5]` from GluonTS. For more information about `time_features`, please see [this](https://github.com/awslabs/gluonts/blob/dev/src/gluonts/time_feature/_base.py). Besides those 4 features, we\\'ll also add an \"age\" feature as we\\'ll see later on in the data transformations.\\n\\nWe now have everything to define the model:\\n\\n\\n```python\\nfrom transformers import InformerConfig, InformerForPrediction\\n\\nconfig = InformerConfig(\\n    # in the multivariate setting, input_size is the number of variates in the time series per time step\\n    input_size=num_of_variates,\\n    # prediction length:\\n    prediction_length=prediction_length,\\n    # context length:\\n    context_length=prediction_length * 2,\\n    # lags value copied from 1 week before:\\n    lags_sequence=[1, 24 * 7],\\n    # we\\'ll add 5 time features (\"hour_of_day\", ..., and \"age\"):\\n    num_time_features=len(time_features) + 1,\\n    \\n    # informer params:\\n    dropout=0.1,\\n    encoder_layers=6,\\n    decoder_layers=4,\\n    # project input from num_of_variates*len(lags_sequence)+num_time_features to:\\n    d_model=64,\\n)\\n\\nmodel = InformerForPrediction(config)\\n```\\n\\nBy default, the model uses a diagonal Student-t distribution (but this is [configurable](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils)):\\n\\n\\n```python\\nmodel.config.distribution_output\\n\\n>>> \\'student_t\\''),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 22516}, page_content=\"```\\n\\n## Define Transformations\\n\\nNext, we define the transformations for the data, in particular for the creation of the time features (based on the dataset or universal ones).\\n\\nAgain, we'll use the GluonTS library for this. We define a `Chain` of transformations (which is a bit comparable to `torchvision.transforms.Compose` for images). It allows us to combine several transformations into a single pipeline.\\n\\n\\n```python\\nfrom gluonts.time_feature import TimeFeature\\nfrom gluonts.dataset.field_names import FieldName\\nfrom gluonts.transform import (\\n    AddAgeFeature,\\n    AddObservedValuesIndicator,\\n    AddTimeFeatures,\\n    AsNumpyArray,\\n    Chain,\\n    ExpectedNumInstanceSampler,\\n    InstanceSplitter,\\n    RemoveFields,\\n    SelectFields,\\n    SetField,\\n    TestSplitSampler,\\n    Transformation,\\n    ValidationSplitSampler,\\n    VstackFeatures,\\n    RenameFields,\\n)\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 23381}, page_content='```\\n\\nThe transformations below are annotated with comments, to explain what they do. At a high level, we will iterate over the individual time series of our dataset and add/remove fields or features:\\n\\n\\n```python\\nfrom transformers import PretrainedConfig\\n\\n\\ndef create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\\n    # create list of fields to remove later\\n    remove_field_names = []\\n    if config.num_static_real_features == 0:\\n        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\\n    if config.num_dynamic_real_features == 0:\\n        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\\n    if config.num_static_categorical_features == 0:\\n        remove_field_names.append(FieldName.FEAT_STATIC_CAT)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 24125}, page_content=\"return Chain(\\n        # step 1: remove static/dynamic fields if not specified\\n        [RemoveFields(field_names=remove_field_names)]\\n        # step 2: convert the data to NumPy (potentially not needed)\\n        + (\\n            [\\n                AsNumpyArray(\\n                    field=FieldName.FEAT_STATIC_CAT,\\n                    expected_ndim=1,\\n                    dtype=int,\\n                )\\n            ]\\n            if config.num_static_categorical_features > 0\\n            else []\\n        )\\n        + (\\n            [\\n                AsNumpyArray(\\n                    field=FieldName.FEAT_STATIC_REAL,\\n                    expected_ndim=1,\\n                )\\n            ]\\n            if config.num_static_real_features > 0\\n            else []\\n        )\\n        + [\\n            AsNumpyArray(\\n                field=FieldName.TARGET,\\n                # we expect an extra dim for the multivariate case:\\n                expected_ndim=1 if config.input_size == 1 else 2,\\n            ),\\n            # step 3: handle the NaN's by filling in the target with zero\\n            # and return the mask (which is in the observed values)\\n            # true for observed values, false for nan's\\n            # the decoder uses this mask (no loss is incurred for unobserved values)\\n            # see loss_weights inside the xxxForPrediction model\\n            AddObservedValuesIndicator(\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName.OBSERVED_VALUES,\\n            ),\\n            # step 4: add temporal features based on freq of the dataset\\n            # these serve as positional encodings\\n            AddTimeFeatures(\\n                start_field=FieldName.START,\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName.FEAT_TIME,\\n                time_features=time_features_from_frequency_str(freq),\\n                pred_length=config.prediction_length,\\n            ),\\n            # step 5: add another temporal feature (just a single number)\\n            # tells the model where in the life the value of the time series is\\n            # sort of running counter\\n            AddAgeFeature(\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName.FEAT_AGE,\\n                pred_length=config.prediction_length,\\n                log_scale=True,\\n            ),\\n            # step 6: vertically stack all the temporal features into the key FEAT_TIME\\n            VstackFeatures(\\n                output_field=FieldName.FEAT_TIME,\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': -1}, page_content='log_scale=True,\\n            ),\\n            # step 6: vertically stack all the temporal features into the key FEAT_TIME\\n            VstackFeatures(\\n                output_field=FieldName.FEAT_TIME,\\n                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\\n                + (\\n                    [FieldName.FEAT_DYNAMIC_REAL]\\n                    if config.num_dynamic_real_features > 0\\n                    else []\\n                ),\\n            ),\\n            # step 7: rename to match HuggingFace names\\n            RenameFields(\\n                mapping={\\n                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\\n                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\\n                    FieldName.FEAT_TIME: \"time_features\",\\n                    FieldName.TARGET: \"values\",\\n                    FieldName.OBSERVED_VALUES: \"observed_mask\",\\n                }\\n            ),\\n        ]\\n    )'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 27372}, page_content='```\\n\\n## Define `InstanceSplitter`\\n\\nFor training/validation/testing we next create an `InstanceSplitter` which is used to sample windows from the dataset (as, remember, we can\\'t pass the entire history of values to the model due to time- and memory constraints).\\n\\nThe instance splitter samples random `context_length` sized and subsequent `prediction_length` sized windows from the data, and appends a `past_` or `future_` key to any temporal keys in `time_series_fields` for the respective windows. The instance splitter can be configured into three different modes:\\n1. `mode=\"train\"`: Here we sample the context and prediction length windows randomly from the dataset given to it (the training dataset)\\n2. `mode=\"validation\"`: Here we sample the very last context length window and prediction window from the dataset given to it (for the back-testing or validation likelihood calculations)\\n3. `mode=\"test\"`: Here we sample the very last context length window only (for the prediction use case)\\n\\n\\n```python\\nfrom gluonts.transform.sampler import InstanceSampler\\nfrom typing import Optional\\n\\n\\ndef create_instance_splitter(\\n    config: PretrainedConfig,\\n    mode: str,\\n    train_sampler: Optional[InstanceSampler] = None,\\n    validation_sampler: Optional[InstanceSampler] = None,\\n) -> Transformation:\\n    assert mode in [\"train\", \"validation\", \"test\"]\\n\\n    instance_sampler = {\\n        \"train\": train_sampler\\n        or ExpectedNumInstanceSampler(\\n            num_instances=1.0, min_future=config.prediction_length\\n        ),\\n        \"validation\": validation_sampler\\n        or ValidationSplitSampler(min_future=config.prediction_length),\\n        \"test\": TestSplitSampler(),\\n    }[mode]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 29061}, page_content='return InstanceSplitter(\\n        target_field=\"values\",\\n        is_pad_field=FieldName.IS_PAD,\\n        start_field=FieldName.START,\\n        forecast_start_field=FieldName.FORECAST_START,\\n        instance_sampler=instance_sampler,\\n        past_length=config.context_length + max(config.lags_sequence),\\n        future_length=config.prediction_length,\\n        time_series_fields=[\"time_features\", \"observed_mask\"],\\n    )'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 29479}, page_content='```\\n\\n## Create DataLoaders\\n\\nNext, it\\'s time to create the DataLoaders, which allow us to have batches of (input, output) pairs - or in other words (`past_values`, `future_values`).\\n\\n\\n```python\\nfrom typing import Iterable\\n\\nimport torch\\nfrom gluonts.itertools import Cached, Cyclic\\nfrom gluonts.dataset.loader import as_stacked_batches\\n\\n\\ndef create_train_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n    num_batches_per_epoch: int,\\n    shuffle_buffer_length: Optional[int] = None,\\n    cache_data: bool = True,\\n    **kwargs,\\n) -> Iterable:\\n    PREDICTION_INPUT_NAMES = [\\n        \"past_time_features\",\\n        \"past_values\",\\n        \"past_observed_mask\",\\n        \"future_time_features\",\\n    ]\\n    if config.num_static_categorical_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\\n\\n    if config.num_static_real_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\\n\\n    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\\n        \"future_values\",\\n        \"future_observed_mask\",\\n    ]\\n\\n    transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(data, is_train=True)\\n    if cache_data:\\n        transformed_data = Cached(transformed_data)\\n\\n    # we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \"train\")'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': -1}, page_content='# we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \"train\")\\n\\n    # the instance splitter will sample a window of\\n    # context length + lags + prediction length (from all the possible transformed time series, 1 in our case)\\n    # randomly from within the target time series and return an iterator.\\n    stream = Cyclic(transformed_data).stream()\\n    training_instances = instance_splitter.apply(stream)\\n    \\n    return as_stacked_batches(\\n        training_instances,\\n        batch_size=batch_size,\\n        shuffle_buffer_length=shuffle_buffer_length,\\n        field_names=TRAINING_INPUT_NAMES,\\n        output_type=torch.tensor,\\n        num_batches_per_epoch=num_batches_per_epoch,\\n    )'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 31477}, page_content='```\\n\\n\\n```python\\ndef create_backtest_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n    **kwargs,\\n):\\n    PREDICTION_INPUT_NAMES = [\\n        \"past_time_features\",\\n        \"past_values\",\\n        \"past_observed_mask\",\\n        \"future_time_features\",\\n    ]\\n    if config.num_static_categorical_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\\n\\n    if config.num_static_real_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\\n\\n    transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(data)\\n\\n    # we create a Validation Instance splitter which will sample the very last\\n    # context window seen during training only for the encoder.\\n    instance_sampler = create_instance_splitter(config, \"validation\")\\n\\n    # we apply the transformations in train mode\\n    testing_instances = instance_sampler.apply(transformed_data, is_train=True)\\n    \\n    return as_stacked_batches(\\n        testing_instances,\\n        batch_size=batch_size,\\n        output_type=torch.tensor,\\n        field_names=PREDICTION_INPUT_NAMES,\\n    )\\n\\ndef create_test_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n    **kwargs,\\n):\\n    PREDICTION_INPUT_NAMES = [\\n        \"past_time_features\",\\n        \"past_values\",\\n        \"past_observed_mask\",\\n        \"future_time_features\",\\n    ]\\n    if config.num_static_categorical_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\\n\\n    if config.num_static_real_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\\n\\n    transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(data, is_train=False)\\n\\n    # We create a test Instance splitter to sample the very last\\n    # context window from the dataset provided.\\n    instance_sampler = create_instance_splitter(config, \"test\")'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': -1}, page_content='# We create a test Instance splitter to sample the very last\\n    # context window from the dataset provided.\\n    instance_sampler = create_instance_splitter(config, \"test\")\\n\\n    # We apply the transformations in test mode\\n    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\\n    \\n    return as_stacked_batches(\\n        testing_instances,\\n        batch_size=batch_size,\\n        output_type=torch.tensor,\\n        field_names=PREDICTION_INPUT_NAMES,\\n    )'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 33728}, page_content=\"```\\n\\n\\n```python\\ntrain_dataloader = create_train_dataloader(\\n    config=config,\\n    freq=freq,\\n    data=multi_variate_train_dataset,\\n    batch_size=256,\\n    num_batches_per_epoch=100,\\n    num_workers=2,\\n)\\n\\ntest_dataloader = create_backtest_dataloader(\\n    config=config,\\n    freq=freq,\\n    data=multi_variate_test_dataset,\\n    batch_size=32,\\n)\\n```\\n\\nLet's check the first batch:\\n\\n\\n```python\\nbatch = next(iter(train_dataloader))\\nfor k, v in batch.items():\\n    print(k, v.shape, v.type())\\n\\n>>> past_time_features torch.Size([256, 264, 5]) torch.FloatTensor\\n    past_values torch.Size([256, 264, 862]) torch.FloatTensor\\n    past_observed_mask torch.Size([256, 264, 862]) torch.FloatTensor\\n    future_time_features torch.Size([256, 48, 5]) torch.FloatTensor\\n    future_values torch.Size([256, 48, 862]) torch.FloatTensor\\n    future_observed_mask torch.Size([256, 48, 862]) torch.FloatTensor\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 34613}, page_content='```\\n\\nAs can be seen, we don\\'t feed `input_ids` and `attention_mask` to the encoder (as would be the case for NLP models), but rather `past_values`, along with `past_observed_mask`, `past_time_features` and `static_real_features`.\\n\\nThe decoder inputs consist of `future_values`, `future_observed_mask` and `future_time_features`. The `future_values` can be seen as the equivalent of `decoder_input_ids` in NLP.\\n\\nWe refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/informer#transformers.InformerModel.forward.past_values) for a detailed explanation for each of them.\\n\\n## Forward Pass\\n\\nLet\\'s perform a single forward pass with the batch we just created:\\n\\n\\n```python\\n# perform forward pass\\noutputs = model(\\n    past_values=batch[\"past_values\"],\\n    past_time_features=batch[\"past_time_features\"],\\n    past_observed_mask=batch[\"past_observed_mask\"],\\n    static_categorical_features=batch[\"static_categorical_features\"]\\n    if config.num_static_categorical_features > 0\\n    else None,\\n    static_real_features=batch[\"static_real_features\"]\\n    if config.num_static_real_features > 0\\n    else None,\\n    future_values=batch[\"future_values\"],\\n    future_time_features=batch[\"future_time_features\"],\\n    future_observed_mask=batch[\"future_observed_mask\"],\\n    output_hidden_states=True,\\n)\\n```\\n\\n\\n```python\\nprint(\"Loss:\", outputs.loss.item())\\n\\n>>> Loss: -1071.5718994140625'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 36005}, page_content=\"```\\n\\nNote that the model is returning a loss. This is possible as the decoder automatically shifts the `future_values` one position to the right in order to have the labels. This allows computing a loss between the predicted values and the labels. The loss is the negative log-likelihood of the predicted distribution with respect to the ground truth values and tends to negative infinity.\\n\\nAlso note that the decoder uses a causal mask to not look into the future as the values it needs to predict are in the `future_values` tensor.\\n\\n## Train the Model\\n\\nIt's time to train the model! We'll use a standard PyTorch training loop.\\n\\nWe will use the 🤗 [Accelerate](https://huggingface.co/docs/accelerate/index) library here, which automatically places the model, optimizer and dataloader on the appropriate `device`.\\n\\n\\n```python\\nfrom accelerate import Accelerator\\nfrom torch.optim import AdamW\\n\\nepochs = 25\\nloss_history = []\\n\\naccelerator = Accelerator()\\ndevice = accelerator.device\\n\\nmodel.to(device)\\noptimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\\n\\nmodel, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_dataloader,\\n)\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': -1}, page_content='model, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_dataloader,\\n)\\n\\nmodel.train()\\nfor epoch in range(epochs):\\n    for idx, batch in enumerate(train_dataloader):\\n        optimizer.zero_grad()\\n        outputs = model(\\n            static_categorical_features=batch[\"static_categorical_features\"].to(device)\\n            if config.num_static_categorical_features > 0\\n            else None,\\n            static_real_features=batch[\"static_real_features\"].to(device)\\n            if config.num_static_real_features > 0\\n            else None,\\n            past_time_features=batch[\"past_time_features\"].to(device),\\n            past_values=batch[\"past_values\"].to(device),\\n            future_time_features=batch[\"future_time_features\"].to(device),\\n            future_values=batch[\"future_values\"].to(device),\\n            past_observed_mask=batch[\"past_observed_mask\"].to(device),\\n            future_observed_mask=batch[\"future_observed_mask\"].to(device),\\n        )\\n        loss = outputs.loss\\n\\n        # Backpropagation\\n        accelerator.backward(loss)\\n        optimizer.step()\\n\\n        loss_history.append(loss.item())\\n        if idx % 100 == 0:\\n            print(loss.item())\\n\\n>>> -1081.978515625\\n    ...\\n    -2877.723876953125'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 38348}, page_content='```\\n\\n```python\\n# view training\\nloss_history = np.array(loss_history).reshape(-1)\\nx = range(loss_history.shape[0])\\nplt.figure(figsize=(10, 5))\\nplt.plot(x, loss_history, label=\"train\")\\nplt.title(\"Loss\", fontsize=15)\\nplt.legend(loc=\"upper right\")\\nplt.xlabel(\"iteration\")\\nplt.ylabel(\"nll\")\\nplt.show()'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 38645}, page_content='```\\n\\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_62_0.png)\\n    \\n\\n## Inference\\n\\nAt inference time, it\\'s recommended to use the `generate()` method for autoregressive generation, similar to NLP models.\\n\\nForecasting involves getting data from the test instance sampler, which will sample the very last `context_length` sized window of values from each time series in the dataset, and pass it to the model. Note that we pass `future_time_features`, which are known ahead of time, to the decoder.\\n\\nThe model will autoregressively sample a certain number of values from the predicted distribution and pass them back to the decoder to return the prediction outputs:\\n\\n\\n```python\\nmodel.eval()\\n\\nforecasts_ = []\\n\\nfor batch in test_dataloader:\\n    outputs = model.generate(\\n        static_categorical_features=batch[\"static_categorical_features\"].to(device)\\n        if config.num_static_categorical_features > 0\\n        else None,\\n        static_real_features=batch[\"static_real_features\"].to(device)\\n        if config.num_static_real_features > 0\\n        else None,\\n        past_time_features=batch[\"past_time_features\"].to(device),\\n        past_values=batch[\"past_values\"].to(device),\\n        future_time_features=batch[\"future_time_features\"].to(device),\\n        past_observed_mask=batch[\"past_observed_mask\"].to(device),\\n    )\\n    forecasts_.append(outputs.sequences.cpu().numpy())'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 40081}, page_content=\"```\\n\\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `input_size`). \\n\\nIn this case, we get `100` possible values for the next `48` hours for each of the `862` time series (for each example in the batch which is of size `1` since we only have a single multivariate time series):\\n\\n\\n```python\\nforecasts_[0].shape\\n\\n>>> (1, 100, 48, 862)\\n```\\n\\nWe'll stack them vertically, to get forecasts for all time-series in the test dataset (just in case there are more time series in the test set):\\n\\n\\n```python\\nforecasts = np.vstack(forecasts_)\\nprint(forecasts.shape)\\n\\n>>> (1, 100, 48, 862)\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 40701}, page_content='```\\n\\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values present in the test set. For that, we\\'ll use the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library, which includes the [MASE](https://huggingface.co/spaces/evaluate-metric/mase) and [sMAPE](https://huggingface.co/spaces/evaluate-metric/smape) metrics.\\n\\nWe calculate both metrics for each time series variate in the dataset:\\n\\n\\n```python\\nfrom evaluate import load\\nfrom gluonts.time_feature import get_seasonality\\n\\nmase_metric = load(\"evaluate-metric/mase\")\\nsmape_metric = load(\"evaluate-metric/smape\")\\n\\nforecast_median = np.median(forecasts, 1).squeeze(0).T\\n\\nmase_metrics = []\\nsmape_metrics = []\\n\\nfor item_id, ts in enumerate(test_dataset):\\n    training_data = ts[\"target\"][:-prediction_length]\\n    ground_truth = ts[\"target\"][-prediction_length:]\\n    mase = mase_metric.compute(\\n        predictions=forecast_median[item_id],\\n        references=np.array(ground_truth),\\n        training=np.array(training_data),\\n        periodicity=get_seasonality(freq),\\n    )\\n    mase_metrics.append(mase[\"mase\"])\\n\\n    smape = smape_metric.compute(\\n        predictions=forecast_median[item_id],\\n        references=np.array(ground_truth),\\n    )\\n    smape_metrics.append(smape[\"smape\"])\\n```\\n\\n\\n```python\\nprint(f\"MASE: {np.mean(mase_metrics)}\")\\n\\n>>> MASE: 1.1913437728068093\\n\\nprint(f\"sMAPE: {np.mean(smape_metrics)}\")\\n\\n>>> sMAPE: 0.5322665081607634'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 42141}, page_content='```\\n\\n\\n```python\\nplt.scatter(mase_metrics, smape_metrics, alpha=0.2)\\nplt.xlabel(\"MASE\")\\nplt.ylabel(\"sMAPE\")\\nplt.show()'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 42259}, page_content='```\\n\\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_73_0.png)\\n    \\nTo plot the prediction for any time series variate with respect the ground truth test data we define the following helper:\\n\\n\\n```python\\nimport matplotlib.dates as mdates\\n\\n\\ndef plot(ts_index, mv_index):\\n    fig, ax = plt.subplots()\\n\\n    index = pd.period_range(\\n        start=multi_variate_test_dataset[ts_index][FieldName.START],\\n        periods=len(multi_variate_test_dataset[ts_index][FieldName.TARGET]),\\n        freq=multi_variate_test_dataset[ts_index][FieldName.START].freq,\\n    ).to_timestamp()\\n\\n    ax.xaxis.set_minor_locator(mdates.HourLocator())\\n\\n    ax.plot(\\n        index[-2 * prediction_length :],\\n        multi_variate_test_dataset[ts_index][\"target\"][mv_index, -2 * prediction_length :],\\n        label=\"actual\",\\n    )\\n\\n    ax.plot(\\n        index[-prediction_length:],\\n        forecasts[ts_index, ..., mv_index].mean(axis=0),\\n        label=\"mean\",\\n    )\\n    ax.fill_between(\\n        index[-prediction_length:],\\n        forecasts[ts_index, ..., mv_index].mean(0)\\n        - forecasts[ts_index, ..., mv_index].std(axis=0),\\n        forecasts[ts_index, ..., mv_index].mean(0)\\n        + forecasts[ts_index, ..., mv_index].std(axis=0),\\n        alpha=0.2,\\n        interpolate=True,\\n        label=\"+/- 1-std\",\\n    )\\n    ax.legend()\\n    fig.autofmt_xdate()\\n```\\n\\nFor example:\\n\\n\\n```python\\nplot(0, 344)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 43687}, page_content='```\\n\\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_77_0.png)\\n    \\n\\n## Conclusion\\n\\nHow do we compare against other models? The [Monash Time Series Repository](https://forecastingdata.org/#results) has a comparison table of test set MASE metrics which we can add to:\\n\\n|Dataset | \\tSES| \\tTheta | \\tTBATS| \\tETS\\t| (DHR-)ARIMA| \\tPR|\\tCatBoost |\\tFFNN\\t| DeepAR | \\tN-BEATS | \\tWaveNet|  Transformer (uni.) | **Informer (mv. our)**| \\n|:------------------:|:-----------------:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:--:|:--:|:--:|:--:|\\n|Traffic Hourly | 1.922\\t| 1.922\\t| 2.482 |\\t2.294|\\t2.535|\\t1.281|\\t1.571\\t|0.892|\\t0.825\\t|1.100|\\t1.066\\t| **0.821** | 1.191 |\\n\\nAs can be seen, and perhaps surprising to some, the multivariate forecasts are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations/relationships. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. We refer to [this paper](https://openreview.net/forum?id=GpW327gxLTF) for further reading. Multivariate models tend to work well when trained on a lot of data.\\n\\nSo the vanilla Transformer still performs best here! In the future, we hope to better benchmark these models in a central place to ease reproducing the results of several papers. Stay tuned for more!\\n\\n## Resources'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': -1}, page_content='So the vanilla Transformer still performs best here! In the future, we hope to better benchmark these models in a central place to ease reproducing the results of several papers. Stay tuned for more!\\n\\n## Resources\\n\\nWe recommend to check out the [Informer docs](https://huggingface.co/docs/transformers/main/en/model_doc/informer) and the [example notebook](https://github.com/huggingface/notebooks/blob/main/examples/multivariate_informer.ipynb) linked at the top of this blog post.'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 1}, page_content='SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) that employs [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block) to enable the network to perform dynamic channel-wise feature recalibration.\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model(\\'seresnet152d\\', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 1186}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]\\n```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model(\\'seresnet152d\\', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 2537}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{hu2019squeezeandexcitation,\\n      title={Squeeze-and-Excitation Networks}, \\n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\\n      year={2019},\\n      eprint={1709.01507},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Kandinsky 2.2\\n\\nKandinsky 2.2 is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Vladimir Arkhipkin](https://github.com/oriBetelgeuse), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey), and [Denis Dimitrov](https://github.com/denndimitrov).\\n\\nThe description from it\\'s GitHub page is:\\n\\n*Kandinsky 2.2 brings substantial improvements upon its predecessor, Kandinsky 2.1, by introducing a new, more powerful image encoder - CLIP-ViT-G and the ControlNet support. The switch to CLIP-ViT-G as the image encoder significantly increases the model\\'s capability to generate more aesthetic pictures and better understand text, thus enhancing the model\\'s overall performance. The addition of the ControlNet mechanism allows the model to effectively control the process of generating images. This leads to more accurate and visually appealing outputs and opens new possibilities for text-guided image manipulation.*\\n\\nThe original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).\\n\\n<Tip>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': -1}, page_content='The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).\\n\\n<Tip>\\n\\nCheck out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\\n\\n</Tip>\\n\\n<Tip>\\n\\nMake sure to check out the schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## KandinskyV22PriorPipeline\\n\\n[[autodoc]] KandinskyV22PriorPipeline\\n\\t- all\\n\\t- __call__\\n\\t- interpolate\\n\\n## KandinskyV22Pipeline\\n\\n[[autodoc]] KandinskyV22Pipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22CombinedPipeline\\n\\n[[autodoc]] KandinskyV22CombinedPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22ControlnetPipeline\\n\\n[[autodoc]] KandinskyV22ControlnetPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22PriorEmb2EmbPipeline\\n\\n[[autodoc]] KandinskyV22PriorEmb2EmbPipeline\\n\\t- all\\n\\t- __call__\\n\\t- interpolate\\n\\n## KandinskyV22Img2ImgPipeline\\n\\n[[autodoc]] KandinskyV22Img2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22Img2ImgCombinedPipeline\\n\\n[[autodoc]] KandinskyV22Img2ImgCombinedPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22ControlnetImg2ImgPipeline'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 2973}, page_content='[[autodoc]] KandinskyV22Img2ImgCombinedPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22ControlnetImg2ImgPipeline\\n\\n[[autodoc]] KandinskyV22ControlnetImg2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22InpaintPipeline\\n\\n[[autodoc]] KandinskyV22InpaintPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22InpaintCombinedPipeline\\n\\n[[autodoc]] KandinskyV22InpaintCombinedPipeline\\n\\t- all\\n\\t- __call__'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 0}, page_content='--\\ntitle: \"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\"\\nthumbnail: blog/assets/156_huggylingo/Huggy_Lingo.png\\nauthors:\\n- user: davanstrien\\n---\\n\\n## Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\n\\n\\n\\n**tl;dr**: We\\'re using machine learning to detect the language of Hub datasets with no language metadata, and [librarian-bots](https://huggingface.co/librarian-bots) to make pull requests to add this metadata. \\n\\nThe Hugging Face Hub has become the repository where the community shares machine learning models, datasets, and applications. As the number of datasets grows, metadata becomes increasingly important as a tool for finding the right resource for your use case.\\n\\nIn this blog post, I\\'m excited to share some early experiments which seek to use machine learning to improve the metadata for datasets hosted on the Hugging Face Hub.\\n\\n### Language Metadata for Datasets on the Hub\\n\\nThere are currently ~50K public datasets on the Hugging Face Hub. Metadata about the language used in a dataset can be specified using a [YAML](https://en.wikipedia.org/wiki/YAML) field at the top of the [dataset card](https://huggingface.co/docs/datasets/upload_dataset#create-a-dataset-card).\\n\\nAll public datasets specify 1,716 unique languages via a language tag in their metadata. Note that some of them will be the result of languages being specified in different ways i.e. `en` vs `eng` vs `english` vs `English`. \\n\\nFor example, the [IMDB dataset](https://huggingface.co/datasets/imdb) specifies `en` in the YAML metadata (indicating English):'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': -1}, page_content='For example, the [IMDB dataset](https://huggingface.co/datasets/imdb) specifies `en` in the YAML metadata (indicating English):\\n\\n<p align=\"center\"> \\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_metadata.png\" alt=\"Screenshot of YAML metadata\"><br> \\n<em>Section of the YAML metadata for the IMDB dataset</em> \\n </p> \\n\\n\\nIt is perhaps unsurprising that English is by far the most common language for datasets on the Hub, with around 19% of datasets on the Hub listing their language as `en` (not including any variations of `en`, so the actual percentage is likely much higher).\\n\\n <p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_freq.png\" alt=\"Distribution of language tags\"><br> \\n     <em>The frequency and percentage frequency for datasets on the Hugging Face Hub</em> \\n </p> \\n\\n\\nWhat does the distribution of languages look like if we exclude English? We can see that there is a grouping of a few dominant languages and after that there is a pretty smooth fall in the frequencies at which languages appear. \\n\\n <p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_freq_distribution.png\" alt=\"Distribution of language tags\"><br> \\n     <em>Distribution of language tags for datasets on the hub excluding English.</em> \\n </p> \\n\\nHowever, there is a major caveat to this. Most datasets (around 87%) do not specify the language used; only approximately 13% of datasets include language information in their metadata.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 2951}, page_content='However, there is a major caveat to this. Most datasets (around 87%) do not specify the language used; only approximately 13% of datasets include language information in their metadata.\\n\\n\\n<p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/has_lang_info_bar.png\" alt=\"Barchart\"><br> \\n     <em>The percent of datasets which have language metadata. True indicates language metadata is specified, False means no language data is listed. No card data means that there isn\\'t any metadata or it couldn\\'t be loaded by the `huggingface_hub` Python library.</em> \\n</p> \\n\\n#### Why is Language Metadata Important?\\n\\nLanguage metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. \\n\\nCurrently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don\\'t specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. \\n\\nMany people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data. \\n\\nIf we switch to the task of finding relevant machine learning models, knowing what languages were included in the training data for a model can help us find models for the language we are interested in. This relies on the dataset specifying this information. \\n\\nFinally, knowing what languages are represented on the Hub (and which are not), helps us understand the language biases of the Hub and helps inform community efforts to address gaps in particular languages. \\n\\n### Predicting the Languages of Datasets Using Machine Learning'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': -1}, page_content='### Predicting the Languages of Datasets Using Machine Learning\\n\\nWe’ve already seen that many of the datasets on the Hugging Face Hub haven’t included metadata for the language used. However, since these datasets are already shared openly, perhaps we can look at the dataset and try to identify the language using machine learning.\\n\\n#### Getting the Data \\n\\nOne way we could access some examples from a dataset is by using the datasets library to download the datasets i.e. \\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"biglam/on_the_books\")'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 5457}, page_content='```\\n\\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole dataset. We could instead try to load a sample of the dataset. However, depending on how the dataset was created, we might still end up downloading more data than we’d need onto the machine we’re working on. \\n\\nLuckily, many datasets on the Hub are available via the [datasets server](https://huggingface.co/docs/datasets-server/index). The datasets server is an API that allows us to access datasets hosted on the Hub without downloading the dataset locally. The Datasets Server powers the Datasets Viewer preview you will see for many datasets hosted on the Hub. \\n\\nFor this first experiment with predicting language for datasets, we define a list of column names and data types likely to contain textual content i.e. `text` or `prompt` column names and `string` features are likely to be relevant `image` is not. This means we can avoid predicting the language for datasets where language information is less relevant, for example, image classification datasets. We use the Datasets Server to get 20 rows of text data to pass to a machine learning model (we could modify this to take more or fewer examples from the dataset). \\n\\nThis approach means that for the majority of datasets on the Hub we can quickly request the contents of likely text columns for the first 20 rows in a dataset. \\n\\n#### Predicting the Language of a Dataset \\n\\nOnce we have some examples of text from a dataset, we need to predict the language. There are various options here, but for this work, we used the [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model created by [Meta](https://huggingface.co/facebook) as part of the [No Language Left Behind](https://ai.facebook.com/research/no-language-left-behind/) work. This model can detect 217 languages which will likely represent the majority of languages for datasets hosted on the Hub. \\n\\nWe pass 20 examples to the model representing rows from a dataset. This results in 20 individual language predictions (one per row) for each dataset.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': -1}, page_content='We pass 20 examples to the model representing rows from a dataset. This results in 20 individual language predictions (one per row) for each dataset.  \\n\\nOnce we have these predictions, we do some additional filtering to determine if we will accept the predictions as a metadata suggestion. This roughly consists of:\\n\\n- Grouping the predictions for each dataset by language: some datasets return predictions for multiple languages. We group these predictions by the language predicted i.e. if a dataset returns predictions for English and Dutch, we group the English and Dutch predictions together. \\n- For datasets with multiple languages predicted, we count how many predictions we have for each language. If a language is predicted less than 20% of the time, we discard this prediction. i.e. if we have 18 predictions for English and only 2 for Dutch we discard the Dutch predictions. \\n- We calculate the mean score for all predictions for a language. If the mean score associated with a languages prediction is below 80% we discard this prediction.\\n\\n <p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/prediction-flow.png\" alt=\"Prediction workflow\"><br> \\n     <em>Diagram showing how predictions are handled.</em> \\n </p> \\n  \\n\\nOnce we’ve done this filtering, we have a further step of deciding how to use these predictions. The fastText language prediction model returns predictions as an [ISO 639-3](https://en.wikipedia.org/wiki/ISO_639-3) code (an international standard for language codes) along with a script type. i.e. `kor_Hang` is the ISO 693-3 language code for Korean (kor) + Hangul script (Hang) a [ISO 15924](https://en.wikipedia.org/wiki/ISO_15924) code representing the script of a language.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 9226}, page_content=\"We discard the script information since this isn't currently captured consistently as metadata on the Hub and, where possible, we convert the language prediction returned by the model from [ISO 639-3](https://en.wikipedia.org/wiki/ISO_639-3) to [ISO 639-1](https://en.wikipedia.org/wiki/ISO_639-1) language codes. This is largely done because these language codes have better support in the Hub UI for navigating datasets. \\n\\nFor some ISO 639-3 codes, there is no ISO 639-1 equivalent. For these cases we manually specify a mapping if we deem it to make sense, for example Standard Arabic (`arb`) is mapped to Arabic (`ar`). Where an obvious mapping is not possible, we currently don't suggest metadata for this dataset. In future iterations of this work we may take a different approach. It is important to recognise this approach does come with downsides, since it reduces the diversity of languages which might be suggested and also relies on subjective judgments about what languages can be mapped to others. \\n\\nBut the process doesn't stop here. After all, what use is predicting the language of the datasets if we can't share that information with the rest of the community?\\n\\n### Using Librarian-Bot to Update Metadata\\n\\nTo ensure this valuable language metadata is incorporated back into the Hub, we turn to Librarian-Bot! Librarian-Bot takes the language predictions generated by Meta's [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model and opens pull requests to add this information to the metadata of each respective dataset. \\n\\nThis system not only updates the datasets with language information, but also does it swiftly and efficiently, without requiring manual work from humans. If the owner of a repo decided to approve and merge the pull request, then the language metadata becomes available for all users, significantly enhancing the usability of the Hugging Face Hub. You can keep track of what the librarian-bot is doing [here](https://huggingface.co/librarian-bot/activity/community)! \\n\\n#### Next Steps\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 11305}, page_content=\"#### Next Steps \\n\\nAs the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.\\n\\nWith the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. \\n\\nAs the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Trainer\\n\\nThe [`Trainer`] class provides an API for feature-complete training in PyTorch, and it supports distributed training on multiple GPUs/TPUs, mixed precision for [NVIDIA GPUs](https://nvidia.github.io/apex/), [AMD GPUs](https://rocm.docs.amd.com/en/latest/rocm.html), and [`torch.amp`](https://pytorch.org/docs/stable/amp.html) for PyTorch. [`Trainer`] goes hand-in-hand with the [`TrainingArguments`] class, which offers a wide range of options to customize how a model is trained. Together, these two classes provide a complete training API.\\n\\n[`Seq2SeqTrainer`] and [`Seq2SeqTrainingArguments`] inherit from the [`Trainer`] and [`TrainingArgument`] classes and they\\'re adapted for training models for sequence-to-sequence tasks such as summarization or translation.\\n\\n<Tip warning={true}>\\n\\nThe [`Trainer`] class is optimized for 🤗 Transformers models and can have surprising behaviors\\nwhen used with other models. When using it with your own model, make sure:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md', 'start_index': -1}, page_content='<Tip warning={true}>\\n\\nThe [`Trainer`] class is optimized for 🤗 Transformers models and can have surprising behaviors\\nwhen used with other models. When using it with your own model, make sure:\\n\\n- your model always return tuples or subclasses of [`~utils.ModelOutput`]\\n- your model can compute the loss if a `labels` argument is provided and that loss is returned as the first\\n  element of the tuple (if your model returns tuples)\\n- your model can accept multiple label arguments (use `label_names` in [`TrainingArguments`] to indicate their name to the [`Trainer`]) but none of them should be named `\"label\"`\\n\\n</Tip>\\n\\n## Trainer[[api-reference]]\\n\\n[[autodoc]] Trainer\\n    - all\\n\\n## Seq2SeqTrainer\\n\\n[[autodoc]] Seq2SeqTrainer\\n    - evaluate\\n    - predict\\n\\n## TrainingArguments\\n\\n[[autodoc]] TrainingArguments\\n    - all\\n\\n## Seq2SeqTrainingArguments\\n\\n[[autodoc]] Seq2SeqTrainingArguments\\n    - all'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: upload_button_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 47}, page_content='```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    \\n    with gr.Row():\\n        with gr.Column():\\n            upload_btn = gr.UploadButton(label=\"Upload Single File\", file_count=\"single\")\\n        with gr.Column():\\n            output_file_1 = gr.File(label=\"Upload Single File Output\", file_count=\"single\")\\n            num_load_btn_1 = gr.Number(label=\"# Load Upload Single File\", value=0)\\n            output_click_1 = gr.Number(label=\"# Click Upload Single File Output\", value=0)\\n            upload_btn.upload(lambda s,n: (s, n + 1), [upload_btn, num_load_btn_1], [output_file_1, num_load_btn_1])\\n            upload_btn.click(lambda n: (n + 1), output_click_1, [output_click_1])\\n    with gr.Row():\\n        with gr.Column():\\n            upload_btn_multiple = gr.UploadButton(label=\"Upload Multiple Files\", file_count=\"multiple\")\\n        with gr.Column():\\n            output_file_2 = gr.File(label=\"Upload Multiple Files Output\", file_count=\"multiple\")\\n            num_load_btn_2 = gr.Number(label=\"# Load Upload Multiple Files\", value=0)\\n            output_click_2 = gr.Number(label=\"# Click Upload Multiple Files Output\", value=0)\\n            upload_btn_multiple.upload(lambda s,n: (s, n + 1), [upload_btn_multiple, num_load_btn_2], [output_file_2, num_load_btn_2])\\n            upload_btn_multiple.click(lambda n: (n + 1), output_click_2, [output_click_2])\\n\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n```'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 1}, page_content='Normalization and pre-tokenization[[normalization-and-pre-tokenization]]\\n\\n<CourseFloatingBanner chapter={6}\\n  classNames=\"absolute z-10 right-0 top-0\"\\n  notebooks={[\\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb\"},\\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb\"},\\n]} />\\n\\nBefore we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we\\'ll first take a look at the preprocessing that each tokenizer applies to text. Here\\'s a high-level overview of the steps in the tokenization pipeline:\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg\" alt=\"The tokenization pipeline.\">\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg\" alt=\"The tokenization pipeline.\">\\n</div>\\n\\nBefore splitting a text into subtokens (according to its model), the tokenizer performs two steps: _normalization_ and _pre-tokenization_.\\n\\n## Normalization[[normalization]]\\n\\n<Youtube id=\"4IIC2jI9CaU\"/>'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': -1}, page_content='## Normalization[[normalization]]\\n\\n<Youtube id=\"4IIC2jI9CaU\"/>\\n\\nThe normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you\\'re familiar with [Unicode normalization](http://www.unicode.org/reports/tr15/) (such as NFC or NFKC), this is also something the tokenizer may apply.\\n\\nThe 🤗 Transformers `tokenizer` has an attribute called `backend_tokenizer` that provides access to the underlying tokenizer from the 🤗 Tokenizers library:\\n\\n```py\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\nprint(type(tokenizer.backend_tokenizer))'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 2028}, page_content='```\\n\\n```python out\\n<class \\'tokenizers.Tokenizer\\'>\\n```\\n\\nThe `normalizer` attribute of the `tokenizer` object has a `normalize_str()` method that we can use to see how the normalization is performed:\\n\\n```py\\nprint(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\\n```\\n\\n```python out\\n\\'hello how are u?\\'\\n```\\n\\nIn this example, since we picked the `bert-base-uncased` checkpoint, the normalization applied lowercasing and removed the accents. \\n\\n<Tip>\\n\\n✏️ **Try it out!** Load a tokenizer from the `bert-base-cased` checkpoint and pass the same example to it. What are the main differences you can see between the cased and uncased versions of the tokenizer?\\n\\n</Tip>\\n\\n## Pre-tokenization[[pre-tokenization]]\\n\\n<Youtube id=\"grlLV8AIXug\"/>\\n\\nAs we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That\\'s where the pre-tokenization step comes in. As we saw in [Chapter 2](/course/chapter2), a word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training.\\n\\nTo see how a fast tokenizer performs pre-tokenization, we can use the `pre_tokenize_str()` method of the `pre_tokenizer` attribute of the `tokenizer` object:\\n\\n```py\\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\\n```\\n\\n```python out\\n[(\\'Hello\\', (0, 5)), (\\',\\', (5, 6)), (\\'how\\', (7, 10)), (\\'are\\', (11, 14)), (\\'you\\', (16, 19)), (\\'?\\', (19, 20))]'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 3602}, page_content='```\\n\\nNotice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between `are` and `you` to account for that.\\n\\nSince we\\'re using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer:\\n\\n```py\\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\\n```\\n\\nit will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a `Ġ` symbol, enabling it to recover the original spaces if we decode the tokens:\\n\\n```python out\\n[(\\'Hello\\', (0, 5)), (\\',\\', (5, 6)), (\\'Ġhow\\', (6, 10)), (\\'Ġare\\', (10, 14)), (\\'Ġ\\', (14, 15)), (\\'Ġyou\\', (15, 19)),\\n (\\'?\\', (19, 20))]\\n```\\n\\nAlso note that unlike the BERT tokenizer, this tokenizer does not ignore the double space.\\n\\nFor a last example, let\\'s have a look at the T5 tokenizer, which is based on the SentencePiece algorithm:\\n\\n```py\\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\\n```\\n\\n```python out\\n[(\\'▁Hello,\\', (0, 6)), (\\'▁how\\', (7, 10)), (\\'▁are\\', (11, 14)), (\\'▁you?\\', (16, 20))]'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 5025}, page_content=\"```\\n\\nLike the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (`_`), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before `Hello`) and ignored the double space between `are` and `you`.\\n\\nNow that we've seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We'll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we'll examine how the three main algorithms used for subword tokenization work.\\n\\n## SentencePiece[[sentencepiece]]\\n\\n[SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `▁`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter7/7)), it doesn't even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).\\n\\nThe other main feature of SentencePiece is *reversible tokenization*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_`s with spaces -- this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.\\n\\n## Algorithm overview[[algorithm-overview]]\\n\\nIn the following sections, we'll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here's a quick overview of how they each work. Don't hesitate to come back to this table after reading each of the next sections if it doesn't make sense to you yet.\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 7035}, page_content=\"Model | BPE | WordPiece | Unigram\\n:----:|:---:|:---------:|:------:\\nTraining | Starts from a small vocabulary and learns rules to merge tokens |  Starts from a small vocabulary and learns rules to merge tokens | Starts from a large vocabulary and learns rules to remove tokens\\nTraining step | Merges the tokens corresponding to the most common pair | Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent | Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus\\nLearns | Merge rules and a vocabulary | Just a vocabulary | A vocabulary with a score for each token\\nEncoding | Splits a word into characters and applies the merges learned during training | Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word | Finds the most likely split into tokens, using the scores learned during training\\n\\nNow let's dive into BPE!\"),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/howto/map_pools.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Map pools\\n\\nMap pools allow you to instantiate multiple versions of your environment on the backend, the enables higher \\nthroughput with parallelization of interaction in simulations and embodied environments.\\nUsing map pools is simple with 🤗 Simulate. First define a function that will generate your environment, we call each environment instance a \"map\".\\n\\n```\\ndef generate_map(index):\\n    root = sm.Asset(name=f\"root_{index}\")\\n    root += sm.Box(\\n        name=f\"floor_{index}\",\\n        position=[0, -0.05, 0],\\n        scaling=[10, 0.1, 10],\\n        material=sm.Material.BLUE,\\n        with_collider=True,\\n    )\\n    root += sm.Box(\\n        name=f\"wall1_{index}\",\\n        position=[-1, 0.5, 0],\\n        scaling=[0.1, 1, 5.1],\\n        material=sm.Material.GRAY75,\\n        with_collider=True,\\n    )\\n    root += sm.Box(\\n        name=f\"wall2_{index}\",\\n        position=[1, 0.5, 0],\\n        scaling=[0.1, 1, 5.1],\\n        material=sm.Material.GRAY75,\\n        with_collider=True,\\n    )\\n    root += sm.Box(\\n        name=f\"wall3_{index}\",\\n        position=[0, 0.5, 4.5],\\n        scaling=[5.9, 1, 0.1],\\n        material=sm.Material.GRAY75,\\n        with_collider=True,\\n    )\\n\\n    # add actors, sensors, reward functions etc ...\\n\\n    return root'),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/howto/map_pools.mdx', 'start_index': 1822}, page_content='```\\n\\nYou can then provide the `generate_map` method as an argument to the `sm.ParallelRLEnv` class, which will instantiate `n_maps`. \\nTraining with a subset of the maps is possible using the `n_show` option. At each environment reset, it cycles through to the next map.\\n\\n[[autodoc]] ParallelRLEnv'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 1}, page_content='@gradio/imageeditor\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story.  Thanks [@hannahblair](https://github.com/hannahblair)!\\n\\n## 0.1.5\\n\\n### Fixes\\n\\n- [#6799](https://github.com/gradio-app/gradio/pull/6799) [`c352811`](https://github.com/gradio-app/gradio/commit/c352811f76d4126613ece0a584f8c552fdd8d1f6) - Adds docstrings for `gr.WaveformOptions`, `gr.Brush`, and `gr.Eraser`, fixes examples for `ImageEditor`, and allows individual images to be used as the initial `value` for `ImageEditor`.  Thanks [@abidlabs](https://github.com/abidlabs)!\\n\\n## 0.1.4\\n\\n### Patch Changes'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 715}, page_content='## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a), [`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:\\n  - @gradio/upload@0.5.4\\n  - @gradio/client@0.9.1\\n  - @gradio/image@0.5.1\\n\\n## 0.1.3\\n\\n### Patch Changes'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 1071}, page_content='- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/gradio/commit/6a9151d5c9432c724098da7d88a539aaaf5ffe88), [`21cfb0a`](https://github.com/gradio-app/gradio/commit/21cfb0acc309bb1a392f4d8a8e42f6be864c5978), [`d76bcaa`](https://github.com/gradio-app/gradio/commit/d76bcaaaf0734aaf49a680f94ea9d4d22a602e70), [`67ddd40`](https://github.com/gradio-app/gradio/commit/67ddd40b4b70d3a37cb1637c33620f8d197dbee0), [`053bec9`](https://github.com/gradio-app/gradio/commit/053bec98be1127e083414024e02cf0bebb0b5142), [`bdf81fe`](https://github.com/gradio-app/gradio/commit/bdf81fead86e1d5a29e6b036f1fff677f6480e6b), [`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00), [`5177132`](https://github.com/gradio-app/gradio/commit/5177132d718c77f6d47869b4334afae6380394cb)]:'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 1890}, page_content='- @gradio/image@0.5.0\\n  - @gradio/upload@0.5.3\\n  - @gradio/client@0.9.0\\n  - @gradio/wasm@0.4.0\\n  - @gradio/icons@0.3.2\\n  - @gradio/atoms@0.4.0\\n  - @gradio/statustracker@0.4.2'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 2066}, page_content='## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b639e04`](https://github.com/gradio-app/gradio/commit/b639e040741e6c0d9104271c81415d7befbd8cf3), [`206af31`](https://github.com/gradio-app/gradio/commit/206af31d7c1a31013364a44e9b40cf8df304ba50)]:\\n  - @gradio/image@0.4.2\\n  - @gradio/icons@0.3.1\\n  - @gradio/atoms@0.3.1\\n  - @gradio/statustracker@0.4.1\\n  - @gradio/upload@0.5.2\\n\\n## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\\n  - @gradio/client@0.8.2\\n  - @gradio/image@0.4.1\\n  - @gradio/upload@0.5.1\\n\\n## 0.1.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))\\n\\nA brand new component, completely separate from `Image` that provides simple editing capabilities.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': -1}, page_content='A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n- Set background images from file uploads, webcam, or just paste!\\n- Crop images with an improved cropping UI. App authors can event set specific crop size, or crop ratios (`1:1`, etc)\\n- Paint on top of any image (or no image) and erase any mistakes!\\n- The ImageEditor supports layers, confining draw and erase actions to that layer.\\n- More flexible access to data. The image component returns a composite image representing the final state of the canvas as well as providing the background and all layers as individual images.\\n- Fully customisable. All features can be enabled and disabled. Even the brush color swatches can be customised.\\n\\n<video src=\"https://user-images.githubusercontent.com/12937446/284027169-31188926-fd16-4a1c-8718-998e7aae4695.mp4\" autoplay muted></video>\\n\\n```py\\n\\ndef fn(im):\\n    im[\"composite\"] # the full canvas\\n    im[\"background\"] # the background image\\n    im[\"layers\"] # a list of individual layers\\n\\n\\nim = gr.ImageEditor(\\n    # decide which sources you\\'d like to accept\\n    sources=[\"upload\", \"webcam\", \"clipboard\"],\\n    # set a cropsize constraint, can either be a ratio or a concrete [width, height]\\n    crop_size=\"1:1\",\\n    # enable crop (or disable it)\\n    transforms=[\"crop\"],\\n    # customise the brush\\n    brush=Brush(\\n      default_size=\"25\", # or leave it as \\'auto\\'\\n      color_mode=\"fixed\", # \\'fixed\\' hides the user swatches and colorpicker, \\'defaults\\' shows it\\n      default_color=\"hotpink\", # html names are supported\\n      colors=[\\n        \"rgba(0, 150, 150, 1)\", # rgb(a)\\n        \"#fff\", # hex rgb\\n        \"hsl(360, 120, 120)\" # in fact any valid colorstring\\n      ]\\n    ),\\n    brush=Eraser(default_size=\"25\")\\n)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 4652}, page_content='```\\n\\nThanks [@pngwn](https://github.com/pngwn)!\\n\\n### Fixes\\n\\n- [#6502](https://github.com/gradio-app/gradio/pull/6502) [`070f71c93`](https://github.com/gradio-app/gradio/commit/070f71c933d846ce8e2fe11cdd9bc0f3f897f29f) - Ensure image editor crop and draw cursor works as expected when the scroll position changes. Thanks [@pngwn](https://github.com/pngwn)!\\n\\n# @gradio/image'),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': 1}, page_content=\"How to contribute to simulate?\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg)](CODE_OF_CONDUCT.md)\\n\\nSimulation environments is an open source project, so all contributions and suggestions are welcome.\\n\\nYou can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, \\nimproving the documentation, fixing bugs,...\\n\\nMany thanks in advance to every contributor.\\n\\nIn order to facilitate healthy, constructive behavior in an open and inclusive community, we all respect and abide by \\nour [code of conduct](CODE_OF_CONDUCT.md).\\n\\n## How to work on an open Issue?\\nYou have the list of open Issues at: https://github.com/huggingface/simulate/issues\\n\\nSome of them may have the label `help wanted`: that means that any contributor is welcomed!\\n\\nIf you would like to work on any of the open Issues:\\n\\n1. Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page.\\n\\n2. You can self-assign it by commenting on the Issue page with one of the keywords: `#take` or `#self-assign`.\\n\\n3. Work on your self-assigned issue and eventually create a Pull Request.\\n\\n## How to create a Pull Request?\\n1. Fork the [repository](https://github.com/huggingface/simulate) by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account.\\n\\n2. Clone your fork to your local disk, and add the base repository as a remote:\\n\\n\\t```bash\\n\\tgit clone git@github.com:<your Github handle>/simulate.git\\n\\tcd simulate\\n\\tgit remote add upstream https://github.com/huggingface/simulate.git\\n\\t```\\n\\n3. Create a new branch to hold your development changes:\\n\\n\\t```bash\\n\\tgit checkout -b a-descriptive-name-for-my-changes\"),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': -1}, page_content='```\\n\\n3. Create a new branch to hold your development changes:\\n\\n\\t```bash\\n\\tgit checkout -b a-descriptive-name-for-my-changes\\n\\t```\\n\\n\\t**do not** work on the `main` branch.\\n\\n4. Set up a development environment by running the following command in a virtual environment:\\n\\n\\t```bash\\n\\tpip install -e \".[dev]\"\\n\\t```\\n\\n   (If simulate was already installed in the virtual environment, remove\\n   it with `pip uninstall simulate` before reinstalling it in editable\\n   mode with the `-e` flag.)\\n\\n5. Develop the features on your branch. If you want to add a dataset see more in-detail instructions in the section [*How to add a dataset*](#how-to-add-a-dataset). \\n\\n6. Format your code. Run black and isort so that your newly added files look nice with the following command:\\n\\n\\t```bash\\n\\tmake style\\n\\t```\\n\\n7. Once you\\'re happy with your dataset script file, add your changes and make a commit to record your changes locally:\\n\\n\\t```bash\\n\\tgit add simulate/<your_dataset_name>\\n\\tgit commit\\n\\t```\\n\\n\\tIt is a good idea to sync your copy of the code with the original\\n\\trepository regularly. This way you can quickly account for changes:\\n\\n\\t```bash\\n\\tgit fetch upstream\\n\\tgit rebase upstream/main\\n    ```\\n\\n   Push the changes to your account using:\\n\\n   ```bash\\n   git push -u origin a-descriptive-name-for-my-changes\\n   ```\\n\\n8. Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review.\\n\\n## Code of conduct\\n\\nThis project adheres to the HuggingFace [code of conduct](CODE_OF_CONDUCT.md). \\nBy participating, you are expected to uphold this code.'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/chatinterface_system_prompt/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: chatinterface_system_prompt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\ndef echo(message, history, system_prompt, tokens):\\n    response = f\"System prompt: {system_prompt}\\\\n Message: {message}.\"\\n    for i in range(min(len(response), int(tokens))):\\n        time.sleep(0.05)\\n        yield response[: i+1]\\n\\ndemo = gr.ChatInterface(echo, \\n                        additional_inputs=[\\n                            gr.Textbox(\"You are helpful AI.\", label=\"System Prompt\"), \\n                            gr.Slider(10, 100)\\n                        ]\\n                       )\\n\\nif __name__ == \"__main__\":\\n    demo.queue().launch()\\n```'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 1}, page_content='CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to [ResNeXt](https://paperswithcode.com/method/resnext). The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model(\\'cspresnext50\\', pretrained=True)\\n>>> model.eval()\\n```\\n\\nTo load and preprocess the image:\\n\\n```py \\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 2161}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspresnext50`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('cspresnext50', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\\n```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{wang2019cspnet,\\n      title={CSPNet: A New Backbone that can Enhance Learning Capability of CNN}, \\n      author={Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh},\\n      year={2019},\\n      eprint={1911.11929},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 3385}, page_content=\"```\\n\\n<!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNeXt\\n  Paper:\\n    Title: 'CSPNet: A New Backbone that can Enhance Learning Capability of CNN'\\n    URL: https://paperswithcode.com/paper/cspnet-a-new-backbone-that-can-enhance\\nModels:\\n- Name: cspresnext50\\n  In Collection: CSP ResNeXt\\n  Metadata:\\n    FLOPs: 3962945536\\n    Parameters: 20570000\\n    File Size: 82562887\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - Max Pooling\\n    - ReLU\\n    - ResNeXt Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - Polynomial Learning Rate Decay\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 1x GPU\\n    ID: cspresnext50\\n    LR: 0.1\\n    Layers: 50\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 128\\n    Image Size: '224'\\n    Weight Decay: 0.005\\n    Interpolation: bilinear\\n    Training Steps: 8000000\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/cspnet.py#L430\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspresnext50_ra_224-648b4713.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.05%\\n      Top 5 Accuracy: 94.94%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# MultiDiffusion\\n\\n[MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation](https://huggingface.co/papers/2302.08113) is by Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\\n\\nThe abstract from the paper is:\\n\\n*Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.*\\n\\nYou can find additional information about MultiDiffusion on the [project page](https://multidiffusion.github.io/), [original codebase](https://github.com/omerbt/MultiDiffusion), and try it out in a [demo](https://huggingface.co/spaces/weizmannscience/MultiDiffusion).\\n\\n## Tips'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 2138}, page_content='## Tips\\n\\nWhile calling [`StableDiffusionPanoramaPipeline`], it\\'s possible to specify the `view_batch_size` parameter to be > 1.\\nFor some GPUs with high performance, this can speedup the generation process and increase VRAM usage.\\n\\nTo generate panorama-like images make sure you pass the width parameter accordingly. We recommend a width value of 2048 which is the default.\\n\\nCircular padding is applied to ensure there are no stitching artifacts when working with panoramas to ensure a seamless transition from the rightmost part to the leftmost part. By enabling circular padding (set `circular_padding=True`), the operation applies additional crops after the rightmost point of the image, allowing the model to \"see” the transition from the rightmost part to the leftmost part. This helps maintain visual consistency in a 360-degree sense and creates a proper “panorama” that can be viewed using 360-degree panorama viewers. When decoding latents in Stable Diffusion, circular padding is applied to ensure that the decoded latents match in the RGB space.\\n\\nFor example, without circular padding, there is a stitching artifact (default):\\n![img](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/indoor_%20no_circular_padding.png)\\n\\nBut with circular padding, the right and the left parts are matching (`circular_padding=True`):\\n![img](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/indoor_%20circular_padding.png)\\n\\n<Tip>\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## StableDiffusionPanoramaPipeline\\n[[autodoc]] StableDiffusionPanoramaPipeline\\n\\t- __call__\\n\\t- all'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': -1}, page_content='</Tip>\\n\\n## StableDiffusionPanoramaPipeline\\n[[autodoc]] StableDiffusionPanoramaPipeline\\n\\t- __call__\\n\\t- all\\n\\n## StableDiffusionPipelineOutput\\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput'),\n",
       " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/api/actors.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Actors\\n\\n[[autodoc]] SimpleActor\\n\\n[[autodoc]] EgocentricCameraActor\\n\\n\\nUnder construction 🚧.'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02c_preprocess-sentence-pairs-pt.md', 'start_index': 0}, page_content='ow to preprocess pairs of sentences? We have seen how to tokenize single sentences and batch them together in the \"Batching inputs together\" video. If this code look unfamiliar to you, be sure to check that video again! Here we will focus on tasks that classify pairs of sentences. For instance, we may want to classify whether two texts are paraphrases or not. Here is an example taken from the Quora Question Pairs dataset, which focuses on identifying duplicate questions. In the first pair, the two questions are duplicates; in the second, they are not. Another pair classification problem is when we want to know if two sentences are logically related or not (a problem called Natural Language Inference or NLI). In this example taken from the MultiNLI dataset, we have a pair of sentences for each possible label: contradiction, neutral or entailment (which is a fancy way of saying the first sentence implies the second). So classifying pairs of sentences is a problem worth studying. In fact, in the GLUE benchmark (which is an academic benchmark for text classification), 8 of the 10 datasets are focused on tasks using pairs of sentences. That\\'s why models like BERT are often pretrained with a dual objective: on top of the language modeling objective, they often have an objective related to sentence pairs. For instance, during pretraining, BERT is shown pairs of sentences and must predict both the value of randomly masked tokens and whether the second sentence follows from the first. Fortunately, the tokenizer from the Transformers library has a nice API to deal with pairs of sentences: you just have to pass them as two arguments to the tokenizer. On top of the input IDs and the attention mask we studied already, it returns a new field called token type IDs, which tells the model which tokens belong to the first sentence and which ones belong to the second sentence. Zooming in a little bit, here are the input IDs, aligned with the tokens they correspond to, their respective token type ID and attention mask. We can see the tokenizer also added special tokens so we have a CLS token, the tokens from the first sentence, a SEP token, the tokens from the second sentence, and a final SEP token. If we have several pairs of sentences, we can tokenize them together by passing the list of first sentences, then the list of second sentences and all the keyword arguments we studied already,'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02c_preprocess-sentence-pairs-pt.md', 'start_index': -1}, page_content='tokens from the second sentence, and a final SEP token. If we have several pairs of sentences, we can tokenize them together by passing the list of first sentences, then the list of second sentences and all the keyword arguments we studied already, like padding=True. Zooming in at the result, we can see how the tokenizer added padding to the second pair of sentences, to make the two outputs the same length, and properly dealt with token type IDS and attention masks for the two sentences. This is then all ready to pass through our model!'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 0}, page_content='!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Download files from the Hub\\n\\nThe `huggingface_hub` library provides functions to download files from the repositories\\nstored on the Hub. You can use these functions independently or integrate them into your\\nown library, making it more convenient for your users to interact with the Hub. This\\nguide will show you how to:\\n\\n* Download and cache a single file.\\n* Download and cache an entire repository.\\n* Download files to a local folder. \\n\\n## Download a single file\\n\\nThe [`hf_hub_download`] function is the main function for downloading files from the Hub.\\nIt downloads the remote file, caches it on disk (in a version-aware way), and returns its local file path.\\n\\n<Tip>\\n\\nThe returned filepath is a pointer to the HF local cache. Therefore, it is important to not modify the file to avoid\\nhaving a corrupted cache. If you are interested in getting to know more about how files are cached, please refer to our\\n[caching guide](./manage-cache).\\n\\n</Tip>\\n\\n### From latest version\\n\\nSelect the file to download using the `repo_id`, `repo_type` and `filename` parameters. By default, the file will\\nbe considered as being part of a `model` repo.\\n\\n```python\\n>>> from huggingface_hub import hf_hub_download\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\")\\n\\'/root/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade/config.json\\''),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 1563}, page_content='# Download from a dataset\\n>>> hf_hub_download(repo_id=\"google/fleurs\", filename=\"fleurs.py\", repo_type=\"dataset\")\\n\\'/root/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34/fleurs.py\\''),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 1794}, page_content='```\\n\\n### From specific version\\n\\nBy default, the latest version from the `main` branch is downloaded. However, in some cases you want to download a file\\nat a particular version (e.g. from a specific branch, a PR, a tag or a commit hash).\\nTo do so, use the `revision` parameter:\\n\\n```python\\n# Download from the `v1.0` tag\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"v1.0\")\\n\\n# Download from the `test-branch` branch\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"test-branch\")\\n\\n# Download from Pull Request #3\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"refs/pr/3\")\\n\\n# Download from a specific commit hash\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"877b84a8f93f2d619faa2a6e514a32beef88ab0a\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 2640}, page_content='```\\n\\n**Note:** When using the commit hash, it must be the full-length hash instead of a 7-character commit hash.\\n\\n### Construct a download URL\\n\\nIn case you want to construct the URL used to download a file from a repo, you can use [`hf_hub_url`] which returns a URL.\\nNote that it is used internally by [`hf_hub_download`].\\n\\n## Download an entire repository\\n\\n[`snapshot_download`] downloads an entire repository at a given revision. It uses internally [`hf_hub_download`] which\\nmeans all downloaded files are also cached on your local disk. Downloads are made concurrently to speed-up the process.\\n\\nTo download a whole repository, just pass the `repo_id` and `repo_type`:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\")\\n\\'/home/lysandre/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade\\'\\n\\n# Or from a dataset\\n>>> snapshot_download(repo_id=\"google/fleurs\", repo_type=\"dataset\")\\n\\'/home/lysandre/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34\\'\\n```\\n\\n[`snapshot_download`] downloads the latest revision by default. If you want a specific repository revision, use the\\n`revision` parameter:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", revision=\"refs/pr/1\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 4026}, page_content='```\\n\\n### Filter files to download\\n\\n[`snapshot_download`] provides an easy way to download a repository. However, you don\\'t always want to download the\\nentire content of a repository. For example, you might want to prevent downloading all `.bin` files if you know you\\'ll\\nonly use the `.safetensors` weights. You can do that using `allow_patterns` and `ignore_patterns` parameters.\\n\\nThese parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing\\npatterns) as documented [here](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm). The pattern matching is\\nbased on [`fnmatch`](https://docs.python.org/3/library/fnmatch.html).\\n\\nFor example, you can use `allow_patterns` to only download JSON configuration files:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", allow_patterns=\"*.json\")\\n```\\n\\nOn the other hand, `ignore_patterns` can exclude certain files from being downloaded. The\\nfollowing example ignores the `.msgpack` and `.h5` file extensions:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", ignore_patterns=[\"*.msgpack\", \"*.h5\"])\\n```\\n\\nFinally, you can combine both to precisely filter your download. Here is an example to download all json and markdown\\nfiles except `vocab.json`.\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"gpt2\", allow_patterns=[\"*.md\", \"*.json\"], ignore_patterns=\"vocab.json\")'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 5561}, page_content='```\\n\\n## Download file(s) to local folder\\n\\nThe recommended (and default) way to download files from the Hub is to use the [cache-system](./manage-cache).\\nYou can define your cache location by setting `cache_dir` parameter (both in [`hf_hub_download`] and [`snapshot_download`]).'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 5840}, page_content='However, in some cases you want to download files and move them to a specific folder. This is useful to get a workflow\\ncloser to what `git` commands offer. You can do that using the `local_dir` and `local_dir_use_symlinks` parameters:\\n- `local_dir` must be a path to a folder on your system. The downloaded files will keep the same file structure as in the\\nrepo. For example if `filename=\"data/train.csv\"` and `local_dir=\"path/to/folder\"`, then the returned filepath will be\\n`\"path/to/folder/data/train.csv\"`.\\n- `local_dir_use_symlinks` defines how the file must be saved in your local folder.\\n  - The default behavior (`\"auto\"`) is to duplicate small files (<5MB) and use symlinks for bigger files. Symlinks allow\\n    to optimize both bandwidth and disk usage. However manually editing a symlinked file might corrupt the cache, hence\\n    the duplication for small files. The 5MB threshold can be configured with the `HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD`\\n    environment variable.\\n  - If `local_dir_use_symlinks=True` is set, all files are symlinked for an optimal disk space optimization. This is\\n    for example useful when downloading a huge dataset with thousands of small files.\\n  - Finally, if you don\\'t want symlinks at all you can disable them (`local_dir_use_symlinks=False`). The cache directory\\n    will still be used to check wether the file is already cached or not. If already cached, the file is **duplicated**\\n    from the cache (i.e. saves bandwidth but increases disk usage). If the file is not already cached, it will be\\n    downloaded and moved directly to the local dir. This means that if you need to reuse it somewhere else later, it\\n    will be **re-downloaded**.\\n\\nHere is a table that summarizes the different options to help you choose the parameters that best suit your use case.'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': -1}, page_content='Here is a table that summarizes the different options to help you choose the parameters that best suit your use case.\\n\\n<!-- Generated with https://www.tablesgenerator.com/markdown_tables -->\\n| Parameters | File already cached | Returned path | Can read path? | Can save to path? | Optimized bandwidth | Optimized disk usage |\\n|---|:---:|:---:|:---:|:---:|:---:|:---:|\\n| `local_dir=None` |  | symlink in cache | ✅ | ❌<br>_(save would corrupt the cache)_ | ✅ | ✅ |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=\"auto\"` |  | file or symlink in folder | ✅ | ✅ _(for small files)_ <br> ⚠️ _(for big files do not resolve path before saving)_ | ✅ | ✅ |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=True` |  | symlink in folder | ✅ | ⚠️<br>_(do not resolve path before saving)_ | ✅ | ✅ |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=False` | No | file in folder | ✅ | ✅ | ❌<br>_(if re-run, file is re-downloaded)_ | ⚠️<br>(multiple copies if ran in multiple folders) |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=False` | Yes | file in folder | ✅ | ✅ | ⚠️<br>_(file has to be cached first)_ | ❌<br>_(file is duplicated)_ |\\n\\n**Note:** if you are on a Windows machine, you need to enable developer mode or run `huggingface_hub` as admin to enable\\nsymlinks. Check out the [cache limitations](../guides/manage-cache#limitations) section for more details.\\n\\n## Download from the CLI'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 8929}, page_content='## Download from the CLI\\n\\nYou can use the `huggingface-cli download` command from the terminal to directly download files from the Hub.\\nInternally, it uses the same [`hf_hub_download`] and [`snapshot_download`] helpers described above and prints the\\nreturned path to the terminal.\\n\\n```bash\\n>>> huggingface-cli download gpt2 config.json\\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 9378}, page_content='```\\n\\nYou can download multiple files at once which displays a progress bar and returns the snapshot path in which the files\\nare located:\\n\\n```bash\\n>>> huggingface-cli download gpt2 config.json model.safetensors\\nFetching 2 files: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 23831.27it/s]\\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10\\n```\\n\\nFor more details about the CLI download command, please refer to the [CLI guide](./cli#huggingface-cli-download).\\n\\n## Faster downloads\\n\\nIf you are running on a machine with high bandwidth, you can increase your download speed with [`hf_transfer`](https://github.com/huggingface/hf_transfer), a Rust-based library developed to speed up file transfers with the Hub. To enable it, install the package (`pip install hf_transfer`) and set `HF_HUB_ENABLE_HF_TRANSFER=1` as an environment variable.\\n\\n<Tip>\\n\\nProgress bars are supported in `hf_transfer` starting from version `0.1.4`. Consider upgrading (`pip install -U hf-transfer`) if you plan to enable faster downloads.\\n\\n</Tip>\\n\\n<Tip warning={true}>\\n\\n`hf_transfer` is a power user tool! It is tested and production-ready, but it lacks user-friendly features like advanced error handling or proxies. For more details, please take a look at this [section](https://huggingface.co/docs/huggingface_hub/hf_transfer).\\n\\n</Tip>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 1}, page_content='Two types of value-based methods [[two-types-value-based-methods]]\\n\\nIn value-based methods,\\xa0**we learn a value function**\\xa0that\\xa0**maps a state to the expected value of being at that state.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" alt=\"Value Based Methods\"/>\\n\\nThe value of a state is the\\xa0**expected discounted return**\\xa0the agent can get if it\\xa0**starts at that state and then acts according to our policy.**\\n\\n<Tip>\\nBut what does it mean to act according to our policy? After all, we don\\'t have a policy in value-based methods since we train a value function and not a policy.\\n</Tip>\\n\\nRemember that the goal of an\\xa0**RL agent is to have an optimal policy π\\\\*.**\\n\\nTo find the optimal policy, we learned about two different methods:\\n\\n- *Policy-based methods:*\\xa0**Directly train the policy**\\xa0to select what action to take given a state (or a probability distribution over actions at that state). In this case, we\\xa0**don\\'t have a value function.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-2.jpg\" alt=\"Two RL approaches\"/>\\n\\nThe policy takes a state as input and outputs what action to take at that state (deterministic policy: a policy that output one action given a state, contrary to stochastic policy that output a probability distribution over actions).\\n\\nAnd consequently,\\xa0**we don\\'t define by hand the behavior of our policy; it\\'s the training that will define it.**\\n\\n- *Value-based methods:*\\xa0**Indirectly, by training a value function**\\xa0that outputs the value of a state or a state-action pair. Given this value function, our policy\\xa0**will take an action.**'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': -1}, page_content='- *Value-based methods:*\\xa0**Indirectly, by training a value function**\\xa0that outputs the value of a state or a state-action pair. Given this value function, our policy\\xa0**will take an action.**\\n\\nSince the policy is not trained/learned,\\xa0**we need to specify its behavior.**\\xa0For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward,\\xa0**we\\'ll create a Greedy Policy.**\\n\\n<figure>\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-3.jpg\" alt=\"Two RL approaches\"/>\\n  <figcaption>Given a state, our action-value function (that we train) outputs the value of each action at that state. Then, our pre-defined Greedy Policy selects the action that will yield the highest value given a state or a state action pair.</figcaption>\\n</figure>\\n\\nConsequently, whatever method you use to solve your problem,\\xa0**you will have a policy**. In the case of value-based methods, you don\\'t train the policy: your policy\\xa0**is just a simple pre-specified function**\\xa0(for instance, the Greedy Policy) that\\xa0uses the values given by the value-function to select its actions.\\n\\nSo the difference is:\\n\\n- In policy-based training,\\xa0**the optimal policy (denoted π\\\\*) is found by training the policy directly.**\\n- In value-based training,\\xa0**finding an optimal value function (denoted Q\\\\* or V\\\\*, we\\'ll study the difference below) leads to having an optimal policy.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link between value and policy\"/>\\n\\nIn fact, most of the time, in value-based methods, you\\'ll use\\xa0**an Epsilon-Greedy Policy**\\xa0that handles the exploration/exploitation trade-off; we\\'ll talk about this when we talk about Q-Learning in the second part of this unit.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 3383}, page_content='As we mentioned above, we have two types of value-based functions:\\n\\n## The state-value function [[state-value-function]]\\n\\nWe write the state value function under a policy π like this:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-1.jpg\" alt=\"State value function\"/>\\n\\nFor each state, the state-value function outputs the expected return if the agent **starts at that state** and then follows the policy forever afterward (for all future timesteps, if you prefer).\\n\\n<figure>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-2.jpg\" alt=\"State value function\"/>\\n  <figcaption>If we take the state with value -7: it\\'s the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right.</figcaption>\\n</figure>\\n\\n## The action-value function [[action-value-function]]\\n\\nIn the action-value function, for each state and action pair, the action-value function\\xa0**outputs the expected return**\\xa0if the agent starts in that state, takes that action, and then follows the policy forever after.\\n\\nThe value of taking action \\\\\\\\(a\\\\\\\\) in state \\\\\\\\(s\\\\\\\\) under a policy \\\\\\\\(π\\\\\\\\) is:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-1.jpg\" alt=\"Action State value function\"/>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-2.jpg\" alt=\"Action State value function\"/>'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 5039}, page_content='We see that the difference is:\\n\\n- For the state-value function, we calculate\\xa0**the value of a state \\\\\\\\(S_t\\\\\\\\)**\\n- For the action-value function, we calculate\\xa0**the value of the state-action pair ( \\\\\\\\(S_t, A_t\\\\\\\\) ) hence the value of taking that action at that state.**\\n\\n<figure>\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-types.jpg\" alt=\"Two types of value function\"/>\\n  <figcaption>\\nNote: We didn\\'t fill all the state-action pairs for the example of Action-value function</figcaption>\\n</figure>\\n\\nIn either case, whichever value function we choose (state-value or action-value function),\\xa0**the returned value is the expected return.**\\n\\nHowever, the problem is that\\xa0**to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.**\\n\\nThis can be a computationally expensive process, and that\\'s\\xa0**where the Bellman equation comes in to help us.**'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1}, page_content=\"The Hugging Face Course\\n\\nThis repo contains the content that's used to create the **[Hugging Face course](https://huggingface.co/course/chapter1/1)**. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you'll learn how to use the [Hugging Face](https://huggingface.co/) ecosystem — [🤗 Transformers](https://github.com/huggingface/transformers), [🤗 Datasets](https://github.com/huggingface/datasets), [🤗 Tokenizers](https://github.com/huggingface/tokenizers), and [🤗 Accelerate](https://github.com/huggingface/accelerate) — as well as the [Hugging Face Hub](https://huggingface.co/models). It's completely free and open-source!\\n\\n## 🌎 Languages and translations\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 737}, page_content='| Language                                                                      | Source                                                                             | Authors                                                                                                                                                                                                                                                                                                                                                  |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1251}, page_content='|:------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1712}, page_content='---------------------------------------------------|'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1765}, page_content='| [English](https://huggingface.co/course/en/chapter1/1)                        | [`chapters/en`](https://github.com/huggingface/course/tree/main/chapters/en)       | [@sgugger](https://github.com/sgugger), [@lewtun](https://github.com/lewtun), [@LysandreJik](https://github.com/LysandreJik), [@Rocketknight1](https://github.com/Rocketknight1), [@sashavor](https://github.com/sashavor), [@osanseviero](https://github.com/osanseviero), [@SaulLu](https://github.com/SaulLu), [@lvwerra](https://github.com/lvwerra) |\\n| [Bengali](https://huggingface.co/course/bn/chapter1/1) (WIP)                  | [`chapters/bn`](https://github.com/huggingface/course/tree/main/chapters/bn)       | [@avishek-018](https://github.com/avishek-018), [@eNipu](https://github.com/eNipu)                                                                                                                                                                                                                                                                       |\\n| [German](https://huggingface.co/course/de/chapter1/1) (WIP)                   | [`chapters/de`](https://github.com/huggingface/course/tree/main/chapters/de)       | [@JesperDramsch](https://github.com/JesperDramsch), [@MarcusFra](https://github.com/MarcusFra), [@fabridamicelli](https://github.com/fabridamicelli)                                                                                                                                                                                                                                                          |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 3360}, page_content='| [Spanish](https://huggingface.co/course/es/chapter1/1) (WIP)                  | [`chapters/es`](https://github.com/huggingface/course/tree/main/chapters/es)       | [@camartinezbu](https://github.com/camartinezbu), [@munozariasjm](https://github.com/munozariasjm), [@fordaz](https://github.com/fordaz)                                                                                                                                                                                                                 |\\n| [Persian](https://huggingface.co/course/fa/chapter1/1) (WIP)                  | [`chapters/fa`](https://github.com/huggingface/course/tree/main/chapters/fa)       | [@jowharshamshiri](https://github.com/jowharshamshiri), [@schoobani](https://github.com/schoobani)                                                                                                                                                                                                                                                       |\\n| [French](https://huggingface.co/course/fr/chapter1/1)                         | [`chapters/fr`](https://github.com/huggingface/course/tree/main/chapters/fr)       | [@lbourdois](https://github.com/lbourdois), [@ChainYo](https://github.com/ChainYo), [@melaniedrevet](https://github.com/melaniedrevet), [@abdouaziz](https://github.com/abdouaziz)                                                                                                                                                                       |\\n| [Gujarati](https://huggingface.co/course/gu/chapter1/1) (WIP)                 | [`chapters/gu`](https://github.com/huggingface/course/tree/main/chapters/gu)       | [@pandyaved98](https://github.com/pandyaved98)                                                                                                                                                                                                                                                                                                           |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 5416}, page_content='| [Hebrew](https://huggingface.co/course/he/chapter1/1) (WIP)                   | [`chapters/he`](https://github.com/huggingface/course/tree/main/chapters/he)       | [@omer-dor](https://github.com/omer-dor)                                                                                                                                                                                                                                                                                                                 |\\n| [Hindi](https://huggingface.co/course/hi/chapter1/1) (WIP)                    | [`chapters/hi`](https://github.com/huggingface/course/tree/main/chapters/hi)       | [@pandyaved98](https://github.com/pandyaved98)                                                                                                                                                                                                                                                                                                           |\\n| [Bahasa Indonesia](https://huggingface.co/course/id/chapter1/1) (WIP)                   | [`chapters/id`](https://github.com/huggingface/course/tree/main/chapters/id)       | [@gstdl](https://github.com/gstdl)                                                                                                                                                                                                                                                                                                           |\\n| [Italian](https://huggingface.co/course/it/chapter1/1) (WIP)                  | [`chapters/it`](https://github.com/huggingface/course/tree/main/chapters/it)       | [@CaterinaBi](https://github.com/CaterinaBi), [@ClonedOne](https://github.com/ClonedOne),    [@Nolanogenn](https://github.com/Nolanogenn), [@EdAbati](https://github.com/EdAbati), [@gdacciaro](https://github.com/gdacciaro)                                                                                                                                                                  |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 7508}, page_content='| [Japanese](https://huggingface.co/course/ja/chapter1/1) (WIP)                 | [`chapters/ja`](https://github.com/huggingface/course/tree/main/chapters/ja)       | [@hiromu166](https://github.com/@hiromu166), [@younesbelkada](https://github.com/@younesbelkada), [@HiromuHota](https://github.com/@HiromuHota)                                                                                                                                                                                                       |\\n| [Korean](https://huggingface.co/course/ko/chapter1/1) (WIP)                   | [`chapters/ko`](https://github.com/huggingface/course/tree/main/chapters/ko)       | [@Doohae](https://github.com/Doohae), [@wonhyeongseo](https://github.com/wonhyeongseo), [@dlfrnaos19](https://github.com/dlfrnaos19), [@nsbg](https://github.com/nsbg)                                                                                                                                                                                                                                                                                                                     |\\n| [Portuguese](https://huggingface.co/course/pt/chapter1/1) (WIP)               | [`chapters/pt`](https://github.com/huggingface/course/tree/main/chapters/pt)       | [@johnnv1](https://github.com/johnnv1), [@victorescosta](https://github.com/victorescosta), [@LincolnVS](https://github.com/LincolnVS)                                                                                                                                                                                                                   |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 9177}, page_content='| [Russian](https://huggingface.co/course/ru/chapter1/1) (WIP)                  | [`chapters/ru`](https://github.com/huggingface/course/tree/main/chapters/ru)       | [@pdumin](https://github.com/pdumin), [@svv73](https://github.com/svv73)                                                                                                                                                                                                                                                                                 |\\n| [Thai](https://huggingface.co/course/th/chapter1/1) (WIP)                     | [`chapters/th`](https://github.com/huggingface/course/tree/main/chapters/th)       | [@peeraponw](https://github.com/peeraponw), [@a-krirk](https://github.com/a-krirk), [@jomariya23156](https://github.com/jomariya23156), [@ckingkan](https://github.com/ckingkan)                                                                                                                                                                         |\\n| [Turkish](https://huggingface.co/course/tr/chapter1/1) (WIP)                  | [`chapters/tr`](https://github.com/huggingface/course/tree/main/chapters/tr)       | [@tanersekmen](https://github.com/tanersekmen), [@mertbozkir](https://github.com/mertbozkir), [@ftarlaci](https://github.com/ftarlaci), [@akkasayaz](https://github.com/akkasayaz)                                                                                                                                                                       |\\n| [Vietnamese](https://huggingface.co/course/vi/chapter1/1)               | [`chapters/vi`](https://github.com/huggingface/course/tree/main/chapters/vi)       | [@honghanhh](https://github.com/honghanhh)                                                                                                                                                                                                                                                                                                               |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 11227}, page_content='| [Chinese (simplified)](https://huggingface.co/course/zh-CN/chapter1/1)  | [`chapters/zh-CN`](https://github.com/huggingface/course/tree/main/chapters/zh-CN) | [@zhlhyx](https://github.com/zhlhyx), [petrichor1122](https://github.com/petrichor1122), [@1375626371](https://github.com/1375626371)                                                                                                                                                                                                                    |\\n| [Chinese (traditional)](https://huggingface.co/course/zh-TW/chapter1/1) (WIP) | [`chapters/zh-TW`](https://github.com/huggingface/course/tree/main/chapters/zh-TW) | [@davidpeng86](https://github.com/davidpeng86)                                                                                                                                                                                                                                                                                                           |'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 12251}, page_content=\"### Translating the course into your language\\n\\nAs part of our mission to democratise machine learning, we'd love to have the course available in many more languages! Please follow the steps below if you'd like to help translate the course into your language 🙏.\\n\\n**🗞️ Open an issue**\\n\\nTo get started, navigate to the [_Issues_](https://github.com/huggingface/course/issues) page of this repo and check if anyone else has opened an issue for your language. If not, open a new issue by selecting the _Translation template_ from the _New issue_ button.\\n\\nOnce an issue is created, post a comment to indicate which chapters you'd like to work on and we'll add your name to the list.\\n\\n**🗣 Join our Discord**\\n\\nSince it can be difficult to discuss translation details quickly over GitHub issues, we have created dedicated channels for each language on our Discord server. If you'd like to join, follow the instructions at this channel 👉: [https://discord.gg/JfAtkvEtRb](https://discord.gg/JfAtkvEtRb)\\n\\n**🍴 Fork the repository**\\n\\nNext, you'll need to [fork this repo](https://docs.github.com/en/get-started/quickstart/fork-a-repo). You can do this by clicking on the **Fork** button on the top-right corner of this repo's page.\\n\\nOnce you've forked the repo, you'll want to get the files on your local machine for editing. You can do that by cloning the fork with Git as follows:\\n\\n```bash\\ngit clone https://github.com/YOUR-USERNAME/course\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 13679}, page_content=\"```\\n\\n**📋 Copy-paste the English files with a new language code**\\n\\nThe course files are organised under a main directory:\\n\\n* [`chapters`](https://github.com/huggingface/course/tree/main/chapters): all the text and code snippets associated with the course.\\n\\nYou'll only need to copy the files in the [`chapters/en`](https://github.com/huggingface/course/tree/main/chapters/en) directory, so first navigate to your fork of the repo and run the following:\\n\\n```bash\\ncd ~/path/to/course\\ncp -r chapters/en/CHAPTER-NUMBER chapters/LANG-ID/CHAPTER-NUMBER\\n```\\n\\nHere, `CHAPTER-NUMBER` refers to the chapter you'd like to work on and `LANG-ID` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](https://www.loc.gov/standards/iso639-2/php/code_list.php) for a handy table.\\n\\n**✍️ Start translating**\\n\\nNow comes the fun part - translating the text! The first thing we recommend is translating the part of the `_toctree.yml` file that corresponds to your chapter. This file is used to render the table of contents on the website and provide the links to the Colab notebooks. The only fields you should change are the `title`, ones -- for example, here are the parts of `_toctree.yml` that we'd translate for [Chapter 0](https://huggingface.co/course/chapter0/1?fw=pt):\\n\\n```yaml\\n- title: 0. Setup # Translate this!\\n  sections:\\n  - local: chapter0/1 # Do not change this!\\n    title: Introduction # Translate this!\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 15097}, page_content=\"```\\n\\n> 🚨 Make sure the `_toctree.yml` file only contains the sections that have been translated! Otherwise you won't be able to build the content on the website or locally (see below how).\\n\\n\\nOnce you have translated the `_toctree.yml` file, you can start translating the [MDX](https://mdxjs.com/) files associated with your chapter.\\n\\n> 🙋 If the `_toctree.yml` file doesn't yet exist for your language, you can simply create one by copy-pasting from the English version and deleting the sections that aren't related to your chapter. Just make sure it exists in the `chapters/LANG-ID/` directory!\\n\\n**👷\\u200d♂️ Build the course locally**\\n\\nOnce you're happy with your changes, you can preview how they'll look by first installing the [`doc-builder`](https://github.com/huggingface/doc-builder) tool that we use for building all documentation at Hugging Face:\\n\\n```\\npip install hf-doc-builder\\n```\\n\\n```\\ndoc-builder preview course ../course/chapters/LANG-ID --not_python_module\\n```\\n\\n**`preview` command does not work with Windows.\\n\\nThis will build and render the course on [http://localhost:3000/](http://localhost:3000/). Although the content looks much nicer on the Hugging Face website, this step will still allow you to check that everything is formatted correctly.\\n\\n**🚀 Submit a pull request**\\n\\nIf the translations look good locally, the final step is to prepare the content for a pull request. Here, the first think to check is that the files are formatted correctly. For that you can run:\\n\\n```\\npip install -r requirements.txt\\nmake style\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 16581}, page_content=\"```\\npip install -r requirements.txt\\nmake style\\n```\\n\\nOnce that's run, commit any changes, open a pull request, and tag [@lewtun](https://github.com/lewtun) for a review. Congratulations, you've now completed your first translation 🥳!\\n\\n> 🚨 To build the course on the website, double-check your language code exists in `languages` field of the `build_documentation.yml` and `build_pr_documentation.yml` files in the `.github` folder. If not, just add them in their alphabetical order.\\n\\n## 📔 Jupyter notebooks\\n\\nThe Jupyter notebooks containing all the code from the course are hosted on the [`huggingface/notebooks`](https://github.com/huggingface/notebooks) repo. If you wish to generate them locally, first install the required dependencies:\\n\\n```bash\\npython -m pip install -r requirements.txt\\n```\\n\\nThen run the following script:\\n\\n```bash\\npython utils/generate_notebooks.py --output_dir nbs\"),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': -1}, page_content=\"```\\n\\nThen run the following script:\\n\\n```bash\\npython utils/generate_notebooks.py --output_dir nbs\\n```\\n\\nThis script extracts all the code snippets from the chapters and stores them as notebooks in the `nbs` folder (which is ignored by Git by default).\\n\\n## ✍️ Contributing a new chapter\\n\\n> Note: we are not currently accepting community contributions for new chapters. These instructions are for the Hugging Face authors.\\n\\nAdding a new chapter to the course is quite simple:\\n\\n1. Create a new directory under `chapters/en/chapterX`, where `chapterX` is the chapter you'd like to add.\\n2. Add numbered MDX files `sectionX.mdx` for each section. If you need to include images, place them in the [huggingface-course/documentation-images](https://huggingface.co/datasets/huggingface-course/documentation-images) repository and use the [HTML Images Syntax](https://www.w3schools.com/html/html_images.asp) with the path `https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/{langY}/{chapterX}/{your-image.png}`.\\n3. Update the `_toctree.yml` file to include your chapter sections -- this information will render the table of contents on the website. If your section involves both the PyTorch and TensorFlow APIs of `transformers`, make sure you include links to both Colabs in the `colab` field.\\n\\nIf you get stuck, check out one of the existing chapters -- this will often show you the expected syntax.\\n\\nOnce you are happy with the content, open a pull request and tag [@lewtun](https://github.com/lewtun) for a review. We recommend adding the first chapter draft as a single pull request -- the team will then provide feedback internally to iterate on the content 🤗!\\n\\n## 🙌 Acknowledgements\\n\\nThe structure of this repo and README are inspired by the wonderful [Advanced NLP with spaCy](https://github.com/ines/spacy-course) course.\"),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 1}, page_content='The advantages and disadvantages of policy-gradient methods\\n\\nAt this point, you might ask, \"but Deep Q-Learning is excellent! Why use policy-gradient methods?\". To answer this question, let\\'s study the **advantages and disadvantages of policy-gradient methods**.\\n\\n## Advantages\\n\\nThere are multiple advantages over value-based methods. Let\\'s see some of them:\\n\\n### The simplicity of integration\\n\\nWe can estimate the policy directly without storing additional data (action values).\\n\\n### Policy-gradient methods can learn a stochastic policy\\n\\nPolicy-gradient methods can\\xa0**learn a stochastic policy while value functions can\\'t**.\\n\\nThis has two consequences:\\n\\n1. We **don\\'t need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent explores\\xa0**the state space without always taking the same trajectory.**\\n\\n2. We also get rid of the problem of **perceptual aliasing**. Perceptual aliasing is when two states seem (or are) the same but need different actions.\\n\\nLet\\'s take an example: we have an intelligent vacuum cleaner whose goal is to suck the dust and avoid killing the hamsters.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster1.jpg\" alt=\"Hamster 1\"/>\\n</figure>\\n\\nOur vacuum cleaner can only perceive where the walls are.\\n\\nThe problem is that the **two red (colored) states are aliased states because the agent perceives an upper and lower wall for each**.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster2.jpg\" alt=\"Hamster 1\"/>\\n</figure>\\n\\nUnder a deterministic policy, the policy will either always move right when in a red state or always move left. **Either case will cause our agent to get stuck and never suck the dust**.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': -1}, page_content='Under a deterministic policy, the policy will either always move right when in a red state or always move left. **Either case will cause our agent to get stuck and never suck the dust**.\\n\\nUnder a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic policy** (\"greedy epsilon strategy\"). Consequently, our agent can **spend a lot of time before finding the dust**.\\n\\nOn the other hand, an optimal stochastic policy **will randomly move left or right in red (colored) states**. Consequently, **it will not be stuck and will reach the goal state with a high probability**.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg\" alt=\"Hamster 1\"/>\\n</figure>\\n\\n### Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces\\n\\nThe problem with Deep Q-learning is that their **predictions assign a score (maximum expected future reward) for each possible action**, at each time step, given the current state.\\n\\nBut what if we have an infinite possibility of actions?\\n\\nFor instance, with a self-driving car, at each state, you can have a (near) infinite choice of actions (turning the wheel at 15°, 17.2°, 19,4°, honking, etc.). **We\\'ll need to output a Q-value for each possible action**! And **taking the max action of a continuous output is an optimization problem itself**!\\n\\nInstead, with policy-gradient methods, we output a\\xa0**probability distribution over actions.**\\n\\n### Policy-gradient methods have better convergence properties\\n\\nIn value-based methods, we use an aggressive operator to **change the value function: we take the maximum over Q-estimates**.\\nConsequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.'),\n",
       " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 3696}, page_content=\"For instance, if during the training, the best action was left (with a Q-value of 0.22) and the training step after it's right (since the right Q-value becomes 0.23), we dramatically changed the policy since now the policy will take most of the time right instead of left.\\n\\nOn the other hand, in policy-gradient methods, stochastic policy action preferences (probability of taking action) **change smoothly over time**.\\n\\n## Disadvantages\\n\\nNaturally, policy-gradient methods also have some disadvantages:\\n\\n- **Frequently, policy-gradient methods converges to a local maximum instead of a global optimum.**\\n- Policy-gradient goes slower,\\xa0**step by step: it can take longer to train (inefficient).**\\n- Policy-gradient can have high variance. We'll see in the actor-critic unit why, and how we can solve this problem.\\n\\n👉 If you want to go deeper into the advantages and disadvantages of policy-gradient methods, [you can check this video](https://youtu.be/y3oqOjHilio).\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/shap_e.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Shap-E\\n\\nThe Shap-E model was proposed in [Shap-E: Generating Conditional 3D Implicit Functions](https://huggingface.co/papers/2305.02463) by Alex Nichol and Heewoo Jun from [OpenAI](https://github.com/openai).\\n\\nThe abstract from the paper is:\\n\\n*We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space.*\\n\\nThe original codebase can be found at [openai/shap-e](https://github.com/openai/shap-e).\\n\\n<Tip>\\n\\nSee the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/shap_e.md', 'start_index': 2012}, page_content='</Tip>\\n\\n## ShapEPipeline\\n[[autodoc]] ShapEPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ShapEImg2ImgPipeline\\n[[autodoc]] ShapEImg2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ShapEPipelineOutput\\n[[autodoc]] pipelines.shap_e.pipeline_shap_e.ShapEPipelineOutput'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/streaming_wav2vec/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: streaming_wav2vec\\n\\n\\n```\\n!pip install -q gradio torch transformers \\n```\\n\\n\\n```\\nfrom transformers import pipeline\\nimport gradio as gr\\nimport time\\n\\np = pipeline(\"automatic-speech-recognition\")\\n\\ndef transcribe(audio, state=\"\"):\\n    time.sleep(2)\\n    text = p(audio)[\"text\"]\\n    state += text + \" \"\\n    return state, state\\n\\ndemo = gr.Interface(\\n    fn=transcribe, \\n    inputs=[\\n        gr.Audio(sources=[\"microphone\"], type=\"filepath\", streaming=True), \\n        \"state\"\\n    ],\\n    outputs=[\\n        \"textbox\",\\n        \"state\"\\n    ],\\n    live=True\\n)\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 1}, page_content='Adding a Sign-In with HF button to your Space\\n\\nYou can enable a built-in sign-in flow in your Space by seamlessly creating and associating an [OAuth/OpenID connect](https://developer.okta.com/blog/2019/10/21/illustrated-guide-to-oauth-and-oidc) app so users can log in with their HF account.\\n\\nThis enables new use cases for your Space. For instance, when combined with [Persistent Storage](https://huggingface.co/docs/hub/spaces-storage), a generative AI Space could allow users to log in to access their previous generations, only accessible to them.\\n\\n<Tip>\\n\\nThis guide will take you through the process of integrating a *Sign-In with HF* button into any Space. If you\\'re seeking a fast and simple method to implement this in a **Gradio** Space, take a look at its [built-in integration](https://www.gradio.app/guides/sharing-your-app#o-auth-login-via-hugging-face).\\n\\n</Tip>\\n\\n<Tip>\\n\\nYou can also use the HF OAuth flow to create a \"Sign in with HF\" flow in any website or App, outside of Spaces. [Read our general OAuth page](./oauth).\\n\\n</Tip>\\n\\n## Create an OAuth app\\n\\nAll you need to do is add `hf_oauth: true` to your Space\\'s metadata inside your `README.md` file.\\n\\nHere\\'s an example of metadata for a Gradio Space:\\n\\n```yaml\\ntitle: Gradio Oauth Test\\nemoji: 🏆\\ncolorFrom: pink\\ncolorTo: pink\\nsdk: gradio\\nsdk_version: 3.40.0\\npython_version: 3.10.6\\napp_file: app.py\\n\\nhf_oauth: true\\n# optional, see \"Scopes\" below. \"openid profile\" is always included.\\nhf_oauth_scopes:\\n - read-repos\\n - write-repos\\n - manage-repos\\n - inference-api'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 1528}, page_content='```\\n\\nYou can check out the [configuration reference docs](./spaces-config-reference) for more information.\\n\\nThis will add the following [environment variables](https://huggingface.co/docs/hub/spaces-overview#helper-environment-variables) to your space:\\n\\n- `OAUTH_CLIENT_ID`: the client ID of your OAuth app (public)\\n- `OAUTH_CLIENT_SECRET`: the client secret of your OAuth app\\n- `OAUTH_SCOPES`: scopes accessible by your OAuth app.\\n- `OPENID_PROVIDER_URL`: The URL of the OpenID provider. The OpenID metadata will be available at [`{OPENID_PROVIDER_URL}/.well-known/openid-configuration`](https://huggingface.co/.well-known/openid-configuration).\\n\\nAs for any other environment variable, you can use them in your code by using `os.getenv(\"OAUTH_CLIENT_ID\")`, for example.\\n\\n## Redirect URLs \\n\\nYou can use any redirect URL you want, as long as it targets your Space.\\n\\nNote that `SPACE_HOST` is [available](https://huggingface.co/docs/hub/spaces-overview#helper-environment-variables) as an environment variable.\\n\\nFor example, you can use `https://{SPACE_HOST}/login/callback` as a redirect URI.\\n\\n## Scopes\\n\\nThe following scopes are always included for Spaces:\\n\\n- `openid`: Get the ID token in addition to the access token.\\n- `profile`: Get the user\\'s profile information (username, avatar, etc.)\\n\\nThose scopes are optional and can be added by setting `hf_oauth_scopes` in your Space\\'s metadata:'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': -1}, page_content='Those scopes are optional and can be added by setting `hf_oauth_scopes` in your Space\\'s metadata:\\n\\n- `email`: Get the user\\'s email address.\\n- `read-repos`: Get read access to the user\\'s personal repos.\\n- `write-repos`: Get write access to the user\\'s personal repos. Does not grant read access on its own, you need to include `read-repos` as well.\\n- `manage-repos`: Get access to a repo\\'s settings. Also grants repo creation and deletion.\\n- `inference-api`: Get access to the [Inference API](https://huggingface.co/docs/api-inference/index), you will be able to make inference requests on behalf of the user.\\n\\n## Adding the button to your Space\\n\\nYou now have all the information to add a \"Sign-in with HF\" button to your Space. Some libraries ([Python](https://github.com/lepture/authlib), [NodeJS](https://github.com/panva/node-openid-client)) can help you implement the OpenID/OAuth protocol. Gradio also provides **built-in support**, making implementing the Sign-in with HF button a breeze; you can [check out the associated guide](https://www.gradio.app/guides/sharing-your-app#o-auth-login-via-hugging-face).\\n\\nBasically, you need to:'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 3937}, page_content='Basically, you need to:\\n\\n- Redirect the user to `https://huggingface.co/oauth/authorize?redirect_uri={REDIRECT_URI}&scope=openid%20profile&client_id={CLIENT_ID}&state={STATE}`, where `STATE` is a random string that you will need to verify later.\\n- Handle the callback on `/auth/callback` or `/login/callback` (or your own custom callback URL) and verify the `state` parameter.\\n- Use the `code` query parameter to get an access token and id token from `https://huggingface.co/oauth/token` (POST request with `client_id`, `code`, `grant_type=authorization_code` and `redirect_uri` as form data, and with `Authorization: Basic {base64(client_id:client_secret)}` as a header).\\n\\n<Tip warning={true}>\\n\\nYou should use `target=_blank` on the button to open the sign-in page in a new tab, unless you run the space outside its `iframe`. Otherwise, you might encounter issues with cookies on some browsers.\\n\\n</Tip>\\n\\n## Examples:\\n\\n- [Gradio test app](https://huggingface.co/spaces/Wauplin/gradio-oauth-test)\\n- [Hugging Chat (NodeJS/SvelteKit)](https://huggingface.co/spaces/huggingchat/chat-ui)\\n- [Inference Widgets (Auth.js/SvelteKit)](https://huggingface.co/spaces/huggingfacejs/inference-widgets), uses the `inference-api` scope to make inference requests on behalf of the user.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# 🧨 Diffusers Examples\\n\\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\\nfor a variety of use cases involving training or fine-tuning.\\n\\n**Note**: If you are looking for **official** examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).\\n\\nOur examples aspire to be **self-contained**, **easy-to-tweak**, **beginner-friendly** and for **one-purpose-only**.\\nMore specifically, this means:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 1159}, page_content='- **Self-contained**: An example script shall only depend on \"pip-install-able\" Python packages that can be found in a `requirements.txt` file. Example scripts shall **not** depend on any local files. This means that one can simply download an example script, *e.g.* [train_unconditional.py](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/train_unconditional.py), install the required dependencies, *e.g.* [requirements.txt](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/requirements.txt) and execute the example script.\\n- **Easy-to-tweak**: While we strive to present as many use cases as possible, the example scripts are just that - examples. It is expected that they won\\'t work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs. To help you with that, most of the examples fully expose the preprocessing of the data and the training loop to allow you to tweak and edit them as required.\\n- **Beginner-friendly**: We do not aim for providing state-of-the-art training scripts for the newest models, but rather examples that can be used as a way to better understand diffusion models and how to use them with the `diffusers` library. We often purposefully leave out certain state-of-the-art methods if we consider them too complex for beginners.\\n- **One-purpose-only**: Examples should show one task and one task only. Even if a task is from a modeling point of view very similar, *e.g.* image super-resolution and image modification tend to use the same model and training method, we want examples to showcase only one task to keep them as readable and easy-to-understand as possible.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 2914}, page_content='We provide **official** examples that cover the most popular tasks of diffusion models.\\n*Official* examples are **actively** maintained by the `diffusers` maintainers and we try to rigorously follow our example philosophy as defined above.\\nIf you feel like another important example should exist, we are more than happy to welcome a [Feature Request](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feature_request.md&title=) or directly a [Pull Request](https://github.com/huggingface/diffusers/compare) from you!\\n\\nTraining examples show how to pretrain or fine-tune diffusion models for a variety of tasks. Currently we support:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': -1}, page_content='Training examples show how to pretrain or fine-tune diffusion models for a variety of tasks. Currently we support:\\n\\n| Task | 🤗 Accelerate | 🤗 Datasets | Colab\\n|---|---|:---:|:---:|\\n| [**Unconditional Image Generation**](./unconditional_image_generation) | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)\\n| [**Text-to-Image fine-tuning**](./text_to_image) | ✅ | ✅ |\\n| [**Textual Inversion**](./textual_inversion) | ✅ | - | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\\n| [**Dreambooth**](./dreambooth) | ✅ | - | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb)\\n| [**ControlNet**](./controlnet) | ✅ | ✅ | -\\n| [**InstructPix2Pix**](./instruct_pix2pix) | ✅ | ✅ | -\\n| [**Reinforcement Learning for Control**](https://github.com/huggingface/diffusers/blob/main/examples/reinforcement_learning/run_diffusers_locomotion.py)                    | - | - | coming soon.\\n\\n## Community'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 4757}, page_content='## Community\\n\\nIn addition, we provide **community** examples, which are examples added and maintained by our community.\\nCommunity examples can consist of both *training* examples or *inference* pipelines.\\nFor such examples, we are more lenient regarding the philosophy defined above and also cannot guarantee to provide maintenance for every issue.\\nExamples that are useful for the community, but are either not yet deemed popular or not yet following our above philosophy should go into the [community examples](https://github.com/huggingface/diffusers/tree/main/examples/community) folder. The community folder therefore includes training examples and inference pipelines.\\n**Note**: Community examples can be a [great first contribution](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) to show to the community how you like to use `diffusers` 🪄.\\n\\n## Research Projects\\n\\nWe also provide **research_projects** examples that are maintained by the community as defined in the respective research project folders. These examples are useful and offer the extended capabilities which are complementary to the official examples. You may refer to [research_projects](https://github.com/huggingface/diffusers/tree/main/examples/research_projects) for details.\\n\\n## Important note\\n\\nTo make sure you can successfully run the latest versions of the example scripts, you have to **install the library from source** and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install .'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 6417}, page_content='```\\nThen cd in the example folder of your choice and run\\n```bash\\npip install -r requirements.txt\\n```'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/examples/image_classification/README.md', 'start_index': 1}, page_content='Fine-tuning for image classification using LoRA and 🤗 PEFT\\n\\n## Vision Transformer model from transformers\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/image_classification/image_classification_peft_lora.ipynb) \\n\\nWe provide a notebook (`image_classification_peft_lora.ipynb`) where we learn how to use [LoRA](https://arxiv.org/abs/2106.09685) from 🤗 PEFT to fine-tune an image classification model by ONLY using **0.7%** of the original trainable parameters of the model. \\n\\nLoRA adds low-rank \"update matrices\" to certain blocks in the underlying model (in this case the attention blocks) and ONLY trains those matrices during fine-tuning. During inference, these update matrices are _merged_ with the original model parameters. For more details, check out the [original LoRA paper](https://arxiv.org/abs/2106.09685). \\n\\n## PoolFormer model from timm\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb) \\n\\nThe notebook `image_classification_timm_peft_lora.ipynb` showcases fine-tuning an image classification model using from the [timm](https://huggingface.co/docs/timm/index) library. Again, LoRA is used to reduce the numberof trainable parameters to a fraction of the total.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 0}, page_content='--\\ntitle: \"Generating Human-level Text with Contrastive Search in Transformers 🤗\"\\nthumbnail: /blog/assets/115_introducing_contrastive_search/thumbnail.png\\nauthors:\\n- user: GMFTBY\\n---\\n\\n# Generating Human-level Text with Contrastive Search in Transformers 🤗\\n\\n\\n****\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n### 1. Introduction:\\n\\nNatural language generation (i.e. text generation) is one of the core tasks in natural language processing (NLP). In this blog, we introduce the current state-of-the-art decoding method, ___Contrastive Search___, for neural text generation. Contrastive search is originally proposed in _\"A Contrastive Framework for Neural Text Generation\"_ <a href=\\'#references\\'>[1]</a> ([[Paper]](https://arxiv.org/abs/2202.06417)[[Official Implementation]](https://github.com/yxuansu/SimCTG)) at NeurIPS 2022. Moreover, in this follow-up work,  _\"Contrastive Search Is What You Need For Neural Text Generation\"_ <a href=\\'#references\\'>[2]</a> ([[Paper]](https://arxiv.org/abs/2210.14140) [[Official Implementation]](https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need)), the authors further demonstrate that contrastive search can generate human-level text using **off-the-shelf** language models across **16** languages.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 1451}, page_content='**[Remark]** For users who are not familiar with text generation, please refer more details to [this blog post](https://huggingface.co/blog/how-to-generate).\\n\\n****\\n\\n<span id=\\'demo\\'/>\\n\\n### 2. Hugging Face 🤗 Demo of Contrastive Search:\\n\\nContrastive Search is now available on 🤗 `transformers`, both on PyTorch and TensorFlow. You can interact with the examples shown in this blog post using your framework of choice in [this Colab notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb), which is linked at the top. We have also built this awesome [demo](https://huggingface.co/spaces/joaogante/contrastive_search_generation) which directly compares contrastive search with other popular decoding methods (e.g. beam search, top-k sampling <a href=\\'#references\\'>[3]</a>, and nucleus sampling <a href=\\'#references\\'>[4]</a>).\\n\\n****\\n\\n<span id=\\'installation\\'/>\\n\\n### 3. Environment Installation:\\n\\nBefore running the experiments in the following sections, please install the update-to-date version of `transformers` as\\n```yaml\\npip install torch\\npip install \"transformers==4.24.0\"'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 2596}, page_content='```\\n\\n****\\n\\n<span id=\\'problems_of_decoding_methods\\'/>\\n\\n### 4. Problems of Existing Decoding Methods:\\n\\nDecoding methods can be divided into two categories: (i) deterministic methods and (ii) stochastic methods. Let\\'s discuss both!\\n\\n\\n<span id=\\'deterministic_methods\\'/>\\n\\n#### 4.1. Deterministic Methods:\\n\\nDeterministic methods, e.g. greedy search and beam search, generate text by selecting the text continuation with the highest likelihood measured by the language model. However, as widely discussed in previous studies <a href=\\'#references\\'>[3]</a><a href=\\'#references\\'>[4]</a>, deterministic methods often lead to the problem of _model degeneration_, i.e., the generated text is unnatural and contains undesirable repetitions.\\n\\nBelow, let\\'s see an example of generated text from greedy search using GPT-2 model.\\n\\n```python\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\'gpt2-large\\')\\ninput_ids = tokenizer(\\'DeepMind Company is\\', return_tensors=\\'pt\\').input_ids\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2-large\\')\\n\\noutput = model.generate(input_ids, max_length=128)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': -1}, page_content=\"```\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeepMind Company is a leading AI research company, with a focus on deep learning and deep\\nlearning-based systems.\\n\\nThe company's research is focused on the development of deep learning-based systems that\\ncan learn from large amounts of data, and that can be used to solve real-world problems.\\n\\nDeepMind's research is also used by the UK government to develop new technologies for the\\nUK's National Health Service.\\n\\nDeepMind's research is also used by the UK government to develop new technologies for the\\nUK's National Health Service.\\n\\nDeepMind's research is also used by the UK government to develop new technologies\\n----------------------------------------------------------------------------------------------------\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 4721}, page_content='```\\n</details>\\n\\n**[Remark]** From the result generated by greedy search, we can see obvious pattern of repetitions.\\n\\n<span id=\\'stochastic_methods\\'/>\\n\\n#### 4.2. Stochastic Methods:\\n\\nTo address the issues posed by deterministic methods, stochastic methods generate text by introducing randomness during the decoding process. Two widely-used stochastic methods are (i) top-k sampling <a href=\\'#references\\'>[3]</a> and (ii) nucleus sampling (also called top-p sampling) <a href=\\'#references\\'>[4]</a>.\\n\\nBelow, we illustrate an example of generated text by nucleus sampling (p=0.95) using the GPT-2 model.\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\'gpt2-large\\')\\ninput_ids = tokenizer(\\'DeepMind Company is\\', return_tensors=\\'pt\\').input_ids\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2-large\\')\\n\\ntorch.manual_seed(0.)\\noutput = model.generate(input_ids, do_sample=True, max_length=128, top_p=0.95, top_k=0)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': -1}, page_content='```\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeepMind Company is a leading provider of AI-based research, development, and delivery of\\nAI solutions for security, infrastructure, machine learning, communications, and so on.\"\\n\\n\\'AI is not journalism\\'\\n\\nWorse still was the message its researchers hoped would reach the world\\'s media — that it\\nwas not really research, but rather a get-rich-quick scheme to profit from living forces\\'\\nignorance.\\n\\n\"The thing is, we know that people don\\'t consciously assess the value of the others\\'\\ninformation. They understand they will get the same on their own.\"\\n\\nOne example? Given the details of today\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 6677}, page_content='```\\n</details>\\n\\n**[Remark]** While nucleus sampling can generate text free of repetitions, the semantic coherence of the generated text is not well-maintained. For instance, the generated phrase _\\'AI is not journalism\\'_ is incoherent with respect to the given prefix, i.e. _\\'DeepMind Company\\'_.\\n\\nWe note that this semantic inconsistency problem can partially be remedied by lowering the temperature. However, reducing the temperature brings nucleus sampling closer to greedy search, which can be seen as a trade-off between greedy search and nucleus sampling. Generally, it is challenging to find a prompt and model-independent temperature that avoids both the pitfalls of greedy search and nucleus sampling.\\n\\n****\\n\\n<span id=\\'contrastive_search\\'/>\\n\\n### 5. Contrastive Search:\\n\\nIn this section, we introduce a new decoding method, ___Contrastive Search___, in details.\\n\\n<span id=\\'contrastive_objective\\'/>\\n\\n#### 5.1. Decoding Objective:\\n\\nGiven the prefix text \\\\\\\\(x_{< t}\\\\\\\\), the selection of the output token \\\\\\\\(x_{t}\\\\\\\\) follows\\n\\n<center class=\"half\">\\n    <img src=\"assets/115_introducing_contrastive_search/formulation.png\" width=\"750\"/>\\n</center>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': -1}, page_content='<center class=\"half\">\\n    <img src=\"assets/115_introducing_contrastive_search/formulation.png\" width=\"750\"/>\\n</center>\\n\\nwhere \\\\\\\\(V^{(k)}\\\\\\\\) is the set of top-k predictions from the language model\\'s probability distribution \\\\\\\\(p_{\\\\theta}(v|x_{< t})\\\\\\\\). The first term, i.e. _model confidence_, is the probability of the candidate \\\\\\\\(v\\\\\\\\) predicted by the language model. The second term, _degeneration penalty_, measures how discriminative of \\\\\\\\(v\\\\\\\\) with respect to the previous context \\\\\\\\( x_{< t}\\\\\\\\) and the function \\\\\\\\(s(\\\\cdot, \\\\cdot)\\\\\\\\) computes the cosine similarity between the token representations. More specifically, the degeneration penalty is defined as the maximum cosine similarity between the token representation of \\\\\\\\(v\\\\\\\\), i.e. \\\\\\\\(h_{v}\\\\\\\\), and that of all tokens in the context \\\\\\\\(x_{< t}\\\\\\\\). Here, the candidate representation \\\\\\\\(h_{v}\\\\\\\\) is computed by the language model given the concatenation of \\\\\\\\(x_{< t}\\\\\\\\) and \\\\\\\\(v\\\\\\\\). Intuitively, a larger degeneration penalty of \\\\\\\\(v\\\\\\\\) means it is more similar (in the representation space) to the context, therefore more likely leading to the problem of model degeneration. The hyperparameter \\\\\\\\(\\\\alpha\\\\\\\\) regulates the importance of these two components. When \\\\\\\\(\\\\alpha=0\\\\\\\\), contrastive search degenerates to the vanilla greedy search.\\n\\n**[Remark]** When generating output, contrastive search jointly considers (i) the probability predicted by the language model to maintain the semantic coherence between the generated text and the prefix text; and (ii) the similarity with respect to the previous context to avoid model degeneration.\\n\\n\\n<span id=\\'contrastive_generation\\'/>\\n\\n#### 5.2. Generating Text with Contrastive Search:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 9310}, page_content='<span id=\\'contrastive_generation\\'/>\\n\\n#### 5.2. Generating Text with Contrastive Search:\\n\\nBelow, we use the same prefix text (i.e. _\"DeepMind Company is\"_) as in Section <a href=\\'#deterministic_methods\\'>4.1</a> and <a href=\\'#stochastic_methods\\'>4.2</a>, and generate the text with contrastive search (k=4 and \\\\\\\\(\\\\alpha=0.6\\\\\\\\)). To fully demonstrate the superior capability of contrastive search, we let the language model generate a **long** document with **512** tokens as\\n\\n```python\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\\n\\nmodel_name = \\'gpt2-large\\'\\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\\nmodel = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\\nmodel.eval()\\n\\n# prepare the prefix\\nprefix_text = r\\'DeepMind Company is\\'\\ninput_ids = tokenizer(prefix_text, return_tensors=\\'pt\\').input_ids\\n\\n# generate the result with contrastive search\\noutput = model.generate(input_ids, penalty_alpha=0.6, top_k=4, max_length=512)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 10398}, page_content='```\\n\\nThe arguments are as follows:\\n* `--top_k`: The hyperparameter \\\\\\\\(k\\\\\\\\) in contrastive search.\\n* `--penalty_alpha`: The hyperparameter \\\\\\\\(\\\\alpha\\\\\\\\) in contrastive search.\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 10629}, page_content='```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeepMind Company is a leader in artificial intelligence (AI). We have a long history of working\\nwith companies such as Google, Facebook, Amazon, and Microsoft to build products that improve\\npeople\\'s lives, and today we are excited to announce that DeepMind\\'s AlphaGo program has won the\\ngame of Go, becoming the first program to defeat a professional Go player.\\n\\nThe victory is a testament to the power of deep learning, and to the incredible work of our\\nresearch team, which has been at the forefront of AI research for the past five years. AlphaGo\\nis one of the most advanced Go programs ever created, and its performance is an important step\\ntowards the goal of human-level AI.\\n\\n\"This is the culmination of a decade of hard work,\" said Andy Ng, co-founder and CTO of DeepMind.\\n\"We are thrilled to have achieved this milestone and look forward to continuing to develop AI that\\ncan be used in a wide range of applications and to help people live better lives.\"\\n\\nDeepMind\\'s work on Go began in 2010, when it began to train a neural network to play Go using\\nmillions of games played by top Go players around the world. Since then, the team has refined the\\nalgorithm, adding more and more layers of reinforcement learning to make it better at recognizing\\npatterns and making decisions based on those patterns. In the past year and a half, the team has\\nmade significant progress in the game, winning a record-tying 13 games in a row to move into the\\ntop four of the world rankings.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 12222}, page_content='\"The game of Go is a complex game in which players have to be very careful not to overextend their\\nterritory, and this is something that we have been able to improve over and over again,\" said\\nDr. Demis Hassabis, co-founder and Chief Scientific Officer of DeepMind. \"We are very proud of our\\nteam\\'s work, and we hope that it will inspire others to take the next step in their research and\\napply the same techniques to other problems.\"\\n\\nIn addition to the win in Go, DeepMind has also developed an AI system that can learn to play a\\nnumber of different games, including poker, Go, and chess. This AI system, called Tarsier, was\\ndeveloped in partnership with Carnegie Mellon University and the University of California,\\nBerkeley, and is being used to teach computer vision and machine learning to identify objects in\\nimages and recognize speech in natural language. Tarsier has been trained to play the game of Go\\nand other games on a\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 13256}, page_content='```\\n</details>\\n\\n**[Remark]** We see that the generated text is of exceptionally high quality. The entire document is grammatically fluent as well as semantically coherent. Meanwhile, the generated text also well maintains its factually correctness. For instance, in the first paragraph, it elaborates _\"AlphaGo\"_ as the _\"first program to defeat a professional Go player\"_.\\n\\n\\n<span id=\\'contrastive_visual_demonstration\\'/>\\n\\n#### 5.3. Visual Demonstration of Contrastive Search:\\n\\nTo better understand how contrastive search works, we provide a visual comparison between greedy search (<a href=\\'#deterministic_methods\\'>Section 4.1</a>) and contrastive search. Specifically, we visualize the token similarity matrix of the generated text from greedy search and contrastive search, respectively. The similarity between two tokens is defined as the cosine similarity between their token representations (i.e. the hidden states of the last transformer layer). The results of greedy search (top) and contrastive search (bottom) are shown in the Figure below.\\n\\n<center class=\"half\">\\n    <img src=\"assets/115_introducing_contrastive_search/greedy_search_visualization.png\" width=\"400\"/>\\n    <img src=\"assets/115_introducing_contrastive_search/contrastive_search_visualization.png\" width=\"400\"/>\\n</center>\\n\\n**[Remark]** From the result of greedy search, we see high similarity scores in the off-diagonal entries which clearly indicates the generated repetitions by greedy search. On the contrary, in the result of contrastive search, the high similarity scores mostly appear in the diagonal entries which verifies that the degeneration problem is successfully addressed. This nice property of contrastive search is achieved by the introduction of degeneration penalty (see <a href=\\'#contrastive_objective\\'>Section 5.1</a>) during the decoding process.\\n\\n\\n****\\n\\n<span id=\\'more_examples\\'/>\\n\\n### 6. More Generated Examples:\\n\\nIn this section, we provide more generated examples to compare different decoding methods.\\n\\n<span id=\\'gpt2_example_one\\'/>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': -1}, page_content='<span id=\\'more_examples\\'/>\\n\\n### 6. More Generated Examples:\\n\\nIn this section, we provide more generated examples to compare different decoding methods.\\n\\n<span id=\\'gpt2_example_one\\'/>\\n\\n#### 6.1. Example One - GPT-2:\\n\\nIn this part, we use GPT-2 to generate text with the prefix text from the original [OpenAI blog](https://openai.com/blog/better-language-models/) that announced the release of GPT-2.\\n\\n> _In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English._\\n\\n\\n<details open>\\n<summary><b> Load the language model and prepare the prefix text:</b></summary>\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\'gpt2-large\\')\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2-large\\')\\n\\nprefix_text = r\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\\ninput_ids = tokenizer(prefix_text, return_tensors=\\'pt\\').input_ids'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 16343}, page_content='```\\n</details>\\n\\n<span id=\\'gpt2_greedy_example_one\\'/>\\n\\n##### 6.1.1. Generating Text with Greedy Search:\\n\\n<details>\\n<summary><b>Code: [click to expand]</b></summary>\\n\\n```python\\noutput = model.generate(input_ids, max_length=512)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n</details>\\n\\n<details>\\n<summary><b>Model Output: [click to expand]</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 16768}, page_content='```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously\\nunexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact\\nthat the unicorns spoke perfect English.\\n\\nThe researchers, led by Dr. David R. Williams of the University of California, Santa Cruz,\\ndiscovered the unicorns in the Andes Mountains of Peru. The area is known for its unique geology\\nand is home to a number of rare species of animals.\\n\\nThe researchers found the unicorns in the Andes Mountains of Peru.\\n\\n\"We were surprised to find that the unicorns were able to communicate with each other,\" Williams\\nsaid. \"We were also surprised to find that they were able to communicate in English.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': -1}, page_content='The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 19232}, page_content='```\\n</details>\\n\\n<span id=\\'gpt2_nucleus_example_one\\'/>\\n\\n##### 6.1.2. Generating Text with Nucleus Sampling:\\n\\n<details>\\n<summary><b>Code: [click to expand]</b></summary>\\n\\n```python\\ntorch.manual_seed(0.)\\noutput = model.generate(input_ids, do_sample=True, max_length=512, top_p=0.95, top_k=0)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n</details>\\n\\n\\n<details>\\n<summary><b>Model Output: [click to expand]</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 19721}, page_content='```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously\\nunexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact\\nthat the unicorns spoke perfect English. The study was published in the Journal of Zoology in\\nMarch 2016.\\n\\nPolygynous mammals such as unicorns have remained largely unknown to science. Professor Gustavo\\nGiacota, from the University of Oxford who led the study, said that they had been documented as\\nfar as Eastern Siberia in Russia, but had only been seen a handful of times in the Gobi Desert.\\n\\nTiny animals with pale and shiny coats live in the presence of human beings and are hardly likely\\nto be victims of any cruelty. However, there is some evidence of the condition occurring in both\\nhumans and animals in remote regions, which might have similarities to \"black moles\" that coexist\\non the skin.\\n\\nIt is thought that Unicorns could be inside themselves, that they have different scents depending\\non their current environment, or just fall out and there are plenty of legends of how they have\\nsurvived. Experts speculate that the moths and other animals could be remnants of the Yezidi Isis\\nand Charon, which literally is both the word which means great bird, and the Greek word for sound.\\nIt is said that the Isis and Charon taught their young the use of voice in the form of calling out\\nto others.\\n\\nThe scientists think that it could be ancient folklore that has survived and is no longer attributed\\nto a real entity\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 21448}, page_content='```\\n</details>\\n\\n\\n<span id=\\'gpt2_contrastive_example_one\\'/>\\n\\n##### 6.1.3. Generating Text with Contrastive Search:\\n\\n<details open>\\n<summary><b>Code:</b></summary>\\n\\n```python\\noutput = model.generate(input_ids, max_length=512, penalty_alpha=0.6, top_k=4)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n</details>\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 21886}, page_content='```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored\\nvalley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns\\nspoke perfect English.\\n\\nAccording to the BBC, a team of scientists led by Dr David MacKay, from the University of Bristol, spent\\ntwo years searching for the unicorn herd, which they discovered during a survey of the area.\\n\\n\"It\\'s a very rare find,\" MacKay told the BBC. \"There are a few in the Himalayas, but this is the first\\ntime we\\'ve been able to find one in such a remote area.\"\\n\\nThe team was surprised to find a herd of unicorns living in a region that has been known to be a hotbed\\nof poaching, with many of the animals poached for their horns, which are used in traditional Chinese\\nmedicine to treat everything from rheumatism to cancer.\\n\\n\"We knew that the area was rich in rhino horn, but we had no idea how many there were, or what they were\\ndoing there,\" MacKay said. \"This is an area of high poaching pressure, and we wanted to find out what was\\ngoing on.\"\\n\\nIn order to do so, the team used GPS collars to track the animals as they moved around the mountain and\\nthe surrounding area. The GPS data was then compared with information gathered from local villagers, who\\nhad a wealth of information about the animals\\' movements, including where they were eating, what they were\\ndoing at night, and how much time they spent in the mountains each day.\\n\\nAfter analyzing the data, the team determined that the herd consisted of at least three species of unicorns,\\nincluding a male and two females. One of the females was the mother of the male, and the other two were her\\ndaughters. All three had the same horn color, which is believed to be a sign of purity in the animal kingdom.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 23793}, page_content='While the discovery is exciting, it\\'s not the first time scientists have discovered an animal that speaks\\nEnglish. Last year, scientists discovered a species of porcupine that can be heard by humans, and has been\\ndubbed \"Porcupine Man\" for his ability to converse with the human race.\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 24179}, page_content='```\\n</details>\\n\\n\\n<span id=\\'opt_example_two\\'/>\\n\\n#### 6.2. Example Two - OPT:\\n\\nIn this part, we use the OPT model <a href=\\'#references\\'>[5]</a> which is recently released by Meta to generate text by taking the first two sentences from the abstract of the prestigious ResNet paper <a href=\\'#references\\'>[6]</a>.\\n\\n> Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.\\n\\n\\n<details open>\\n<summary><b> Load the language model and prepare the prefix text:</b></summary>\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, OPTForCausalLM\\nmodel_name = r\\'facebook/opt-1.3b\\'\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = OPTForCausalLM.from_pretrained(model_name)\\n\\nprefix_text = r\"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.\"\\ninput_ids = tokenizer(prefix_text, return_tensors=\\'pt\\').input_ids\\n```\\n</details>\\n\\n\\n\\n<span id=\\'opt_greedy_example_two\\'/>\\n\\n##### 6.2.1. Generating Text with Greedy Search:\\n\\n<details>\\n<summary><b>Code: [click to expand]</b></summary>\\n\\n```python\\noutput = model.generate(input_ids, max_length=256)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n</details>\\n\\n<details>\\n<summary><b>Model Output: [click to expand]</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': -1}, page_content='```\\n</details>\\n\\n<details>\\n<summary><b>Model Output: [click to expand]</b></summary>\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease\\nthe training of networks that are substantially deeper than those used previously. We show that the\\nresidual learning framework can be used to train deep neural networks that are significantly more\\ndifficult to train than those used previously. We also show that the residual learning framework can\\nbe used to train deep neural networks that are significantly more difficult to train than those used\\npreviously.\\n\\nThe paper presents a new residual learning framework for deep neural networks that is based on the\\nconcept of residuals. The residuals are the residuals of the network that are not used in the training\\nprocess. The residuals are computed by taking the residuals of the network that are used in the training\\nprocess and subtracting the residuals of the network that are not used in the training process. The\\nresiduals are then used to train the network. The residuals are computed by taking the residuals of\\nthe network that are used in the training process and subtracting the residuals of the network that\\nare not used in the training process. The residuals are then used to train the network. The residuals\\nare computed by taking the residuals of the network that are used in the training process and\\nsubtracting the residuals of the\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 27248}, page_content='```\\n</details>\\n\\n\\n<span id=\\'opt_greedy_example_two\\'/>\\n\\n##### 6.2.2. Generating Text with Nucleus Sampling:\\n\\n<details>\\n<summary><b>Code: [click to expand]</b></summary>\\n\\n```python\\ntorch.manual_seed(0.)\\noutput = model.generate(input_ids, do_sample=True, max_length=256, top_p=0.95, top_k=0)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n</details>\\n\\n\\n<details>\\n<summary><b>Model Output: [click to expand]</b></summary>\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease the\\ntraining of networks that are substantially deeper than those used previously. The theory focuses on\\nseveral aspects of learning, including the dynamics of replicative and non-replicative aspects of learning.\\nThis framework emphasizes learning by entropy. New randomized algorithms enable training networks with\\nresidual learning, so that deep networks can be deployed as reliably and as efficiently as their more\\nconventional counterparts.\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 28496}, page_content='```\\n</details>\\n\\n\\n\\n<span id=\\'opt_contrastive_example_two\\'/>\\n\\n##### 6.2.3. Generating Text with Contrastive Search:\\n\\n<details open>\\n<summary><b>Code:</b></summary>\\n\\n```python\\noutput = model.generate(input_ids, max_length=256, penalty_alpha=0.6, top_k=6)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n</details>\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': -1}, page_content='```\\n</details>\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeeper neural networks are more difficult to train. We present a residual learning framework to ease\\nthe training of networks that are substantially deeper than those used previously.\\n\\nIn this paper, we propose a model-based residual learning (MBRL) framework that is based on neural\\nnetworks trained on data that is sparse in terms of dimensionality (e.g., 1, 2, 3, etc.). The network\\nparameters are chosen such that there is a high probability of convergence, i.e., the number of\\niterations is large enough to minimize the variance of the residuals. This is achieved by training\\nthe network on a set of training data, in which the data is sparse in terms of dimensionality, and\\nthen discarding the nonparametric part of the data after training is complete.\\n\\nWe show that MBRL outperforms other methods for deep reinforcement learning (RL) and deep convolutional\\nneural networks (CNNs) by a factor of at least 2. In addition, we show that, compared to CNNs, MBRL\\nperforms better in two-dimensional (2D) and three-dimensional (3D) cases.\\n----------------------------------------------------------------------------------------------------'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 30186}, page_content=\"```\\n</details>\\n\\n****\\n\\n<span id='resources'/>\\n\\n### 7. Resources:\\n\\nFor more details of contrastive search, please check our papers and code as\\n* **A Contrastive Framework for Neural Text Generation**: (1) [Paper](https://arxiv.org/abs/2202.06417) and (2) [Official Implementation](https://github.com/yxuansu/SimCTG).\\n* **Contrastive Search Is What You Need For Neural Text Generation**: (1) [Paper](https://arxiv.org/abs/2210.14140) and (2) [Official Implementation](https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need).\\n\\n****\\n\\n<span id='citation'/>\\n\\n### 8. Citation:\\n\\n```bibtex\\n@inproceedings{su2022a,\\n   title={A Contrastive Framework for Neural Text Generation},\\n   author={Yixuan Su and Tian Lan and Yan Wang and Dani Yogatama and Lingpeng Kong and Nigel Collier},\\n   booktitle={Advances in Neural Information Processing Systems},\\n   editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},\\n   year={2022},\\n   url={https://openreview.net/forum?id=V88BafmH9Pj}\\n}\\n\\n@article{su2022contrastiveiswhatyouneed,\\n  title={Contrastive Search Is What You Need For Neural Text Generation},\\n  author={Su, Yixuan and Collier, Nigel},\\n  journal={arXiv preprint arXiv:2210.14140},\\n  year={2022}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 31403}, page_content='```\\n\\n\\n\\n****\\n\\n<span id=\\'references\\'/>\\n\\n## Reference:\\n> [1] Su et al., 2022 [\"A Contrastive Framework for Neural Text Generation\"](https://arxiv.org/abs/2202.06417), NeurIPS 2022\\n\\n> [2] Su and Collier, 2022 [\"Contrastive Search Is What You Need For Neural Text Generation\"](https://arxiv.org/abs/2210.14140), Arxiv 2022\\n\\n> [3] Fan et al., 2018 [\"Hierarchical Neural Story Generation\"](https://arxiv.org/abs/1805.04833), ACL 2018\\n\\n> [4] Holtzman et al., 2020 [\"The Curious Case of Neural Text Degeneration\"](https://arxiv.org/abs/1904.09751), ICLR 2020\\n\\n> [5] Zhang et al., 2022 [\"OPT: Open Pre-trained Transformer Language Models\"](https://arxiv.org/abs/2205.01068), Arxiv 2022\\n\\n> [6] He et al., 2016 [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/abs/1512.03385), CVPR 2016\\n\\n****\\n\\n*- Written by Yixuan Su and Tian Lan*\\n\\n****\\n\\n\\n\\n<span id=\\'acknowledgements\\'/>\\n\\n\\n## Acknowledgements:\\n\\nWe would like to thank Joao Gante ([@joaogante](https://huggingface.co/joaogante)), Patrick von Platen ([@patrickvonplaten](https://huggingface.co/patrickvonplaten)), and Sylvain Gugger ([@sgugger](https://github.com/sgugger)) for their help and guidance in adding contrastive search mentioned in this blog post into the `transformers` library.'),\n",
       " Document(metadata={'source': 'huggingface/optimum/blob/main/docs/source/exporters/tflite/package_reference/export.mdx', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Export functions\\n\\n## Main functions\\n\\n[[autodoc]] exporters.tflite.convert.export\\n\\n## Utility functions\\n\\n[[autodoc]] exporters.tflite.convert.validate_model_outputs'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggingface-and-optimum-amd.md', 'start_index': 0}, page_content='--\\ntitle: \"AMD + 🤗: Large Language Models Out-of-the-Box Acceleration with AMD GPU\"\\nthumbnail: /blog/assets/optimum_amd/amd_hf_logo_fixed.png\\nauthors:\\n- user: fxmarty\\n- user: IlyasMoutawwakil\\n- user: mohitsha\\n- user: echarlaix\\n- user: seungrokj\\n  guest: true\\n- user: mfuntowicz\\n---\\n\\n# AMD + 🤗: Large Language Models Out-of-the-Box Acceleration with AMD GPU\\n\\nEarlier this year, [AMD and Hugging Face announced a partnership](https://huggingface.co/blog/huggingface-and-amd) to accelerate AI models during the AMD\\'s  AI Day event. We have been hard at work to bring this vision to reality, and make it easy for the Hugging Face community to run the latest AI models on AMD hardware with the best possible performance.\\n\\nAMD is powering some of the most powerful supercomputers in the World, including the fastest European one, [LUMI](https://www.lumi-supercomputer.eu/lumi-retains-its-position-as-europes-fastest-supercomputer/), which operates over 10,000 MI250X AMD GPUs. At this event, AMD revealed their latest generation of server GPUs, the AMD [Instinct™  MI300](https://www.amd.com/fr/graphics/instinct-server-accelerators) series accelerators, which will soon become generally available.\\n\\nIn this blog post, we provide an update on our progress towards providing great out-of-the-box support for AMD GPUs, and improving the interoperability for the latest server-grade AMD Instinct GPUs\\n\\n## Out-of-the-box Acceleration\\n\\nCan you spot AMD-specific code changes below? Don\\'t hurt your eyes, there\\'s none compared to running on NVIDIA GPUs 🤗.\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggingface-and-optimum-amd.md', 'start_index': -1}, page_content='```python\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\nmodel_id = \"01-ai/Yi-6B\"\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\nwith torch.device(\"cuda\"):\\n\\tmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\\n\\ninp = tokenizer([\"Today I am in Paris and\"], padding=True, return_tensors=\"pt\").to(\"cuda\")\\nres = model.generate(**inp, max_new_tokens=30)\\n\\nprint(tokenizer.batch_decode(res))'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggingface-and-optimum-amd.md', 'start_index': 1992}, page_content='```\\n\\nOne of the major aspects we have been working on is the ability to run Hugging Face Transformers models without any code change. We now support all Transformers models and tasks on AMD Instinct GPUs. And our collaboration is not stopping here, as we explore out-of-the-box support for diffusers models, and other libraries as well as other AMD GPUs.\\n\\nAchieving this milestone has been a significant effort and collaboration between our teams and companies. To maintain support and performances for the Hugging Face community, we have built integrated testing of Hugging Face open source libraries on AMD Instinct GPUs in our datacenters - and were able to minimize the carbon impact of these new workloads working with Verne Global to deploy the AMD Instinct servers in [Iceland](https://verneglobal.com/about-us/locations/iceland/).\\n\\nOn top of native support, another major aspect of our collaboration is to provide integration for the latest innovations and features available on AMD GPUs. Through the collaboration of Hugging Face team, AMD engineers and open source community members, we are happy to announce [support for](https://huggingface.co/docs/optimum/amd/index):'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggingface-and-optimum-amd.md', 'start_index': 3174}, page_content='* Flash Attention v2 from AMD Open Source efforts in [ROCmSoftwarePlatform/flash-attention](https://github.com/ROCmSoftwarePlatform/flash-attention) integrated natively in [Transformers](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2) and [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/quicktour).\\n* Paged Attention from [vLLM](https://github.com/vllm-project/vllm/pull/1313), and various fused kernels available in [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/quicktour) for ROCm.\\n* [DeepSpeed](https://github.com/microsoft/DeepSpeed) for ROCm-powered GPUs using Transformers is also now officially validated and supported.\\n* GPTQ, a common weight compression technique used to reduce the model memory requirements, is supported on ROCm GPUs through a direct integration with [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) and [Transformers](https://huggingface.co/blog/gptq-integration).\\n* [Optimum-Benchmark](https://github.com/huggingface/optimum-benchmark), a utility to easily benchmark the performance of Transformers on AMD GPUs, in normal and distributed settings, with supported optimizations and quantization schemes.\\n* Support of ONNX models execution on ROCm-powered GPUs using ONNX Runtime through the [ROCMExecutionProvider](https://onnxruntime.ai/docs/execution-providers/ROCm-ExecutionProvider.html) using [Optimum library](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/amdgpu).'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggingface-and-optimum-amd.md', 'start_index': 4690}, page_content='We are very excited to make these state of the art acceleration tools available and easy to use to Hugging Face users, and offer maintained support and performance with direct integration in our new continuous integration and development pipeline for AMD Instinct GPUs.\\n\\nOne AMD Instinct MI250 GPU with 128 GB of High Bandwidth Memory has two distinct ROCm devices (GPU 0 and 1), each of them having 64 GB of High Bandwidth Memory.\\n\\n<br>\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img alt=\"\" src=\"assets/optimum_amd/rocmsmi.png\" />\\n  <figcaption>MI250 two devices as displayed by `rocm-smi`</figcaption>\\n</figure>\\n<br>\\n\\nThis means that with just one MI250 GPU card, we have two PyTorch devices that can be used very easily with tensor and data parallelism to achieve higher throughputs and lower latencies.\\n\\nIn the rest of the blog post, we report performance results for the two steps involved during the text generation through large language models:\\n* **Prefill latency**: The time it takes for the model to compute the representation for the user\\'s provided input or prompt (also referred to as \"Time To First Token\").\\n* **Decoding per token latency**: The time it takes to generate each new token in an autoregressive manner after the prefill step.\\n* **Decoding throughput**: The number of tokens generated per second during the decoding phase.\\n\\nUsing [`optimum-benchmark`](https://github.com/huggingface/optimum-benchmark) and running [inference benchmarks](https://github.com/huggingface/optimum-benchmark/tree/main/examples/running-llamas) on an MI250 and an A100 GPU with and without optimizations, we get the following results:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggingface-and-optimum-amd.md', 'start_index': 6341}, page_content='<br>\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img alt=\"\" src=\"assets/optimum_amd/transformers_bench.png\" />\\n  <figcaption>Inference benchmarks using Transformers and PEFT libraries. FA2 stands for \"Flash Attention 2\", TP for \"Tensor Parallelism\", DDP for \"Distributed Data Parallel\".</figcaption>\\n</figure>\\n<br>\\n\\nIn the plots above, we can see how performant the MI250 is, especially for production settings where requests are processed in big batches, delivering more than 2.33x more tokens (decode throughput) and taking half the time to the first token (prefill latency), compared to an A100 card.\\n\\nRunning [training benchmarks](https://github.com/huggingface/optimum-benchmark/tree/main/examples/training-llamas) as seen below, one MI250 card fits larger batches of training samples and reaches higher training throughput.\\n\\n<br>\\n<figure class=\"image table text-center m-0 w-9/12\">\\n  <img alt=\"\" src=\"assets/optimum_amd/training_bench.png\" />\\n  <figcaption>Training benchmark using Transformers library at maximum batch size (power of two) that can fit on a given card</figcaption>\\n</figure>\\n<br>\\n\\n## Production Solutions\\n\\nAnother important focus for our collaboration is to build support for Hugging Face production solutions, starting with Text Generation Inference (TGI). TGI provides an end-to-end solution to deploy large language models for inference at scale.\\n\\nInitially, TGI was mostly driven towards Nvidia GPUs, leveraging most of the recent optimizations made for post Ampere architecture, such as Flash Attention v1 and v2, GPTQ weight quantization and Paged Attention.\\n\\nToday, we are happy to announce initial support for AMD Instinct MI210 and MI250 GPUs in TGI, leveraging all the great open-source work detailed above, integrated in a complete end-to-end solution, ready to be deployed.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggingface-and-optimum-amd.md', 'start_index': 8162}, page_content='Performance-wise, we spent a lot of time benchmarking Text Generation Inference on AMD Instinct GPUs to validate and discover where we should focus on optimizations. As such, and with the support of AMD GPUs Engineers, we have been able to achieve matching performance compared to what TGI was already offering.\\n\\nIn this context, and with the long-term relationship we are building between AMD and Hugging Face, we have been integrating and testing with the AMD GeMM Tuner tool which allows us to tune the GeMM (matrix multiplication) kernels we are using in TGI to find the best setup towards increased performances. GeMM Tuner tool is expected to be released [as part of PyTorch](https://github.com/pytorch/pytorch/pull/114894) in a coming release for everyone to benefit from it.\\n\\nWith all of the above being said, we are thrilled to show the very first performance numbers demonstrating the latest AMD technologies, putting Text Generation Inference on AMD GPUs at the forefront of efficient inferencing solutions with Llama model family.\\n\\n<br>\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img alt=\"\" src=\"assets/optimum_amd/tgi_34b.png\" />\\n  <figcaption>TGI latency results for Llama 34B, comparing one AMD Instinct MI250 against A100-SXM4-80GB. As explained above one MI250 corresponds to two PyTorch devices.</figcaption>\\n</figure>\\n<br>\\n\\n<br>\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img alt=\"\" src=\"assets/optimum_amd/tgi_70b.png\" />\\n  <figcaption>TGI latency results for Llama 70B, comparing two AMD Instinct MI250 against two A100-SXM4-80GB (using tensor parallelism)</figcaption>\\n</figure>\\n<br>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/huggingface-and-optimum-amd.md', 'start_index': 9793}, page_content=\"Missing bars for A100 correspond to out of memory errors, as Llama 70B weights 138 GB in float16, and enough free memory is necessary for intermediate activations, KV cache buffer (>5GB for 2048 sequence length, batch size 8), CUDA context, etc. The Instinct MI250 GPU has 128 GB global memory while an A100 has 80GB which explains the ability to run larger workloads (longer sequences, larger batches) on MI250.\\n\\nText Generation Inference is [ready to be deployed](https://huggingface.co/docs/text-generation-inference/quicktour) in production on AMD Instinct GPUs through the docker image `ghcr.io/huggingface/text-generation-inference:1.2-rocm`. Make sure to refer to the [documentation](https://huggingface.co/docs/text-generation-inference/supported_models#supported-hardware) concerning the support and its limitations.\\n\\n## What's next?\\n\\nWe hope this blog post got you as excited as we are at Hugging Face about this partnership with AMD. Of course, this is just the very beginning of our journey, and we look forward to enabling more use cases on more AMD hardware.\\n\\nIn the coming months, we will be working on bringing more support and validation for AMD Radeon GPUs, the same GPUs you can put in your own desktop for local usage, lowering down the accessibility barrier and paving the way for even more versatility for our users.\\n\\nOf course we'll soon be working on performance optimization for the MI300 lineup, ensuring that both the Open Source and the Solutions provide with the latest innovations at the highest stability level we are always looking for at Hugging Face.\\n\\nAnother area of focus for us will be around AMD Ryzen AI technology, powering the latest generation of AMD laptop CPUs, allowing to run AI at the edge, on the device. At the time where Coding Assistant, Image Generation tools and Personal Assistant are becoming more and more broadly available, it is important to offer solutions which can meet the needs of privacy to leverage these powerful tools. In this context, Ryzen AI compatible models are already being made available on the [Hugging Face Hub](https://huggingface.co/models?other=RyzenAI) and we're working closely with AMD to bring more of them in the coming months.\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/rexnet.md', 'start_index': 1}, page_content='RexNet\\n\\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bottlenecks in image classification models. Authors refine each layer by 1) expanding the input channel size of the convolution layer and 2) replacing the [ReLU6s](https://www.paperswithcode.com/method/relu6).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model(\\'rexnet_100\\', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/rexnet.md', 'start_index': 1208}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]\\n```\\n\\nReplace the model name with the variant you want to use, e.g. `rexnet_100`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model(\\'rexnet_100\\', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/rexnet.md', 'start_index': 2555}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{han2020rexnet,\\n      title={ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network}, \\n      author={Dongyoon Han and Sangdoo Yun and Byeongho Heo and YoungJoon Yoo},\\n      year={2020},\\n      eprint={2007.00992},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/rexnet.md', 'start_index': 3248}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: RexNet\\n  Paper:\\n    Title: 'ReXNet: Diminishing Representational Bottleneck on Convolutional Neural\\n      Network'\\n    URL: https://paperswithcode.com/paper/rexnet-diminishing-representational\\nModels:\\n- Name: rexnet_100\\n  In Collection: RexNet\\n  Metadata:\\n    FLOPs: 509989377\\n    Parameters: 4800000\\n    File Size: 19417552\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Dropout\\n    - ReLU6\\n    - Residual Connection\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - Linear Warmup With Cosine Annealing\\n    - Nesterov Accelerated Gradient\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x NVIDIA V100 GPUs\\n    ID: rexnet_100\\n    LR: 0.5\\n    Epochs: 400\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    Label Smoothing: 0.1\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/b9843f954b0457af2db4f9dea41a8538f51f5d78/timm/models/rexnet.py#L212\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_100-1b4dddf4.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.86%\\n      Top 5 Accuracy: 93.88%\\n- Name: rexnet_130\\n  In Collection: RexNet\\n  Metadata:\\n    FLOPs: 848364461\\n    Parameters: 7560000\\n    File Size: 30508197\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Dropout\\n    - ReLU6\\n    - Residual Connection\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - Linear Warmup With Cosine Annealing\\n    - Nesterov Accelerated Gradient\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x NVIDIA V100 GPUs\\n    ID: rexnet_130\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/rexnet.md', 'start_index': -1}, page_content=\"Training Techniques:\\n    - Label Smoothing\\n    - Linear Warmup With Cosine Annealing\\n    - Nesterov Accelerated Gradient\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x NVIDIA V100 GPUs\\n    ID: rexnet_130\\n    LR: 0.5\\n    Epochs: 400\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    Label Smoothing: 0.1\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/b9843f954b0457af2db4f9dea41a8538f51f5d78/timm/models/rexnet.py#L218\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_130-590d768e.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.49%\\n      Top 5 Accuracy: 94.67%\\n- Name: rexnet_150\\n  In Collection: RexNet\\n  Metadata:\\n    FLOPs: 1122374469\\n    Parameters: 9730000\\n    File Size: 39227315\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Dropout\\n    - ReLU6\\n    - Residual Connection\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - Linear Warmup With Cosine Annealing\\n    - Nesterov Accelerated Gradient\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x NVIDIA V100 GPUs\\n    ID: rexnet_150\\n    LR: 0.5\\n    Epochs: 400\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    Label Smoothing: 0.1\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/rexnet.md', 'start_index': 6270}, page_content=\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    Label Smoothing: 0.1\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/b9843f954b0457af2db4f9dea41a8538f51f5d78/timm/models/rexnet.py#L224\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_150-bd1a6aa8.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.31%\\n      Top 5 Accuracy: 95.16%\\n- Name: rexnet_200\\n  In Collection: RexNet\\n  Metadata:\\n    FLOPs: 1960224938\\n    Parameters: 16370000\\n    File Size: 65862221\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Dropout\\n    - ReLU6\\n    - Residual Connection\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - Linear Warmup With Cosine Annealing\\n    - Nesterov Accelerated Gradient\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x NVIDIA V100 GPUs\\n    ID: rexnet_200\\n    LR: 0.5\\n    Epochs: 400\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 1.0e-05\\n    Interpolation: bicubic\\n    Label Smoothing: 0.1\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/b9843f954b0457af2db4f9dea41a8538f51f5d78/timm/models/rexnet.py#L230\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rexnet/rexnetv1_200-8c0b7f2d.pth\\n  Results:\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/rexnet.md', 'start_index': 7783}, page_content='Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81.63%\\n      Top 5 Accuracy: 95.67%\\n-->'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': 1}, page_content='RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parameters: depth $d$, initial width $w\\\\_{0} > 0$, and slope $w\\\\_{a} > 0$, and generates a different block width $u\\\\_{j}$ for each block $j < d$. The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure):\\n\\n$$ u\\\\_{j} = w\\\\_{0} + w\\\\_{a}\\\\cdot{j} $$\\n\\nFor **RegNetX** authors have additional restrictions: we set $b = 1$ (the bottleneck ratio), $12 \\\\leq d \\\\leq 28$, and $w\\\\_{m} \\\\geq 2$ (the width multiplier).\\n\\nFor **RegNetY** authors make one change, which is to include [Squeeze-and-Excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model(\\'regnety_002\\', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': 1457}, page_content='```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])\\n```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': 2429}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model('regnety_002', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\\n```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{radosavovic2020designing,\\n      title={Designing Network Design Spaces}, \\n      author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Dollár},\\n      year={2020},\\n      eprint={2003.13678},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': 3710}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: RegNetY\\n  Paper:\\n    Title: Designing Network Design Spaces\\n    URL: https://paperswithcode.com/paper/designing-network-design-spaces\\nModels:\\n- Name: regnety_002\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 255754236\\n    Parameters: 3160000\\n    File Size: 12782926\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_002\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L409\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_002-e68ca334.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 70.28%\\n      Top 5 Accuracy: 89.55%\\n- Name: regnety_004\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 515664568\\n    Parameters: 4340000\\n    File Size: 17542753\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_004\\n    Epochs: 100\\n    Crop Pct: '0.875'\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': -1}, page_content=\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_004\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L415\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_004-0db870e6.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74.02%\\n      Top 5 Accuracy: 91.76%\\n- Name: regnety_006\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 771746928\\n    Parameters: 6060000\\n    File Size: 24394127\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_006\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L421\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': 6866}, page_content=\"Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_006-c67e57ec.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75.27%\\n      Top 5 Accuracy: 92.53%\\n- Name: regnety_008\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 1023448952\\n    Parameters: 6260000\\n    File Size: 25223268\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_008\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L427\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_008-dc900dbe.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76.32%\\n      Top 5 Accuracy: 93.07%\\n- Name: regnety_016\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 2070895094\\n    Parameters: 11200000\\n    File Size: 45115589\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': -1}, page_content=\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_016\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L433\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_016-54367f74.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.87%\\n      Top 5 Accuracy: 93.73%\\n- Name: regnety_032\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 4081118714\\n    Parameters: 19440000\\n    File Size: 78084523\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_032\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': 9809}, page_content=\"Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L439\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/regnety_032_ra-7f2439f9.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82.01%\\n      Top 5 Accuracy: 95.91%\\n- Name: regnety_040\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 5105933432\\n    Parameters: 20650000\\n    File Size: 82913909\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_040\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L445\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth\\n  Results:\\n  - Task: Image Classification\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': 11283}, page_content=\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.23%\\n      Top 5 Accuracy: 94.64%\\n- Name: regnety_064\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 8167730444\\n    Parameters: 30580000\\n    File Size: 122751416\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_064\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L451\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_064-0a48325c.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.73%\\n      Top 5 Accuracy: 94.76%\\n- Name: regnety_080\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 10233621420\\n    Parameters: 39180000\\n    File Size: 157124671\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_080\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': -1}, page_content=\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_080\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L457\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_080-e7f3eb93.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.87%\\n      Top 5 Accuracy: 94.83%\\n- Name: regnety_120\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 15542094856\\n    Parameters: 51820000\\n    File Size: 207743949\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_120\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L463\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': 14394}, page_content=\"Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_120-721ba79a.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.38%\\n      Top 5 Accuracy: 95.12%\\n- Name: regnety_160\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 20450196852\\n    Parameters: 83590000\\n    File Size: 334916722\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_160\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 512\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L469\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_160-d64013cd.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.28%\\n      Top 5 Accuracy: 94.97%\\n- Name: regnety_320\\n  In Collection: RegNetY\\n  Metadata:\\n    FLOPs: 41492618394\\n    Parameters: 145050000\\n    File Size: 580891965\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/regnety.md', 'start_index': -1}, page_content=\"- Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - ReLU\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA V100 GPUs\\n    ID: regnety_320\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 5.0e-05\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/regnet.py#L475\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_320-ba464b29.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.8%\\n      Top 5 Accuracy: 95.25%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/feature_extraction.md', 'start_index': 1}, page_content='Feature Extraction\\n\\nAll of the models in `timm` have consistent mechanisms for obtaining various types of features from the model for tasks besides classification.\\n\\n## Penultimate Layer Features (Pre-Classifier Features)\\n\\nThe features from the penultimate model layer can be obtained in several ways without requiring model surgery (although feel free to do surgery). One must first decide if they want pooled or un-pooled features.\\n\\n### Unpooled\\n\\nThere are three ways to obtain unpooled features.\\n\\nWithout modifying the network, one can call `model.forward_features(input)` on any model instead of the usual `model(input)`. This will bypass the head classifier and global pooling for networks.\\n\\nIf one wants to explicitly modify the network to return unpooled features, they can either create the model without a classifier and pooling, or remove it later. Both paths remove the parameters associated with the classifier from the network.\\n\\n#### forward_features()\\n```python hl_lines=\"3 6\"\\nimport torch\\nimport timm\\nm = timm.create_model(\\'xception41\\', pretrained=True)\\no = m(torch.randn(2, 3, 299, 299))\\nprint(f\\'Original shape: {o.shape}\\')\\no = m.forward_features(torch.randn(2, 3, 299, 299))\\nprint(f\\'Unpooled shape: {o.shape}\\')\\n```\\nOutput:\\n```text\\nOriginal shape: torch.Size([2, 1000])\\nUnpooled shape: torch.Size([2, 2048, 10, 10])\\n```\\n\\n#### Create with no classifier and pooling\\n```python hl_lines=\"3\"\\nimport torch\\nimport timm\\nm = timm.create_model(\\'resnet50\\', pretrained=True, num_classes=0, global_pool=\\'\\')\\no = m(torch.randn(2, 3, 224, 224))\\nprint(f\\'Unpooled shape: {o.shape}\\')\\n```\\nOutput:\\n```text\\nUnpooled shape: torch.Size([2, 2048, 7, 7])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/feature_extraction.md', 'start_index': -1}, page_content='```\\nOutput:\\n```text\\nUnpooled shape: torch.Size([2, 2048, 7, 7])\\n```\\n\\n#### Remove it later\\n```python hl_lines=\"3 6\"\\nimport torch\\nimport timm\\nm = timm.create_model(\\'densenet121\\', pretrained=True)\\no = m(torch.randn(2, 3, 224, 224))\\nprint(f\\'Original shape: {o.shape}\\')\\nm.reset_classifier(0, \\'\\')\\no = m(torch.randn(2, 3, 224, 224))\\nprint(f\\'Unpooled shape: {o.shape}\\')\\n```\\nOutput:\\n```text\\nOriginal shape: torch.Size([2, 1000])\\nUnpooled shape: torch.Size([2, 1024, 7, 7])\\n```\\n\\n### Pooled\\n\\nTo modify the network to return pooled features, one can use `forward_features()` and pool/flatten the result themselves, or modify the network like above but keep pooling intact. \\n\\n#### Create with no classifier\\n```python hl_lines=\"3\"\\nimport torch\\nimport timm\\nm = timm.create_model(\\'resnet50\\', pretrained=True, num_classes=0)\\no = m(torch.randn(2, 3, 224, 224))\\nprint(f\\'Pooled shape: {o.shape}\\')\\n```\\nOutput:\\n```text\\nPooled shape: torch.Size([2, 2048])\\n```\\n\\n#### Remove it later\\n```python hl_lines=\"3 6\"\\nimport torch\\nimport timm\\nm = timm.create_model(\\'ese_vovnet19b_dw\\', pretrained=True)\\no = m(torch.randn(2, 3, 224, 224))\\nprint(f\\'Original shape: {o.shape}\\')\\nm.reset_classifier(0)\\no = m(torch.randn(2, 3, 224, 224))\\nprint(f\\'Pooled shape: {o.shape}\\')'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/feature_extraction.md', 'start_index': 2811}, page_content='```\\nOutput:\\n```text\\nOriginal shape: torch.Size([2, 1000])\\nPooled shape: torch.Size([2, 1024])\\n```\\n\\n\\n## Multi-scale Feature Maps (Feature Pyramid)\\n\\nObject detection, segmentation, keypoint, and a variety of dense pixel tasks require access to feature maps from the backbone network at multiple scales. This is often done by modifying the original classification network. Since each network varies quite a bit in structure, it\\'s not uncommon to see only a few backbones supported in any given obj detection or segmentation library.\\n\\n`timm` allows a consistent interface for creating any of the included models as feature backbones that output feature maps for selected levels. \\n\\nA feature backbone can be created by adding the argument `features_only=True` to any `create_model` call. By default 5 strides will be output from most models (not all have that many), with the first starting at 2 (some start at 1 or 4).\\n\\n### Create a feature map extraction model\\n```python hl_lines=\"3\"\\nimport torch\\nimport timm\\nm = timm.create_model(\\'resnest26d\\', features_only=True, pretrained=True)\\no = m(torch.randn(2, 3, 224, 224))\\nfor x in o:\\n  print(x.shape)\\n```\\nOutput:\\n```text\\ntorch.Size([2, 64, 112, 112])\\ntorch.Size([2, 256, 56, 56])\\ntorch.Size([2, 512, 28, 28])\\ntorch.Size([2, 1024, 14, 14])\\ntorch.Size([2, 2048, 7, 7])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/feature_extraction.md', 'start_index': 4120}, page_content='```\\n\\n### Query the feature information\\n\\nAfter a feature backbone has been created, it can be queried to provide channel or resolution reduction information to the downstream heads without requiring static config or hardcoded constants. The `.feature_info` attribute is a class encapsulating the information about the feature extraction points.\\n\\n```python hl_lines=\"3 4\"\\nimport torch\\nimport timm\\nm = timm.create_model(\\'regnety_032\\', features_only=True, pretrained=True)\\nprint(f\\'Feature channels: {m.feature_info.channels()}\\')\\no = m(torch.randn(2, 3, 224, 224))\\nfor x in o:\\n  print(x.shape)\\n```\\nOutput:\\n```text\\nFeature channels: [32, 72, 216, 576, 1512]\\ntorch.Size([2, 32, 112, 112])\\ntorch.Size([2, 72, 56, 56])\\ntorch.Size([2, 216, 28, 28])\\ntorch.Size([2, 576, 14, 14])\\ntorch.Size([2, 1512, 7, 7])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/feature_extraction.md', 'start_index': 4916}, page_content='```\\n\\n### Select specific feature levels or limit the stride\\n\\nThere are two additional creation arguments impacting the output features. \\n\\n* `out_indices` selects which indices to output\\n* `output_stride` limits the feature output stride of the network (also works in classification mode BTW)\\n\\n`out_indices` is supported by all models, but not all models have the same index to feature stride mapping. Look at the code or check feature_info to compare. The out indices generally correspond to the `C(i+1)th` feature level (a `2^(i+1)` reduction). For most models, index 0 is the stride 2 features, and index 4 is stride 32.\\n\\n`output_stride` is achieved by converting layers to use dilated convolutions. Doing so is not always straightforward, some networks only support `output_stride=32`.\\n\\n```python hl_lines=\"3 4 5\"\\nimport torch\\nimport timm\\nm = timm.create_model(\\'ecaresnet101d\\', features_only=True, output_stride=8, out_indices=(2, 4), pretrained=True)\\nprint(f\\'Feature channels: {m.feature_info.channels()}\\')\\nprint(f\\'Feature reduction: {m.feature_info.reduction()}\\')\\no = m(torch.randn(2, 3, 320, 320))\\nfor x in o:\\n  print(x.shape)\\n```\\nOutput:\\n```text\\nFeature channels: [512, 2048]\\nFeature reduction: [8, 8]\\ntorch.Size([2, 512, 40, 40])\\ntorch.Size([2, 2048, 40, 40])\\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/style.md', 'start_index': 1}, page_content='component-styles\\n\\n## Textbox\\n\\n| name        | type                                 | description                    |\\n| ----------- | ------------------------------------ | ------------------------------ |\\n| `rounded`   | `bool` or `(bool, bool, bool, bool)` | corners of text input          |\\n| `border`    | `bool` or `(bool, bool, bool, bool)` | borders of text input          |\\n| `container` | `bool`                               | show or hide the container box |\\n\\n## Number\\n\\n| name        | type                                 | description                    |\\n| ----------- | ------------------------------------ | ------------------------------ |\\n| `rounded`   | `bool` or `(bool, bool, bool, bool)` | corners of text input          |\\n| `border`    | `bool` or `(bool, bool, bool, bool)` | borders of text input          |\\n| `container` | `bool`                               | show or hide the container box |\\n\\n## Slider\\n\\n| name        | type   | description                    |\\n| ----------- | ------ | ------------------------------ |\\n| `container` | `bool` | show or hide the container box |\\n\\n## Checkbox'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/style.md', 'start_index': 1110}, page_content='## Checkbox\\n\\n| name        | type                                 | description                    |\\n| ----------- | ------------------------------------ | ------------------------------ |\\n| `rounded`   | `bool` or `(bool, bool, bool, bool)` | corners of checkbox            |\\n| `border`    | `bool` or `(bool, bool, bool, bool)` | borders of checkbox            |\\n| `container` | `bool`                               | show or hide the container box |\\n\\n## Checkbox Group\\n\\n| name             | type                                 | description                               |\\n| ---------------- | ------------------------------------ | ----------------------------------------- |\\n| `rounded`        | `bool` or `(bool, bool, bool, bool)` | corners of checkboxes                     |\\n| `container`      | `bool`                               | show or hide the container box            |\\n| `item_container` | `bool`                               | show or hide the checkbox container boxes |\\n\\n## Radio\\n\\n| name             | type   | description                            |\\n| ---------------- | ------ | -------------------------------------- |\\n| `container`      | `bool` | show or hide the container box         |\\n| `item_container` | `bool` | show or hide the radio container boxes |\\n\\n## Dropdown'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/style.md', 'start_index': 2399}, page_content='## Dropdown\\n\\n| name        | type                                 | description                    |\\n| ----------- | ------------------------------------ | ------------------------------ |\\n| `rounded`   | `bool` or `(bool, bool, bool, bool)` | corners of input               |\\n| `border`    | `bool` or `(bool, bool, bool, bool)` | borders of input               |\\n| `container` | `bool`                               | show or hide the container box |\\n\\n## Image\\n\\n| name      | type                                 | description         |\\n| --------- | ------------------------------------ | ------------------- |\\n| `rounded` | `bool` or `(bool, bool, bool, bool)` | corners of main box |\\n\\n## Video\\n\\n| name      | type                                 | description         |\\n| --------- | ------------------------------------ | ------------------- |\\n| `rounded` | `bool` or `(bool, bool, bool, bool)` | corners of main box |\\n\\n## Audio\\n\\n| name      | type                                 | description         |\\n| --------- | ------------------------------------ | ------------------- |\\n| `rounded` | `bool` or `(bool, bool, bool, bool)` | corners of main box |\\n\\n## File'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/style.md', 'start_index': 3561}, page_content='## File\\n\\n| name      | type                                 | description         |\\n| --------- | ------------------------------------ | ------------------- |\\n| `rounded` | `bool` or `(bool, bool, bool, bool)` | corners of main box |\\n\\n## Dataframe\\n\\n| name      | type                                 | description         |\\n| --------- | ------------------------------------ | ------------------- |\\n| `rounded` | `bool` or `(bool, bool, bool, bool)` | corners of main box |\\n\\n## Timeseries\\n\\n| name      | type                                 | description         |\\n| --------- | ------------------------------------ | ------------------- |\\n| `rounded` | `bool` or `(bool, bool, bool, bool)` | corners of main box |\\n\\n## Label\\n\\n| name        | type   | description                    |\\n| ----------- | ------ | ------------------------------ |\\n| `container` | `bool` | show or hide the container box |\\n\\n## HighlightedText'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/style.md', 'start_index': 4462}, page_content='## HighlightedText\\n\\n| name        | type                                 | description                    |\\n| ----------- | ------------------------------------ | ------------------------------ |\\n| `rounded`   | `bool` or `(bool, bool, bool, bool)` | corners of labels              |\\n| `color_map` | `Dict[str, str]`                     | color map of labels and colors |\\n| `container` | `bool`                               | show or hide the container box |\\n\\n## JSON\\n\\n| name        | type   | description                    |\\n| ----------- | ------ | ------------------------------ |\\n| `container` | `bool` | show or hide the container box |\\n\\n## HTML\\n\\nNothing\\n\\n## Gallery\\n\\n| name        | type                                      | description                         |\\n| ----------- | ----------------------------------------- | ----------------------------------- |\\n| `rounded`   | `bool` or `(bool, bool, bool, bool)`      | corners of images                   |\\n| `grid`      | `int` or `(int, int, int, int, int, int)` | grid for images                     |\\n| `height`    | `\"auto\"`                                  | height of gallery (auto or default) |\\n| `container` | `bool`                                    | show or hide the container box      |\\n\\n## Chatbot'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/style.md', 'start_index': 5726}, page_content='## Chatbot\\n\\n| name        | type                                 | description                                      |\\n| ----------- | ------------------------------------ | ------------------------------------------------ |\\n| `rounded`   | `bool` or `(bool, bool, bool, bool)` | corners of chat bubbles                          |\\n| `color_map` | `Dict[str, str]`                     | color map of user and bot color for chat bubbles |\\n\\n## Model3D\\n\\n| name      | type                                 | description         |\\n| --------- | ------------------------------------ | ------------------- |\\n| `rounded` | `bool` or `(bool, bool, bool, bool)` | corners of main box |\\n\\n## Plot\\n\\nNothing (yet)\\n\\n## Markdown\\n\\nNothing\\n\\n## Button\\n\\n| name         | type                                 | description                             |\\n| ------------ | ------------------------------------ | --------------------------------------- |\\n| `rounded`    | `bool` or `(bool, bool, bool, bool)` | corners of button                       |\\n| `border`     | `bool` or `(bool, bool, bool, bool)` | borders of button                       |\\n| `full_width` | `bool`                               | whether button expand to fill container |\\n\\n## Dataset\\n\\nNothing\\n\\n## Variable\\n\\nNothing'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/big_bird.md', 'start_index': 0}, page_content='!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# BigBird\\n\\n## Overview\\n\\nThe BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by\\nZaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon,\\nSantiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others. BigBird, is a sparse-attention\\nbased transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparse\\nattention, BigBird also applies global attention as well as random attention to the input sequence. Theoretically, it\\nhas been shown that applying sparse, global, and random attention approximates full attention, while being\\ncomputationally much more efficient for longer sequences. As a consequence of the capability to handle longer context,\\nBigBird has shown improved performance on various long document NLP tasks, such as question answering and\\nsummarization, compared to BERT or RoBERTa.\\n\\nThe abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/big_bird.md', 'start_index': 1737}, page_content=\"The abstract from the paper is the following:\\n\\n*Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP.\\nUnfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence\\nlength due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that\\nreduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and\\nis Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our\\ntheoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire\\nsequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to\\n8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context,\\nBigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also\\npropose novel applications to genomics data.*\\n\\nThis model was contributed by [vasudevgupta](https://huggingface.co/vasudevgupta). The original code can be found\\n[here](https://github.com/google-research/bigbird).\\n\\n## Usage tips\\n\\n- For an in-detail explanation on how BigBird's attention works, see [this blog post](https://huggingface.co/blog/big-bird).\\n- BigBird comes with 2 implementations: **original_full** & **block_sparse**. For the sequence length < 1024, using\\n  **original_full** is advised as there is no benefit in using **block_sparse** attention.\\n- The code currently uses window size of 3 blocks and 2 global blocks.\\n- Sequence length must be divisible by block size.\\n- Current implementation supports only **ITC**.\\n- Current implementation doesn't support **num_random_blocks = 0**\\n- BigBird is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\\n  the left.\\n\\n\\n## Resources\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/big_bird.md', 'start_index': 3771}, page_content='## Resources\\n\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\\n- [Multiple choice task guide](../tasks/multiple_choice)\\n\\n## BigBirdConfig\\n\\n[[autodoc]] BigBirdConfig\\n\\n## BigBirdTokenizer\\n\\n[[autodoc]] BigBirdTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## BigBirdTokenizerFast\\n\\n[[autodoc]] BigBirdTokenizerFast\\n\\n## BigBird specific outputs\\n\\n[[autodoc]] models.big_bird.modeling_big_bird.BigBirdForPreTrainingOutput\\n\\n<frameworkcontent>\\n<pt>\\n\\n## BigBirdModel\\n\\n[[autodoc]] BigBirdModel\\n    - forward\\n\\n## BigBirdForPreTraining\\n\\n[[autodoc]] BigBirdForPreTraining\\n    - forward\\n\\n## BigBirdForCausalLM\\n\\n[[autodoc]] BigBirdForCausalLM\\n    - forward\\n\\n## BigBirdForMaskedLM\\n\\n[[autodoc]] BigBirdForMaskedLM\\n    - forward\\n\\n## BigBirdForSequenceClassification\\n\\n[[autodoc]] BigBirdForSequenceClassification\\n    - forward\\n\\n## BigBirdForMultipleChoice\\n\\n[[autodoc]] BigBirdForMultipleChoice\\n    - forward\\n\\n## BigBirdForTokenClassification\\n\\n[[autodoc]] BigBirdForTokenClassification\\n    - forward\\n\\n## BigBirdForQuestionAnswering\\n\\n[[autodoc]] BigBirdForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<jax>\\n\\n## FlaxBigBirdModel\\n\\n[[autodoc]] FlaxBigBirdModel\\n    - __call__\\n\\n## FlaxBigBirdForPreTraining'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/big_bird.md', 'start_index': -1}, page_content='</pt>\\n<jax>\\n\\n## FlaxBigBirdModel\\n\\n[[autodoc]] FlaxBigBirdModel\\n    - __call__\\n\\n## FlaxBigBirdForPreTraining\\n\\n[[autodoc]] FlaxBigBirdForPreTraining\\n    - __call__\\n\\n## FlaxBigBirdForCausalLM\\n\\n[[autodoc]] FlaxBigBirdForCausalLM\\n    - __call__\\n\\n## FlaxBigBirdForMaskedLM\\n\\n[[autodoc]] FlaxBigBirdForMaskedLM\\n    - __call__\\n\\n## FlaxBigBirdForSequenceClassification\\n\\n[[autodoc]] FlaxBigBirdForSequenceClassification\\n    - __call__\\n\\n## FlaxBigBirdForMultipleChoice\\n\\n[[autodoc]] FlaxBigBirdForMultipleChoice\\n    - __call__\\n\\n## FlaxBigBirdForTokenClassification\\n\\n[[autodoc]] FlaxBigBirdForTokenClassification\\n    - __call__\\n\\n## FlaxBigBirdForQuestionAnswering\\n\\n[[autodoc]] FlaxBigBirdForQuestionAnswering\\n    - __call__\\n\\n</jax>\\n</frameworkcontent>'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-tokens.md', 'start_index': 1}, page_content='User access tokens\\n\\n## What are User Access Tokens?\\n\\nUser Access Tokens are the preferred way to authenticate an application or notebook to Hugging Face services. You can manage your access tokens in your [settings](https://huggingface.co/settings/tokens).\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/User-Access-Token.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/User-Access-Token-dark.png\"/>\\n</div>\\n\\nAccess tokens allow applications and notebooks to perform specific actions specified by the scope of the roles shown in the following:\\n\\n- `read`: tokens with this role can only be used to provide read access to repositories you could read. That includes public and private repositories that you, or an organization you\\'re a member of, own. Use this role if you only need to read content from the Hugging Face Hub (e.g. when downloading private models or doing inference).\\n\\n- `write`: tokens with this role additionally grant write access to the repositories you have write access to. Use this token if you need to create or push content to a repository (e.g., when training a model or modifying a model card).\\n\\nNote that Organization API Tokens have been deprecated: \\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/API-token.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/API-token_dark.png\"/>\\n</div>'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-tokens.md', 'start_index': 1678}, page_content='If you are a member of an organization with read/write/admin role, then your User Access Tokens will be able to read/write the resources according to the token permission (read/write) and organization membership (read/write/admin).\\n\\n## How to manage User Access Tokens?\\n\\nTo create an access token, go to your settings, then click on the [Access Tokens tab](https://huggingface.co/settings/tokens). Click on the **New token** button to create a new User Access Token.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new-token.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/new-token-dark.png\"/>\\n</div>\\n\\nSelect a role and a name for your token and voilà - you\\'re ready to go!\\n\\nYou can delete and refresh User Access Tokens by clicking on the **Manage** button.\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" width=\"350\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/delete-token.png\"/>\\n<img class=\"hidden dark:block\" width=\"350\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/delete-token-dark.png\"/>\\n</div>\\n\\n## How to use User Access Tokens?\\n\\nThere are plenty of ways to use a User Access Token to access the Hugging Face Hub, granting you the flexibility you need to build awesome apps on top of it.'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-tokens.md', 'start_index': -1}, page_content='## How to use User Access Tokens?\\n\\nThere are plenty of ways to use a User Access Token to access the Hugging Face Hub, granting you the flexibility you need to build awesome apps on top of it.\\n\\nUser Access Tokens can be:\\n- used **in place of a password** to access the Hugging Face Hub with git or with basic authentication.\\n- passed as a **bearer token** when calling the [Inference API](https://huggingface.co/inference-api).\\n- used in the Hugging Face Python libraries, such as `transformers` or `datasets`:\\n\\n```python\\nfrom transformers import AutoModel\\n\\naccess_token = \"hf_...\"\\n\\nmodel = AutoModel.from_pretrained(\"private/model\", token=access_token)'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-tokens.md', 'start_index': 3622}, page_content='```\\n\\n<Tip warning={true}>\\nTry not to leak your token! Though you can always rotate it, anyone will be able to read or write your private repos in the meantime which is 💩\\n</Tip>\\n\\n### Best practices\\n\\nWe recommend you create one access token per app or usage. For instance, you could have a separate token for:\\n * A local machine.\\n * A Colab notebook.\\n * An awesome custom inference server. \\n \\n This way, you can invalidate one token without impacting your other usages.\\n\\nWe also recommend only giving the appropriate role to each token you create. If you only need read access (e.g., loading a dataset with the `datasets` library or retrieving the weights of a model), only give your access token the `read` role.'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/wide-resnet.md', 'start_index': 1}, page_content='Wide ResNet\\n\\n**Wide Residual Networks** are a variant on [ResNets](https://paperswithcode.com/method/resnet) where we decrease depth and increase the width of residual networks. This is achieved through the use of [wide residual blocks](https://paperswithcode.com/method/wide-residual-block).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model(\\'wide_resnet101_2\\', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/wide-resnet.md', 'start_index': 1199}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]\\n```\\n\\nReplace the model name with the variant you want to use, e.g. `wide_resnet101_2`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model(\\'wide_resnet101_2\\', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/wide-resnet.md', 'start_index': 2558}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@article{DBLP:journals/corr/ZagoruykoK16,\\n  author    = {Sergey Zagoruyko and\\n               Nikos Komodakis},\\n  title     = {Wide Residual Networks},\\n  journal   = {CoRR},\\n  volume    = {abs/1605.07146},\\n  year      = {2016},\\n  url       = {http://arxiv.org/abs/1605.07146},\\n  archivePrefix = {arXiv},\\n  eprint    = {1605.07146},\\n  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},\\n  biburl    = {https://dblp.org/rec/journals/corr/ZagoruykoK16.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/wide-resnet.md', 'start_index': 3470}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: Wide ResNet\\n  Paper:\\n    Title: Wide Residual Networks\\n    URL: https://paperswithcode.com/paper/wide-residual-networks\\nModels:\\n- Name: wide_resnet101_2\\n  In Collection: Wide ResNet\\n  Metadata:\\n    FLOPs: 29304929280\\n    Parameters: 126890000\\n    File Size: 254695146\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Connection\\n    - Softmax\\n    - Wide Residual Block\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: wide_resnet101_2\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/5f9aff395c224492e9e44248b15f44b5cc095d9c/timm/models/resnet.py#L802\\n  Weights: https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.85%\\n      Top 5 Accuracy: 94.28%\\n- Name: wide_resnet50_2\\n  In Collection: Wide ResNet\\n  Metadata:\\n    FLOPs: 14688058368\\n    Parameters: 68880000\\n    File Size: 275853271\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Connection\\n    - Softmax\\n    - Wide Residual Block\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: wide_resnet50_2\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation: bicubic\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/wide-resnet.md', 'start_index': -1}, page_content=\"- Softmax\\n    - Wide Residual Block\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: wide_resnet50_2\\n    Crop Pct: '0.875'\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/5f9aff395c224492e9e44248b15f44b5cc095d9c/timm/models/resnet.py#L790\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81.45%\\n      Top 5 Accuracy: 95.52%\\n-->\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/persimmon.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Persimmon\\n\\n## Overview\\n\\nThe Persimmon model was created by [ADEPT](https://www.adept.ai/blog/persimmon-8b), and authored by Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.\\n\\nThe authors introduced Persimmon-8B, a decoder model based on the classic transformers architecture, with query and key normalization. Persimmon-8B is a fully permissively-licensed model with approximately 8 billion parameters, released under the Apache license.  Some of the key attributes of Persimmon-8B are long context size (16K), performance, and capabilities for multimodal extensions.\\n\\nThe authors showcase their approach to model evaluation, focusing on practical text generation, mirroring how users interact with language models. The work also includes a comparative analysis, pitting Persimmon-8B against other prominent models (MPT 7B Instruct and Llama 2 Base 7B 1-Shot), across various evaluation tasks. The results demonstrate Persimmon-8B\\'s competitive performance, even with limited training data.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/persimmon.md', 'start_index': 1813}, page_content='In terms of model details, the work outlines the architecture and training methodology of Persimmon-8B, providing insights into its design choices, sequence length, and dataset composition. The authors present a fast inference code that outperforms traditional implementations through operator fusion and CUDA graph utilization while maintaining code coherence. They express their anticipation of how the community will leverage this contribution to drive innovation, hinting at further upcoming releases as part of an ongoing series of developments.\\n\\nThis model was contributed by [ArthurZ](https://huggingface.co/ArthurZ).\\nThe original code can be found [here](https://github.com/persimmon-ai-labs/adept-inference).\\n\\n## Usage tips\\n\\n<Tip warning={true}>\\n\\nThe `Persimmon` models were trained using `bfloat16`, but the original inference uses `float16` The checkpoints uploaded on the hub use `torch_dtype = \\'float16\\'` which will be\\nused by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`. \\n\\nThe `dtype` of the online weights is mostly irrelevant, unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online) then it will be cast to the default `dtype` of `torch` (becomes `torch.float32`). Users should specify the `torch_dtype` they want, and if they don\\'t it will be `torch.float32`.\\n\\nFinetuning the model in `float16` is not recommended and known to produce `nan`, as such the model should be fine-tuned in `bfloat16`.\\n\\n</Tip>\\n\\n\\nTips:\\n\\n- To convert the model, you need to clone the original repository using `git clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/persimmon.md', 'start_index': -1}, page_content='Tips:\\n\\n- To convert the model, you need to clone the original repository using `git clone https://github.com/persimmon-ai-labs/adept-inference`, then get the checkpoints:\\n\\n```bash\\ngit clone https://github.com/persimmon-ai-labs/adept-inference\\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar\\ntar -xvf 8b_base_model_release.tar\\npython src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py  --input_dir /path/to/downloaded/persimmon/weights/ --output_dir /output/path \\\\\\n    --pt_model_path /path/to/8b_chat_model_release/iter_0001251/mp_rank_00/model_optim_rng.pt\\n    --ada_lib_path /path/to/adept-inference'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/persimmon.md', 'start_index': 4182}, page_content='```\\n\\nFor the chat model:\\n```bash\\nwget https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_chat_model_release.tar\\ntar -xvf 8b_base_model_release.tar\\n```\\n\\nThereafter, models can be loaded via:\\n\\n```py\\nfrom transformers import PersimmonForCausalLM, PersimmonTokenizer\\n\\nmodel = PersimmonForCausalLM.from_pretrained(\"/output/path\")\\ntokenizer = PersimmonTokenizer.from_pretrained(\"/output/path\")\\n```\\n\\n\\n- Perismmon uses a `sentencepiece` based tokenizer, with a `Unigram` model. It supports bytefallback, which is only available in `tokenizers==0.14.0` for the fast tokenizer.\\nThe `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece. The `chat` template will be updated with the templating functions in a follow up PR!\\n\\n- The authors suggest to use the following prompt format for the chat mode: `f\"human: {prompt}\\\\n\\\\nadept:\"`\\n\\n\\n## PersimmonConfig\\n\\n[[autodoc]] PersimmonConfig\\n\\n## PersimmonModel\\n\\n[[autodoc]] PersimmonModel\\n    - forward\\n\\n## PersimmonForCausalLM\\n\\n[[autodoc]] PersimmonForCausalLM\\n    - forward\\n\\n## PersimmonForSequenceClassification\\n\\n[[autodoc]] PersimmonForSequenceClassification\\n    - forward'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/blocks_webcam/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: blocks_webcam\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\n\\nimport gradio as gr\\n\\n\\ndef snap(image):\\n    return np.flipud(image)\\n\\n\\ndemo = gr.Interface(snap, gr.Image(sources=[\"webcam\"]), \"image\")\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/on_listener_live/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: on_listener_live\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        num1 = gr.Slider(1, 10)\\n        num2 = gr.Slider(1, 10)\\n        num3 = gr.Slider(1, 10)\\n    output = gr.Number(label=\"Sum\")\\n\\n    @gr.on(inputs=[num1, num2, num3], outputs=output)\\n    def sum(a, b, c):\\n        return a + b + c\\n\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n\\n```'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/schedulers/score_sde_ve.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# ScoreSdeVeScheduler\\n\\n`ScoreSdeVeScheduler` is a variance exploding stochastic differential equation (SDE) scheduler. It was introduced in the [Score-Based Generative Modeling through Stochastic Differential Equations](https://huggingface.co/papers/2011.13456) paper by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole.\\n\\nThe abstract from the paper is:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/schedulers/score_sde_ve.md', 'start_index': 955}, page_content='The abstract from the paper is:\\n\\n*Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.*\\n\\n## ScoreSdeVeScheduler\\n[[autodoc]] ScoreSdeVeScheduler\\n\\n## SdeVeOutput\\n[[autodoc]] schedulers.scheduling_sde_ve.SdeVeOutput'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 1}, page_content='What to do when you get an error[[what-to-do-when-you-get-an-error]]\\n\\n<CourseFloatingBanner chapter={8}\\n  classNames=\"absolute z-10 right-0 top-0\"\\n  notebooks={[\\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb\"},\\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb\"},\\n]} />\\n\\nIn this section we\\'ll look at some common errors that can occur when you\\'re trying to generate predictions from your freshly tuned Transformer model. This will prepare you for [section 4](/course/chapter8/section4), where we\\'ll explore how to debug the training phase itself.\\n\\n<Youtube id=\"DQ-CpJn6Rc4\"/>\\n\\nWe\\'ve prepared a [template model repository](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28) for this section, and if you want to run the code in this chapter you\\'ll first need to copy the model to your account on the [Hugging Face Hub](https://huggingface.co). To do so, first log in by running either the following in a Jupyter notebook:\\n\\n```python\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()\\n```\\n\\nor the following in your favorite terminal:\\n\\n```bash\\nhuggingface-cli login'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': -1}, page_content='```\\n\\nor the following in your favorite terminal:\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nThis will prompt you to enter your username and password, and will save a token under *~/.cache/huggingface/*. Once you\\'ve logged in, you can copy the template repository with the following function:\\n\\n```python\\nfrom distutils.dir_util import copy_tree\\nfrom huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name\\n\\n\\ndef copy_repository_template():\\n    # Clone the repo and extract the local path\\n    template_repo_id = \"lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\"\\n    commit_hash = \"be3eaffc28669d7932492681cd5f3e8905e358b4\"\\n    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)\\n    # Create an empty repo on the Hub\\n    model_name = template_repo_id.split(\"/\")[1]\\n    create_repo(model_name, exist_ok=True)\\n    # Clone the empty repo\\n    new_repo_id = get_full_repo_name(model_name)\\n    new_repo_dir = model_name\\n    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)\\n    # Copy files\\n    copy_tree(template_repo_dir, new_repo_dir)\\n    # Push to Hub\\n    repo.push_to_hub()'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 2363}, page_content='```\\n\\nNow when you call `copy_repository_template()`, it will create a copy of the template repository under your account.\\n\\n## Debugging the pipeline from 🤗 Transformers[[debugging-the-pipeline-from-transformers]]\\n\\nTo kick off our journey into the wonderful world of debugging Transformer models, consider the following scenario: you\\'re working with a colleague on a question answering project to help the customers of an e-commerce website find answers about consumer products. Your colleague shoots you a message like:\\n\\n> G\\'day! I just ran an experiment using the techniques in [Chapter 7](/course/chapter7/7) of the Hugging Face course and got some great results on SQuAD! I think we can use this model as a starting point for our project. The model ID on the Hub is \"lewtun/distillbert-base-uncased-finetuned-squad-d5716d28\". Feel free to test it out :)\\n\\nand the first thing you think of is to load the model using the `pipeline` from 🤗 Transformers:\\n\\n```python\\nfrom transformers import pipeline\\n\\nmodel_checkpoint = get_full_repo_name(\"distillbert-base-uncased-finetuned-squad-d5716d28\")\\nreader = pipeline(\"question-answering\", model=model_checkpoint)\\n```\\n\\n```python out\\n\"\"\"\\nOSError: Can\\'t load config for \\'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28\\'. Make sure that:\\n\\n- \\'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28\\' is a correct model identifier listed on \\'https://huggingface.co/models\\'\\n\\n- or \\'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28\\' is the correct path to a directory containing a config.json file\\n\"\"\"'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 3916}, page_content='```\\n\\nOh no, something seems to have gone wrong! If you\\'re new to programming, these kind of errors can seem a bit cryptic at first (what even is an `OSError`?!). The error displayed here is just the last part of a much larger error report called a _Python traceback_ (aka stack trace). For example, if you\\'re running this code on Google Colab, you should see something like the following screenshot:\\n\\n<div class=\"flex justify-center\">\\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png\" alt=\"A Python traceback.\" width=\"100%\"/>\\n</div>\\n\\nThere\\'s a lot of information contained in these reports, so let\\'s walk through the key parts together. The first thing to note is that tracebacks should be read _from bottom to top_. This might sound weird if you\\'re used to reading English text from top to bottom, but it reflects the fact that the traceback shows the sequence of function calls that the `pipeline` makes when downloading the model and tokenizer. (Check out [Chapter 2](/course/chapter2) for more details on how the `pipeline` works under the hood.)\\n\\n<Tip>\\n\\n🚨 See that blue box around \"6 frames\" in the traceback from Google Colab? That\\'s a special feature of Colab, which  compresses the traceback into \"frames.\" If you can\\'t seem to find the source of an error, make sure you expand the full traceback by clicking on those two little arrows.\\n\\n</Tip>\\n\\nThis means that the last line of the traceback indicates the last error message and gives the name of the exception that was raised. In this case, the exception type is `OSError`, which indicates a system-related error. If we read the accompanying error message, we can see that there seems to be a problem with the model\\'s *config.json* file, and we\\'re given two suggestions to fix it:\\n\\n```python out\\n\"\"\"\\nMake sure that:'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 5730}, page_content='```python out\\n\"\"\"\\nMake sure that:\\n\\n- \\'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28\\' is a correct model identifier listed on \\'https://huggingface.co/models\\'\\n\\n- or \\'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28\\' is the correct path to a directory containing a config.json file\\n\"\"\"'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 6032}, page_content='```\\n\\n<Tip>\\n\\n💡 If you encounter an error message that is difficult to understand, just copy and paste the message into the Google or [Stack Overflow](https://stackoverflow.com/) search bar (yes, really!). There\\'s a good chance that you\\'re not the first person to encounter the error, and this is a good way to find solutions that others in the community have posted. For example, searching for `OSError: Can\\'t load config for` on Stack Overflow gives several [hits](https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+) that could be used as a starting point for solving the problem.\\n\\n</Tip>\\n\\nThe first suggestion is asking us to check whether the model ID is actually correct, so the first order of business is to copy the identifier and paste it into the Hub\\'s search bar:\\n\\n<div class=\"flex justify-center\">\\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png\" alt=\"The wrong model name.\" width=\"100%\"/>\\n</div>\\n\\nHmm, it indeed looks like our colleague\\'s model is not on the Hub... aha, but there\\'s a typo in the name of the model! DistilBERT only has one \"l\" in its name, so let\\'s fix that and look for \"lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\" instead:\\n\\n<div class=\"flex justify-center\">\\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png\" alt=\"The right model name.\" width=\"100%\"/>\\n</div>\\n\\nOkay, this got a hit. Now let\\'s try to download the model again with the correct model ID:'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': -1}, page_content='Okay, this got a hit. Now let\\'s try to download the model again with the correct model ID:\\n\\n```python\\nmodel_checkpoint = get_full_repo_name(\"distilbert-base-uncased-finetuned-squad-d5716d28\")\\nreader = pipeline(\"question-answering\", model=model_checkpoint)'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 7763}, page_content='```\\n\\n```python out\\n\"\"\"\\nOSError: Can\\'t load config for \\'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\\'. Make sure that:\\n\\n- \\'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\\' is a correct model identifier listed on \\'https://huggingface.co/models\\'\\n\\n- or \\'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\\' is the correct path to a directory containing a config.json file\\n\"\"\"\\n```\\n\\nArgh, foiled again -- welcome to the daily life of a machine learning engineer! Since we\\'ve fixed the model ID, the problem must lie in the repository itself. A quick way to access the contents of a repository on the 🤗 Hub is via the `list_repo_files()` function of the `huggingface_hub` library:\\n\\n```python\\nfrom huggingface_hub import list_repo_files\\n\\nlist_repo_files(repo_id=model_checkpoint)\\n```\\n\\n```python out\\n[\\'.gitattributes\\', \\'README.md\\', \\'pytorch_model.bin\\', \\'special_tokens_map.json\\', \\'tokenizer_config.json\\', \\'training_args.bin\\', \\'vocab.txt\\']'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 8716}, page_content='```\\n\\nInteresting -- there doesn\\'t seem to be a *config.json* file in the repository! No wonder our `pipeline` couldn\\'t load the model; our colleague must have forgotten to push this file to the Hub after they fine-tuned it. In this case, the problem seems pretty straightforward to fix: we could ask them to add the file, or, since we can see from the model ID that the pretrained model used was [`distilbert-base-uncased`](https://huggingface.co/distilbert-base-uncased), we can download the config for this model and push it to our repo to see if that resolves the problem. Let\\'s try that. Using the techniques we learned in [Chapter 2](/course/chapter2), we can download the model\\'s configuration with the `AutoConfig` class:\\n\\n```python\\nfrom transformers import AutoConfig\\n\\npretrained_checkpoint = \"distilbert-base-uncased\"\\nconfig = AutoConfig.from_pretrained(pretrained_checkpoint)\\n```\\n\\n<Tip warning={true}>\\n\\n🚨 The approach we\\'re taking here is not foolproof, since our colleague may have tweaked the configuration of `distilbert-base-uncased` before fine-tuning the model. In real life, we\\'d want to check with them first, but for the purposes of this section we\\'ll assume they used the default configuration.\\n\\n</Tip>\\n\\nWe can then push this to our model repository with the configuration\\'s `push_to_hub()` function:\\n\\n```python\\nconfig.push_to_hub(model_checkpoint, commit_message=\"Add config.json\")'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 10119}, page_content='```\\n\\nNow we can test if this worked by loading the model from the latest commit on the `main` branch:\\n\\n```python\\nreader = pipeline(\"question-answering\", model=model_checkpoint, revision=\"main\")\\n\\ncontext = r\"\"\"\\nExtractive Question Answering is the task of extracting an answer from a text\\ngiven a question. An example of a question answering dataset is the SQuAD\\ndataset, which is entirely based on that task. If you would like to fine-tune a\\nmodel on a SQuAD task, you may leverage the\\nexamples/pytorch/question-answering/run_squad.py script.\\n\\n🤗 Transformers is interoperable with the PyTorch, TensorFlow, and JAX\\nframeworks, so you can use your favourite tools for a wide variety of tasks!\\n\"\"\"\\n\\nquestion = \"What is extractive question answering?\"\\nreader(question=question, context=context)\\n```\\n\\n```python out\\n{\\'score\\': 0.38669535517692566,\\n \\'start\\': 34,\\n \\'end\\': 95,\\n \\'answer\\': \\'the task of extracting an answer from a text given a question\\'}'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 11062}, page_content='```\\n\\nWoohoo, it worked! Let\\'s recap what you\\'ve just learned:\\n\\n- The error messages in Python are known as _tracebacks_ and are read from bottom to top. The last line of the error message usually contains the information you need to locate the source of the problem.\\n- If the last line does not contain sufficient information, work your way up the traceback and see if you can identify where in the source code the error occurs.\\n- If none of the error messages can help you debug the problem, try searching online for a solution to a similar issue.\\n- The `huggingface_hub` \\n// 🤗 Hub?\\nlibrary provides a suite of tools that you can use to interact with and debug repositories on the Hub.\\n\\nNow that you know how to debug a pipeline, let\\'s take a look at a trickier example in the forward pass of the model itself.\\n\\n## Debugging the forward pass of your model[[debugging-the-forward-pass-of-your-model]]\\n\\nAlthough the `pipeline` is great for most applications where you need to quickly generate predictions, sometimes you\\'ll need to access the model\\'s logits (say, if you have some custom post-processing that you\\'d like to apply). To see what can go wrong in this case, let\\'s first grab the model and tokenizer from our `pipeline`:\\n\\n```python\\ntokenizer = reader.tokenizer\\nmodel = reader.model\\n```\\n\\nNext we need a question, so let\\'s see if our favorite frameworks are supported:\\n\\n```python\\nquestion = \"Which frameworks can I use?\"'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': -1}, page_content='```\\n\\nNext we need a question, so let\\'s see if our favorite frameworks are supported:\\n\\n```python\\nquestion = \"Which frameworks can I use?\"\\n```\\n\\nAs we saw in [Chapter 7](/course/chapter7), the usual steps we need to take are tokenizing the inputs, extracting the logits of the start and end tokens, and then decoding the answer span:\\n\\n```python\\nimport torch\\n\\ninputs = tokenizer(question, context, add_special_tokens=True)\\ninput_ids = inputs[\"input_ids\"][0]\\noutputs = model(**inputs)\\nanswer_start_scores = outputs.start_logits\\nanswer_end_scores = outputs.end_logits\\n# Get the most likely beginning of answer with the argmax of the score\\nanswer_start = torch.argmax(answer_start_scores)\\n# Get the most likely end of answer with the argmax of the score\\nanswer_end = torch.argmax(answer_end_scores) + 1\\nanswer = tokenizer.convert_tokens_to_string(\\n    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\\n)\\nprint(f\"Question: {question}\")\\nprint(f\"Answer: {answer}\")'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 13326}, page_content='```\\n\\n```python out\\n\"\"\"\\n---------------------------------------------------------------------------\\nAttributeError                            Traceback (most recent call last)\\n/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>\\n      1 inputs = tokenizer(question, text, add_special_tokens=True)\\n      2 input_ids = inputs[\"input_ids\"]\\n----> 3 outputs = model(**inputs)\\n      4 answer_start_scores = outputs.start_logits\\n      5 answer_end_scores = outputs.end_logits\\n\\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\\n   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\\n   1050                 or _global_forward_hooks or _global_forward_pre_hooks):\\n-> 1051             return forward_call(*input, **kwargs)\\n   1052         # Do not call functions when jit is used\\n   1053         full_backward_hooks, non_full_backward_hooks = [], []'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 14343}, page_content='~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\\n    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n    724\\n--> 725         distilbert_output = self.distilbert(\\n    726             input_ids=input_ids,\\n    727             attention_mask=attention_mask,\\n\\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\\n   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\\n   1050                 or _global_forward_hooks or _global_forward_pre_hooks):\\n-> 1051             return forward_call(*input, **kwargs)\\n   1052         # Do not call functions when jit is used\\n   1053         full_backward_hooks, non_full_backward_hooks = [], []\\n\\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\\n    471             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\\n    472         elif input_ids is not None:\\n--> 473             input_shape = input_ids.size()\\n    474         elif inputs_embeds is not None:\\n    475             input_shape = inputs_embeds.size()[:-1]\\n\\nAttributeError: \\'list\\' object has no attribute \\'size\\'\\n\"\"\"'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 15984}, page_content='```\\n\\nOh dear, it looks like we have a bug in our code! But we\\'re not afraid of a little debugging. You can use the Python debugger in a notebook:\\n\\n<Youtube id=\"rSPyvPw0p9k\"/>\\n\\nor in a terminal:\\n\\n<Youtube id=\"5PkZ4rbHL6c\"/>\\n\\nHere, reading the error message tells us that `\\'list\\' object has no attribute \\'size\\'`, and we can see a `-->` arrow pointing to the line where the problem was raised in `model(**inputs)`.You can debug this interactively using the Python debugger, but for now we\\'ll simply print out a slice of `inputs` to see what we have:\\n\\n```python\\ninputs[\"input_ids\"][:5]\\n```\\n\\n```python out\\n[101, 2029, 7705, 2015, 2064]\\n```\\n\\nThis certainly looks like an ordinary Python `list`, but let\\'s double-check the type:\\n\\n```python\\ntype(inputs[\"input_ids\"])\\n```\\n\\n```python out\\nlist\\n```\\n\\nYep, that\\'s a Python `list` for sure. So what went wrong? Recall from [Chapter 2](/course/chapter2) that the `AutoModelForXxx` classes in 🤗 Transformers operate on _tensors_ (either in PyTorch or TensorFlow), and a common operation is to extract the dimensions of a tensor using `Tensor.size()` in, say, PyTorch. Let\\'s take another look at the traceback, to see which line triggered the exception:'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 17171}, page_content='```\\n~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\\n    471             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\\n    472         elif input_ids is not None:\\n--> 473             input_shape = input_ids.size()\\n    474         elif inputs_embeds is not None:\\n    475             input_shape = inputs_embeds.size()[:-1]\\n\\nAttributeError: \\'list\\' object has no attribute \\'size\\''),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 17778}, page_content='```\\n\\nIt looks like our code tried to call `input_ids.size()`, but this clearly won\\'t work for a Python `list`, which is just a container. How can we solve this problem? Searching for the error message on Stack Overflow gives quite a few relevant [hits](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f). Clicking on the first one displays a similar question to ours, with the answer shown in the screenshot below:\\n\\n<div class=\"flex justify-center\">\\n<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png\" alt=\"An answer from Stack Overflow.\" width=\"100%\"/>\\n</div>\\n\\nThe answer recommends that we add `return_tensors=\\'pt\\'` to the tokenizer, so let\\'s see if that works for us:\\n\\n```python out\\ninputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\\ninput_ids = inputs[\"input_ids\"][0]\\noutputs = model(**inputs)\\nanswer_start_scores = outputs.start_logits\\nanswer_end_scores = outputs.end_logits\\n# Get the most likely beginning of answer with the argmax of the score\\nanswer_start = torch.argmax(answer_start_scores)\\n# Get the most likely end of answer with the argmax of the score\\nanswer_end = torch.argmax(answer_end_scores) + 1\\nanswer = tokenizer.convert_tokens_to_string(\\n    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\\n)\\nprint(f\"Question: {question}\")\\nprint(f\"Answer: {answer}\")'),\n",
       " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter8/2.mdx', 'start_index': 19265}, page_content='```\\n\\n```python out\\n\"\"\"\\nQuestion: Which frameworks can I use?\\nAnswer: pytorch, tensorflow, and jax\\n\"\"\"\\n```\\n\\nNice, it worked! This is a great example of how useful Stack Overflow can be: by identifying a similar problem, we were able to benefit from the experience of others in the community. However, a search like this won\\'t always yield a relevant answer, so what can you do in such cases? Fortunately, there is a welcoming community of developers on the [Hugging Face forums](https://discuss.huggingface.co/) that can help you out! In the next section, we\\'ll take a look at how you can craft good forum questions that are likely to get answered.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 0}, page_content=\"*TEMPLATE**\\n=====================================\\n\\n*search & replace the following keywords, e.g.:*\\n`:%s/\\\\[name of model\\\\]/brand_new_bert/g`\\n\\n-[lowercase name of model]  # e.g. brand_new_bert\\n\\n-[camelcase name of model]  # e.g. BrandNewBert\\n\\n-[name of mentor]  # e.g. [Peter](https://github.com/peter)\\n\\n-[link to original repo]\\n\\n-[start date]\\n\\n-[end date]\\n\\n\\n\\nHow to add [camelcase name of model] to 🤗 Transformers?\\n=====================================\\n\\nMentor: [name of mentor]\\n\\nBegin: [start date]\\n\\nEstimated End: [end date]\\n\\nAdding a new model is often difficult and requires an in-depth knowledge\\nof the 🤗 Transformers library and ideally also of the model's original\\nrepository. At Hugging Face, we are trying to empower the community more\\nand more to add models independently. \\n\\nThe following sections explain in detail how to add [camelcase name of model] \\nto Transformers. You will work closely with [name of mentor] to\\nintegrate [camelcase name of model] into Transformers. By doing so, you will both gain a \\ntheoretical and deep practical understanding of [camelcase name of model]. \\nBut more importantly, you will have made a major\\nopen-source contribution to Transformers. Along the way, you will:\\n\\n-   get insights into open-source best practices\\n-   understand the design principles of one of the most popular NLP\\n    libraries\\n-   learn how to do efficiently test large NLP models\\n-   learn how to integrate Python utilities like `black`, `ruff`,\\n    `make fix-copies` into a library to always ensure clean and readable\\n    code\\n\\nTo start, let's try to get a general overview of the Transformers\\nlibrary.\\n\\nGeneral overview of 🤗 Transformers\\n----------------------------------\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': -1}, page_content=\"General overview of 🤗 Transformers\\n----------------------------------\\n\\nFirst, you should get a general overview of 🤗 Transformers. Transformers \\nis a very opinionated library, so there is a chance that\\nyou don't agree with some of the library's philosophies or design\\nchoices. From our experience, however, we found that the fundamental\\ndesign choices and philosophies of the library are crucial to\\nefficiently scale Transformers while keeping maintenance costs at a\\nreasonable level.\\n\\nA good first starting point to better understand the library is to read\\nthe [documentation of our philosophy](https://huggingface.co/transformers/philosophy.html).\\nAs a result of our way of working, there are some choices that we try to apply to all models:\\n\\n-   Composition is generally favored over abstraction\\n-   Duplicating code is not always bad if it strongly improves the\\n    readability or accessibility of a model\\n-   Model files are as self-contained as possible so that when you read\\n    the code of a specific model, you ideally only have to look into the\\n    respective `modeling_....py` file.\\n\\nIn our opinion, the library's code is not just a means to provide a\\nproduct, *e.g.*, the ability to use BERT for inference, but also as the\\nvery product that we want to improve. Hence, when adding a model, the\\nuser is not only the person that will use your model, but also everybody\\nthat will read, try to understand, and possibly tweak your code.\\n\\nWith this in mind, let's go a bit deeper into the general library\\ndesign.\\n\\n### Overview of models\\n\\nTo successfully add a model, it is important to understand the\\ninteraction between your model and its config,\\n`PreTrainedModel`, and `PretrainedConfig`. For\\nexemplary purposes, we will call the PyTorch model to be added to 🤗 Transformers\\n`BrandNewBert`.\\n\\nLet's take a look:\\n\\n![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png)\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 3419}, page_content='Let\\'s take a look:\\n\\n![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png)\\n\\nAs you can see, we do make use of inheritance in 🤗 Transformers, but we\\nkeep the level of abstraction to an absolute minimum. There are never\\nmore than two levels of abstraction for any model in the library.\\n`BrandNewBertModel` inherits from\\n`BrandNewBertPreTrainedModel` which in\\nturn inherits from `PreTrainedModel` and that\\'s it. \\nAs a general rule, we want to make sure\\nthat a new model only depends on `PreTrainedModel`. The\\nimportant functionalities that are automatically provided to every new\\nmodel are\\n`PreTrainedModel.from_pretrained` and `PreTrainedModel.save_pretrained`, which are \\nused for serialization and deserialization. All\\nof the other important functionalities, such as\\n`BrandNewBertModel.forward` should be\\ncompletely defined in the new `modeling_brand_new_bert.py` module. Next,\\nwe want to make sure that a model with a specific head layer, such as\\n`BrandNewBertForMaskedLM` does not inherit\\nfrom `BrandNewBertModel`, but rather uses\\n`BrandNewBertModel` as a component that\\ncan be called in its forward pass to keep the level of abstraction low.\\nEvery new model requires a configuration class, called\\n`BrandNewBertConfig`. This configuration\\nis always stored as an attribute in\\n`PreTrainedModel`, and\\nthus can be accessed via the `config` attribute for all classes\\ninheriting from `BrandNewBertPreTrainedModel`\\n\\n```python\\n# assuming that `brand_new_bert` belongs to the organization `brandy`\\nmodel = BrandNewBertModel.from_pretrained(\"brandy/brand_new_bert\")\\nmodel.config  # model has access to its config'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 5083}, page_content='```\\n\\nSimilar to the model, the configuration inherits basic serialization and\\ndeserialization functionalities from\\n`PretrainedConfig`. Note\\nthat the configuration and the model are always serialized into two\\ndifferent formats - the model to a `pytorch_model.bin` file\\nand the configuration to a `config.json` file. Calling\\n`PreTrainedModel.save_pretrained` will automatically call\\n`PretrainedConfig.save_pretrained`, so that both model and configuration are saved.\\n\\n### Overview of tokenizers\\n\\nNot quite ready yet :-( This section will be added soon!\\n\\nStep-by-step recipe to add a model to 🤗 Transformers\\n----------------------------------------------------\\n\\nEveryone has different preferences of how to port a model so it can be\\nvery helpful for you to take a look at summaries of how other\\ncontributors ported models to Hugging Face. Here is a list of community\\nblog posts on how to port a model:\\n\\n1.  [Porting GPT2\\n    Model](https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28)\\n    by [Thomas](https://huggingface.co/thomwolf)\\n2.  [Porting WMT19 MT Model](https://huggingface.co/blog/porting-fsmt)\\n    by [Stas](https://huggingface.co/stas)\\n\\nFrom experience, we can tell you that the most important things to keep\\nin mind when adding a model are:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': -1}, page_content=\"From experience, we can tell you that the most important things to keep\\nin mind when adding a model are:\\n\\n-   Don't reinvent the wheel! Most parts of the code you will add for\\n    the new 🤗 Transformers model already exist somewhere in 🤗\\n    Transformers. Take some time to find similar, already existing\\n    models and tokenizers you can copy from.\\n    [grep](https://www.gnu.org/software/grep/) and\\n    [rg](https://github.com/BurntSushi/ripgrep) are your friends. Note\\n    that it might very well happen that your model's tokenizer is based\\n    on one model implementation, and your model's modeling code on\\n    another one. *E.g.*, FSMT's modeling code is based on BART, while\\n    FSMT's tokenizer code is based on XLM.\\n-   It's more of an engineering challenge than a scientific challenge.\\n    You should spend more time on creating an efficient debugging\\n    environment than trying to understand all theoretical aspects of the\\n    model in the paper.\\n-   Ask for help when you're stuck! Models are the core component of 🤗\\n    Transformers so we, at Hugging Face, are more than happy to help\\n    you at every step to add your model. Don't hesitate to ask if you\\n    notice you are not making progress.\\n\\nIn the following, we try to give you a general recipe that we found most\\nuseful when porting a model to 🤗 Transformers.\\n\\nThe following list is a summary of everything that has to be done to add\\na model and can be used by you as a To-Do List:\\n\\n1.  [ ] (Optional) Understood theoretical aspects\\n\\n2.  [ ] Prepared transformers dev environment\\n\\n3.  [ ] Set up debugging environment of the original repository\\n\\n4.  [ ] Created script that successfully runs forward pass using\\n    original repository and checkpoint\\n\\n5.  [ ] Successfully opened a PR and added the model skeleton to Transformers\\n\\n6.  [ ] Successfully converted original checkpoint to Transformers\\n    checkpoint\\n\\n7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original checkpoint\\n\\n8.  [ ] Finished model tests in Transformers\\n\\n9.  [ ] Successfully added Tokenizer in Transformers\\n\\n10. [ ] Run end-to-end integration tests\\n\\n11. [ ] Finished docs\\n\\n12. [ ] Uploaded model weights to the hub\\n\\n13. [ ] Submitted the pull request for review\\n\\n14. [ ] (Optional) Added a demo notebook\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 8340}, page_content=\"10. [ ] Run end-to-end integration tests\\n\\n11. [ ] Finished docs\\n\\n12. [ ] Uploaded model weights to the hub\\n\\n13. [ ] Submitted the pull request for review\\n\\n14. [ ] (Optional) Added a demo notebook\\n\\nTo begin with, we usually recommend to start by getting a good\\ntheoretical understanding of `[camelcase name of model]`. However, if you prefer to\\nunderstand the theoretical aspects of the model *on-the-job*, then it is\\ntotally fine to directly dive into the `[camelcase name of model]`'s code-base. This\\noption might suit you better, if your engineering skills are better than\\nyour theoretical skill, if you have trouble understanding\\n`[camelcase name of model]`'s paper, or if you just enjoy programming much more than\\nreading scientific papers.\\n\\n### 1. (Optional) Theoretical aspects of [camelcase name of model]\\n\\nYou should take some time to read *[camelcase name of model]'s* paper, if such\\ndescriptive work exists. There might be large sections of the paper that\\nare difficult to understand. If this is the case, this is fine - don't\\nworry! The goal is not to get a deep theoretical understanding of the\\npaper, but to extract the necessary information required to effectively\\nre-implement the model in 🤗 Transformers. That being said, you don't\\nhave to spend too much time on the theoretical aspects, but rather focus\\non the practical ones, namely:\\n\\n-   What type of model is *[camelcase name of model]*? BERT-like encoder-only\\n    model? GPT2-like decoder-only model? BART-like encoder-decoder\\n    model? Look at the `model_summary` if\\n    you're not familiar with the differences between those.\\n-   What are the applications of *[camelcase name of model]*? Text\\n    classification? Text generation? Seq2Seq tasks, *e.g.,*\\n    summarization?\\n-   What is the novel feature of the model making it different from\\n    BERT/GPT-2/BART?\\n-   Which of the already existing [🤗 Transformers\\n    models](https://huggingface.co/transformers/#contents) is most\\n    similar to *[camelcase name of model]*?\\n-   What type of tokenizer is used? A sentencepiece tokenizer? Word\\n    piece tokenizer? Is it the same tokenizer as used for BERT or BART?\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 10477}, page_content=\"After you feel like you have gotten a good overview of the architecture\\nof the model, you might want to write to [name of mentor] with any\\nquestions you might have. This might include questions regarding the\\nmodel's architecture, its attention layer, etc. We will be more than\\nhappy to help you.\\n\\n\\n#### Additional resources\\n\\n Before diving into the code, here are some additional resources that might be worth taking a look at:\\n \\n - [link 1]\\n - [link 2]\\n - [link 3]\\n - ...\\n\\n#### Make sure you've understood the fundamental aspects of [camelcase name of model]\\n\\nAlright, now you should be ready to take a closer look into the actual code of [camelcase name of model].\\nYou should have understood the following aspects of [camelcase name of model] by now:\\n\\n- [characteristic 1 of [camelcase name of model]]\\n- [characteristic 2 of [camelcase name of model]]\\n- ...\\n\\nIf any of the mentioned aspects above are **not** clear to you, now is a great time to talk to [name of mentor].\\n\\n### 2. Next prepare your environment\\n\\n1.  Fork the [repository](https://github.com/huggingface/transformers)\\n    by clicking on the 'Fork' button on the repository's page. This\\n    creates a copy of the code under your GitHub user account.\\n\\n2.  Clone your `transformers` fork to your local disk, and add the base\\n    repository as a remote:\\n\\n    ```bash\\n    git clone https://github.com/[your Github handle]/transformers.git\\n    cd transformers\\n    git remote add upstream https://github.com/huggingface/transformers.git\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 11977}, page_content='```\\n\\n3.  Set up a development environment, for instance by running the\\n    following command:\\n\\n    ```bash\\n    python -m venv .env\\n    source .env/bin/activate\\n    pip install -e \".[dev]\"\\n    ```\\n\\nand return to the parent directory\\n\\n```bash\\ncd ..\\n```\\n\\n4.  We recommend adding the PyTorch version of *[camelcase name of model]* to\\n    Transformers. To install PyTorch, please follow the instructions [here](https://pytorch.org/get-started/locally/).\\n\\n**Note:** You don\\'t need to have CUDA installed. Making the new model\\nwork on CPU is sufficient.\\n\\n5.  To port *[camelcase name of model]*, you will also need access to its\\n    original repository:\\n\\n```bash\\ngit clone [link to original repo].git \\ncd [lowercase name of model]\\npip install -e .'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 12718}, page_content='```\\n\\nNow you have set up a development environment to port *[camelcase name of model]*\\nto 🤗 Transformers.\\n\\n### Run a pretrained checkpoint using the original repository\\n\\n**3. Set up debugging environment**\\n\\nAt first, you will work on the original *[camelcase name of model]* repository.\\nOften, the original implementation is very \"researchy\". Meaning that\\ndocumentation might be lacking and the code can be difficult to\\nunderstand. But this should be exactly your motivation to reimplement\\n*[camelcase name of model]*. At Hugging Face, one of our main goals is to *make\\npeople stand on the shoulders of giants* which translates here very well\\ninto taking a working model and rewriting it to make it as **accessible,\\nuser-friendly, and beautiful** as possible. This is the number-one\\nmotivation to re-implement models into 🤗 Transformers - trying to make\\ncomplex new NLP technology accessible to **everybody**.\\n\\nYou should start thereby by diving into the [original repository]([link to original repo]).\\n\\nSuccessfully running the official pretrained model in the original\\nrepository is often **the most difficult** step. From our experience, it\\nis very important to spend some time getting familiar with the original\\ncode-base. You need to figure out the following:\\n\\n-   Where to find the pretrained weights?\\n-   How to load the pretrained weights into the corresponding model?\\n-   How to run the tokenizer independently from the model?\\n-   Trace one forward pass so that you know which classes and functions\\n    are required for a simple forward pass. Usually, you only have to\\n    reimplement those functions.\\n-   Be able to locate the important components of the model: Where is\\n    the model\\'s class? Are there model sub-classes, *e.g.*,\\n    EncoderModel, DecoderModel? Where is the self-attention layer? Are\\n    there multiple different attention layers, *e.g.*, *self-attention*,\\n    *cross-attention*...?\\n-   How can you debug the model in the original environment of the repo?\\n    Do you have to add `print` statements, can you work with\\n    an interactive debugger like [ipdb](https://pypi.org/project/ipdb/), or should you use\\n    an efficient IDE to debug the model, like PyCharm?'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 14910}, page_content='It is very important that before you start the porting process, that you\\ncan **efficiently** debug code in the original repository! Also,\\nremember that you are working with an open-source library, so do not\\nhesitate to open an issue, or even a pull request in the original\\nrepository. The maintainers of this repository are most likely very\\nhappy about someone looking into their code!\\n\\nAt this point, it is really up to you which debugging environment and\\nstrategy you prefer to use to debug the original model. We strongly\\nadvise against setting up a costly GPU environment, but simply work on a\\nCPU both when starting to dive into the original repository and also\\nwhen starting to write the 🤗 Transformers implementation of the model.\\nOnly at the very end, when the model has already been successfully\\nported to 🤗 Transformers, one should verify that the model also works as\\nexpected on GPU.\\n\\nIn general, there are two possible debugging environments for running\\nthe original model\\n\\n-   [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)\\n-   Local python scripts.\\n\\nJupyter notebooks have the advantage that they allow for cell-by-cell\\nexecution which can be helpful to better split logical components from\\none another and to have faster debugging cycles as intermediate results\\ncan be stored. Also, notebooks are often easier to share with other\\ncontributors, which might be very helpful if you want to ask the Hugging\\nFace team for help. If you are familiar with Jupyter notebooks, we\\nstrongly recommend you to work with them.\\n\\nThe obvious disadvantage of Jupyter notebooks is that if you are not\\nused to working with them you will have to spend some time adjusting to\\nthe new programming environment and that you might not be able to use\\nyour known debugging tools anymore, like `ipdb`.\\n\\n**4. Successfully run forward pass**\\n\\nFor each code-base, a good first step is always to load a **small**\\npretrained checkpoint and to be able to reproduce a single forward pass\\nusing a dummy integer vector of input IDs as an input. Such a script\\ncould look like this (in pseudocode):'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 17054}, page_content='```python\\nmodel = [camelcase name of model]Model.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\\ninput_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids\\noriginal_output = model.predict(input_ids)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 17254}, page_content=\"```\\n\\nNext, regarding the debugging strategy, there are generally a few from\\nwhich to choose from:\\n\\n-   Decompose the original model into many small testable components and\\n    run a forward pass on each of those for verification\\n-   Decompose the original model only into the original *tokenizer* and\\n    the original *model*, run a forward pass on those, and use\\n    intermediate print statements or breakpoints for verification\\n\\nAgain, it is up to you which strategy to choose. Often, one or the other\\nis advantageous depending on the original code base.\\n\\nIf the original code-base allows you to decompose the model into smaller\\nsub-components, *e.g.*, if the original code-base can easily be run in\\neager mode, it is usually worth the effort to do so. There are some\\nimportant advantages to taking the more difficult road in the beginning:\\n\\n-   at a later stage when comparing the original model to the Hugging\\n    Face implementation, you can verify automatically for each component\\n    individually that the corresponding component of the 🤗 Transformers\\n    implementation matches instead of relying on visual comparison via\\n    print statements\\n-   it can give you some rope to decompose the big problem of porting a\\n    model into smaller problems of just porting individual components\\n    and thus structure your work better\\n-   separating the model into logical meaningful components will help\\n    you to get a better overview of the model's design and thus to\\n    better understand the model\\n-   at a later stage those component-by-component tests help you to\\n    ensure that no regression occurs as you continue changing your code\\n\\n[Lysandre's](https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed)\\nintegration checks for ELECTRA gives a nice example of how this can be\\ndone.\\n\\nHowever, if the original code-base is very complex or only allows\\nintermediate components to be run in a compiled mode, it might be too\\ntime-consuming or even impossible to separate the model into smaller\\ntestable sub-components. A good example is [T5's\\nMeshTensorFlow](https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow)\\nlibrary which is very complex and does not offer a simple way to\\ndecompose the model into its sub-components. For such libraries, one\\noften relies on verifying print statements.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 19571}, page_content='No matter which strategy you choose, the recommended procedure is often\\nthe same in that you should start to debug the starting layers first and\\nthe ending layers last.\\n\\nIt is recommended that you retrieve the output, either by print\\nstatements or sub-component functions, of the following layers in the\\nfollowing order:\\n\\n1.  Retrieve the input IDs passed to the model\\n2.  Retrieve the word embeddings\\n3.  Retrieve the input of the first Transformer layer\\n4.  Retrieve the output of the first Transformer layer\\n5.  Retrieve the output of the following n - 1 Transformer layers\\n6.  Retrieve the output of the whole [camelcase name of model] Model\\n\\nInput IDs should thereby consists of an array of integers, *e.g.*,\\n`input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]`\\n\\nThe outputs of the following layers often consist of multi-dimensional\\nfloat arrays and can look like this:\\n\\n```bash\\n[[\\n [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],\\n [-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],\\n [-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],\\n ...,\\n [-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],\\n [-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],\\n [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 20835}, page_content='```\\n\\nWe expect that every model added to 🤗 Transformers passes a couple of\\nintegration tests, meaning that the original model and the reimplemented\\nversion in 🤗 Transformers have to give the exact same output up to a\\nprecision of 0.001! Since it is normal that the exact same model written\\nin different libraries can give a slightly different output depending on\\nthe library framework, we accept an error tolerance of 1e-3 (0.001). It\\nis not enough if the model gives nearly the same output, they have to be\\nthe almost identical. Therefore, you will certainly compare the\\nintermediate outputs of the 🤗 Transformers version multiple times\\nagainst the intermediate outputs of the original implementation of\\n*[camelcase name of model]* in which case an **efficient** debugging environment\\nof the original repository is absolutely important. Here is some advice\\nto make your debugging environment as efficient as possible.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 21755}, page_content=\"-   Find the best way of debugging intermediate results. Is the original\\n    repository written in PyTorch? Then you should probably take the\\n    time to write a longer script that decomposes the original model\\n    into smaller sub-components to retrieve intermediate values. Is the\\n    original repository written in Tensorflow 1? Then you might have to\\n    rely on TensorFlow print operations like\\n    [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) to\\n    output intermediate values. Is the original repository written in\\n    Jax? Then make sure that the model is **not jitted** when running\\n    the forward pass, *e.g.*, check-out [this\\n    link](https://github.com/google/jax/issues/196).\\n-   Use the smallest pretrained checkpoint you can find. The smaller the\\n    checkpoint, the faster your debug cycle becomes. It is not efficient\\n    if your pretrained model is so big that your forward pass takes more\\n    than 10 seconds. In case only very large checkpoints are available,\\n    it might make more sense to create a dummy model in the new\\n    environment with randomly initialized weights and save those weights\\n    for comparison with the 🤗 Transformers version of your model\\n-   Make sure you are using the easiest way of calling a forward pass in\\n    the original repository. Ideally, you want to find the function in\\n    the original repository that **only** calls a single forward pass,\\n    *i.e.* that is often called `predict`, `evaluate`, `forward` or\\n    `__call__`. You don't want to debug a function that calls `forward`\\n    multiple times, *e.g.*, to generate text, like\\n    `autoregressive_sample`, `generate`.\\n-   Try to separate the tokenization from the model's\\n    forward pass. If the original repository shows\\n    examples where you have to input a string, then try to find out\\n    where in the forward call the string input is changed to input ids\\n    and start from this point. This might mean that you have to possibly\\n    write a small script yourself or change the original code so that\\n    you can directly input the ids instead of an input string.\\n-   Make sure that the model in your debugging setup is **not** in\\n    training mode, which often causes the model to yield random outputs\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': -1}, page_content='you can directly input the ids instead of an input string.\\n-   Make sure that the model in your debugging setup is **not** in\\n    training mode, which often causes the model to yield random outputs\\n    due to multiple dropout layers in the model. Make sure that the\\n    forward pass in your debugging environment is **deterministic** so\\n    that the dropout layers are not used. Or use\\n    `transformers.utils.set_seed` if the old and new\\n    implementations are in the same framework.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 24283}, page_content='#### More details on how to create a debugging environment for [camelcase name of model] \\n\\n[TODO FILL: Here the mentor should add very specific information on what the student should do]\\n[to set up an efficient environment for the special requirements of this model]\\n\\n### Port [camelcase name of model] to 🤗 Transformers\\n\\nNext, you can finally start adding new code to 🤗 Transformers. Go into\\nthe clone of your 🤗 Transformers\\' fork:\\n\\n    cd transformers\\n\\nIn the special case that you are adding a model whose architecture\\nexactly matches the model architecture of an existing model you only\\nhave to add a conversion script as described in [this\\nsection](#write-a-conversion-script). In this case, you can just re-use\\nthe whole model architecture of the already existing model.\\n\\nOtherwise, let\\'s start generating a new model with the amazing\\nCookiecutter!\\n\\n**Use the Cookiecutter to automatically generate the model\\'s code**\\n\\nTo begin with head over to the [🤗 Transformers\\ntemplates](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model)\\nto make use of our `cookiecutter` implementation to automatically\\ngenerate all the relevant files for your model. Again, we recommend only\\nadding the PyTorch version of the model at first. Make sure you follow\\nthe instructions of the `README.md` on the [🤗 Transformers\\ntemplates](https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model)\\ncarefully.\\n\\n**Open a Pull Request on the main huggingface/transformers repo**\\n\\nBefore starting to adapt the automatically generated code, now is the\\ntime to open a \"Work in progress (WIP)\" pull request, *e.g.*, \"\\\\[WIP\\\\]\\nAdd *[camelcase name of model]*\", in 🤗 Transformers so that you and the Hugging\\nFace team can work side-by-side on integrating the model into 🤗\\nTransformers.\\n\\nYou should do the following:\\n\\n1.  Create a branch with a descriptive name from your main branch'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 26190}, page_content='```\\n    git checkout -b add_[lowercase name of model]\\n```\\n\\n2.  Commit the automatically generated code:\\n\\n```\\n    git add .\\n    git commit\\n```\\n\\n3.  Fetch and rebase to current main\\n\\n```\\n    git fetch upstream\\n    git rebase upstream/main\\n```\\n\\n4.  Push the changes to your account using:\\n\\n```\\n    git push -u origin a-descriptive-name-for-my-changes'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 26538}, page_content='```\\n\\n5.  Once you are satisfied, go to the webpage of your fork on GitHub.\\n    Click on \"Pull request\". Make sure to add the GitHub handle of \\n\\t\\t[name of mentor] as a reviewer, so that the Hugging\\n    Face team gets notified for future changes.\\n\\n6.  Change the PR into a draft by clicking on \"Convert to draft\" on the\\n    right of the GitHub pull request web page.\\n\\nIn the following, whenever you have done some progress, don\\'t forget to\\ncommit your work and push it to your account so that it shows in the\\npull request. Additionally, you should make sure to update your work\\nwith the current main from time to time by doing:\\n\\n    git fetch upstream\\n    git merge upstream/main\\n\\nIn general, all questions you might have regarding the model or your\\nimplementation should be asked in your PR and discussed/solved in the\\nPR. This way, [name of mentor] will always be notified when you are\\ncommitting new code or if you have a question. It is often very helpful\\nto point [name of mentor] to your added code so that the Hugging\\nFace team can efficiently understand your problem or question.\\n\\nTo do so, you can go to the \"Files changed\" tab where you see all of\\nyour changes, go to a line regarding which you want to ask a question,\\nand click on the \"+\" symbol to add a comment. Whenever a question or\\nproblem has been solved, you can click on the \"Resolve\" button of the\\ncreated comment.\\n\\nIn the same way, [name of mentor] will open comments when reviewing\\nyour code. We recommend asking most questions on GitHub on your PR. For\\nsome very general questions that are not very useful for the public,\\nfeel free to ping [name of mentor] by Slack or email.\\n\\n**5. Adapt the generated models code for [camelcase name of model]**\\n\\nAt first, we will focus only on the model itself and not care about the\\ntokenizer. All the relevant code should be found in the generated files\\n`src/transformers/models/[lowercase name of model]/modeling_[lowercase name of model].py` and\\n`src/transformers/models/[lowercase name of model]/configuration_[lowercase name of model].py`.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 28591}, page_content='Now you can finally start coding :). The generated code in\\n`src/transformers/models/[lowercase name of model]/modeling_[lowercase name of model].py` will\\neither have the same architecture as BERT if it\\'s an encoder-only model\\nor BART if it\\'s an encoder-decoder model. At this point, you should\\nremind yourself what you\\'ve learned in the beginning about the\\ntheoretical aspects of the model: *How is the model different from BERT\\nor BART?*\\\\\". Implement those changes which often means to change the\\n*self-attention* layer, the order of the normalization layer, etc...\\nAgain, it is often useful to look at the similar architecture of already\\nexisting models in Transformers to get a better feeling of how your\\nmodel should be implemented.\\n\\n**Note** that at this point, you don\\'t have to be very sure that your\\ncode is fully correct or clean. Rather, it is advised to add a first\\n*unclean*, copy-pasted version of the original code to\\n`src/transformers/models/[lowercase name of model]/modeling_[lowercase name of model].py`\\nuntil you feel like all the necessary code is added. From our\\nexperience, it is much more efficient to quickly add a first version of\\nthe required code and improve/correct the code iteratively with the\\nconversion script as described in the next section. The only thing that\\nhas to work at this point is that you can instantiate the 🤗 Transformers\\nimplementation of *[camelcase name of model]*, *i.e.* the following command\\nshould work:\\n\\n```python\\nfrom transformers import [camelcase name of model]Model, [camelcase name of model]Config\\nmodel = [camelcase name of model]Model([camelcase name of model]Config())'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 30223}, page_content=\"```\\n\\nThe above command will create a model according to the default\\nparameters as defined in `[camelcase name of model]Config()` with random weights,\\nthus making sure that the `init()` methods of all components works.\\n\\n[TODO FILL: Here the mentor should add very specific information on what exactly has to be changed for this model]\\n[...]\\n[...]\\n\\n**6. Write a conversion script**\\n\\nNext, you should write a conversion script that lets you convert the\\ncheckpoint you used to debug *[camelcase name of model]* in the original\\nrepository to a checkpoint compatible with your just created 🤗\\nTransformers implementation of *[camelcase name of model]*. It is not advised to\\nwrite the conversion script from scratch, but rather to look through\\nalready existing conversion scripts in 🤗 Transformers for one that has\\nbeen used to convert a similar model that was written in the same\\nframework as *[camelcase name of model]*. Usually, it is enough to copy an\\nalready existing conversion script and slightly adapt it for your use\\ncase. Don't hesitate to ask [name of mentor] to point you to a\\nsimilar already existing conversion script for your model.\\n\\n-   If you are porting a model from TensorFlow to PyTorch, a good\\n    starting point might be BERT's conversion script\\n    [here](https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91)\\n-   If you are porting a model from PyTorch to PyTorch, a good starting\\n    point might be BART's conversion script\\n    [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py)\\n\\nIn the following, we'll quickly explain how PyTorch models store layer\\nweights and define layer names. In PyTorch, the name of a layer is\\ndefined by the name of the class attribute you give the layer. Let's\\ndefine a dummy model in PyTorch, called `SimpleModel` as follows:\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 32176}, page_content='```python\\nfrom torch import nn\\n\\nclass SimpleModel(nn.Module):\\n    def __init__(self):\\n            super().__init__()\\n            self.dense = nn.Linear(10, 10)\\n            self.intermediate = nn.Linear(10, 10)\\n            self.layer_norm = nn.LayerNorm(10)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 32433}, page_content='```\\n\\nNow we can create an instance of this model definition which will fill\\nall weights: `dense`, `intermediate`, `layer_norm` with random weights.\\nWe can print the model to see its architecture\\n\\n```python\\nmodel = SimpleModel()\\n\\nprint(model)\\n```\\n\\nThis will print out the following:\\n\\n```bash\\nSimpleModel(\\n  (dense): Linear(in_features=10, out_features=10, bias=True)\\n  (intermediate): Linear(in_features=10, out_features=10, bias=True)\\n  (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\\n)\\n```\\n\\nWe can see that the layer names are defined by the name of the class\\nattribute in PyTorch. You can print out the weight values of a specific\\nlayer:\\n\\n```python\\nprint(model.dense.weight.data)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 33134}, page_content='```\\n\\nto see that the weights were randomly initialized'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 33190}, page_content='```bash\\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\\n         -0.2077,  0.2157],\\n        [ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, -0.2190,\\n          0.2166, -0.0212],\\n        [-0.2000,  0.1107, -0.1999, -0.3119,  0.1559,  0.0993,  0.1776, -0.1950,\\n         -0.1023, -0.0447],\\n        [-0.0888, -0.1092,  0.2281,  0.0336,  0.1817, -0.0115,  0.2096,  0.1415,\\n         -0.1876, -0.2467],\\n        [ 0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465,\\n          0.2577,  0.0402],\\n        [ 0.1502,  0.2465,  0.2566,  0.0693,  0.2352, -0.0530,  0.1859, -0.0604,\\n          0.2132,  0.1680],\\n        [ 0.1733, -0.2407, -0.1721,  0.1484,  0.0358, -0.0633, -0.0721, -0.0090,\\n          0.2707, -0.2509],\\n        [-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,\\n          0.1829, -0.1568],\\n        [-0.1164, -0.2228, -0.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': -1}, page_content='[-0.1164, -0.2228, -0.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,\\n          0.0333, -0.0536],\\n        [-0.1492, -0.1616,  0.1057,  0.1950, -0.2807, -0.2710, -0.1586,  0.0739,\\n          0.2220,  0.2358]]).'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 34290}, page_content='```\\n\\nIn the conversion script, you should fill those randomly initialized\\nweights with the exact weights of the corresponding layer in the\\ncheckpoint. *E.g.*,\\n\\n```python\\n# retrieve matching layer weights, e.g. by \\n# recursive algorithm\\nlayer_name = \"dense\"\\npretrained_weight = array_of_dense_layer\\n\\nmodel_pointer = getattr(model, \"dense\")\\n\\nmodel_pointer.weight.data = torch.from_numpy(pretrained_weight)\\n```\\n\\nWhile doing so, you must verify that each randomly initialized weight of\\nyour PyTorch model and its corresponding pretrained checkpoint weight\\nexactly match in both **shape and name**. To do so, it is **necessary**\\nto add assert statements for the shape and print out the names of the\\ncheckpoints weights. *E.g.*, you should add statements like:\\n\\n```python\\nassert (\\n     model_pointer.weight.shape == pretrained_weight.shape\\n), f\"Pointer shape of random weight {model_pointer.shape} and array shape of checkpoint weight {pretrained_weight.shape} mismatched\"\\n```\\n\\nBesides, you should also print out the names of both weights to make\\nsure they match, *e.g.*,\\n\\n```python\\nlogger.info(f\"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 35452}, page_content='```\\n\\nIf either the shape or the name doesn\\'t match, you probably assigned\\nthe wrong checkpoint weight to a randomly initialized layer of the 🤗\\nTransformers implementation.\\n\\nAn incorrect shape is most likely due to an incorrect setting of the\\nconfig parameters in `[camelcase name of model]Config()` that do not exactly match\\nthose that were used for the checkpoint you want to convert. However, it\\ncould also be that PyTorch\\'s implementation of a layer requires the\\nweight to be transposed beforehand.\\n\\nFinally, you should also check that **all** required weights are\\ninitialized and print out all checkpoint weights that were not used for\\ninitialization to make sure the model is correctly converted. It is\\ncompletely normal, that the conversion trials fail with either a wrong\\nshape statement or wrong name assignment. This is most likely because\\neither you used incorrect parameters in `[camelcase name of model]Config()`, have a\\nwrong architecture in the 🤗 Transformers implementation, you have a bug\\nin the `init()` functions of one of the components of the 🤗 Transformers\\nimplementation or you need to transpose one of the checkpoint weights.\\n\\nThis step should be iterated with the previous step until all weights of\\nthe checkpoint are correctly loaded in the Transformers model. Having\\ncorrectly loaded the checkpoint into the 🤗 Transformers implementation,\\nyou can then save the model under a folder of your choice\\n`/path/to/converted/checkpoint/folder` that should then contain both a\\n`pytorch_model.bin` file and a `config.json` file:\\n\\n```python\\nmodel.save_pretrained(\"/path/to/converted/checkpoint/folder\")'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 37070}, page_content='```\\n\\n[TODO FILL: Here the mentor should add very specific information on what exactly has to be done for the conversion of this model]\\n[...]\\n[...]\\n\\n**7. Implement the forward pass**\\n\\nHaving managed to correctly load the pretrained weights into the 🤗\\nTransformers implementation, you should now make sure that the forward\\npass is correctly implemented. In [Get familiar with the original\\nrepository](#34-run-a-pretrained-checkpoint-using-the-original-repository),\\nyou have already created a script that runs a forward pass of the model\\nusing the original repository. Now you should write an analogous script\\nusing the 🤗 Transformers implementation instead of the original one. It\\nshould look as follows:\\n\\n[TODO FILL: Here the model name might have to be adapted, *e.g.*, maybe [camelcase name of model]ForConditionalGeneration instead of [camelcase name of model]Model]\\n\\n```python\\nmodel = [camelcase name of model]Model.from_pretrained(\"/path/to/converted/checkpoint/folder\")\\ninput_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]\\noutput = model(input_ids).last_hidden_states'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 38131}, page_content='```\\n\\nIt is very likely that the 🤗 Transformers implementation and the\\noriginal model implementation don\\'t give the exact same output the very\\nfirst time or that the forward pass throws an error. Don\\'t be\\ndisappointed - it\\'s expected! First, you should make sure that the\\nforward pass doesn\\'t throw any errors. It often happens that the wrong\\ndimensions are used leading to a `\"Dimensionality mismatch\"`\\nerror or that the wrong data type object is used, *e.g.*, `torch.long`\\ninstead of `torch.float32`. Don\\'t hesitate to ask [name of mentor]\\nfor help, if you don\\'t manage to solve certain errors.\\n\\nThe final part to make sure the 🤗 Transformers implementation works\\ncorrectly is to ensure that the outputs are equivalent to a precision of\\n`1e-3`. First, you should ensure that the output shapes are identical,\\n*i.e.* `outputs.shape` should yield the same value for the script of the\\n🤗 Transformers implementation and the original implementation. Next, you\\nshould make sure that the output values are identical as well. This one\\nof the most difficult parts of adding a new model. Common mistakes why\\nthe outputs are not identical are:\\n\\n-   Some layers were not added, *i.e.* an activation layer\\n    was not added, or the residual connection was forgotten\\n-   The word embedding matrix was not tied\\n-   The wrong positional embeddings are used because the original\\n    implementation uses on offset\\n-   Dropout is applied during the forward pass. To fix this make sure\\n    `model.training is False` and that no dropout layer is\\n    falsely activated during the forward pass, *i.e.* pass\\n    `self.training` to [PyTorch\\'s functional\\n    dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 39872}, page_content=\"The best way to fix the problem is usually to look at the forward pass\\nof the original implementation and the 🤗 Transformers implementation\\nside-by-side and check if there are any differences. Ideally, you should\\ndebug/print out intermediate outputs of both implementations of the\\nforward pass to find the exact position in the network where the 🤗\\nTransformers implementation shows a different output than the original\\nimplementation. First, make sure that the hard-coded `input_ids` in both\\nscripts are identical. Next, verify that the outputs of the first\\ntransformation of the `input_ids` (usually the word embeddings) are\\nidentical. And then work your way up to the very last layer of the\\nnetwork. At some point, you will notice a difference between the two\\nimplementations, which should point you to the bug in the 🤗 Transformers\\nimplementation. From our experience, a simple and efficient way is to\\nadd many print statements in both the original implementation and 🤗\\nTransformers implementation, at the same positions in the network\\nrespectively, and to successively remove print statements showing the\\nsame values for intermediate presentions.\\n\\nWhen you're confident that both implementations yield the same output,\\nverifying the outputs with\\n`torch.allclose(original_output, output, atol=1e-3)`, you're done with\\nthe most difficult part! Congratulations - the work left to be done\\nshould be a cakewalk 😊.\\n\\n**8. Adding all necessary model tests**\\n\\nAt this point, you have successfully added a new model. However, it is\\nvery much possible that the model does not yet fully comply with the\\nrequired design. To make sure, the implementation is fully compatible\\nwith 🤗 Transformers, all common tests should pass. The Cookiecutter\\nshould have automatically added a test file for your model, probably\\nunder the same `tests/test_modeling_[lowercase name of model].py`. Run this test\\nfile to verify that all common tests pass:\\n\\n```python\\npytest tests/test_modeling_[lowercase name of model].py\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 41865}, page_content='```\\n\\n[TODO FILL: Here the mentor should add very specific information on what tests are likely to fail after having implemented the model\\n, e.g. given the model, it might be very likely that `test_attention_output` fails]\\n[...]\\n[...]\\n\\nHaving fixed all common tests, it is now crucial to ensure that all the\\nnice work you have done is well tested, so that\\n\\n-   a)  The community can easily understand your work by looking at\\n        specific tests of *[camelcase name of model]*\\n\\n-   b)  Future changes to your model will not break any important\\n        feature of the model.\\n\\nAt first, integration tests should be added. Those integration tests\\nessentially do the same as the debugging scripts you used earlier to\\nimplement the model to 🤗 Transformers. A template of those model tests\\nis already added by the Cookiecutter, called\\n`[camelcase name of model]ModelIntegrationTests` and only has to be filled out by\\nyou. To ensure that those tests are passing, run\\n\\n```python\\nRUN_SLOW=1 pytest -sv tests/test_modeling_[lowercase name of model].py::[camelcase name of model]ModelIntegrationTests'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 42956}, page_content='```\\n\\n**Note:** In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\\n\\nSecond, all features that are special to *[camelcase name of model]* should be\\ntested additionally in a separate test under\\n`[camelcase name of model]ModelTester`/`[camelcase name of model]ModelTest`. This part is often\\nforgotten but is extremely useful in two ways:\\n\\n-   It helps to transfer the knowledge you have acquired during the\\n    model addition to the community by showing how the special features\\n    of *[camelcase name of model]* should work.\\n-   Future contributors can quickly test changes to the model by running\\n    those special tests.\\n\\n[TODO FILL: Here the mentor should add very specific information on what special features of the model should be tested additionally]\\n[...]\\n[...]\\n\\n**9. Implement the tokenizer**\\n\\nNext, we should add the tokenizer of *[camelcase name of model]*. Usually, the\\ntokenizer is equivalent or very similar to an already existing tokenizer\\nof 🤗 Transformers.\\n\\n[TODO FILL: Here the mentor should add a comment whether a new tokenizer is required or if this is not the case which existing tokenizer closest resembles \\n [camelcase name of model]\\'s tokenizer and how the tokenizer should be implemented]\\n [...]\\n [...]\\n\\nIt is very important to find/extract the original tokenizer file and to\\nmanage to load this file into the 🤗 Transformers\\' implementation of the\\ntokenizer.\\n\\nFor [camelcase name of model], the tokenizer files can be found here:\\n- [To be filled out by mentor]\\n\\nand having implemented the 🤗 Transformers\\' version of the tokenizer can be loaded as follows:\\n\\n[To be filled out by mentor]\\n\\nTo ensure that the tokenizer works correctly, it is recommended to first\\ncreate a script in the original repository that inputs a string and\\nreturns the `input_ids`. It could look similar to this (in pseudo-code):\\n\\n```bash\\ninput_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\\nmodel = [camelcase name of model]Model.load_pretrained_checkpoint(\"/path/to/checkpoint/\")\\ninput_ids = model.tokenize(input_str)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 45069}, page_content='```\\n\\nYou might have to take a deeper look again into the original repository\\nto find the correct tokenizer function or you might even have to do\\nchanges to your clone of the original repository to only output the\\n`input_ids`. Having written a functional tokenization script that uses\\nthe original repository, an analogous script for 🤗 Transformers should\\nbe created. It should look similar to this:\\n\\n```python\\nfrom transformers import [camelcase name of model]Tokenizer\\ninput_str = \"This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.\"\\n\\ntokenizer = [camelcase name of model]Tokenizer.from_pretrained(\"/path/to/tokenizer/folder/\")\\n\\ninput_ids = tokenizer(input_str).input_ids'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 45794}, page_content='```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer\\ntest file should also be added.\\n\\n[TODO FILL: Here mentor should point the student to test files of similar tokenizers]\\n\\nAnalogous to the modeling test files of *[camelcase name of model]*, the\\ntokenization test files of *[camelcase name of model]* should contain a couple of\\nhard-coded integration tests.\\n\\n[TODO FILL: Here mentor should again point to an existing similar test of another model that the student can copy & adapt]\\n\\n**10. Run End-to-end integration tests**\\n\\nHaving added the tokenizer, you should also add a couple of end-to-end\\nintegration tests using both the model and the tokenizer to\\n`tests/test_modeling_[lowercase name of model].py` in 🤗 Transformers. Such a test\\nshould show on a meaningful text-to-text sample that the 🤗 Transformers\\nimplementation works as expected. A meaningful text-to-text sample can\\ninclude *e.g.* a source-to-target-translation pair, an\\narticle-to-summary pair, a question-to-answer pair, etc... If none of\\nthe ported checkpoints has been fine-tuned on a downstream task it is\\nenough to simply rely on the model tests. In a final step to ensure that\\nthe model is fully functional, it is advised that you also run all tests\\non GPU. It can happen that you forgot to add some `.to(self.device)`\\nstatements to internal tensors of the model, which in such a test would\\nshow in an error. In case you have no access to a GPU, the Hugging Face\\nteam can take care of running those tests for you.\\n\\n**11. Add Docstring**'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 47306}, page_content=\"**11. Add Docstring**\\n\\nNow, all the necessary functionality for *[camelcase name of model]* is added -\\nyou're almost done! The only thing left to add is a nice docstring and\\na doc page. The Cookiecutter should have added a template file called\\n`docs/source/model_doc/[lowercase name of model].rst` that you should fill out.\\nUsers of your model will usually first look at this page before using\\nyour model. Hence, the documentation must be understandable and concise.\\nIt is very useful for the community to add some *Tips* to show how the\\nmodel should be used. Don't hesitate to ping [name of mentor]\\nregarding the docstrings.\\n\\nNext, make sure that the docstring added to\\n`src/transformers/models/[lowercase name of model]/modeling_[lowercase name of model].py` is\\ncorrect and included all necessary inputs and outputs. It is always to\\ngood to remind oneself that documentation should be treated at least as\\ncarefully as the code in 🤗 Transformers since the documentation is\\nusually the first contact point of the community with the model.\\n\\n**Code refactor**\\n\\nGreat, now you have added all the necessary code for *[camelcase name of model]*.\\nAt this point, you should correct some potential incorrect code style by\\nrunning:\\n\\n```bash\\nmake style\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 48549}, page_content='```\\n\\nand verify that your coding style passes the quality check:\\n\\n```bash\\nmake quality'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 48636}, page_content=\"```\\n\\nThere are a couple of other very strict design tests in 🤗 Transformers\\nthat might still be failing, which shows up in the tests of your pull\\nrequest. This is often because of some missing information in the\\ndocstring or some incorrect naming. [name of mentor] will surely\\nhelp you if you're stuck here.\\n\\nLastly, it is always a good idea to refactor one's code after having\\nensured that the code works correctly. With all tests passing, now it's\\na good time to go over the added code again and do some refactoring.\\n\\nYou have now finished the coding part, congratulation! 🎉 You are\\nAwesome! 😎\\n\\n**12. Upload the models to the model hub**\\n\\nIn this final part, you should convert and upload all checkpoints to the\\nmodel hub and add a model card for each uploaded model checkpoint. You\\nshould work alongside [name of mentor] here to decide on a fitting\\nname for each checkpoint and to get the required access rights to be\\nable to upload the model under the author's organization of\\n*[camelcase name of model]*.\\n\\nIt is worth spending some time to create fitting model cards for each\\ncheckpoint. The model cards should highlight the specific\\ncharacteristics of this particular checkpoint, *e.g.*, On which dataset\\nwas the checkpoint pretrained/fine-tuned on? On what down-stream task\\nshould the model be used? And also include some code on how to correctly\\nuse the model.\\n\\n**13. (Optional) Add notebook**\\n\\nIt is very helpful to add a notebook that showcases in-detail how\\n*[camelcase name of model]* can be used for inference and/or fine-tuned on a\\ndownstream task. This is not mandatory to merge your PR, but very useful\\nfor the community.\\n\\n**14. Submit your finished PR**\\n\\nYou're done programming now and can move to the last step, which is\\ngetting your PR merged into main. Usually, [name of mentor]\\nshould have helped you already at this point, but it is worth taking\\nsome time to give your finished PR a nice description and eventually add\\ncomments to your code, if you want to point out certain design choices\\nto your reviewer.\\n\\n### Share your work!!\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/templates/adding_a_new_model/ADD_NEW_MODEL_PROPOSAL_TEMPLATE.md', 'start_index': 50668}, page_content=\"### Share your work!!\\n\\nNow, it's time to get some credit from the community for your work!\\nHaving completed a model addition is a major contribution to\\nTransformers and the whole NLP community. Your code and the ported\\npre-trained models will certainly be used by hundreds and possibly even\\nthousands of developers and researchers. You should be proud of your\\nwork and share your achievement with the community.\\n\\n**You have made another model that is super easy to access for everyone\\nin the community! 🤯**\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 0}, page_content='--\\ntitle: \"Machine Learning Experts - Lewis Tunstall\"\\nthumbnail: /blog/assets/60_lewis_tunstall_interview/thumbnail.png\\nauthors:\\n- user: britneymuller\\n---\\n\\n# Machine Learning Experts - Lewis Tunstall\\n\\n\\n\\n## 🤗 Welcome to Machine Learning Experts - Lewis Tunstall\\n\\nHey friends! Welcome to Machine Learning Experts. I\\'m your host, Britney Muller and today’s guest is [Lewis Tunstall](https://twitter.com/_lewtun). Lewis is a Machine Learning Engineer at Hugging Face where he works on applying Transformers to automate business processes and solve MLOps challenges.\\n\\nLewis has built ML applications for startups and enterprises in the domains of NLP, topological data analysis, and time series. \\n\\nYou’ll hear Lewis talk about his [new book](https://transformersbook.com/), transformers, large scale model evaluation, how he’s helping ML engineers optimize for faster latency and higher throughput, and more.\\n\\nIn a previous life, Lewis was a theoretical physicist and outside of work loves to play guitar, go trail running, and contribute to open-source projects.\\n\\n<a href=\"https://huggingface.co/support?utm_source=blog&utm_medium=blog&utm_campaign=ml_experts&utm_content=lewis_interview_article\"><img src=\"/blog/assets/60_lewis_tunstall_interview/lewis-cta.png\"></a>\\n\\nVery excited to introduce this fun and brilliant episode to you! Here’s my conversation with Lewis Tunstall:\\n\\n<iframe width=\"100%\" style=\"aspect-ratio: 16 / 9;\"src=\"https://www.youtube.com/embed/igW5VWewuLE\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\n\\n*Note: Transcription has been slightly modified/reformatted to deliver the highest-quality reading experience.*'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': -1}, page_content=\"*Note: Transcription has been slightly modified/reformatted to deliver the highest-quality reading experience.*\\n\\n### Welcome, Lewis! Thank you so much for taking time out of your busy schedule to chat with me today about your awesome work!\\n\\n**Lewis:** Thanks, Britney. It’s a pleasure to be here.\\n\\n### Curious if you can do a brief self-introduction and highlight what brought you to Hugging Face? \\n\\n**Lewis:** What brought me to Hugging Face was transformers. In 2018, I was working with transformers at a startup in Switzerland. My first project was a question answering task where you input some text and train a model to try and find the answer to a question within that text.\\n\\nIn those days the library was called: pytorch-pretrained-bert, it was a very focused code base with a couple of scripts and it was the first time I worked with transformers. I had no idea what was going on so I read the original [‘Attention Is All You Need’](https://arxiv.org/abs/1706.03762) paper but I couldn’t understand it. So I started looking around for other resources to learn from. \\n\\nIn the process, Hugging Face exploded with their library growing into many architectures and I got really excited about contributing to open-source software. So around 2019, I had this kinda crazy idea to write a book about transformers because I felt there was an information gap that was missing. So I partnered up with my friend, [Leandro](https://twitter.com/lvwerra) (von Werra) and we sent [Thom](https://twitter.com/Thom_Wolf) (Wolf) a cold email out of nowhere saying, “Hey we are going to write a book about transformers, are you interested?” and I was expecting no response. But to our great surprise, he responded “Yea, sure let’s have a chat.” and around 1.5 years later this is our book: [NLP with Transformers](https://transformersbook.com/).\\n\\nThis collaboration set the seeds for Leandro and I to eventually join Hugging Face. And I've been here now for around nine months. \\n\\n### That is incredible. How does it feel to have a copy of your book in your hands?\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 3476}, page_content=\"This collaboration set the seeds for Leandro and I to eventually join Hugging Face. And I've been here now for around nine months. \\n\\n### That is incredible. How does it feel to have a copy of your book in your hands? \\n\\n**Lewis:** I have to say, I just became a parent about a year and a half ago and it feels kind of similar to my son being born. You're holding this thing that you created. \\n\\nIt's quite an exciting feeling and so different to actually hold it (compared to reading a PDF). Confirms that it’s actually real and I didn't just dream about it.\\n\\n### Exactly. Congratulations!\\n\\nWant to briefly read one endorsement that I love about this book;\\n\\n“_Complexity made simple. This is a rare and precious book about NLP, transformers, and the growing ecosystem around them, Hugging Face. Whether these are still buzzwords to you or you already have a solid grasp of it all, the authors will navigate you with humor, scientific rigor, and plenty of code examples into the deepest secrets of the coolest technology around. From “off-the-shelf pre-trained” to “from-scratch custom” models, and from performance to missing labels issues, the authors address practically every real-life struggle of an ML engineer and provide state-of-the-art solutions, making this book destined to dictate the standards in the field for years to come._” \\n—Luca Perrozi Ph.D., Data Science and Machine Learning Associate Manager at Accenture.\\n\\nCheckout [Natural Language Processing with Transformers](https://transformersbook.com/).\\n\\n### Can you talk about the work you've done with the transformers library?\\n\\n**Lewis:** One of the things that I experienced in my previous jobs before Hugging Face was there's this challenge in the industry when deploying these models into production; these models are really large in terms of the number of parameters and this adds a lot of complexity to the requirements you might have. \\n\\nSo for example, if you're trying to build a chatbot you need this model to be very fast and responsive. And most of the time these models are a bit too slow if you just take an off-the-shelf model, train it, and then try to integrate it into your application.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 5647}, page_content=\"So what I've been working on for the last few months on the transformers library is providing the functionality to export these models into a format that lets you run them much more efficiently using tools that we have at Hugging Face, but also just general tools in the open-source ecosystem.\\n\\nIn a way, the philosophy of the transformers library is like writing lots of code so that the users don't have to write that code.\\n\\nIn this particular example, what we're talking about is something called the ONNX format. It's a special format that is used in industry where you can basically have a model that's written in PyTorch but you can then convert it to TensorFlow or you can run it on some very dedicated hardware.\\n\\nAnd if you actually look at what's needed to make this conversion happen in the transformers library, it's fairly gnarly. But we make it so that you only really have to run one line of code and the library will take care of you. \\n\\nSo the idea is that this particular feature lets machine learning engineers or even data scientists take their model, convert it to this format, and then optimize it to get faster latency and higher throughput. \\n\\n### That's very cool. Have there been, any standout applications of transformers?\\n\\n**Lewis:** I think there are a few. One is maybe emotional or personal, for example many of us when OpenAI released GPT-2, this very famous language model which can generate text.\\n\\nOpenAI actually provided in their blog posts some examples of the essays that this model had created. And one of them was really funny. One was an essay about why we shouldn't recycle or why recycling is bad.\\n\\nAnd the model wrote a compelling essay on why recycling was bad. Leandro and I were working at a startup at the time and I printed it out and stuck it right above the recycling bin in the office as a joke. And people were like, “Woah, who wrote this?” and I said, “An algorithm.”\\n\\nI think there's something sort of strangely human, right? Where if we see generated text we get more surprised when it looks like something I (or another human) might have written versus other applications that have been happening like classifying text or more conventional tasks.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 7849}, page_content=\"### That's incredible. I remember when they released those examples for GPT-2, and one of my favorites (that almost gave me this sense of, whew, we're not quite there yet) were some of the more inaccurate mentions like “underwater fires”.\\n\\n**Lewis:** Exactly!\\n\\n**Britney:** But, then something had happened with an oil spill that next year, where there were actually fires underwater! And I immediately thought about that text and thought, maybe AI is onto something already that we're not quite aware of?\\n\\n### You and other experts at Hugging Face have been working hard on the Hugging Face Course. How did that come about & where is it headed? \\n\\n**Lewis:** When I joined Hugging Face, [Sylvian](https://twitter.com/GuggerSylvain) and [Lysandre](https://twitter.com/LysandreJik), two of the core maintainers of the transformers library, were developing a course to basically bridge the gap between people who are more like software engineers who are curious about natural language processing but specifically curious about the transformers revolution that's been happening. So I worked with them and others in the open-source team to create a free course called the [Hugging Face Course](https://huggingface.co/course/chapter1/1). And this course is designed to really help people go from knowing kind of not so much about ML all the way through to having the ability to train models on many different tasks.\\n\\nAnd, we've released two parts of this course and planning to release the third part this year. I'm really excited about the next part that we're developing right now where we're going to explore different modalities where transformers are really powerful. Most of the time we think of transformers for NLP, but likely there's been this explosion where transformers are being used in things like audio or in computer vision and we're going to be looking at these in detail. \\n\\n### What are some transformers applications that you're excited about? \\n\\n**Lewis:** So one that's kind of fun is in the course we had an event last year where we got people in the community to use the course material to build applications.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': -1}, page_content=\"**Lewis:** So one that's kind of fun is in the course we had an event last year where we got people in the community to use the course material to build applications.\\n\\nAnd one of the participants in this event created a cover letter generator for jobs. So the idea is that when you apply for a job there's always this annoying thing you have to write a cover letter and it's always like a bit like you have to be witty. So this guy created a cover letter generator where you provide some information about yourself and then it generates it from that.\\n\\nAnd he actually used that to apply to Hugging Face.\\n\\n### No way?!\\n\\n**Lewis:** He's joining the Big Science team as an intern. So. I mean this is a super cool thing, right? When you learn something and then use that thing to apply which I thought was pretty awesome. \\n\\n### Where do you want to see more ML applications?\\n\\n**Lewis:** So I think personally, the area that I'm most excited about is the application of machine learning into natural sciences. And that's partly because of my background. I used to be a Physicist in a previous lifetime but I think what's also very exciting here is that in a lot of fields. For example, in physics or chemistry you already know what the say underlying laws are in terms of equations that you can write down but it turns out that many of the problems that you're interested in studying often require a simulation. Or they often require very hardcore supercomputers to understand and solve these equations. And one of the most exciting things to me is the combination of deep learning with the prior knowledge that scientists have gathered to make breakthroughs that weren't previously possible.\\n\\nAnd I think a great example is [DeepMind’s Alpha Fold](https://www.deepmind.com/research/highlighted-research/alphafold) model for protein structure prediction where they were basically using a combination of transformers with some extra information to generate predictions of proteins that I think previously were taking on the order of months and now they can do them in days.\\n\\nSo this accelerates the whole field in a really powerful way. And I can imagine these applications ultimately lead to hopefully a better future for humanity.\\n\\n### How you see the world of model evaluation evolving?\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 11877}, page_content=\"So this accelerates the whole field in a really powerful way. And I can imagine these applications ultimately lead to hopefully a better future for humanity.\\n\\n### How you see the world of model evaluation evolving?\\n\\n**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.\\n\\nAnd at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.\\n\\nSo one thing that we've been working on is to develop a way that you can evaluate models and datasets directly through the Hub. We're still trying to experiment there with the direction. But I'm hoping that we have something cool to show later this year. \\n\\nAnd there's another side to this which is that a large part of the measuring progress in machine learning is through the use of benchmarks. These benchmarks are traditionally a set of datasets with some tasks but what's been maybe missing is that a lot of researchers speak to us and say, “Hey, I've got this cool idea for a benchmark, but I don't really want to implement all of the nitty-gritty infrastructure for the submissions, and the maintenance, and all those things.”\\n\\nAnd so we've been working with some really cool partners on hosting benchmarks on the Hub directly. So that then people in the research community can use the tooling that we have and then simplify the evaluation of these models. \\n\\n### That is super interesting and powerful.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 13842}, page_content=\"### That is super interesting and powerful.\\n\\n**Lewis:** Maybe one thing to mention is that the whole evaluation question is a very subtle one. We know from previous benchmarks, such as SQuAD, a famous benchmark to measure how good models are at question answering, that many of these transformer models are good at taking shortcuts.\\n\\nWell, that's the aim but it turns out that many of these transformer models are really good at taking shortcuts. So, what they’re actually doing is they're getting a very high score on a benchmark which doesn't necessarily translate into the actual thing you were interested in which was answering questions.\\n\\nAnd you have all these subtle failure modes where the models will maybe provide completely wrong answers or they should not even answer at all. And so at the moment in the research community there's a very active and vigorous discussion about what role benchmarks play in the way we measure progress.\\n\\nBut also, how do these benchmarks encode our values as a community? And one thing that I think Hugging Face can really offer the community here is the means to diversify the space of values because traditionally most of these research papers come from the U.S. which is a great country but it's a small slice of the human experience, right?\\n\\n### What are some common mistakes machine learning engineers or teams make?\\n\\n**Lewis:** I can maybe tell you the ones that I've done.\\n\\nProbably a good representative of the rest of the things. So I think the biggest lesson I learned when I was starting out in the field is using baseline models when starting out. It’s a common problem that I did and then later saw other junior engineers doing is reaching for the fanciest state-of-the-art model.\\n\\nAlthough that may work, a lot of the time what happens is you introduce a lot of complexity into the problem and your state-of-the-art model may have a bug and you won't really know how to fix it because the model is so complex. It’s a very common pattern in industry and especially within NLP is that you can actually get quite far with regular expressions and linear models like logistic regression and these kinds of things will give you a good start. Then if you can build a better model then great, you should do that, but it's great to have a reference point.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 16147}, page_content=\"And then I think the second big lesson I’ve learned from building a lot of projects is that you can get a bit obsessed with the modeling part of the problem because that's the exciting bit when you're doing machine learning but there's this whole ecosystem. Especially if you work in a large company there'll be this whole ecosystem of services and things that are around your application. \\n\\nSo the lesson there is you should really try to build something end to end that maybe doesn't even have any machine learning at all. But it's the scaffolding upon which you can build the rest of the system because you could spend all this time training an awesome mode, and then you go, oh, oops.\\n\\nIt doesn't integrate with the requirements we have in our application. And then you've wasted all this time. \\n\\n### That's a good one! Don't over-engineer. Something I always try to keep in mind. \\n\\n**Lewis:** Exactly. And it's a natural thing I think as humans especially if you're nerdy you really want to find the most interesting way to do something and most of the time simple is better.\\n\\n### If you could go back and do one thing differently at the beginning of your career in machine learning, what would it be? \\n\\n**Lewis:** Oh, wow. That's a tough one. Hmm. So, the reason this is a really hard question to answer is that now that I’m working at  Hugging Face, it's the most fulfilling type of work that I've really done in my whole life. And the question is if I changed something when I started out maybe I wouldn't be here, right? \\n\\nIt's one of those things where it's a tricky one in that sense. I suppose one thing that maybe I would've done slightly differently is when I started out working as a data scientist you tend to develop the skills which are about mapping business problems to software problems or ultimately machine learning problems.\\n\\nAnd this is a really great skill to have. But what I later discovered is that my true driving passion is doing open source software development. So probably the thing I would have done differently would have been to start that much earlier. Because at the end of the day most open source is really driven by community members.\\n\\nSo that would have been maybe a way to shortcut my path to doing this full-time.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': -1}, page_content=\"So that would have been maybe a way to shortcut my path to doing this full-time. \\n\\n### I love the idea of had you done something differently maybe you wouldn't be at Hugging Face. \\n\\n**Lewis:** It’s like the butterfly effect movie, right? You go back in time and then you don't have any legs or something.\\n\\n### Totally. Don't want to mess with a good thing!\\n\\n**Lewis:** Exactly.\\n\\n### Rapid Fire Questions:\\n\\n### Best piece of advice for someone looking to get into AI/Machine Learning?\\n\\n**Lewis:** Just start. Just start coding. Just start contributing if you want to do open-source. You can always find reasons not to do it but you just have to get your hands dirty.\\n\\n### What are some of the industries you're most excited to see machine learning applied? \\n\\n**Lewis:** As I mentioned before, I think the natural sciences is the area I’m most excited about\\n\\nThis is where I think that's most exciting. If we look at something, say at the industrial side, I guess some of the development of new drugs through machine learning is very exciting. Personally, I'd be really happy if there were advancements in robotics where I could finally have a robot to like fold my laundry because I really hate doing this and it would be nice if like there was an automated way of handling that. \\n\\n### Should people be afraid of AI taking over the world?\\n\\n**Lewis:** Maybe. It’s a tough one because I think we have reasons to think that we may create systems that are quite dangerous in the sense that they could be used to cause a lot of harm. An analogy is perhaps with weapons you can use within the sports like archery and shooting, but you can also use them for war. One big risk is probably if we think about combining these techniques with the military perhaps this leads to some tricky situations.\\n\\nBut, I'm not super worried about the Terminator. I'm more worried about, I don't know, a rogue agent on the financial stock market bankrupting the whole world. \\n\\n### That's a good point.\\n\\n**Lewis:** Sorry, that's a bit dark.\\n\\n### No, that was great. The next question is a follow-up on your folding laundry robot. When will AI-assisted robots be in homes everywhere?\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 20303}, page_content=\"**Lewis:** Sorry, that's a bit dark.\\n\\n### No, that was great. The next question is a follow-up on your folding laundry robot. When will AI-assisted robots be in homes everywhere?\\n\\n**Lewis:** Honest answer. I don't know. Everyone, I know who's working on robotics says this is still an extremely difficult task in the sense that robotics hasn't quite experienced the same kind of revolutions that NLP and deep learning have had. But on the other hand, you can see some pretty exciting developments in the last year, especially around the idea of being able to transfer knowledge from a simulation into the real world.\\n\\nI think there's hope that in my lifetime I will have a laundry-folding robot.\\n\\n### What have you been interested in lately? It could be a movie, a recipe, a podcast, literally anything. And I'm just curious what that is and how someone interested in that might find it or get started. \\n\\n**Lewis:** It's a great question. So for me, I like podcasts in general. It’s my new way of reading books because I have a young baby so I'm just doing chores and listening at the same time. \\n\\nOne podcast that really stands out recently is actually the [DeepMind podcast](https://www.deepmind.com/the-podcast) produced by Hannah Fry who's a mathematician in the UK and she gives this beautiful journey through not just what Deep Mind does, but more generally, what deep learning and especially reinforcement learning does and how they're impacting the world. Listening to this podcast feels like you're listening to like a BBC documentary because you know the English has such great accents and you feel really inspired because a lot of the work that she discusses in this podcast has a strong overlap with what we do at Hugging Face. You see this much bigger picture of trying to pave the way for a better future.\\n\\nIt resonated strongly. And I just love it because the explanations are super clear and you can share it with your family and your friends and say, “Hey, if you want to know what I'm doing? This can give you a rough idea.”\\n\\nIt gives you a very interesting insight into the Deep Mind researchers and their backstory as well.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': -1}, page_content=\"It gives you a very interesting insight into the Deep Mind researchers and their backstory as well.\\n\\n### I'm definitely going to give that a listen. [Update: It’s one of my new favorite podcasts. :) Thank you, Lewis!]\\n\\n### What are some of your favorite Machine Learning papers?\\n\\n**Lewis:** Depends on how we measure this, but there's [one paper that stands out to me, which is quite an old paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf). It’s by the creator of random forests, Leo Breiman. Random forests is a very famous classic machine learning technique that's useful for tabular data that you see in industry and I had to teach random forests at university a year ago.\\n\\nAnd I was like, okay, I'll read this paper from the 2000s and see if I understand it. And it's a model of clarity. It's very short, and very clearly explains how the algorithm is implemented. You can basically just take this paper and implement the code very very easily. And that to me was a really nice example of how papers were written in medieval times. \\n\\nWhereas nowadays, most papers, have this formulaic approach of, okay, here's an introduction, here's a table with some numbers that get better, and here's like some random related work section. So, I think that's one that like stands out to me a lot.\\n\\nBut another one that's a little bit more recent is [a paper by DeepMind](https://www.nature.com/articles/d41586-021-03593-1) again on using machine learning techniques to prove fundamental theorems like algebraic topology, which is a special branch of abstract mathematics. And at one point in my life, I used to work on these related topics.\\n\\nSo, to me, it's a very exciting, perspective of augmenting the knowledge that a mathematician would have in trying to narrow down the space of theorems that they might have to search for. I think this to me was surprising because a lot of the time I've been quite skeptical that machine learning will lead to this fundamental scientific insight beyond the obvious ones like making predictions.\\n\\nBut this example showed that you can actually be quite creative and help mathematicians find new ideas.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 24395}, page_content=\"But this example showed that you can actually be quite creative and help mathematicians find new ideas. \\n\\n### What is the meaning of life?\\n\\n**Lewis:** I think that the honest answer is, I don't know. And probably anyone who does tell you an answer probably is lying. That's a bit sarcastic. I dunno, I guess being a site scientist by training and especially a physicist, you develop this worldview that is very much that there isn't really some sort of deeper meaning to this.\\n\\nIt's very much like the universe is quite random and I suppose the only thing you can take from that beyond being very sad is that you derive your own meaning, right? And most of the time this comes either from the work that you do or from the family or from your friends that you have.\\n\\nBut I think when you find a way to derive your own meaning and discover what you do is actually interesting and meaningful that that's the best part. Life is very up and down, right? At least for me personally, the things that have always been very meaningful are generally in creating things. So, I used to be a musician, so that was a way of creating music for other people and there was great pleasure in doing that. And now I kind of, I guess, create code which is a form of creativity.\\n\\n### Absolutely. I think that's beautiful, Lewis! Is there anything else you would like to share or mention before we sign off? \\n\\n**Lewis:** Maybe [buy my book](https://transformersbook.com/).  \\n\\n### It is so good!\\n\\n**Lewis:** [shows book featuring a parrot on the cover] Do you know the story about the parrot?  \\n\\n### I don't think so.\\n\\n**Lewis:** So when O’Reilly is telling you “We're going to get our illustrator now to design the cover,” it's a secret, right?\\n\\nThey don't tell you what the logic is or you have no say in the matter. So, basically, the illustrator comes up with an idea and in one of the last chapters of the book we have a section where we basically train a GPT-2 like model on Python code, this was Thom's idea, and he decided to call it code parrot.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 26427}, page_content=\"I think the idea or the joke he had was that there's a lot of discussion in the community about this paper that Meg Mitchell and others worked on called, ‘Stochastic Parrots’. And the idea was that you have these very powerful language models which seem to exhibit human-like traits in their writing as we discussed earlier but deep down maybe they're just doing some sort of like parrot parenting thing.\\n\\nYou know, if you talk to like a cockatoo it will swear at you or make jokes. That may not be a true measure of intelligence, right? So I think that the illustrator somehow maybe saw that and decided to put a parrot which I think is a perfect metaphor for the book.\\n\\nAnd the fact that there are transformers in it.\\n\\n### Had no idea that that was the way O'Reilly's covers came about. They don't tell you and just pull context from the book and create something?\\n\\n**Lewis:** It seems like it. I mean, we don't really know the process. I'm just sort of guessing that maybe the illustrator was trying to get an idea and saw a few animals in the book. In one of the chapters we have a discussion about giraffes and zebras and stuff. But yeah I'm happy with the parrot cover.\\n\\n### I love it. Well, it looks absolutely amazing. A lot of these types of books tend to be quite dry and technical and this one reads almost like a novel mixed with great applicable technical information, which is beautiful.\\n\\n**Lewis:** Thanks. Yeah, that’s one thing we realized afterward because it was the first time we were writing a book we thought we should be sort of serious, right? But if you sort of know me I'm like never really serious about anything. And in hindsight, we should have been even more silly in the book.\\n\\nI had to control my humor in various places but maybe there'll be a second edition one day and then we can just inject it with memes.\\n\\n### Please do, I look forward to that!\\n\\n**Lewis:** In fact, there is one meme in the book. We tried to sneak this in past the Editor and have the DOGE dog inside the book and we use a special vision transformer to try and classify what this meme is.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/lewis-tunstall-interview.md', 'start_index': 28522}, page_content='### So glad you got that one in there. Well done! Look forward to many more in the next edition. Thank you so much for joining me today. I really appreciate it. Where can our listeners find you online?\\n\\n**Lewis:** I\\'m fairly active on Twitter. You can just find me my handle [@_lewtun](https://twitter.com/_lewtun). LinkedIn is a strange place and I\\'m not really on there very much. And of course, there\\'s [Hugging Face](https://huggingface.co/lewtun), the [Hugging Face Forums](https://discuss.huggingface.co/), and [Discord](https://discuss.huggingface.co/t/join-the-hugging-face-discord/11263).\\n\\n### Perfect. Thank you so much, Lewis. And I\\'ll chat with you soon!\\n\\n**Lewis:** See ya, Britney. Bye.\\n\\nThank you for listening to Machine Learning Experts!\\n\\n<a href=\"https://huggingface.co/support?utm_source=blog&utm_medium=blog&utm_campaign=ml_experts&utm_content=lewis_interview_article\"><img src=\"/blog/assets/60_lewis_tunstall_interview/lewis-cta.png\"></a>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': 0}, page_content='--\\ntitle: \\'Welcome fastai to the Hugging Face Hub\\'\\nthumbnail: /blog/assets/64_fastai/fastai_hf_blog.png\\nauthors:\\n- user: espejelomar\\n---\\n\\n# Welcome fastai to the Hugging Face Hub\\n\\n\\n## Making neural nets uncool again... and sharing them\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/64_fastai_hub.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\nFew have done as much as the [fast.ai](https://www.fast.ai/) ecosystem to make Deep Learning accessible. Our mission at Hugging Face is to democratize good Machine Learning. Let\\'s make exclusivity in access to Machine Learning, including [pre-trained models](https://huggingface.co/models), a thing of the past and let\\'s push this amazing field even further.\\n\\nfastai is an [open-source Deep Learning library](https://github.com/fastai/fastai) that leverages PyTorch and Python to provide high-level components to train fast and accurate neural networks with state-of-the-art outputs on text, vision, and tabular data. However, fast.ai, the company, is more than just a library; it has grown into a thriving ecosystem of open source contributors and people learning about neural networks. As some examples, check out their [book](https://github.com/fastai/fastbook) and [courses](https://course.fast.ai/). Join the fast.ai [Discord](https://discord.com/invite/YKrxeNn) and [forums](https://forums.fast.ai/). It is a guarantee that you will learn by being part of their community!'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': 1542}, page_content=\"Because of all this, and more (the writer of this post started his journey thanks to the fast.ai course), we are proud to announce that fastai practitioners can now share and upload models to Hugging Face Hub with a single line of Python.\\n\\n 👉 In this post, we will introduce the integration between fastai and the Hub. Additionally, you can open this tutorial as a [Colab notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/64_fastai_hub.ipynb).\\n\\nWe want to thank the fast.ai community, notably [Jeremy Howard](https://twitter.com/jeremyphoward), [Wayde Gilliam](https://twitter.com/waydegilliam), and [Zach Mueller](https://twitter.com/TheZachMueller) for their feedback 🤗. This blog is heavily inspired by the [Hugging Face Hub section](https://docs.fast.ai/huggingface.html) in the fastai docs.\\n\\n\\n## Why share to the Hub?\\n\\nThe Hub is a central platform where anyone can share and explore models, datasets, and ML demos. It has the most extensive collection of Open Source models, datasets, and demos.\\n\\nSharing on the Hub amplifies the impact of your fastai models by making them available for others to download and explore. You can also use transfer learning with fastai models; load someone else's model as the basis for your task.\\n\\nAnyone can access all the fastai models in the Hub by filtering the [hf.co/models](https://huggingface.co/models?library=fastai&sort=downloads) webpage by the fastai library, as in the image below.\\n\\n![Fastai Models in the Hub](assets/64_fastai/hf_hub_fastai.png)\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': -1}, page_content='![Fastai Models in the Hub](assets/64_fastai/hf_hub_fastai.png)\\n\\nIn addition to free model hosting and exposure to the broader community, the Hub has built-in [version control based on git](https://huggingface.co/docs/transformers/model_sharing#repository-features) (git-lfs, for large files) and [model cards](https://huggingface.co/docs/hub/models-cards) for discoverability and reproducibility. For more information on navigating the Hub, see [this introduction](https://github.com/huggingface/education-toolkit/blob/main/01_huggingface-hub-tour.md).\\n\\n\\n\\n## Joining Hugging Face and installation\\n\\nTo share models in the Hub, you will need to have a user. Create it on the [Hugging Face website](https://huggingface.co/join).\\n\\nThe `huggingface_hub` library is a lightweight Python client with utility functions to interact with the Hugging Face Hub. To push fastai models to the hub, you need to have some libraries pre-installed (fastai>=2.4, fastcore>=1.3.27 and toml). You can install them automatically by specifying [\"fastai\"] when installing `huggingface_hub`, and your environment is good to go:\\n\\n```bash\\npip install huggingface_hub[\"fastai\"]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': 4166}, page_content=\"```\\n\\n## Creating a fastai `Learner`\\n\\nHere we train the [first model in the fastbook](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb) to identify cats 🐱. We fully recommended reading the entire fastbook.\\n\\n```py\\n# Training of 6 lines in chapter 1 of the fastbook.\\nfrom fastai.vision.all import *\\npath = untar_data(URLs.PETS)/'images'\\n\\ndef is_cat(x): return x[0].isupper()\\ndls = ImageDataLoaders.from_name_func(\\n    path, get_image_files(path), valid_pct=0.2, seed=42,\\n    label_func=is_cat, item_tfms=Resize(224))\\n\\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\\nlearn.fine_tune(1)\\n```\\n\\n\\n## Sharing a `Learner` to the Hub\\n\\nA [`Learner` is a fastai object](https://docs.fast.ai/learner.html#Learner) that bundles a model, data loaders, and a loss function. We will use the words `Learner` and Model interchangeably throughout this post.\\n\\nFirst, log in to the Hugging Face Hub. You will need to create a `write` token in your [Account Settings](http://hf.co/settings/tokens). Then there are three options to log in:\\n\\n1. Type `huggingface-cli login` in your terminal and enter your token.\\n\\n2. If in a python notebook, you can use `notebook_login`.\\n\\n```py\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': 5402}, page_content='```\\n\\n3. Use the `token` argument of the `push_to_hub_fastai` function.\\n\\nYou can input `push_to_hub_fastai` with the `Learner` you want to upload and the repository id for the Hub in the format of \"namespace/repo_name\". The namespace can be an individual account or an organization you have write access to (for example, \\'fastai/stanza-de\\'). For more details, refer to the [Hub Client documentation](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/mixins#huggingface_hub.push_to_hub_fastai).\\n\\n```py\\nfrom huggingface_hub import push_to_hub_fastai\\n\\n# repo_id = \"YOUR_USERNAME/YOUR_LEARNER_NAME\"\\nrepo_id = \"espejelomar/identify-my-cat\"\\n\\npush_to_hub_fastai(learner=learn, repo_id=repo_id)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': 6108}, page_content='```\\n\\nThe `Learner` is now in the Hub in the repo named [`espejelomar/identify-my-cat`](https://huggingface.co/espejelomar/identify-my-cat). An automatic model card is created with some links and next steps. When uploading a fastai `Learner` (or any other model) to the Hub, it is helpful to edit its model card (image below) so that others better understand your work (refer to the [Hugging Face documentation](https://huggingface.co/docs/hub/models-cards)).\\n\\n![Fastai Model Card](assets/64_fastai/hf_model_card.png)\\n\\nif you want to learn more about `push_to_hub_fastai` go to the [Hub Client Documentation](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/mixins#huggingface_hub.from_pretrained_fastai). There are some cool arguments you might be interested in 👀. Remember, your model is a [Git repository](https://huggingface.co/docs/transformers/model_sharing#repository-features) with all the advantages that this entails: version control, commits, branches...\\n\\n## Loading a `Learner` from the Hugging Face Hub\\n\\nLoading a model from the Hub is even simpler. We will load our `Learner`, \"espejelomar/identify-my-cat\", and test it with a cat image (🦮?). This code is adapted from\\nthe [first chapter of the fastbook](https://github.com/fastai/fastbook/blob/master/01_intro.ipynb).\\n\\nFirst, upload an image of a cat (or possibly a dog?). The [Colab notebook with this tutorial](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/64_fastai_hub.ipynb) uses `ipywidgets` to interactively upload a cat image (or not?). Here we will use this cute cat 🐅:'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': 7705}, page_content='![Fastai Model Card](assets/64_fastai/cat.jpeg)\\n\\nNow let\\'s load the `Learner` we just shared in the Hub and test it.\\n\\n```py\\nfrom huggingface_hub import from_pretrained_fastai\\n\\n# repo_id = \"YOUR_USERNAME/YOUR_LEARNER_NAME\"\\nrepo_id = \"espejelomar/identify-my-cat\"\\n\\nlearner = from_pretrained_fastai(repo_id)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': 8010}, page_content='```\\nIt works 👇!\\n\\n```py\\n_,_,probs = learner.predict(img)\\nprint(f\"Probability it\\'s a cat: {100*probs[1].item():.2f}%\")\\n\\nProbability it\\'s a cat: 100.00%\\n```\\n\\nThe [Hub Client documentation](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/mixins#huggingface_hub.from_pretrained_fastai) includes addtional details on `from_pretrained_fastai`.\\n\\n\\n## `Blurr` to mix fastai and Hugging Face Transformers (and share them)!\\n\\n> [Blurr is] a library designed for fastai developers who want to train and deploy Hugging Face transformers - [Blurr Docs](https://github.com/ohmeow/blurr).\\n\\nWe will:\\n1. Train a `blurr` Learner with the [high-level Blurr API](https://github.com/ohmeow/blurr#using-the-high-level-blurr-api). It will load the `distilbert-base-uncased` model from the Hugging Face Hub and prepare a sequence classification model.\\n2. Share it to the Hub with the namespace `fastai/blurr_IMDB_distilbert_classification` using `push_to_hub_fastai`.\\n3. Load it with `from_pretrained_fastai` and try it with `learner_blurr.predict()`.\\n\\nCollaboration and open-source are fantastic!\\n\\nFirst, install `blurr` and train the Learner.\\n\\n```bash\\ngit clone https://github.com/ohmeow/blurr.git\\ncd blurr\\npip install -e \".[dev]\"'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': 9238}, page_content='```\\n\\n```python\\nimport torch\\nimport transformers\\nfrom fastai.text.all import *\\n\\nfrom blurr.text.data.all import *\\nfrom blurr.text.modeling.all import *\\n\\npath = untar_data(URLs.IMDB_SAMPLE)\\nmodel_path = Path(\"models\")\\nimdb_df = pd.read_csv(path / \"texts.csv\")\\n\\nlearn_blurr = BlearnerForSequenceClassification.from_data(imdb_df, \"distilbert-base-uncased\", dl_kwargs={\"bs\": 4})\\nlearn_blurr.fit_one_cycle(1, lr_max=1e-3)\\n```\\n\\nUse `push_to_hub_fastai` to share with the Hub.\\n\\n```python\\nfrom huggingface_hub import push_to_hub_fastai\\n\\n# repo_id = \"YOUR_USERNAME/YOUR_LEARNER_NAME\"\\nrepo_id = \"fastai/blurr_IMDB_distilbert_classification\"\\n\\npush_to_hub_fastai(learn_blurr, repo_id)\\n```\\n\\nUse `from_pretrained_fastai` to load a `blurr` model from the Hub.\\n\\n\\n```python\\nfrom huggingface_hub import from_pretrained_fastai\\n\\n# repo_id = \"YOUR_USERNAME/YOUR_LEARNER_NAME\"\\nrepo_id = \"fastai/blurr_IMDB_distilbert_classification\"\\n\\nlearner_blurr = from_pretrained_fastai(repo_id)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': 10197}, page_content='```\\n\\nTry it with a couple sentences and review their sentiment (negative or positive) with `learner_blurr.predict()`.\\n\\n```python\\nsentences = [\"This integration is amazing!\",\\n             \"I hate this was not available before.\"]\\n\\nprobs = learner_blurr.predict(sentences)\\n\\nprint(f\"Probability that sentence \\'{sentences[0]}\\' is negative is: {100*probs[0][\\'probs\\'][0]:.2f}%\")\\nprint(f\"Probability that sentence \\'{sentences[1]}\\' is negative is: {100*probs[1][\\'probs\\'][0]:.2f}%\")\\n```\\nAgain, it works!\\n\\n```python\\nProbability that sentence \\'This integration is amazing!\\' is negative is: 29.46%\\nProbability that sentence \\'I hate this was not available before.\\' is negative is: 70.04%'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/fastai.md', 'start_index': -1}, page_content=\"```\\nAgain, it works!\\n\\n```python\\nProbability that sentence 'This integration is amazing!' is negative is: 29.46%\\nProbability that sentence 'I hate this was not available before.' is negative is: 70.04%\\n```\\n\\n\\n## What's next?\\n\\nTake the [fast.ai course](https://course.fast.ai/) (a new version is coming soon), follow [Jeremy Howard](https://twitter.com/jeremyphoward?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) and [fast.ai](https://twitter.com/FastDotAI) on Twitter for updates, and start sharing your fastai models on the Hub 🤗. Or load one of the [models that are already in the Hub](https://huggingface.co/models?library=fastai&sort=downloads).\\n\\n📧 Feel free to contact us via the [Hugging Face Discord](https://discord.gg/YRAq8fMnUG) and share if you have an idea for a project. We would love to hear your feedback 💖.\\n\\n\\n### Would you like to integrate your library to the Hub?\\n\\nThis integration is made possible by the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library. If you want to add your library to the Hub, we have a [guide](https://huggingface.co/docs/hub/models-adding-libraries) for you! Or simply tag someone from the Hugging Face team.\\n\\nA shout out to the Hugging Face team for all the work on this integration, in particular [@osanseviero](https://twitter.com/osanseviero) 🦙.\\n\\nThank you fastlearners and hugging learners 🤗.\"),\n",
       " Document(metadata={'source': 'huggingface/optimum/blob/main/docs/source/utils/dummy_input_generators.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Dummy Input Generators\\n\\nIt is very common to have to generate dummy inputs to perform a task (tracing, exporting a model to some backend,\\ntesting model outputs, etc). The goal of [`~optimum.utils.input_generators.DummyInputGenerator`] classes is to make this\\ngeneration easy and re-usable.\\n\\n\\n## Base class\\n\\n[[autodoc]] optimum.utils.input_generators.DummyInputGenerator\\n\\n\\n## Existing dummy input generators\\n\\n[[autodoc]] optimum.utils.input_generators.DummyTextInputGenerator\\n\\n[[autodoc]] optimum.utils.input_generators.DummyDecoderTextInputGenerator\\n\\n[[autodoc]] optimum.utils.input_generators.DummyPastKeyValuesGenerator\\n\\n[[autodoc]] optimum.utils.input_generators.DummySeq2SeqPastKeyValuesGenerator\\n\\n[[autodoc]] optimum.utils.input_generators.DummyBboxInputGenerator\\n\\n[[autodoc]] optimum.utils.input_generators.DummyVisionInputGenerator\\n\\n[[autodoc]] optimum.utils.input_generators.DummyAudioInputGenerator'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/source/package_reference/auto_class.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# AutoPeftModels\\n\\nThe `AutoPeftModel` classes loads the appropriate PEFT model for the task type by automatically inferring it from the configuration file. They are designed to quickly and easily load a PEFT model in a single line of code without having to worry about which exact model class you need or manually loading a [`PeftConfig`].\\n\\n## AutoPeftModel\\n\\n[[autodoc]] auto.AutoPeftModel\\n    - from_pretrained\\n\\n## AutoPeftModelForCausalLM\\n\\n[[autodoc]] auto.AutoPeftModelForCausalLM\\n\\n## AutoPeftModelForSeq2SeqLM\\n\\n[[autodoc]] auto.AutoPeftModelForSeq2SeqLM\\n\\n## AutoPeftModelForSequenceClassification\\n\\n[[autodoc]] auto.AutoPeftModelForSequenceClassification\\n\\n## AutoPeftModelForTokenClassification\\n\\n[[autodoc]] auto.AutoPeftModelForTokenClassification\\n\\n## AutoPeftModelForQuestionAnswering\\n\\n[[autodoc]] auto.AutoPeftModelForQuestionAnswering\\n\\n## AutoPeftModelForFeatureExtraction\\n\\n[[autodoc]] auto.AutoPeftModelForFeatureExtraction'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 0}, page_content='--\\ntitle: \"StackLLaMA: A hands-on guide to train LLaMA with RLHF\" \\nthumbnail: /blog/assets/138_stackllama/thumbnail.png\\nauthors:\\n- user: edbeeching\\n- user: kashif\\n- user: ybelkada\\n- user: lewtun\\n- user: lvwerra\\n- user: nazneen\\n- user: natolambert\\n---\\n\\n# StackLLaMA: A hands-on guide to train LLaMA with RLHF\\n\\n\\nModels such as [ChatGPT]([https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)), [GPT-4]([https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)), and [Claude]([https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude)) are powerful language models that have been fine-tuned using a method called Reinforcement Learning from Human Feedback (RLHF) to be better aligned with how we expect them to behave and would like to use them.\\n\\nIn this blog post, we show all the steps involved in training a [LlaMa model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai) to answer questions on [Stack Exchange](https://stackexchange.com) with RLHF through a combination of:\\n\\n- Supervised Fine-tuning (SFT)\\n- Reward / preference modeling (RM)\\n- Reinforcement Learning from Human Feedback (RLHF)\\n\\n![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/instructGPT.png)\\n*From InstructGPT paper: Ouyang, Long, et al. \"Training language models to follow instructions with human feedback.\" arXiv preprint arXiv:2203.02155 (2022).*'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 1459}, page_content='By combining these approaches, we are releasing the StackLLaMA model. This model is available on the [🤗 Hub](https://huggingface.co/trl-lib/llama-se-rl-peft) (see [Meta\\'s LLaMA release](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) for the original LLaMA model) and [the entire training pipeline](https://huggingface.co/docs/trl/index) is available as part of the Hugging Face TRL library. To give you a taste of what the model can do, try out the demo below!\\n\\n<script\\n\\ttype=\"module\"\\n\\tsrc=\"https://gradio.s3-us-west-2.amazonaws.com/3.23.0/gradio.js\"></script>\\n\\n<gradio-app theme_mode=\"light\" src=\"https://trl-lib-stack-llama.hf.space\"></gradio-app>\\n\\n## The LLaMA model\\n\\nWhen doing RLHF, it is important to start with a capable model: the RLHF step is only a fine-tuning step to align the model with how we want to interact with it and how we expect it to respond.  Therefore, we choose to use the recently introduced and performant [LLaMA models](https://arxiv.org/abs/2302.13971). The LLaMA models are the latest large language models developed by Meta AI. They come in sizes ranging from 7B to 65B parameters and were trained on between 1T and 1.4T tokens, making them very capable. We use the 7B model as the base for all the following steps!\\nTo access the model, use the [form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) from Meta AI.\\n\\n## Stack Exchange dataset'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 2877}, page_content='## Stack Exchange dataset\\n\\nGathering human feedback is a complex and expensive endeavor. In order to bootstrap the process for this example while still building a useful model, we make use of the [StackExchange dataset](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences). The dataset includes questions and their corresponding answers from the StackExchange platform (including StackOverflow for code and many other topics). It is attractive for this use case because the answers come together with the number of upvotes and a label for the accepted answer.\\n\\nWe follow the approach described in [Askell et al. 2021](https://arxiv.org/abs/2112.00861) and assign each answer a score:\\n\\n`score = log2 (1 + upvotes) rounded to the nearest integer, plus 1 if the questioner accepted the answer (we assign a score of −1 if the number of upvotes is negative).`\\n\\nFor the reward model, we will always need two answers per question to compare, as we’ll see later. Some questions have dozens of answers, leading to many possible pairs. We sample at most ten answer pairs per question to limit the number of data points per question. Finally, we cleaned up formatting by converting HTML to Markdown to make the model’s outputs more readable. You can find the dataset as well as the processing notebook [here](https://huggingface.co/datasets/lvwerra/stack-exchange-paired).\\n\\n\\n## Efficient training strategies'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 4262}, page_content='## Efficient training strategies\\n\\nEven training the smallest LLaMA model requires an enormous amount of memory. Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer (see the [performance docs](https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer) in Transformers for more info). So a 7B parameter model would use `(2+8)*7B=70GB` just to fit in memory and would likely need more when you compute intermediate values such as attention scores. So you couldn’t train the model even on a single 80GB A100 like that. You can use some tricks, like more efficient optimizers of half-precision training, to squeeze a bit more into memory, but you’ll run out sooner or later.\\n\\nAnother option is to use Parameter-Efficient Fine-Tuning (PEFT) techniques, such as the [`peft`](https://github.com/huggingface/peft) library, which can perform Low-Rank Adaptation (LoRA) on a model loaded in 8-bit. \\n\\n![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/lora-animated.gif)   \\n*Low-Rank Adaptation of linear layers: extra parameters (in orange) are added next to the frozen layer (in blue), and the resulting encoded hidden states are added together with the hidden states of the frozen layer.*\\n\\nLoading the model in 8bit reduces the memory footprint drastically since you only need one byte per parameter for the weights (e.g. 7B LlaMa is 7GB in memory). Instead of training the original weights directly, LoRA adds small adapter layers on top of some specific layers (usually the attention layers); thus, the number of trainable parameters is drastically reduced.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 5954}, page_content='In this scenario, a rule of thumb is to allocate ~1.2-1.4GB per billion parameters (depending on the batch size and sequence length) to fit the entire fine-tuning setup. As detailed in the attached blog post above, this enables fine-tuning larger models (up to 50-60B scale models on a NVIDIA A100 80GB) at low cost. \\n\\nThese techniques have enabled fine-tuning large models on consumer devices and Google Colab. Notable demos are fine-tuning `facebook/opt-6.7b` (13GB in `float16` ), and `openai/whisper-large` on Google Colab (15GB GPU RAM). To learn more about using `peft`, refer to our [github repo](https://github.com/huggingface/peft) or the [previous blog post](https://huggingface.co/blog/trl-peft)(https://huggingface.co/blog/trl-peft)) on training 20b parameter models on consumer hardware.\\n\\nNow we can fit very large models into a single GPU, but the training might still be very slow. The simplest strategy in this scenario is data parallelism: we replicate the same training setup into separate GPUs and pass different batches to each GPU. With this, you can parallelize the forward/backward passes of the model and scale with the number of GPUs. \\n\\n![chapter10_ddp.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_ddp.png)\\n\\nWe use either the `transformers.Trainer` or `accelerate`, which both support data parallelism without any code changes, by simply passing arguments when calling the scripts with `torchrun` or `accelerate launch`. The following runs a training script with 8 GPUs on a single machine with `accelerate` and `torchrun`, respectively.\\n\\n```bash\\naccelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py\\ntorchrun --nnodes 1  --nproc_per_node 8 my_torch_script.py'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 7742}, page_content=\"```\\n\\n## Supervised fine-tuning\\n\\nBefore we start training reward models and tuning our model with RL, it helps if the model is already good in the domain we are interested in. In our case, we want it to answer questions, while for other use cases, we might want it to follow instructions, in which case instruction tuning is a great idea. The easiest way to achieve this is by continuing to train the language model with the language modeling objective on texts from the domain or task. The [StackExchange dataset](https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences) is enormous (over 10 million instructions), so we can easily train the language model on a subset of it.\\n\\nThere is nothing special about fine-tuning the model before doing RLHF - it’s just the causal language modeling objective from pretraining that we apply here. To use the data efficiently, we use a technique called packing: instead of having one text per sample in the batch and then padding to either the longest text or the maximal context of the model, we concatenate a lot of texts with a EOS token in between and cut chunks of the context size to fill the batch without any padding.\\n\\n![chapter10_preprocessing-clm.png](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/chapter10_preprocessing-clm.png)\\n\\nWith this approach the training is much more efficient as each token that is passed through the model is also trained in contrast to padding tokens which are usually masked from the loss. If you don't have much data and are more concerned about occasionally cutting off some tokens that are overflowing the context you can also use a classical data loader.\\n\\nThe packing is handled by the `ConstantLengthDataset` and we can then use the `Trainer` after loading the model with `peft`. First, we load the model in int8, prepare it for training, and then add the LoRA adapters.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 9665}, page_content='```python\\n# load model in 8bit\\nmodel = AutoModelForCausalLM.from_pretrained(\\n        args.model_path,\\n        load_in_8bit=True,\\n        device_map={\"\": Accelerator().local_process_index}\\n    )\\nmodel = prepare_model_for_int8_training(model)\\n\\n# add LoRA to model\\nlora_config = LoraConfig(\\n    r=16,\\n    lora_alpha=32,\\n    lora_dropout=0.05,\\n    bias=\"none\",\\n    task_type=\"CAUSAL_LM\",\\n)\\n\\nmodel = get_peft_model(model, config)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 10090}, page_content=\"```\\n\\nWe train the model for a few thousand steps with the causal language modeling objective and save the model. Since we will tune the model again with different objectives, we merge the adapter weights with the original model weights.\\n\\n**Disclaimer:** due to LLaMA's license, we release only the adapter weights for this and the model checkpoints in the following sections. You can apply for access to the base model's weights by filling out Meta AI's [form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) and then converting them to the 🤗 Transformers format by running this [script](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). Note that you'll also need to install 🤗 Transformers from source until the `v4.28` is released.\\n\\nNow that we have fine-tuned the model for the task, we are ready to train a reward model.\\n\\n## Reward modeling and human preferences\\n\\nIn principle, we could fine-tune the model using RLHF directly with the human annotations. However, this would require us to send some samples to humans for rating after each optimization iteration. This is expensive and slow due to the number of training samples needed for convergence and the inherent latency of human reading and annotator speed.\\n\\nA trick that works well instead of direct feedback is training a reward model on human annotations collected before the RL loop. The goal of the reward model is to imitate how a human would rate a text. There are several possible strategies to build a reward model: the most straightforward way would be to predict the annotation (e.g. a rating score or a binary value for “good”/”bad”). In practice, what works better is to predict the ranking of two examples, where the reward model is presented with two candidates \\\\\\\\( (y_k, y_j) \\\\\\\\) for a given prompt \\\\\\\\( x \\\\\\\\) and has to predict which one would be rated higher by a human annotator.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 12069}, page_content='This can be translated into the following loss function:\\n\\n\\n\\\\\\\\( \\\\operatorname{loss}(\\\\theta)=- E_{\\\\left(x, y_j, y_k\\\\right) \\\\sim D}\\\\left[\\\\log \\\\left(\\\\sigma\\\\left(r_\\\\theta\\\\left(x, y_j\\\\right)-r_\\\\theta\\\\left(x, y_k\\\\right)\\\\right)\\\\right)\\\\right] \\\\\\\\)\\n\\nwhere \\\\\\\\( r \\\\\\\\) is the model’s score and \\\\\\\\( y_j \\\\\\\\) is the preferred candidate.\\n\\nWith the StackExchange dataset, we can infer which of the two answers was preferred by the users based on the score. With that information and the loss defined above, we can then modify the `transformers.Trainer` by adding a custom loss function. \\n\\n```python\\nclass RewardTrainer(Trainer):\\n    def compute_loss(self, model, inputs, return_outputs=False):\\n        rewards_j = model(input_ids=inputs[\"input_ids_j\"],  attention_mask=inputs[\"attention_mask_j\"])[0]\\n        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\\n        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\\n        if return_outputs:\\n            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\\n        return loss'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 13147}, page_content='```\\n\\nWe utilize a subset of a 100,000 pair of candidates and evaluate on a held-out set of 50,000. With a modest training batch size of 4, we train the LLaMA model using the LoRA `peft` adapter for a single epoch using the Adam optimizer with BF16 precision. Our LoRA configuration is:\\n\\n```python\\npeft_config = LoraConfig(\\n    task_type=TaskType.SEQ_CLS,\\n    inference_mode=False,\\n    r=8,\\n    lora_alpha=32,\\n    lora_dropout=0.1,\\n)\\n```\\n\\nThe training is logged via [Weights & Biases](https://wandb.ai/krasul/huggingface/runs/wmd8rvq6?workspace=user-krasul) and took a few hours on 8-A100 GPUs using the 🤗 research cluster and the model achieves a final **accuracy of 67%**. Although this sounds like a low score, the task is also very hard, even for human annotators.\\n\\nAs detailed in the next section, the resulting adapter can be merged into the frozen model and saved for further downstream use.\\n\\n## Reinforcement Learning from Human Feedback\\n\\nWith the fine-tuned language model and the reward model at hand, we are now ready to run the RL loop. It follows roughly three steps:\\n\\n1. Generate responses from prompts\\n2. Rate the responses with the reward model\\n3. Run a reinforcement learning policy-optimization step with the ratings\\n\\n![Untitled](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/trl_loop.png)\\n\\nThe Query and Response prompts are templated as follows before being tokenized and passed to the model:\\n\\n```bash\\nQuestion: <Query>\\n\\nAnswer: <Response>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 14657}, page_content='```\\n\\nThe same template was used for SFT, RM and RLHF stages.\\n\\nA common issue with training the language model with RL is that the model can learn to exploit the reward model by generating complete gibberish, which causes the reward model to assign high rewards. To balance this, we add a penalty to the reward: we keep a reference of the model that we don’t train and compare the new model’s generation to the reference one by computing the KL-divergence:\\n\\n\\n\\\\\\\\( \\\\operatorname{R}(x, y)=\\\\operatorname{r}(x, y)- \\\\beta \\\\operatorname{KL}(x, y) \\\\\\\\)\\n\\nwhere \\\\\\\\( r \\\\\\\\) is the reward from the reward model and  \\\\\\\\( \\\\operatorname{KL}(x,y) \\\\\\\\) is the KL-divergence between the current  policy and the reference model. \\n\\nOnce more, we utilize `peft` for memory-efficient training, which offers an extra advantage in the RLHF context. Here, the reference model and policy share the same base, the SFT model, which we load in 8-bit and freeze during training. We exclusively optimize the policy\\'s LoRA weights using PPO while sharing the base model\\'s weights.\\n\\n```python\\nfor epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\\n    question_tensors = batch[\"input_ids\"]\\n\\t\\t\\n\\t# sample from the policy and generate responses\\n    response_tensors = ppo_trainer.generate(\\n        question_tensors,\\n        return_prompt=False,\\n        length_sampler=output_length_sampler,\\n        **generation_kwargs,\\n    )\\n    batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\\n\\n    # Compute sentiment score\\n    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\\n    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\\n    rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 16409}, page_content='# Run PPO step\\n    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\\n\\t# Log stats to WandB\\n    ppo_trainer.log_stats(stats, batch, rewards)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 16569}, page_content=\"```\\n\\nWe train for 20 hours on 3x8 A100-80GB GPUs, using the 🤗 research cluster, but you can also get decent results much quicker (e.g. after ~20h on 8 A100 GPUs). All the training statistics of the training run are available on [Weights & Biases](https://wandb.ai/lvwerra/trl/runs/ie2h4q8p).\\n\\n![Per batch reward at each step during training. The model’s performance plateaus after around 1000 steps.](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/wandb_reward.png)\\n*Per batch reward at each step during training. The model’s performance plateaus after around 1000 steps.*\\n\\nSo what can the model do after training? Let's have a look!\\n\\n![llama prompt](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/llama_prompt.png)\\n\\nAlthough we shouldn't trust its advice on LLaMA matters just, yet, the answer looks coherent and even provides a Google link. Let's have a look and some of the training challenges next.\\n\\n## Challenges, instabilities and workarounds\\n\\nTraining LLMs with RL is not always plain sailing. The model we demo today is the result of many experiments, failed runs and hyper-parameter sweeps. Even then, the model is far from perfect. Here we will share a few of the observations and headaches we encountered on the way to making this example.\\n\\n### Higher reward means better performance, right?\\n\\n![Wow this run must be great, look at that sweet, sweet, reward!](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_high_reward.png)\\n*Wow this run must be great, look at that sweet, sweet, reward!*\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 18224}, page_content=\"In general in RL, you want to achieve the highest reward. In RLHF we use a Reward Model, which is imperfect and given the chance, the PPO algorithm will exploit these imperfections. This can manifest itself as sudden increases in reward, however when we look at the text generations from the policy, they mostly contain repetitions of the string ```, as the reward model found the stack exchange answers containing blocks of code usually rank higher than ones without it. Fortunately this issue was observed fairly rarely and in general the KL penalty should counteract such exploits.\\n\\n### KL is always a positive value, isn’t it?\\n\\nAs we previously mentioned, a KL penalty term is used in order to push the model’s outputs remain close to that of the base policy. In general, KL divergence measures the distances between two distributions and is always a positive quantity. However, in `trl` we use an estimate of the KL which in expectation is equal to the real KL divergence.\\n\\n\\n\\\\\\\\( KL_{pen}(x,y) = \\\\log \\\\left(\\\\pi_\\\\phi^{\\\\mathrm{RL}}(y \\\\mid x) / \\\\pi^{\\\\mathrm{SFT}}(y \\\\mid x)\\\\right) \\\\\\\\)\\n\\nClearly, when a token is sampled from the policy which has a lower probability than the SFT model, this will lead to a negative KL penalty, but on average it will be positive otherwise you wouldn't be properly sampling from the policy. However, some generation strategies can force some tokens to be generated or some tokens can suppressed. For example when generating in batches finished sequences are padded and when setting a minimum length the EOS token is suppressed. The model can assign very high or low probabilities to those tokens which leads to negative KL. As the PPO algorithm optimizes for reward, it will chase after these negative penalties, leading to instabilities.\\n\\n![Negative KL](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_neg_kl.png)\\n\\nOne needs to be careful when generating the responses and we suggest to always use a simple sampling strategy first before resorting to more sophisticated generation methods.\\n\\n### Ongoing issues\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': -1}, page_content=\"One needs to be careful when generating the responses and we suggest to always use a simple sampling strategy first before resorting to more sophisticated generation methods.\\n\\n### Ongoing issues\\n\\nThere are still a number of issues that we need to better understand and resolve. For example, there are occassionally spikes in the loss, which can lead to further instabilities. \\n\\n![Loss spikes](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/stackllama/logs_loss_spikes.png)\\n\\nAs we identify and resolve these issues, we will upstream the changes `trl`, to ensure the community can benefit.\\n\\n## Conclusion\\n\\nIn this post, we went through the entire training cycle for RLHF, starting with preparing a dataset with human annotations, adapting the language model to the domain, training a reward model, and finally training a model with RL. \\n\\nBy using `peft`, anyone can run our example on a single GPU! If training is too slow, you can use data parallelism with no code changes and scale training by adding more GPUs.\\n\\nFor a real use case, this is just the first step! Once you have a trained model, you must evaluate it and compare it against other models to see how good it is. This can be done by ranking generations of different model versions, similar to how we built the reward dataset. \\n\\nOnce you add the evaluation step, the fun begins: you can start iterating on your dataset and model training setup to see if there are ways to improve the model. You could add other datasets to the mix or apply better filters to the existing one. On the other hand, you could try different model sizes and architecture for the reward model or train for longer.\\n\\nWe are actively improving TRL to make all steps involved in RLHF more accessible and are excited to see the things people build with it! Check out the [issues on GitHub](https://github.com/lvwerra/trl/issues) if you're interested in contributing.\\n\\n\\n## Citation\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 22065}, page_content='## Citation\\n\\n```bibtex\\n@misc {beeching2023stackllama,\\n    author       = { Edward Beeching and\\n                     Younes Belkada and\\n                     Kashif Rasul and\\n                     Lewis Tunstall and\\n                     Leandro von Werra and\\n                     Nazneen Rajani and\\n                     Nathan Lambert\\n                   },\\n    title        = { StackLLaMA: An RL Fine-tuned LLaMA Model for Stack Exchange Question and Answering },\\n    year         = 2023,\\n    url          = { https://huggingface.co/blog/stackllama },\\n    doi          = { 10.57967/hf/0513 },\\n    publisher    = { Hugging Face Blog }\\n}'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/stackllama.md', 'start_index': 22698}, page_content='```\\n\\n## Acknowledgements\\n\\nWe thank Philipp Schmid for sharing his wonderful [demo](https://huggingface.co/spaces/philschmid/igel-playground) of streaming text generation upon which our demo was based. We also thank Omar Sanseviero and Louis Castricato for giving valuable and detailed feedback on the draft of the blog post.'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/cache.mdx', 'start_index': 1}, page_content='Cache management\\n\\nWhen you download a dataset, the processing scripts and data are stored locally on your computer. The cache allows 🤗 Datasets to avoid re-downloading or processing the entire dataset every time you use it. \\n\\nThis guide will show you how to:\\n\\n- Change the cache directory.\\n- Control how a dataset is loaded from the cache.\\n- Clean up cache files in the directory.\\n- Enable or disable caching.\\n\\n## Cache directory\\n\\nThe default cache directory is `~/.cache/huggingface/datasets`. Change the cache location by setting the shell environment variable, `HF_DATASETS_CACHE` to another directory:\\n\\n```\\n$ export HF_DATASETS_CACHE=\"/path/to/another/directory\"\\n```\\n\\nWhen you load a dataset, you also have the option to change where the data is cached. Change the `cache_dir` parameter to the path you want:\\n\\n```py\\n>>> from datasets import load_dataset\\n>>> dataset = load_dataset(\\'LOADING_SCRIPT\\', cache_dir=\"PATH/TO/MY/CACHE/DIR\")\\n```\\n\\nSimilarly, you can change where a metric is cached with the `cache_dir` parameter:\\n\\n```py\\n>>> from datasets import load_metric\\n>>> metric = load_metric(\\'glue\\', \\'mrpc\\', cache_dir=\"MY/CACHE/DIRECTORY\")\\n```\\n\\n## Download mode\\n\\nAfter you download a dataset, control how it is loaded by [`load_dataset`] with the `download_mode` parameter. By default, 🤗 Datasets will reuse a dataset if it exists. But if you need the original dataset without any processing functions applied, re-download the files as shown below:\\n\\n```py\\n>>> from datasets import load_dataset\\n>>> dataset = load_dataset(\\'squad\\', download_mode=\\'force_redownload\\')\\n```\\n\\nRefer to [`DownloadMode`] for a full list of download modes.\\n\\n## Cache files\\n\\nClean up the cache files in the directory with [`Dataset.cleanup_cache_files`]:\\n\\n```py\\n# Returns the number of removed cache files\\n>>> dataset.cleanup_cache_files()\\n2'),\n",
       " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/cache.mdx', 'start_index': 1817}, page_content=\"```\\n\\n## Enable or disable caching\\n\\nIf you're using a cached file locally, it will automatically reload the dataset with any previous transforms you applied to the dataset. Disable this behavior by setting the argument `load_from_cache_file=False` in [`Dataset.map`]:\\n\\n```py\\n>>> updated_dataset = small_dataset.map(add_prefix, load_from_cache_file=False)\\n```\\n\\nIn the example above, 🤗 Datasets will execute the function `add_prefix` over the entire dataset again instead of loading the dataset from its previous state.\\n\\nDisable caching on a global scale with [`disable_caching`]:\\n\\n```py\\n>>> from datasets import disable_caching\\n>>> disable_caching()\\n```\\n\\nWhen you disable caching, 🤗 Datasets will no longer reload cached files when applying transforms to datasets. Any transform you apply on your dataset will be need to be reapplied.\\n\\n<Tip>\\n\\nIf you want to reuse a dataset from scratch, try setting the `download_mode` parameter in [`load_dataset`] instead.\\n\\n</Tip>\\n\\nYou can also avoid caching your metric entirely, and keep it in CPU memory instead:\\n\\n```py\\n>>> from datasets import load_metric\\n>>> metric = load_metric('glue', 'mrpc', keep_in_memory=True)\\n```\\n\\n<Tip warning={true}>\\n\\nKeeping the predictions in-memory is not possible in a distributed setting since the CPU memory spaces of the various processes are not shared.\\n\\n</Tip>\\n\\n<a id='load_dataset_enhancing_performance'></a>\\n\\n## Improve performance\\n\\nDisabling the cache and copying the dataset in-memory will speed up dataset operations. There are two options for copying the dataset in-memory:\\n\\n1. Set `datasets.config.IN_MEMORY_MAX_SIZE` to a nonzero value (in bytes) that fits in your RAM memory. \\n\\n2. Set the environment variable `HF_DATASETS_IN_MEMORY_MAX_SIZE` to a nonzero value. Note that the first method takes higher precedence.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/mlm_wwm/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n## Whole Word Mask Language Model\\n\\n\\nThese scripts leverage the 🤗 Datasets library and the Trainer API. You can easily customize them to your needs if you\\nneed extra processing on your datasets.\\n\\nThe following examples, will run on a datasets hosted on our [hub](https://huggingface.co/datasets) or with your own\\ntext files for training and validation. We give examples of both below.\\n\\n\\n\\nThe BERT authors released a new version of BERT using Whole Word Masking in May 2019. Instead of masking randomly\\nselected tokens (which may be part of words), they mask randomly selected words (masking all the tokens corresponding\\nto that word). This technique has been refined for Chinese in [this paper](https://arxiv.org/abs/1906.08101).\\n\\nTo fine-tune a model using whole word masking, use the following script:\\n```bash\\npython run_mlm_wwm.py \\\\\\n    --model_name_or_path roberta-base \\\\\\n    --dataset_name wikitext \\\\\\n    --dataset_config_name wikitext-2-raw-v1 \\\\\\n    --do_train \\\\\\n    --do_eval \\\\\\n    --output_dir /tmp/test-mlm-wwm'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/mlm_wwm/README.md', 'start_index': 1612}, page_content=\"```\\n\\nFor Chinese models, we need to generate a reference files (which requires the ltp library), because it's tokenized at\\nthe character level.\\n\\n**Q :** Why a reference file?\\n\\n**A :** Suppose we have a Chinese sentence like: `我喜欢你` The original Chinese-BERT will tokenize it as\\n`['我','喜','欢','你']` (character level). But `喜欢` is a whole word. For whole word masking proxy, we need a result\\nlike `['我','喜','##欢','你']`, so we need a reference file to tell the model which position of the BERT original token\\nshould be added `##`.\\n\\n**Q :** Why LTP ?\\n\\n**A :** Cause the best known Chinese WWM BERT is [Chinese-BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm) by HIT.\\nIt works well on so many Chines Task like CLUE (Chinese GLUE). They use LTP, so if we want to fine-tune their model,\\nwe need LTP.\\n\\nYou could run the following:\\n\\n\\n```bash\\nexport TRAIN_FILE=/path/to/train/file\\nexport LTP_RESOURCE=/path/to/ltp/tokenizer\\nexport BERT_RESOURCE=/path/to/bert/tokenizer\\nexport SAVE_PATH=/path/to/data/ref.txt\\n\\npython run_chinese_ref.py \\\\\\n    --file_name=$TRAIN_FILE \\\\\\n    --ltp=$LTP_RESOURCE \\\\\\n    --bert=$BERT_RESOURCE \\\\\\n    --save_path=$SAVE_PATH\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/mlm_wwm/README.md', 'start_index': 2754}, page_content=\"```\\n\\nThen you can run the script like this: \\n\\n\\n```bash\\nexport TRAIN_FILE=/path/to/train/file\\nexport VALIDATION_FILE=/path/to/validation/file\\nexport TRAIN_REF_FILE=/path/to/train/chinese_ref/file\\nexport VALIDATION_REF_FILE=/path/to/validation/chinese_ref/file\\nexport OUTPUT_DIR=/tmp/test-mlm-wwm\\n\\npython run_mlm_wwm.py \\\\\\n    --model_name_or_path roberta-base \\\\\\n    --train_file $TRAIN_FILE \\\\\\n    --validation_file $VALIDATION_FILE \\\\\\n    --train_ref_file $TRAIN_REF_FILE \\\\\\n    --validation_ref_file $VALIDATION_REF_FILE \\\\\\n    --do_train \\\\\\n    --do_eval \\\\\\n    --output_dir $OUTPUT_DIR\\n```\\n\\n**Note1:** On TPU, you should the flag `--pad_to_max_length` to make sure all your batches have the same length.\\n\\n**Note2:** And if you have any questions or something goes wrong when runing this code, don't hesitate to pin @wlhgtc.\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-zenml.md', 'start_index': 1}, page_content=\"ZenML on Spaces\\n\\n[ZenML](https://github.com/zenml-io/zenml) is an extensible, open-source MLOps framework for creating portable, production-ready MLOps pipelines. It's built for Data Scientists, ML Engineers, and MLOps Developers to collaborate as they develop to production.\\n\\nZenML offers a simple and flexible syntax, is cloud- and tool-agnostic, and has\\ninterfaces/abstractions catered toward ML workflows. With ZenML you'll have all\\nyour favorite tools in one place, so you can tailor a workflow that caters to\\nyour specific needs.\\n\\nThe ZenML Huggingface Space allows you to get up and running with a deployed version\\nof ZenML with just a few clicks. Within a few minutes, you'll have this default\\nZenML dashboard deployed and ready for you to connect to from your local\\nmachine.\\n\\nIn the sections that follow, you'll learn to deploy your own instance of ZenML and use\\nit to view and manage your machine learning pipelines right from the Hub. ZenML\\non Huggingface Spaces is a **self-contained application completely hosted on the\\nHub using Docker**. The diagram below illustrates the complete process.\\n\\n![ZenML on HuggingFace Spaces -- default deployment](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/zenml/hf_spaces_chart.png)\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-zenml.md', 'start_index': 1270}, page_content='Visit [the ZenML documentation](https://docs.zenml.io/) to learn more about its\\nfeatures and how to get started with running your machine learning pipelines\\nthrough your Huggingface Spaces deployment. You can check out [some small sample\\nexamples](https://github.com/zenml-io/zenml/tree/main/examples) of ZenML pipelines to get started or take your pick of some more\\ncomplex production-grade projects at [the ZenML Projects\\nrepository](https://github.com/zenml-io/zenml-projects). ZenML integrates with\\nmany of your favorite tools out of the box, [including\\nHuggingface](https://zenml.io/integrations/huggingface) of course! If there\\'s\\nsomething else you want to use, we\\'re built to be extensible and you can easily\\nmake it work with whatever your custom tool or workflow is.\\n\\n## ⚡️ Deploy ZenML on Spaces\\n\\nYou can deploy ZenML on Spaces with just a few clicks:\\n\\n<a  href=\"https://huggingface.co/new-space?template=zenml/zenml-template-space\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/badges/resolve/main/deploy-to-spaces-lg.svg\" />\\n</a>\\n\\nTo set up your ZenML app, you need to specify three main components: the Owner\\n(either your personal account or an organization), a Space name, and the\\nVisibility (a bit lower down the page). Note that the space visibility needs to\\nbe set to \\'Public\\' if you wish to connect to the ZenML server from your local\\nmachine.\\n\\n![Choose the ZenML Docker template](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/zenml/choose_space.png)\\n\\nYou have the option here to select a higher tier machine to use for your server.\\nThe advantage of selecting a paid CPU instance is that it is not subject to\\nauto-shutdown policies and thus will stay up as long as you leave it up. In\\norder to make use of a persistent CPU, you\\'ll likely want to create and set up a\\nMySQL database to connect to (see below).'),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-zenml.md', 'start_index': 3146}, page_content='To personalize your Space\\'s appearance, such as the title, emojis, and colors,\\nnavigate to \"Files and Versions\" and modify the metadata in your README.md file.\\nFull information on Spaces configuration parameters can be found on the\\nHuggingFace [documentation reference guide](https://huggingface.co/docs/hub/spaces-config-reference).\\n\\nAfter creating your Space, you\\'ll notice a \\'Building\\' status along with logs\\ndisplayed on the screen. When this switches to \\'Running\\', your Space is ready for use. If the\\nZenML login UI isn\\'t visible, try refreshing the page.\\n\\nIn the upper-right hand corner of your space you\\'ll see a button with three dots\\nwhich, when you click on it, will offer you a menu option to \"Embed this Space\".\\n(See [the HuggingFace\\ndocumentation](https://huggingface.co/docs/hub/spaces-embed) for more details on\\nthis feature.) Copy the \"Direct URL\" shown in the box that you can now see on\\nthe screen. This should look something like this:\\n`https://<YOUR_USERNAME>-<SPACE_NAME>.hf.space`. Open that URL and use our default \\nlogin to access the dashboard (username: \\'default\\', password: (leave it empty)).\\n\\n## Connecting to your ZenML Server from your Local Machine\\n\\nOnce you have your ZenML server up and running, you can connect to it from your\\nlocal machine. To do this, you\\'ll need to get your Space\\'s \\'Direct URL\\' (see above).\\n\\n<Tip warning={true}>\\nYour Space\\'s URL will only be available and usable for connecting from your\\nlocal machine if the visibility of the space is set to \\'Public\\'.\\n</Tip>\\n\\nYou can use the \\'Direct URL\\' to connect to your ZenML server from your local machine\\nwith the following CLI command (after installing ZenML, and using your custom\\nURL instead of the placeholder):\\n\\n```shell\\nzenml connect --url \\'<YOUR_HF_SPACES_DIRECT_URL>\\' --username=\\'default\\' --password=\\'\\''),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-zenml.md', 'start_index': 4954}, page_content=\"```\\n\\nYou can also use the Direct URL in your browser to use the ZenML dashboard as a\\nfullscreen application (i.e. without the HuggingFace Spaces wrapper around it).\\n\\n<Tip warning={true}>\\nThe ZenML dashboard will currently not work when viewed from within the Huggingface \\nwebpage (i.e. wrapped in the main `https://huggingface.co/...` website). This is on \\naccount of a limitation in how cookies are handled between ZenML and Huggingface. \\nYou **must** view the dashboard from the 'Direct URL' (see above).\\n</Tip>\\n\\n## Extra Configuration Options\\n\\nBy default the ZenML application will be configured to use a SQLite\\nnon-persistent database. If you want to use a persistent database, you can\\nconfigure this by amending the `Dockerfile` in your Space's root directory. For\\nfull details on the various parameters you can change, see [our reference\\ndocumentation](https://docs.zenml.io/getting-started/deploying-zenml/docker#zenml-server-configuration-options) on configuring\\nZenML when deployed with Docker.\\n\\n<Tip>\\nIf you are using the space just for testing and experimentation, you don't need\\nto make any changes to the configuration. Everything will work out of the box.\\n</Tip>\\n\\nYou can also use an external secrets backend together with your HuggingFace\\nSpaces as described in [our\\ndocumentation](https://docs.zenml.io/getting-started/deploying-zenml/docker#zenml-server-configuration-options). You should be\\nsure to use HuggingFace's inbuilt 'Repository secrets' functionality to\\nconfigure any secrets you need to use in your`Dockerfile` configuration. [See the\\ndocumentation](https://huggingface.co/docs/hub/spaces-sdks-docker#secret-management)\\nfor more details how to set this up.\\n\\n<Tip warning={true}>\\nIf you wish to use a cloud secrets backend together with ZenML for secrets\\nmanagement, **you must take the following minimal security precautions** on your ZenML Server on the\\nDashboard:\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-zenml.md', 'start_index': -1}, page_content=\"<Tip warning={true}>\\nIf you wish to use a cloud secrets backend together with ZenML for secrets\\nmanagement, **you must take the following minimal security precautions** on your ZenML Server on the\\nDashboard:\\n\\n- change your password on the `default` account that you get when you start. You\\n  can do this from the Dashboard or via the CLI.\\n- create a new user account with a password and assign it the `admin` role. This\\n  can also be done from the Dashboard (by 'inviting' a new user) or via the CLI.\\n- reconnect to the server using the new user account and password as described\\n  above, and use this new user account as your working account.\\n\\nThis is because the default user created by the\\nHuggingFace Spaces deployment process has no password assigned to it and as the\\nSpace is publicly accessible (since the Space is public) *potentially anyone\\ncould access your secrets without this extra step*. To change your password\\nnavigate to the Settings page by clicking the button in the upper right hand\\ncorner of the Dashboard and then click 'Update Password'.\\n</Tip>\\n\\n## Upgrading your ZenML Server on HF Spaces\\n\\nThe default space will use the latest version of ZenML automatically. If you\\nwant to update your version, you can simply select the 'Factory reboot' option\\nwithin the 'Settings' tab of the space. Note that this will wipe any data\\ncontained within the space and so if you are not using a MySQL persistent\\ndatabase (as described above) you will lose any data contained within your ZenML\\ndeployment on the space. You can also configure the space to use an earlier\\nversion by updating the `Dockerfile`'s `FROM` import statement at the very top.\\n\\n## Next Steps\\n\\nAs a next step, check out our [Starter Guide to MLOps with\\nZenML](https://docs.zenml.io/starter-guide/pipelines) which is a series of short\\npractical pages on how to get going quickly. Alternatively, check out [our\\n`quickstart`\\nexample](https://github.com/zenml-io/zenml/tree/main/examples/quickstart) which\\nis a full end-to-end example of many of the features of ZenML.\\n\\n## 🤗 Feedback and support\"),\n",
       " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-zenml.md', 'start_index': 8683}, page_content='## 🤗 Feedback and support\\n\\nIf you are having trouble with your ZenML server on HuggingFace Spaces, you can\\nview the logs by clicking on the \"Open Logs\" button at the top of the space.\\nThis will give you more context of what\\'s happening with your server.\\n\\nIf you have suggestions or need specific support for anything else which isn\\'t\\nworking, please [join the ZenML Slack community](https://zenml.io/slack-invite/)\\nand we\\'ll be happy to help you out!'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 1}, page_content='主题 Theming\\n\\nTags: THEMES\\n\\n## 介绍\\n\\nGradio 具有内置的主题引擎，可让您自定义应用的外观和感觉。您可以选择各种主题，或者创建自己的主题。要这样做，请将 `theme=` kwarg 传递给 `Blocks` 或 `Interface` 构造函数。例如：\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\\n    ...\\n```\\n\\n<div class=\"wrapper\">\\n<iframe\\n\\tsrc=\"https://gradio-theme-soft.hf.space?__theme=light\"\\n\\tframeborder=\"0\"\\n></iframe>\\n</div>\\n\\nGradio 带有一组预构建的主题，您可以从 `gr.themes.*` 中加载这些主题。这些主题包括：\\n\\n- `gr.themes.Base()`\\n- `gr.themes.Default()`\\n- `gr.themes.Glass()`\\n- `gr.themes.Monochrome()`\\n- `gr.themes.Soft()`\\n\\n这些主题为数百个 CSS 变量设置了值。您可以使用预构建的主题作为自定义主题的起点，也可以从头开始创建自己的主题。让我们看看每种方法。\\n\\n## 使用主题构建器\\n\\n使用主题构建器构建主题最简单。要在本地启动主题构建器，请运行以下代码：\\n\\n```python\\nimport gradio as gr\\n\\ngr.themes.builder()'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 678}, page_content='```\\n\\n$demo_theme_builder\\n\\n您可以使用上面的 Spaces 上运行的 Theme Builder，但通过 `gr.themes.builder()` 在本地启动时运行速度更快。\\n\\n在 Theme Builder 中编辑值时，应用程序将实时预览更新。您可以下载生成的主题代码，以便在任何 Gradio 应用程序中使用它。\\n\\n在本指南的其余部分，我们将介绍如何以编程方式构建主题。\\n\\n## 通过构造函数扩展主题\\n\\n尽管每个主题都有数百个 CSS 变量，但大多数这些变量的值都是从 8 个核心变量中获取的，可以通过每个预构建主题的构造函数设置这些变量。通过修改这 8 个参数的值，您可以快速更改应用程序的外观和感觉。\\n\\n### 核心颜色\\n\\n前 3 个构造函数参数设置主题的颜色，并且是 `gradio.themes.Color` 对象。在内部，这些 Color 对象包含单个色调的调色板的亮度值，范围从 50，100，200...，800，900，950。其他 CSS 变量是从这 3 种颜色派生的。\\n\\n3 个颜色构造函数参数是：\\n\\n- `primary_hue`：这是主题中的主色。在默认主题中，此值设置为 `gradio.themes.colors.orange`。\\n- `secondary_hue`：这是主题中用于辅助元素的颜色。在默认主题中，此值设置为 `gradio.themes.colors.blue`。\\n- `neutral_hue`：这是主题中用于文本和其他中性元素的颜色。在默认主题中，此值设置为 `gradio.themes.colors.gray`。\\n\\n您可以使用字符串快捷方式修改这些值，例如'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 1377}, page_content='您可以使用字符串快捷方式修改这些值，例如\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(primary_hue=\"red\", secondary_hue=\"pink\")) as demo:\\n    ...'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 1507}, page_content='```\\n\\n或者直接使用 `Color` 对象，如下所示：\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes.colors.red, secondary_hue=gr.themes.colors.pink)) as demo:\\n    ...'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 1675}, page_content='```\\n\\n<div class=\"wrapper\">\\n<iframe\\n\\tsrc=\"https://gradio-theme-extended-step-1.hf.space?__theme=light\"\\n\\tframeborder=\"0\"\\n></iframe>\\n</div>\\n\\n预定义的颜色包括：\\n\\n- `slate`\\n- `gray`\\n- `zinc`\\n- `neutral`\\n- `stone`\\n- `red`\\n- `orange`\\n- `amber`\\n- `yellow`\\n- `lime`\\n- `green`\\n- `emerald`\\n- `teal`\\n- `cyan`\\n- `sky`\\n- `blue`\\n- `indigo`\\n- `violet`\\n- `purple`\\n- `fuchsia`\\n- `pink`\\n- `rose`\\n\\n您还可以创建自己的自定义 `Color` 对象并传递它们。\\n\\n### 核心大小 （Core Sizing）\\n\\n接下来的 3 个构造函数参数设置主题的大小，并且是 `gradio.themes.Size` 对象。在内部，这些 Size 对象包含从 `xxs` 到 `xxl` 的像素大小值。其他 CSS 变量是从这 3 个大小派生的。\\n\\n- `spacing_size`：此设置了元素内部的填充和元素之间的间距。在默认主题中，此值设置为 `gradio.themes.sizes.spacing_md`。\\n- `radius_size`：此设置了元素的圆角弧度。在默认主题中，此值设置为 `gradio.themes.sizes.radius_md`。\\n- `text_size`：此设置了文本的字体大小。在默认主题中，此值设置为 `gradio.themes.sizes.text_md`。\\n\\n您可以使用字符串快捷方式修改这些值，例如\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=\"sm\", radius_size=\"none\")) as demo:\\n    ...'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 2569}, page_content='```\\n\\n或者直接使用 `Size` 对象，如下所示：\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=gr.themes.sizes.spacing_sm, radius_size=gr.themes.sizes.radius_none)) as demo:\\n    ...'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 2747}, page_content='```\\n\\n<div class=\"wrapper\">\\n<iframe\\n\\tsrc=\"https://gradio-theme-extended-step-2.hf.space?__theme=light\"\\n\\tframeborder=\"0\"\\n></iframe>\\n</div>\\n\\n预定义的大小对象包括：\\n\\n- `radius_none`\\n- `radius_sm`\\n- `radius_md`\\n- `radius_lg`\\n- `spacing_sm`\\n- `spacing_md`\\n- `spacing_lg`\\n- `text_sm`\\n- `text_md`\\n- `text_lg`\\n\\n您还可以创建自己的自定义 `Size` 对象并传递它们。\\n\\n### 核心字体（Core Fonts）\\n\\n最后的 2 个构造函数参数设置主题的字体。您可以将一系列字体传递给这些参数，以指定回退字体。如果提供了字符串，它将被加载为系统字体。如果提供了 `gradio.themes.GoogleFont`，则将从 Google Fonts 加载该字体。\\n\\n- `font`：此设置主题的主要字体。在默认主题中，此值设置为 `gradio.themes.GoogleFont(\"Source Sans Pro\")`。\\n- `font_mono`：此设置主题的等宽字体。在默认主题中，此值设置为 `gradio.themes.GoogleFont(\"IBM Plex Mono\")`。\\n\\n您可以修改这些值，例如以下方式：\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(font=[gr.themes.GoogleFont(\"Inconsolata\"), \"Arial\", \"sans-serif\"])) as demo:\\n    ...'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 3530}, page_content='```\\n\\n<div class=\"wrapper\">\\n<iframe\\n\\tsrc=\"https://gradio-theme-extended-step-3.hf.space?__theme=light\"\\n\\tframeborder=\"0\"\\n></iframe>\\n</div>\\n\\n## 通过 `.set()` 扩展主题\\n\\n主题加载后，您还可以修改 CSS 变量的值。为此，请使用主题对象的 `.set()` 方法来访问 CSS 变量。例如：\\n\\n```python\\ntheme = gr.themes.Default(primary_hue=\"blue\").set(    loader_color=\"#FF0000\",    slider_color=\"#FF0000\",)\\n使用`gr.Blocks(theme=theme)`创建演示块    ...'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 3905}, page_content='```\\n\\n在上面的示例中，我们将 `loader_color` 和 `slider_color` 变量设置为`#FF0000`，尽管整体 `primary_color` 使用蓝色调色板。您可以以这种方式设置主题中定义的任何 CSS 变量。\\n您的 IDE 类型提示应该帮助您导航这些变量。由于有很多 CSS 变量，让我们看一下这些变量的命名和组织方式。\\n\\n### CSS 变量命名规范\\n\\nCSS 变量名可能会变得很长，例如 `button_primary_background_fill_hover_dark`！但是它们遵循一种常见的命名约定，使得理解变量功能和查找您要查找的变量变得容易。变量名由下划线分隔，由以下组成：\\n\\n1. 目标元素，例如 `button`、`slider` 或 `block`。2. 目标元素类型或子元素，例如 `button_primary` 或 `block_label`。3. 属性，例如 `button_primary_background_fill` 或 `block_label_border_width`。4. 任何相关状态，例如 `button_primary_background_fill_hover`。5. 如果在暗模式中值不同，则使用后缀 `_dark`。例如，`input_border_color_focus_dark`。\\n   当然，许多 CSS 变量名都比这个短，例如 `table_border_color` 或 `input_shadow`。\\n\\n### CSS 变量组织\\n\\n虽然有数百个 CSS 变量，但并不需要为每个变量都指定单独的值。它们通过引用一组核心变量和彼此引用来获取值。这样做可以仅修改少量变量以改变整个主题的外观和感觉，同时也可以更精细地控制我们可能想要修改的个别元素。'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 4679}, page_content='#### 引用核心变量\\n\\n要引用其中一个核心构造函数变量，请在变量名前加上星号。要引用核心颜色，请使用`*primary_`、`*secondary_` 或`*neutral_` 前缀，后跟亮度值。例如：\\n\\n```python\\ntheme = gr.themes.Default(primary_hue=\"blue\").set(\\n    button_primary_background_fill=\"*primary_200\",\\n    button_primary_background_fill_hover=\"*primary_300\",\\n)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 4954}, page_content='```\\n\\n在上面的示例中，我们将 `button_primary_background_fill` 和 `button_primary_background_fill_hover` 变量分别设置为`*primary_200` 和`*primary_300`。这些变量将分别设置为蓝色主色调调色板的 200 和 300 亮度值。\\n同样地，要引用核心大小，请使用`*spacing_`、`*radius_` 或`*text_` 前缀，后跟大小值。例如：\\n\\n```python\\ntheme = gr.themes.Default(radius_size=\"md\").set(\\n    button_primary_border_radius=\"*radius_xl\",\\n)\\n```\\n\\n在上面的示例中，我们将 `button_primary_border_radius` 变量设置为`*radius_xl`。此变量将设置为中等半径大小范围的 `xl` 设置。\\n\\n#### 引用其他变量\\n\\n变量也可以引用彼此。例如，请看下面的示例：\\n\\n```python\\ntheme = gr.themes.Default().set(\\n    button_primary_background_fill=\"#FF0000\",\\n    button_primary_background_fill_hover=\"#FF0000\",\\n    button_primary_border=\"#FF0000\",\\n)\\n```\\n\\n将这些值设置为相同的颜色有点繁琐。相反，我们可以在 `button_primary_background_fill_hover` 和 `button_primary_border` 变量中使用`*` 前缀引用 `button_primary_background_fill` 变量。\\n\\n```python\\ntheme = gr.themes.Default().set(\\n    button_primary_background_fill=\"#FF0000\",\\n    button_primary_background_fill_hover=\"*button_primary_background_fill\",\\n    button_primary_border=\"*button_primary_background_fill\",\\n)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 5973}, page_content='```\\n\\n现在，如果我们更改 `button_primary_background_fill` 变量，`button_primary_background_fill_hover` 和 `button_primary_border` 变量将自动更新。\\n如果您打算共享主题，这将非常有用- 它使得修改主题变得容易，而无需更改每个变量。\\n请注意，暗模式变量自动相互引用。例如：\\n\\n```python\\ntheme = gr.themes.Default().set(\\n    button_primary_background_fill=\"#FF0000\",\\n    button_primary_background_fill_dark=\"#AAAAAA\",\\n    button_primary_border=\"*button_primary_background_fill\",\\n    button_primary_border_dark=\"*button_primary_background_fill_dark\",\\n)'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 6434}, page_content='```\\n\\n`button_primary_border_dark` 将从 `button_primary_background_fill_dark` 获取其值，因为暗模式总是使用变量的暗版本。\\n\\n## 创建一个完整的主题\\n\\n假设您想从头开始创建一个主题！我们将逐步进行 - 您还可以参考 gradio 源代码库中预构建主题的源代码，请看这里的示例：[Monochrome theme 的源代码](https://github.com/gradio-app/gradio/blob/main/gradio/themes/monochrome.py)\\n我们的新主题类将继承自 `gradio.themes.Base`，这是一个设置了许多方便默认值的主题。让我们创建一个名为 Seafoam 的简单演示，以及使用它的简单应用程序。\\n$code_theme_new_step_1\\n\\n<div class=\"wrapper\">\\n<iframe\\n\\tsrc=\"https://gradio-theme-new-step-1.hf.space?__theme=light\"\\n\\tframeborder=\"0\"\\n></iframe>\\n</div>\\n\\nBase 主题非常简洁，使用 `gr.themes.Blue` 作为其主要颜色-由于此原因，主按钮和加载动画都是蓝色的。让我们改变应用程序的默认核心参数。我们将覆盖构造函数并传递新的默认值给核心构造函数参数。\\n我们将使用 `gr.themes.Emerald` 作为我们的主要颜色，并将次要和中性色调设置为 `gr.themes.Blue`。我们将使用 `text_lg` 使文本更大。我们将使用 `Quicksand` 作为我们的默认字体，从 Google Fonts 加载。\\n$code_theme_new_step_2'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 7213}, page_content='<div class=\"wrapper\">\\n<iframe\\n\\tsrc=\"https://gradio-theme-new-step-2.hf.space?__theme=light\"\\n\\tframeborder=\"0\"\\n></iframe>\\n</div>\\n\\n注意到主按钮和加载动画现在是绿色的了吗？这些 CSS 变量与 `primary_hue` 相关联。\\n我们来直接修改主题。我们将调用 `set()` 方法来明确覆盖 CSS 变量值。我们可以使用任何 CSS 逻辑，并使用`*` 前缀引用我们的核心构造函数的参数。\\n\\n$code_theme_new_step_3\\n\\n<div class=\"wrapper\">\\n<iframe\\n\\tsrc=\"https://gradio-theme-new-step-3.hf.space?__theme=light\"\\n\\tframeborder=\"0\"\\n></iframe>\\n</div>\\n\\n看看我们的主题现在多么有趣！仅通过几个变量的更改，我们的主题完全改变了。\\n\\n您可能会发现探索[其他预建主题的源代码](https://github.com/gradio-app/gradio/blob/main/gradio/themes)会很有帮助，以了解他们如何修改基本主题。您还可以使用浏览器的检查工具，选择 UI 中的元素并查看在样式面板中使用的 CSS 变量。\\n\\n## 分享主题\\n\\n在创建主题后，您可以将其上传到 HuggingFace Hub，让其他人查看、使用和构建主题！\\n\\n### 上传主题\\n\\n有两种上传主题的方式，通过主题类实例或命令行。我们将使用之前创建的“seafoam”主题来介绍这两种方式。\\n\\n- 通过类实例'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 7935}, page_content='- 通过类实例\\n\\n每个主题实例都有一个名为“push_to_hub”的方法，我们可以使用它来将主题上传到 HuggingFace Hub。\\n\\n```python\\nseafoam.push_to_hub(repo_name=\"seafoam\",\\n                    version=\"0.0.1\",\\n\\t\\t\\t\\t\\thf_token=\"<token>\")'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 8119}, page_content='```\\n\\n- 通过命令行\\n\\n首先将主题保存到磁盘\\n\\n```python\\nseafoam.dump(filename=\"seafoam.json\")\\n```\\n\\n然后使用“upload_theme”命令：\\n\\n```bash\\nupload_theme\\\\\\n\"seafoam.json\"\\\\\\n\"seafoam\"\\\\\\n--version \"0.0.1\"\\\\\\n--hf_token \"<token>\"'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 8310}, page_content='```\\n\\n要上传主题，您必须拥有一个 HuggingFace 账户，并通过 `hf_token` 参数传递您的[访问令牌](https://huggingface.co/docs/huggingface_hub/quick-start#login)。\\n但是，如果您通过[HuggingFace 命令行](https://huggingface.co/docs/huggingface_hub/quick-start#login)登录（与 `gradio` 一起安装），\\n那么您可以省略 `hf_token` 参数。\\n\\n`version` 参数允许您为主题指定一个有效的[语义版本](https://www.geeksforgeeks.org/introduction-semantic-versioning/)字符串。\\n这样，您的用户就可以在他们的应用程序中指定要使用的主题版本。这还允许您发布主题更新而不必担心\\n以前创建的应用程序的外观如何更改。`version` 参数是可选的。如果省略，下一个修订版本将自动应用。\\n\\n### 主题预览\\n\\n通过调用 `push_to_hub` 或 `upload_theme`，主题资源将存储在[HuggingFace 空间](https://huggingface.co/docs/hub/spaces-overview)中。\\n\\n我们的 seafoam 主题的预览在这里：[seafoam 预览](https://huggingface.co/spaces/gradio/seafoam)。\\n\\n<div class=\"wrapper\">\\n<iframe\\n\\tsrc=\"https://gradio-seafoam.hf.space?__theme=light\"\\n\\tframeborder=\"0\"\\n></iframe>\\n</div>\\n\\n### 发现主题'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 9095}, page_content='### 发现主题\\n\\n[主题库](https://huggingface.co/spaces/gradio/theme-gallery)显示了所有公开的 gradio 主题。在发布主题之后，\\n它将在几分钟后自动显示在主题库中。\\n\\n您可以按照空间上点赞的数量以及按创建时间从最近到最近对主题进行排序，也可以在浅色和深色模式之间切换主题。\\n\\n<div class=\"wrapper\">\\n<iframe\\n\\tsrc=\"https://gradio-theme-gallery.hf.space\"\\n\\tframeborder=\"0\"\\n></iframe>\\n</div>\\n\\n### 下载\\n\\n要使用 Hub 中的主题，请在 `ThemeClass` 上使用 `from_hub` 方法，然后将其传递给您的应用程序：\\n\\n```python\\nmy_theme = gr.Theme.from_hub(\"gradio/seafoam\")\\n\\nwith gr.Blocks(theme=my_theme) as demo:\\n    ....'),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/theming-guide.md', 'start_index': 9552}, page_content='```\\n\\n您也可以直接将主题字符串传递给 `Blocks` 或 `Interface`（`gr.Blocks(theme=\"gradio/seafoam\")`）\\n\\n您可以通过使用语义版本表达式将您的应用程序固定到上游主题版本。\\n\\n例如，以下内容将确保我们从“seafoam”仓库中加载的主题位于 `0.0.1` 和 `0.1.0` 版本之间：\\n\\n```python\\nwith gr.Blocks(theme=\"gradio/seafoam@>=0.0.1,<0.1.0\") as demo:\\n    ....\\n```\\n\\n享受创建自己的主题吧！如果您制作了一个自豪的主题，请将其上传到 Hub 与世界分享！\\n如果在[Twitter](https://twitter.com/gradio)上标记我们，我们可以给您的主题一个宣传！\\n\\n<style>\\n.wrapper {\\n    position: relative;\\n    padding-bottom: 56.25%;\\n    padding-top: 25px;\\n    height: 0;\\n}\\n.wrapper iframe {\\n    position: absolute;\\n    top: 0;\\n    left: 0;\\n    width: 100%;\\n    height: 100%;\\n}\\n</style>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/inference-endpoints-llm.md', 'start_index': 0}, page_content=\"--\\ntitle:  Deploy LLMs with Hugging Face Inference Endpoints\\nthumbnail: /blog/assets/155_inference_endpoints_llm/thumbnail.jpg\\nauthors:\\n- user: philschmid\\n---\\n\\n# Deploy LLMs with Hugging Face Inference Endpoints\\n\\n\\nOpen-source LLMs like [Falcon](https://huggingface.co/tiiuae/falcon-40b), [(Open-)LLaMA](https://huggingface.co/openlm-research/open_llama_13b), [X-Gen](https://huggingface.co/Salesforce/xgen-7b-8k-base), [StarCoder](https://huggingface.co/bigcode/starcoder) or [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base), have come a long way in recent months and can compete with closed-source models like ChatGPT or GPT4 for certain use cases. However, deploying these models in an efficient and optimized way still presents a challenge.\\n\\nIn this blog post, we will show you how to deploy open-source LLMs to [Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/), our managed SaaS solution that makes it easy to deploy models. Additionally, we will teach you how to stream responses and test the performance of our endpoints. So let's get started!\\n\\n1. [How to deploy Falcon 40B instruct](#1-how-to-deploy-falcon-40b-instruct)\\n2. [Test the LLM endpoint](#2-test-the-llm-endpoint)\\n3. [Stream responses in Javascript and Python](#3-stream-responses-in-javascript-and-python)\\n\\nBefore we start, let's refresh our knowledge about Inference Endpoints. \\n\\n## What is Hugging Face Inference Endpoints\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/inference-endpoints-llm.md', 'start_index': -1}, page_content=\"Before we start, let's refresh our knowledge about Inference Endpoints. \\n\\n## What is Hugging Face Inference Endpoints\\n\\n[Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/) offers an easy and secure way to deploy Machine Learning models for use in production. Inference Endpoints empower developers and data scientists alike to create AI applications without managing infrastructure: simplifying the deployment process to a few clicks, including handling large volumes of requests with autoscaling, reducing infrastructure costs with scale-to-zero, and offering advanced security. \\n\\nHere are some of the most important features for LLM deployment:\\n\\n1. [Easy Deployment](https://huggingface.co/docs/inference-endpoints/index): Deploy models as production-ready APIs with just a few clicks, eliminating the need to handle infrastructure or MLOps.\\n2. [Cost Efficiency](https://huggingface.co/docs/inference-endpoints/autoscaling): Benefit from automatic scale to zero capability, reducing costs by scaling down the infrastructure when the endpoint is not in use, while paying based on the uptime of the endpoint, ensuring cost-effectiveness.\\n3. [Enterprise Security](https://huggingface.co/docs/inference-endpoints/security): Deploy models in secure offline endpoints accessible only through direct VPC connections, backed by SOC2 Type 2 certification, and offering BAA and GDPR data processing agreements for enhanced data security and compliance.\\n4. [LLM Optimization](https://huggingface.co/text-generation-inference): Optimized for LLMs, enabling high throughput with Paged Attention and low latency through custom transformers code and Flash Attention power by Text Generation Inference\\n5. [Comprehensive Task Support](https://huggingface.co/docs/inference-endpoints/supported_tasks): Out of the box support for 🤗 Transformers, Sentence-Transformers, and Diffusers tasks and models, and easy customization to enable advanced tasks like speaker diarization or any Machine Learning task and library.\\n\\nYou can get started with Inference Endpoints at: [https://ui.endpoints.huggingface.co/](https://ui.endpoints.huggingface.co/)\\n\\n## 1. How to deploy Falcon 40B instruct\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/inference-endpoints-llm.md', 'start_index': 3474}, page_content='## 1. How to deploy Falcon 40B instruct\\n\\nTo get started, you need to be logged in with a User or Organization account with a payment method on file (you can add one **[here](https://huggingface.co/settings/billing)**), then access Inference Endpoints at **[https://ui.endpoints.huggingface.co](https://ui.endpoints.huggingface.co/endpoints)**\\n\\nThen, click on “New endpoint”. Select the repository, the cloud, and the region, adjust the instance and security settings, and deploy in our case `tiiuae/falcon-40b-instruct`.\\n\\n![Select Hugging Face Repository](assets/155_inference_endpoints_llm/repository.png \"Select Hugging Face Repository\")\\n\\nInference Endpoints suggest an instance type based on the model size, which should be big enough to run the model. Here `4x NVIDIA T4` GPUs. To get the best performance for the LLM, change the instance to `GPU [xlarge] · 1x Nvidia A100`.\\n\\n*Note: If the instance type cannot be selected, you need to\\xa0[contact us](mailto:api-enterprise@huggingface.co?subject=Quota%20increase%20HF%20Endpoints&body=Hello,%0D%0A%0D%0AI%20would%20like%20to%20request%20access/quota%20increase%20for%20{INSTANCE%20TYPE}%20for%20the%20following%20account%20{HF%20ACCOUNT}.)\\xa0and request an instance quota.*\\n\\n![Select Instance Type](assets/155_inference_endpoints_llm/instance-selection.png \"Select Instance Type\")\\n\\nYou can then deploy your model with a click on “Create Endpoint”. After 10 minutes, the Endpoint should be online and available to serve requests. \\n\\n## 2. Test the LLM endpoint'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/inference-endpoints-llm.md', 'start_index': -1}, page_content='You can then deploy your model with a click on “Create Endpoint”. After 10 minutes, the Endpoint should be online and available to serve requests. \\n\\n## 2. Test the LLM endpoint\\n\\nThe Endpoint overview provides access to the Inference Widget, which can be used to manually send requests. This allows you to quickly test your Endpoint with different inputs and share it with team members. Those Widgets do not support parameters - in this case this results to a “short” generation. \\n\\n![Test Inference Widget](assets/155_inference_endpoints_llm/widget.png \"Test Inference Widget\")\\n\\nThe widget also generates a cURL command you can use. Just add your `hf_xxx` and test. \\n\\n```python\\ncurl https://j4xhm53fxl9ussm8.us-east-1.aws.endpoints.huggingface.cloud \\\\\\n-X POST \\\\\\n-d \\'{\"inputs\":\"Once upon a time,\"}\\' \\\\\\n-H \"Authorization: Bearer <hf_token>\" \\\\\\n-H \"Content-Type: application/json\"'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/inference-endpoints-llm.md', 'start_index': 5681}, page_content='```\\n\\nYou can use different parameters to control the generation, defining them in the `parameters` attribute of the payload. As of today, the following parameters are supported:\\n\\n- `temperature`: Controls randomness in the model. Lower values will make the model more deterministic and higher values will make the model more random. Default value is 1.0.\\n- `max_new_tokens`: The maximum number of tokens to generate. Default value is 20, max value is 512.\\n- `repetition_penalty`: Controls the likelihood of repetition. Default is\\xa0`null`.\\n- `seed`: The seed to use for random generation. Default is\\xa0`null`.\\n- `stop`: A list of tokens to stop the generation. The generation will stop when one of the tokens is generated.\\n- `top_k`: The number of highest probability vocabulary tokens to keep for top-k-filtering. Default value is\\xa0`null`, which disables top-k-filtering.\\n- `top_p`: The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling, default to\\xa0`null`\\n- `do_sample`: Whether or not to use sampling; use greedy decoding otherwise. Default value is\\xa0`false`.\\n- `best_of`: Generate best_of sequences and return the one if the highest token logprobs, default to\\xa0`null`.\\n- `details`: Whether or not to return details about the generation. Default value is\\xa0`false`.\\n- `return_full_text`: Whether or not to return the full text or only the generated part. Default value is\\xa0`false`.\\n- `truncate`: Whether or not to truncate the input to the maximum length of the model. Default value is\\xa0`true`.\\n- `typical_p`: The typical probability of a token. Default value is\\xa0`null`.\\n- `watermark`: The watermark to use for the generation. Default value is\\xa0`false`.\\n\\n## 3. Stream responses in Javascript and Python'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/inference-endpoints-llm.md', 'start_index': 7379}, page_content='## 3. Stream responses in Javascript and Python\\n\\nRequesting and generating text with LLMs can be a time-consuming and iterative process. A great way to improve the user experience is streaming tokens to the user as they are generated. Below are two examples of how to stream tokens using Python and JavaScript. For Python, we are going to use the [client from Text Generation Inference](https://github.com/huggingface/text-generation-inference/tree/main/clients/python), and for JavaScript, the [HuggingFace.js library](https://huggingface.co/docs/huggingface.js/main/en/index)\\n\\n### Streaming requests with Python\\n\\nFirst, you need to install the `huggingface_hub` library:\\n\\n```python\\npip install -U huggingface_hub'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/inference-endpoints-llm.md', 'start_index': 8094}, page_content='```\\n\\nWe can create a `InferenceClient` providing our endpoint URL and credential alongside the hyperparameters we want to use\\n\\n```python\\nfrom huggingface_hub import InferenceClient\\n\\n# HF Inference Endpoints parameter\\nendpoint_url = \"https://YOUR_ENDPOINT.endpoints.huggingface.cloud\"\\nhf_token = \"hf_YOUR_TOKEN\"\\n\\n# Streaming Client\\nclient = InferenceClient(endpoint_url, token=hf_token)\\n\\n# generation parameter\\ngen_kwargs = dict(\\n    max_new_tokens=512,\\n    top_k=30,\\n    top_p=0.9,\\n    temperature=0.2,\\n    repetition_penalty=1.02,\\n    stop_sequences=[\"\\\\nUser:\", \"<|endoftext|>\", \"</s>\"],\\n)\\n# prompt\\nprompt = \"What can you do in Nuremberg, Germany? Give me 3 Tips\"\\n\\nstream = client.text_generation(prompt, stream=True, details=True, **gen_kwargs)\\n\\n# yield each generated token\\nfor r in stream:\\n    # skip special tokens\\n    if r.token.special:\\n        continue\\n    # stop if we encounter a stop sequence\\n    if r.token.text in gen_kwargs[\"stop_sequences\"]:\\n        break\\n    # yield the generated token\\n    print(r.token.text, end = \"\")\\n    # yield r.token.text\\n```\\n\\nReplace the `print` command with the `yield` or with a function you want to stream the tokens to. \\n\\n![Python Streaming](assets/155_inference_endpoints_llm/python-stream.gif \"Python Streaming\")\\n\\n### Streaming requests with JavaScript\\n\\nFirst, you need to install the `@huggingface/inference` library.\\n\\n```python\\nnpm install @huggingface/inference'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/inference-endpoints-llm.md', 'start_index': 9506}, page_content=\"```\\n\\nWe can create a `HfInferenceEndpoint` providing our endpoint URL and credential alongside the hyperparameter we want to use.\\n\\n```jsx\\nimport { HfInferenceEndpoint } from '@huggingface/inference'\\n\\nconst hf = new HfInferenceEndpoint('https://YOUR_ENDPOINT.endpoints.huggingface.cloud', 'hf_YOUR_TOKEN')\\n\\n//generation parameter\\nconst gen_kwargs = {\\n  max_new_tokens: 512,\\n  top_k: 30,\\n  top_p: 0.9,\\n  temperature: 0.2,\\n  repetition_penalty: 1.02,\\n  stop_sequences: ['\\\\nUser:', '<|endoftext|>', '</s>'],\\n}\\n// prompt\\nconst prompt = 'What can you do in Nuremberg, Germany? Give me 3 Tips'\\n\\nconst stream = hf.textGenerationStream({ inputs: prompt, parameters: gen_kwargs })\\nfor await (const r of stream) {\\n  // # skip special tokens\\n  if (r.token.special) {\\n    continue\\n  }\\n  // stop if we encounter a stop sequence\\n  if (gen_kwargs['stop_sequences'].includes(r.token.text)) {\\n    break\\n  }\\n  // yield the generated token\\n  process.stdout.write(r.token.text)\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/inference-endpoints-llm.md', 'start_index': 10465}, page_content='```\\n\\nReplace the `process.stdout` call with the `yield` or with a function you want to stream the tokens to. \\n\\n![Javascript Streaming](assets/155_inference_endpoints_llm/js-stream.gif \"Javascript Streaming\")\\n\\n## Conclusion\\n\\nIn this blog post, we showed you how to deploy open-source LLMs using Hugging Face Inference Endpoints, how to control the text generation with advanced parameters, and how to stream responses to a Python or JavaScript client to improve the user experience. By using Hugging Face Inference Endpoints you can deploy models as production-ready APIs with just a few clicks, reduce your costs with automatic scale to zero, and deploy models into secure offline endpoints backed by SOC2 Type 2 certification.\\n\\n---\\n\\nThanks for reading! If you have any questions, feel free to contact me on [Twitter](https://twitter.com/_philschmid) or [LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/source/package_reference/loha.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# LoHa\\n\\nLow-Rank Hadamard Product ([LoHa](https://huggingface.co/papers/2108.06098)), is similar to LoRA except it approximates the large weight matrix with more low-rank matrices and combines them with the Hadamard product. This method is even more parameter-efficient than LoRA and achieves comparable performance.\\n\\nThe abstract from the paper is:\\n\\n*In this work, we propose a communication-efficient parameterization, FedPara, for federated learning (FL) to overcome the burdens on frequent model uploads and downloads. Our method re-parameterizes weight parameters of layers using low-rank weights followed by the Hadamard product. Compared to the conventional low-rank parameterization, our FedPara method is not restricted to low-rank constraints, and thereby it has a far larger capacity. This property enables to achieve comparable performance while requiring 3 to 10 times lower communication costs than the model with the original layers, which is not achievable by the traditional low-rank methods. The efficiency of our method can be further improved by combining with other efficient FL optimizers. In addition, we extend our method to a personalized FL application, pFedPara, which separates parameters into global and local ones. We show that pFedPara outperforms competing personalized FL methods with more than three times fewer parameters*.\\n\\n## LoHaConfig\\n\\n[[autodoc]] tuners.loha.config.LoHaConfig\\n\\n## LoHaModel\\n\\n[[autodoc]] tuners.loha.model.LoHaModel'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 1}, page_content=\"Quicktour\\n\\nLet's have a quick look at the 🤗 Tokenizers library features. The\\nlibrary provides an implementation of today's most used tokenizers that\\nis both easy to use and blazing fast.\\n\\n## Build a tokenizer from scratch\\n\\nTo illustrate how fast the 🤗 Tokenizers library is, let's train a new\\ntokenizer on [wikitext-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)\\n(516M of text) in just a few seconds. First things first, you will need\\nto download this dataset and unzip it with:\\n\\n``` bash\\nwget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\\nunzip wikitext-103-raw-v1.zip\"),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 649}, page_content='```\\n\\n### Training the tokenizer\\n\\nIn this tour, we will build and train a Byte-Pair Encoding (BPE)\\ntokenizer. For more information about the different type of tokenizers,\\ncheck out this [guide](https://huggingface.co/transformers/tokenizer_summary.html) in\\nthe 🤗 Transformers documentation. Here, training the tokenizer means it\\nwill learn merge rules by:\\n\\n-   Start with all the characters present in the training corpus as\\n    tokens.\\n-   Identify the most common pair of tokens and merge it into one token.\\n-   Repeat until the vocabulary (e.g., the number of tokens) has reached\\n    the size we want.\\n\\nThe main API of the library is the `class` `Tokenizer`, here is how\\nwe instantiate one with a BPE model:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START init_tokenizer\",\\n\"end-before\": \"END init_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_init_tokenizer\",\\n\"end-before\": \"END quicktour_init_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START init_tokenizer\",\\n\"end-before\": \"END init_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 2105}, page_content='To train our tokenizer on the wikitext files, we will need to\\ninstantiate a [trainer]{.title-ref}, in this case a\\n`BpeTrainer`\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START init_trainer\",\\n\"end-before\": \"END init_trainer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_init_trainer\",\\n\"end-before\": \"END quicktour_init_trainer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START init_trainer\",\\n\"end-before\": \"END init_trainer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nWe can set the training arguments like `vocab_size` or `min_frequency` (here\\nleft at their default values of 30,000 and 0) but the most important\\npart is to give the `special_tokens` we\\nplan to use later on (they are not used at all during training) so that\\nthey get inserted in the vocabulary.\\n\\n<Tip>\\n\\nThe order in which you write the special tokens list matters: here `\"[UNK]\"` will get the ID 0,\\n`\"[CLS]\"` will get the ID 1 and so forth.\\n\\n</Tip>'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': -1}, page_content='<Tip>\\n\\nThe order in which you write the special tokens list matters: here `\"[UNK]\"` will get the ID 0,\\n`\"[CLS]\"` will get the ID 1 and so forth.\\n\\n</Tip>\\n\\nWe could train our tokenizer right now, but it wouldn\\'t be optimal.\\nWithout a pre-tokenizer that will split our inputs into words, we might\\nget tokens that overlap several words: for instance we could get an\\n`\"it is\"` token since those two words\\noften appear next to each other. Using a pre-tokenizer will ensure no\\ntoken is bigger than a word returned by the pre-tokenizer. Here we want\\nto train a subword BPE tokenizer, and we will use the easiest\\npre-tokenizer possible by splitting on whitespace.\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START init_pretok\",\\n\"end-before\": \"END init_pretok\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_init_pretok\",\\n\"end-before\": \"END quicktour_init_pretok\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START init_pretok\",\\n\"end-before\": \"END init_pretok\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nNow, we can just call the `Tokenizer.train` method with any list of files we want to use:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 4645}, page_content='Now, we can just call the `Tokenizer.train` method with any list of files we want to use:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START train\",\\n\"end-before\": \"END train\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_train\",\\n\"end-before\": \"END quicktour_train\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START train\",\\n\"end-before\": \"END train\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThis should only take a few seconds to train our tokenizer on the full\\nwikitext dataset! To save the tokenizer in one file that contains all\\nits configuration and vocabulary, just use the\\n`Tokenizer.save` method:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': -1}, page_content='This should only take a few seconds to train our tokenizer on the full\\nwikitext dataset! To save the tokenizer in one file that contains all\\nits configuration and vocabulary, just use the\\n`Tokenizer.save` method:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START save\",\\n\"end-before\": \"END save\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_save\",\\n\"end-before\": \"END quicktour_save\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START save\",\\n\"end-before\": \"END save\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nand you can reload your tokenizer from that file with the\\n`Tokenizer.from_file`\\n`classmethod`:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 6326}, page_content='and you can reload your tokenizer from that file with the\\n`Tokenizer.from_file`\\n`classmethod`:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START reload_tokenizer\",\\n\"end-before\": \"END reload_tokenizer\",\\n\"dedent\": 12}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_reload_tokenizer\",\\n\"end-before\": \"END quicktour_reload_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START reload_tokenizer\",\\n\"end-before\": \"END reload_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\n### Using the tokenizer\\n\\nNow that we have trained a tokenizer, we can use it on any text we want\\nwith the `Tokenizer.encode` method:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': -1}, page_content='### Using the tokenizer\\n\\nNow that we have trained a tokenizer, we can use it on any text we want\\nwith the `Tokenizer.encode` method:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START encode\",\\n\"end-before\": \"END encode\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_encode\",\\n\"end-before\": \"END quicktour_encode\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START encode\",\\n\"end-before\": \"END encode\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThis applied the full pipeline of the tokenizer on the text, returning\\nan `Encoding` object. To learn more\\nabout this pipeline, and how to apply (or customize) parts of it, check out [this page](pipeline).\\n\\nThis `Encoding` object then has all the\\nattributes you need for your deep learning model (or other). The\\n`tokens` attribute contains the\\nsegmentation of your text in tokens:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 8218}, page_content='This `Encoding` object then has all the\\nattributes you need for your deep learning model (or other). The\\n`tokens` attribute contains the\\nsegmentation of your text in tokens:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START print_tokens\",\\n\"end-before\": \"END print_tokens\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_print_tokens\",\\n\"end-before\": \"END quicktour_print_tokens\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START print_tokens\",\\n\"end-before\": \"END print_tokens\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nSimilarly, the `ids` attribute will\\ncontain the index of each of those tokens in the tokenizer\\'s\\nvocabulary:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': -1}, page_content='Similarly, the `ids` attribute will\\ncontain the index of each of those tokens in the tokenizer\\'s\\nvocabulary:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START print_ids\",\\n\"end-before\": \"END print_ids\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_print_ids\",\\n\"end-before\": \"END quicktour_print_ids\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START print_ids\",\\n\"end-before\": \"END print_ids\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nAn important feature of the 🤗 Tokenizers library is that it comes with\\nfull alignment tracking, meaning you can always get the part of your\\noriginal sentence that corresponds to a given token. Those are stored in\\nthe `offsets` attribute of our\\n`Encoding` object. For instance, let\\'s\\nassume we would want to find back what caused the\\n`\"[UNK]\"` token to appear, which is the\\ntoken at index 9 in the list, we can just ask for the offset at the\\nindex:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 10400}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START print_offsets\",\\n\"end-before\": \"END print_offsets\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_print_offsets\",\\n\"end-before\": \"END quicktour_print_offsets\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START print_offsets\",\\n\"end-before\": \"END print_offsets\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nand those are the indices that correspond to the emoji in the original\\nsentence:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': -1}, page_content='and those are the indices that correspond to the emoji in the original\\nsentence:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START use_offsets\",\\n\"end-before\": \"END use_offsets\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_use_offsets\",\\n\"end-before\": \"END quicktour_use_offsets\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START use_offsets\",\\n\"end-before\": \"END use_offsets\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\n### Post-processing\\n\\nWe might want our tokenizer to automatically add special tokens, like\\n`\"[CLS]\"` or `\"[SEP]\"`. To do this, we use a post-processor.\\n`TemplateProcessing` is the most\\ncommonly used, you just have to specify a template for the processing of\\nsingle sentences and pairs of sentences, along with the special tokens\\nand their IDs.\\n\\nWhen we built our tokenizer, we set `\"[CLS]\"` and `\"[SEP]\"` in positions 1\\nand 2 of our list of special tokens, so this should be their IDs. To\\ndouble-check, we can use the `Tokenizer.token_to_id` method:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 12499}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START check_sep\",\\n\"end-before\": \"END check_sep\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_check_sep\",\\n\"end-before\": \"END quicktour_check_sep\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START check_sep\",\\n\"end-before\": \"END check_sep\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nHere is how we can set the post-processing to give us the traditional\\nBERT inputs:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': -1}, page_content='Here is how we can set the post-processing to give us the traditional\\nBERT inputs:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START init_template_processing\",\\n\"end-before\": \"END init_template_processing\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_init_template_processing\",\\n\"end-before\": \"END quicktour_init_template_processing\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START init_template_processing\",\\n\"end-before\": \"END init_template_processing\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nLet\\'s go over this snippet of code in more details. First we specify\\nthe template for single sentences: those should have the form\\n`\"[CLS] $A [SEP]\"` where\\n`$A` represents our sentence.\\n\\nThen, we specify the template for sentence pairs, which should have the\\nform `\"[CLS] $A [SEP] $B [SEP]\"` where\\n`$A` represents the first sentence and\\n`$B` the second one. The\\n`:1` added in the template represent the `type IDs` we want for each part of our input: it defaults\\nto 0 for everything (which is why we don\\'t have\\n`$A:0`) and here we set it to 1 for the\\ntokens of the second sentence and the last `\"[SEP]\"` token.\\n\\nLastly, we specify the special tokens we used and their IDs in our\\ntokenizer\\'s vocabulary.'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 14714}, page_content='Lastly, we specify the special tokens we used and their IDs in our\\ntokenizer\\'s vocabulary.\\n\\nTo check out this worked properly, let\\'s try to encode the same\\nsentence as before:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START print_special_tokens\",\\n\"end-before\": \"END print_special_tokens\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_print_special_tokens\",\\n\"end-before\": \"END quicktour_print_special_tokens\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START print_special_tokens\",\\n\"end-before\": \"END print_special_tokens\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nTo check the results on a pair of sentences, we just pass the two\\nsentences to `Tokenizer.encode`:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': -1}, page_content='To check the results on a pair of sentences, we just pass the two\\nsentences to `Tokenizer.encode`:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START print_special_tokens_pair\",\\n\"end-before\": \"END print_special_tokens_pair\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_print_special_tokens_pair\",\\n\"end-before\": \"END quicktour_print_special_tokens_pair\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START print_special_tokens_pair\",\\n\"end-before\": \"END print_special_tokens_pair\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nYou can then check the type IDs attributed to each token is correct with'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 16583}, page_content='You can then check the type IDs attributed to each token is correct with\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START print_type_ids\",\\n\"end-before\": \"END print_type_ids\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_print_type_ids\",\\n\"end-before\": \"END quicktour_print_type_ids\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START print_type_ids\",\\n\"end-before\": \"END print_type_ids\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nIf you save your tokenizer with `Tokenizer.save`, the post-processor will be saved along.\\n\\n### Encoding multiple sentences in a batch\\n\\nTo get the full speed of the 🤗 Tokenizers library, it\\'s best to\\nprocess your texts by batches by using the\\n`Tokenizer.encode_batch` method:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': -1}, page_content='### Encoding multiple sentences in a batch\\n\\nTo get the full speed of the 🤗 Tokenizers library, it\\'s best to\\nprocess your texts by batches by using the\\n`Tokenizer.encode_batch` method:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START encode_batch\",\\n\"end-before\": \"END encode_batch\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_encode_batch\",\\n\"end-before\": \"END quicktour_encode_batch\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START encode_batch\",\\n\"end-before\": \"END encode_batch\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThe output is then a list of `Encoding`\\nobjects like the ones we saw before. You can process together as many\\ntexts as you like, as long as it fits in memory.\\n\\nTo process a batch of sentences pairs, pass two lists to the\\n`Tokenizer.encode_batch` method: the\\nlist of sentences A and the list of sentences B:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 18571}, page_content='To process a batch of sentences pairs, pass two lists to the\\n`Tokenizer.encode_batch` method: the\\nlist of sentences A and the list of sentences B:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START encode_batch_pair\",\\n\"end-before\": \"END encode_batch_pair\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_encode_batch_pair\",\\n\"end-before\": \"END quicktour_encode_batch_pair\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START encode_batch_pair\",\\n\"end-before\": \"END encode_batch_pair\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nWhen encoding multiple sentences, you can automatically pad the outputs\\nto the longest sentence present by using\\n`Tokenizer.enable_padding`, with the\\n`pad_token` and its ID (which we can\\ndouble-check the id for the padding token with\\n`Tokenizer.token_to_id` like before):'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 19755}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START enable_padding\",\\n\"end-before\": \"END enable_padding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_enable_padding\",\\n\"end-before\": \"END quicktour_enable_padding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START enable_padding\",\\n\"end-before\": \"END enable_padding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nWe can set the `direction` of the padding\\n(defaults to the right) or a given `length` if we want to pad every sample to that specific number (here\\nwe leave it unset to pad to the size of the longest text).'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 20707}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START print_batch_tokens\",\\n\"end-before\": \"END print_batch_tokens\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_print_batch_tokens\",\\n\"end-before\": \"END quicktour_print_batch_tokens\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START print_batch_tokens\",\\n\"end-before\": \"END print_batch_tokens\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nIn this case, the `attention mask` generated by the\\ntokenizer takes the padding into account:'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': -1}, page_content='In this case, the `attention mask` generated by the\\ntokenizer takes the padding into account:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_quicktour.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START print_attention_mask\",\\n\"end-before\": \"END print_attention_mask\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START quicktour_print_attention_mask\",\\n\"end-before\": \"END quicktour_print_attention_mask\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/quicktour.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START print_attention_mask\",\\n\"end-before\": \"END print_attention_mask\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\n## Pretrained\\n\\n<tokenizerslangcontent>\\n<python>\\n### Using a pretrained tokenizer\\n\\nYou can load any tokenizer from the Hugging Face Hub as long as a\\n`tokenizer.json` file is available in the repository.\\n\\n```python\\nfrom tokenizers import Tokenizer\\n\\ntokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")'),\n",
       " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/quicktour.mdx', 'start_index': 22658}, page_content='```\\n\\n### Importing a pretrained tokenizer from legacy vocabulary files\\n\\nYou can also import a pretrained tokenizer directly in, as long as you\\nhave its vocabulary file. For instance, here is how to import the\\nclassic pretrained BERT tokenizer:\\n\\n```python\\nfrom tokenizers import BertWordPieceTokenizer\\n\\ntokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\\n```\\n\\nas long as you have downloaded the file `bert-base-uncased-vocab.txt` with\\n\\n```bash\\nwget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\\n```\\n</python>\\n</tokenizerslangcontent>'),\n",
       " Document(metadata={'source': 'huggingface/optimum/blob/main/examples/onnxruntime/training/translation/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# Translation\\n\\nBy running the script [`run_translation.py`](https://github.com/huggingface/optimum/blob/main/examples/onnxruntime/training/translation/run_translation.py),\\nwe will be able to leverage the [`ONNX Runtime`](https://github.com/microsoft/onnxruntime) to fine-tune the models from the\\n[HuggingFace hub](https://huggingface.co/models) for translation tasks.\\n\\n### Supported Architectures\\n\\n- `BartForConditionalGeneration`\\n- `T5ForConditionalGeneration`\\n\\n`run_translation.py` is a lightweight examples of how to download and preprocess a dataset from the [🤗 Datasets](https://github.com/huggingface/datasets) library\\nor use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.\\n\\nFor custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_datasets.html#json-files.\\n\\n__The following example applies the acceleration features powered by ONNX Runtime.__\\n\\n\\n### Onnxruntime Training\\n\\nThe following example fine-tunes a T5 large model on the wmt16 dataset.'),\n",
       " Document(metadata={'source': 'huggingface/optimum/blob/main/examples/onnxruntime/training/translation/README.md', 'start_index': -1}, page_content='__The following example applies the acceleration features powered by ONNX Runtime.__\\n\\n\\n### Onnxruntime Training\\n\\nThe following example fine-tunes a T5 large model on the wmt16 dataset.\\n\\n```bash\\ntorchrun --nproc_per_node=NUM_GPUS_YOU_HAVE run_translation.py \\\\\\n    --model_name_or_path t5-large \\\\\\n    --dataset_name wmt16 \\\\\\n    --dataset_config ro-en \\\\\\n    --label_smoothing 0.1 \\\\\\n    --predict_with_generate \\\\\\n    --source_lang en \\\\\\n    --target_lang ro \\\\\\n    --do_train \\\\\\n    --max_train_samples 30000 \\\\\\n    --fp16 \\\\\\n    --output_dir /tmp/ort_t5_translation/'),\n",
       " Document(metadata={'source': 'huggingface/optimum/blob/main/examples/onnxruntime/training/translation/README.md', 'start_index': 1999}, page_content='```\\n\\n### Performance\\n\\nWe get the following results for [t5-large](https://huggingface.co/t5-large) mixed precision training(fp16) on the previous\\ntask under PyTorch and ONNX Runtime backends. A single Nvidia A100 card was used to run the experiment for 3 epochs::\\n\\n| Model    | Backend      | Runtime(s) | Train samples(/s) |\\n| -------- | ------------ | ---------- | ----------------- |\\n| t5-large | PyTorch      | 2038.8     | 44.1              |\\n| t5-large | ONNX Runtime | 1536.7     | 58.6              |\\n\\nWe observe the gain of ONNX Runtime compared to PyTorch as follow:\\n\\n|       | Latency | Throughput |\\n| ----- | ------- | ---------- |\\n| Gain  | 24.63%  | 32.67%     |\\n\\n\\n__Note__\\n\\n> *To enable ONNX Runtime training, your devices need to be equipped with GPU. Install the dependencies either with our prepared*\\n*[Dockerfiles](https://github.com/huggingface/optimum/blob/main/examples/onnxruntime/training/docker/) or follow the instructions*\\n*in [`torch_ort`](https://github.com/pytorch/ort/blob/main/torch_ort/docker/README.md).*\\n\\n> *The inference will use PyTorch by default, if you want to use ONNX Runtime backend instead, add the flag `--inference_with_ort`.*\\n---'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/res2next.md', 'start_index': 1}, page_content='Res2NeXt\\n\\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https://paperswithcode.com/method/resnext) bottleneck residual blocks. The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model(\\'res2next50\\', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/res2next.md', 'start_index': 1392}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]\\n```\\n\\nReplace the model name with the variant you want to use, e.g. `res2next50`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model(\\'res2next50\\', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/res2next.md', 'start_index': 2739}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@article{Gao_2021,\\n   title={Res2Net: A New Multi-Scale Backbone Architecture},\\n   volume={43},\\n   ISSN={1939-3539},\\n   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},\\n   DOI={10.1109/tpami.2019.2938758},\\n   number={2},\\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\\n   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\\n   year={2021},\\n   month={Feb},\\n   pages={652–662}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/res2next.md', 'start_index': 3672}, page_content=\"```\\n\\n<!--\\nType: model-index\\nCollections:\\n- Name: Res2NeXt\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale Backbone Architecture'\\n    URL: https://paperswithcode.com/paper/res2net-a-new-multi-scale-backbone\\nModels:\\n- Name: res2next50\\n  In Collection: Res2NeXt\\n  Metadata:\\n    FLOPs: 5396798208\\n    Parameters: 24670000\\n    File Size: 99019592\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2NeXt Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2next50\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L207\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2next50_4s-6ef7e7bf.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.24%\\n      Top 5 Accuracy: 93.91%\\n-->\"),\n",
       " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/dashboard/DESCRIPTION.md', 'start_index': 0}, page_content=\"his demo shows how you can build an interactive dashboard with gradio. Click on a python library on the left hand side and then on the right hand side click on the metric you'd like to see plot over time. Data is pulled from HuggingFace Hub datasets.\"),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Token merging\\n\\n[Token merging](https://huggingface.co/papers/2303.17604) (ToMe) merges redundant tokens/patches progressively in the forward pass of a Transformer-based network which can speed-up the inference latency of [`StableDiffusionPipeline`].\\n\\nInstall ToMe from `pip`:\\n\\n```bash\\npip install tomesd\\n```\\n\\nYou can use ToMe from the [`tomesd`](https://github.com/dbolya/tomesd) library with the [`apply_patch`](https://github.com/dbolya/tomesd?tab=readme-ov-file#usage) function:\\n\\n```diff\\n  from diffusers import StableDiffusionPipeline\\n  import torch\\n  import tomesd\\n\\n  pipeline = StableDiffusionPipeline.from_pretrained(\\n        \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True,\\n  ).to(\"cuda\")\\n+ tomesd.apply_patch(pipeline, ratio=0.5)\\n\\n  image = pipeline(\"a photo of an astronaut riding a horse on mars\").images[0]'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md', 'start_index': 1442}, page_content='```\\n\\nThe `apply_patch` function exposes a number of [arguments](https://github.com/dbolya/tomesd#usage) to help strike a balance between pipeline inference speed and the quality of the generated tokens. The most important argument is `ratio` which controls the number of tokens that are merged during the forward pass.\\n\\nAs reported in the [paper](https://huggingface.co/papers/2303.17604), ToMe can greatly preserve the quality of the generated images while boosting inference speed. By increasing the `ratio`, you can speed-up inference even further, but at the cost of some degraded image quality.\\n\\nTo test the quality of the generated images, we sampled a few prompts from [Parti Prompts](https://parti.research.google/) and performed inference with the [`StableDiffusionPipeline`] with the following settings:\\n\\n<div class=\"flex justify-center\">\\n      <img src=\"https://huggingface.co/datasets/diffusers/docs-images/resolve/main/tome/tome_samples.png\">\\n</div>\\n\\nWe didn’t notice any significant decrease in the quality of the generated samples, and you can check out the generated samples in this [WandB report](https://wandb.ai/sayakpaul/tomesd-results/runs/23j4bj3i?workspace=). If you\\'re interested in reproducing this experiment, use this [script](https://gist.github.com/sayakpaul/8cac98d7f22399085a060992f411ecbd).\\n\\n## Benchmarks\\n\\nWe also benchmarked the impact of `tomesd` on the [`StableDiffusionPipeline`] with [xFormers](https://huggingface.co/docs/diffusers/optimization/xformers) enabled across several image resolutions. The results are obtained from A100 and V100 GPUs in the following development environment:'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md', 'start_index': 3070}, page_content='```bash\\n- `diffusers` version: 0.15.1\\n- Python version: 3.8.16\\n- PyTorch version (GPU?): 1.13.1+cu116 (True)\\n- Huggingface_hub version: 0.13.2\\n- Transformers version: 4.27.2\\n- Accelerate version: 0.18.0\\n- xFormers version: 0.0.16\\n- tomesd version: 0.1.2'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md', 'start_index': 3324}, page_content='```\\n\\nTo reproduce this benchmark, feel free to use this [script](https://gist.github.com/sayakpaul/27aec6bca7eb7b0e0aa4112205850335). The results are reported in seconds, and where applicable we report the speed-up percentage over the vanilla pipeline when using ToMe and ToMe + xFormers.'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md', 'start_index': 3614}, page_content='| **GPU**  | **Resolution** | **Batch size** | **Vanilla** | **ToMe**       | **ToMe + xFormers** |\\n|----------|----------------|----------------|-------------|----------------|---------------------|\\n| **A100** |            512 |             10 |        6.88 | 5.26 (+23.55%) |      4.69 (+31.83%) |\\n|          |            768 |             10 |         OOM |          14.71 |                  11 |\\n|          |                |              8 |         OOM |          11.56 |                8.84 |\\n|          |                |              4 |         OOM |           5.98 |                4.66 |\\n|          |                |              2 |        4.99 | 3.24 (+35.07%) |       2.1 (+37.88%) |\\n|          |                |              1 |        3.29 | 2.24 (+31.91%) |       2.03 (+38.3%) |\\n|          |           1024 |             10 |         OOM |            OOM |                 OOM |\\n|          |                |              8 |         OOM |            OOM |                 OOM |\\n|          |                |              4 |         OOM |          12.51 |                9.09 |\\n|          |                |              2 |         OOM |           6.52 |                4.96 |\\n|          |                |              1 |         6.4 | 3.61 (+43.59%) |      2.81 (+56.09%) |\\n| **V100** |            512 |             10 |         OOM |          10.03 |                9.29 |\\n|          |                |              8 |         OOM |           8.05 |                7.47 |\\n|          |                |              4 |         5.7 |  4.3 (+24.56%) |      3.98 (+30.18%) |\\n|          |                |              2 |        3.14 | 2.43 (+22.61%) |      2.27 (+27.71%) |'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md', 'start_index': -1}, page_content='|          |                |              2 |        3.14 | 2.43 (+22.61%) |      2.27 (+27.71%) |\\n|          |                |              1 |        1.88 | 1.57 (+16.49%) |      1.57 (+16.49%) |\\n|          |            768 |             10 |         OOM |            OOM |               23.67 |\\n|          |                |              8 |         OOM |            OOM |               18.81 |\\n|          |                |              4 |         OOM |          11.81 |                 9.7 |\\n|          |                |              2 |         OOM |           6.27 |                 5.2 |\\n|          |                |              1 |        5.43 | 3.38 (+37.75%) |      2.82 (+48.07%) |\\n|          |           1024 |             10 |         OOM |            OOM |                 OOM |\\n|          |                |              8 |         OOM |            OOM |                 OOM |\\n|          |                |              4 |         OOM |            OOM |               19.35 |\\n|          |                |              2 |         OOM |             13 |               10.78 |\\n|          |                |              1 |         OOM |           6.66 |                5.54 |'),\n",
       " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/optimization/tome.md', 'start_index': 6415}, page_content='As seen in the tables above, the speed-up from `tomesd` becomes more pronounced for larger image resolutions. It is also interesting to note that with `tomesd`, it is possible to run the pipeline on a higher resolution like 1024x1024. You may be able to speed-up inference even more with [`torch.compile`](torch2.0).'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/measurements/word_length/README.md', 'start_index': 0}, page_content='--\\ntitle: Word Length\\nemoji: 🤗\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- measurement\\ndescription: >-\\n  Returns the average length (in terms of the number of words) of the input data.\\n---\\n\\n# Measurement Card for Word Length\\n\\n\\n## Measurement Description\\n\\nThe `word_length` measurement returns the average word count of the input strings, based on tokenization using [NLTK word_tokenize](https://www.nltk.org/api/nltk.tokenize.html).\\n\\n## How to Use\\n\\nThis measurement requires a list of strings as input:\\n\\n```python\\n>>> data = [\"hello world\"]\\n>>> wordlength = evaluate.load(\"word_length\", module_type=\"measurement\")\\n>>> results = wordlength.compute(data=data)\\n```\\n\\n### Inputs\\n- **data** (list of `str`): The input list of strings for which the word length is calculated.\\n- **tokenizer** (`Callable`) : approach used for tokenizing `data` (optional). The default tokenizer is [NLTK\\'s `word_tokenize`](https://www.nltk.org/api/nltk.tokenize.html). This can be replaced by any function that takes a string as input and returns a list of tokens as output.\\n\\n### Output Values\\n- **average_word_length**(`float`): the average number of words in the input string(s).\\n\\nOutput Example(s):\\n\\n```python\\n{\"average_word_length\": 245}\\n```\\n\\nThis metric outputs a dictionary containing the number of words in the input string (`word length`).\\n\\n### Examples\\n\\nExample for a single string\\n\\n```python\\n>>> data = [\"hello sun and goodbye moon\"]\\n>>> wordlength = evaluate.load(\"word_length\", module_type=\"measurement\")\\n>>> results = wordlength.compute(data=data)\\n>>> print(results)\\n{\\'average_word_length\\': 5}'),\n",
       " Document(metadata={'source': 'huggingface/evaluate/blob/main/measurements/word_length/README.md', 'start_index': 1652}, page_content='```\\n\\nExample for a multiple strings\\n```python\\n>>> data = [\"hello sun and goodbye moon\", \"foo bar foo bar\"]\\n>>> wordlength = evaluate.load(\"word_length\", module_type=\"measurement\")\\n>>> results = wordlength.compute(data=text)\\n{\\'average_word_length\\': 4.5}\\n```\\n\\n## Citation(s)\\n\\n\\n## Further References\\n- [NLTK\\'s `word_tokenize`](https://www.nltk.org/api/nltk.tokenize.html)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ml-for-games-4.md', 'start_index': 0}, page_content='--\\ntitle: \"2D Asset Generation: AI for Game Development #4\"\\nthumbnail: /blog/assets/124_ml-for-games/thumbnail4.png\\nauthors:\\n- user: dylanebert\\n---\\n\\n# 2D Asset Generation: AI for Game Development #4\\n\\n\\n\\n**Welcome to AI for Game Development!** In this series, we\\'ll be using AI tools to create a fully functional farming game in just 5 days. By the end of this series, you will have learned how you can incorporate a variety of AI tools into your game development workflow. I will show you how you can use AI tools for:\\n\\n1. Art Style\\n2. Game Design\\n3. 3D Assets\\n4. 2D Assets\\n5. Story\\n\\nWant the quick video version? You can watch it [here](https://www.tiktok.com/@individualkex/video/7192994527312137518). Otherwise, if you want the technical details, keep reading!\\n\\n**Note:** This tutorial is intended for readers who are familiar with Unity development and C#. If you\\'re new to these technologies, check out the [Unity for Beginners](https://www.tiktok.com/@individualkex/video/7086863567412038954) series before continuing.\\n\\n## Day 4: 2D Assets\\n\\nIn [Part 3](https://huggingface.co/blog/ml-for-games-3) of this tutorial series, we discussed how **text-to-3D** isn\\'t quite ready yet. However, the story is much different for 2D.\\n\\nIn this part, we\\'ll talk about how you can use AI to generate 2D Assets.\\n\\n### Preface\\n\\nThis tutorial describes a collaborative process for generating 2D Assets, where Stable Diffusion is incorporated as a tool in a conventional 2D workflow. This is intended for readers with some knowledge of image editing and 2D asset creation but may otherwise be helpful for beginners and experts alike.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ml-for-games-4.md', 'start_index': 1620}, page_content='Requirements:\\n- Your preferred image-editing software, such as [Photoshop](https://www.adobe.com/products/photoshop.html) or [GIMP](https://www.gimp.org/) (free).\\n- Stable Diffusion. For instructions on setting up Stable Diffusion, refer to [Part 1](https://huggingface.co/blog/ml-for-games-1#setting-up-stable-diffusion).\\n\\n### Image2Image\\n\\n[Diffusion models](https://en.wikipedia.org/wiki/Diffusion_model) such as Stable Diffusion work by reconstructing images from noise, guided by text. Image2Image uses the same process but starts with real images as input rather than noise. This means that the outputs will, to some extent, resemble the input image.\\n\\nAn important parameter in Image2Image is **denoising strength**. This controls the extent to which the model changes the input. A denoising strength of 0 will reproduce the input image exactly, while a denoising strength of 1 will generate a very different image. Another way to think about denoising strength is **creativity**. The image below demonstrates image-to-image with an input image of a circle and the prompt \"moon\", at various denoising strengths.\\n\\n<div align=\"center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/moons.png\" alt=\"Denoising Strength Example\">\\n</div>\\n\\nImage2Image allows Stable Diffusion to be used as a tool, rather than as a replacement for the conventional artistic workflow. That is, you can pass your own handmade assets to Image2Image, iterate back on the result by hand, and so on. Let\\'s take an example for the farming game.\\n\\n### Example: Corn\\n\\nIn this section, I\\'ll walk through how I generated a corn icon for the farming game. As a starting point, I sketched a very rough corn icon, intended to lay out the composition of the image.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ml-for-games-4.md', 'start_index': -1}, page_content='### Example: Corn\\n\\nIn this section, I\\'ll walk through how I generated a corn icon for the farming game. As a starting point, I sketched a very rough corn icon, intended to lay out the composition of the image.\\n\\n<div align=\"center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/corn1.png\" alt=\"Corn 1\">\\n</div>\\n\\nNext, I used Image2Image to generate some icons using the following prompt:\\n\\n> corn, james gilleard, atey ghailan, pixar concept artists, stardew valley, animal crossing\\n\\nI used a denoising strength of 0.8, to encourage the model to be more creative. After generating several times, I found a result I liked.\\n\\n<div align=\"center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/corn2.png\" alt=\"Corn 2\">\\n</div>\\n\\nThe image doesn\\'t need to be perfect, just in the direction you\\'re going for, since we\\'ll keep iterating. In my case, I liked the style that was produced, but thought the stalk was a bit too intricate. So, I made some modifications in photoshop.\\n\\n<div align=\"center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/corn3.png\" alt=\"Corn 3\">\\n</div>\\n\\nNotice that I roughly painted over the parts I wanted to change, allowing Stable Diffusion to fill the details in. I dropped my modified image back into Image2Image, this time using a lower denoising strength of 0.6 since I didn\\'t want to deviate too far from the input. This resulted in an icon I was *almost* happy with.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ml-for-games-4.md', 'start_index': 4796}, page_content='<div align=\"center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/corn4.png\" alt=\"Corn 4\">\\n</div>\\n\\nThe base of the corn stalk was just a bit too painterly for me, and there was a sprout coming out of the top. So, I painted over these in photoshop, made one more pass in Stable Diffusion, and removed the background.\\n\\n<div align=\"center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/corn5.png\" alt=\"Corn 5\">\\n</div>\\n\\nVoilà, a game-ready corn icon in less than 10 minutes. However, you could spend much more time to get a better result. I recommend [this video](https://youtu.be/blXnuyVgA_Y) for a more detailed walkthrough of making a more intricate asset.\\n\\n### Example: Scythe\\n\\nIn many cases, you may need to fight Stable Diffusion a bit to get the result you\\'re going for. For me, this was definitely the case for the scythe icon, which required a lot of iteration to get in the direction I was going for.\\n\\n<div align=\"center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/scythe.png\" alt=\"Scythe\">\\n</div>\\n\\nThe issue likely lies in the fact that there are way more images online of scythes as *weapons* rather than as *farming tools*. One way around this is prompt engineering, or fiddling with the prompt to try to push it in the right direction, i.e. writing **scythe, scythe tool** in the prompt or **weapon** in the negative prompt. However, this isn\\'t the only solution.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/ml-for-games-4.md', 'start_index': 6375}, page_content=\"[Dreambooth](https://dreambooth.github.io/), [textual inversion](https://textual-inversion.github.io/), and [LoRA](https://huggingface.co/blog/lora) are techniques for customizing diffusion models, making them capable of producing results much more specific to what you're going for. These are outside the scope of this tutorial, but are worth mentioning, as they're becoming increasingly prominent in the area of 2D Asset generation.\\n\\nGenerative services such as [layer.ai](https://layer.ai/) and [scenario.gg](https://www.scenario.gg/) are specifically targeted toward game asset generation, likely using techniques such as dreambooth and textual inversion to allow game developers to generate style-consistent assets. However, it remains to be seen which approaches will rise to the top in the emerging generative game development toolkit.\\n\\nIf you're interested in diving deeper into these advanced workflows, check out this [blog post](https://huggingface.co/blog/dreambooth) and [space](https://huggingface.co/spaces/multimodalart/dreambooth-training) on Dreambooth training.\\n\\nClick [here](https://huggingface.co/blog/ml-for-games-5) to read Part 5, where we use **AI for Story**.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# MarianMT\\n\\n<div class=\"flex flex-wrap space-x-1\">\\n<a href=\"https://huggingface.co/models?filter=marian\">\\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-marian-blueviolet\">\\n</a>\\n<a href=\"https://huggingface.co/spaces/docs-demos/opus-mt-zh-en\">\\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\\n</a>\\n</div>\\n\\n## Overview\\n\\nA framework for translation models, using the same models as BART. Translations should be similar, but not identical to output in the test set linked to in each model card.\\nThis model was contributed by [sshleifer](https://huggingface.co/sshleifer).\\n\\n\\n## Implementation Notes'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 1390}, page_content='## Implementation Notes\\n\\n- Each model is about 298 MB on disk, there are more than 1,000 models.\\n- The list of supported language pairs can be found [here](https://huggingface.co/Helsinki-NLP).\\n- Models were originally trained by [Jörg Tiedemann](https://researchportal.helsinki.fi/en/persons/j%C3%B6rg-tiedemann) using the [Marian](https://marian-nmt.github.io/) C++ library, which supports fast training and translation.\\n- All models are transformer encoder-decoders with 6 layers in each component. Each model\\'s performance is documented\\n  in a model card.\\n- The 80 opus models that require BPE preprocessing are not supported.\\n- The modeling code is the same as [`BartForConditionalGeneration`] with a few minor modifications:\\n\\n  - static (sinusoid) positional embeddings (`MarianConfig.static_position_embeddings=True`)\\n  - no layernorm_embedding (`MarianConfig.normalize_embedding=False`)\\n  - the model starts generating with `pad_token_id` (which has 0 as a token_embedding) as the prefix (Bart uses\\n    `<s/>`),\\n- Code to bulk convert models can be found in `convert_marian_to_pytorch.py`.\\n\\n\\n## Naming\\n\\n- All model names use the following format: `Helsinki-NLP/opus-mt-{src}-{tgt}`\\n- The language codes used to name models are inconsistent. Two digit codes can usually be found [here](https://developers.google.com/admin-sdk/directory/v1/languages), three digit codes require googling \"language\\n  code {code}\".\\n- Codes formatted like `es_AR` are usually `code_{region}`. That one is Spanish from Argentina.\\n- The models were converted in two stages. The first 1000 models use ISO-639-2 codes to identify languages, the second\\n  group use a combination of ISO-639-5 codes and ISO-639-2 codes.\\n\\n\\n## Examples'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 3092}, page_content='## Examples\\n\\n- Since Marian models are smaller than many other translation models available in the library, they can be useful for\\n  fine-tuning experiments and integration tests.\\n- [Fine-tune on GPU](https://github.com/huggingface/transformers/blob/master/examples/legacy/seq2seq/train_distil_marian_enro.sh)\\n\\n## Multilingual Models\\n\\n- All model names use the following format: `Helsinki-NLP/opus-mt-{src}-{tgt}`:\\n- If a model can output multiple languages, and you should specify a language code by prepending the desired output\\n  language to the `src_text`.\\n- You can see a models\\'s supported language codes in its model card, under target constituents, like in [opus-mt-en-roa](https://huggingface.co/Helsinki-NLP/opus-mt-en-roa).\\n- Note that if a model is only multilingual on the source side, like `Helsinki-NLP/opus-mt-roa-en`, no language\\n  codes are required.\\n\\nNew multi-lingual models from the [Tatoeba-Challenge repo](https://github.com/Helsinki-NLP/Tatoeba-Challenge)\\nrequire 3 character language codes:\\n\\n```python\\n>>> from transformers import MarianMTModel, MarianTokenizer\\n\\n>>> src_text = [\\n...     \">>fra<< this is a sentence in english that we want to translate to french\",\\n...     \">>por<< This should go to portuguese\",\\n...     \">>esp<< And this to Spanish\",\\n... ]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 4376}, page_content='>>> model_name = \"Helsinki-NLP/opus-mt-en-roa\"\\n>>> tokenizer = MarianTokenizer.from_pretrained(model_name)\\n>>> print(tokenizer.supported_language_codes)\\n[\\'>>zlm_Latn<<\\', \\'>>mfe<<\\', \\'>>hat<<\\', \\'>>pap<<\\', \\'>>ast<<\\', \\'>>cat<<\\', \\'>>ind<<\\', \\'>>glg<<\\', \\'>>wln<<\\', \\'>>spa<<\\', \\'>>fra<<\\', \\'>>ron<<\\', \\'>>por<<\\', \\'>>ita<<\\', \\'>>oci<<\\', \\'>>arg<<\\', \\'>>min<<\\']\\n\\n>>> model = MarianMTModel.from_pretrained(model_name)\\n>>> translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\\n>>> [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\\n[\"c\\'est une phrase en anglais que nous voulons traduire en français\",\\n \\'Isto deve ir para o português.\\',\\n \\'Y esto al español\\']'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 5066}, page_content='```\\n\\nHere is the code to see all available pretrained models on the hub:\\n\\n```python\\nfrom huggingface_hub import list_models\\n\\nmodel_list = list_models()\\norg = \"Helsinki-NLP\"\\nmodel_ids = [x.modelId for x in model_list if x.modelId.startswith(org)]\\nsuffix = [x.split(\"/\")[1] for x in model_ids]\\nold_style_multi_models = [f\"{org}/{s}\" for s in suffix if s != s.lower()]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 5432}, page_content='```\\n\\n## Old Style Multi-Lingual Models\\n\\nThese are the old style multi-lingual models ported from the OPUS-MT-Train repo: and the members of each language\\ngroup:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 5594}, page_content=\"```python no-style\\n['Helsinki-NLP/opus-mt-NORTH_EU-NORTH_EU',\\n 'Helsinki-NLP/opus-mt-ROMANCE-en',\\n 'Helsinki-NLP/opus-mt-SCANDINAVIA-SCANDINAVIA',\\n 'Helsinki-NLP/opus-mt-de-ZH',\\n 'Helsinki-NLP/opus-mt-en-CELTIC',\\n 'Helsinki-NLP/opus-mt-en-ROMANCE',\\n 'Helsinki-NLP/opus-mt-es-NORWAY',\\n 'Helsinki-NLP/opus-mt-fi-NORWAY',\\n 'Helsinki-NLP/opus-mt-fi-ZH',\\n 'Helsinki-NLP/opus-mt-fi_nb_no_nn_ru_sv_en-SAMI',\\n 'Helsinki-NLP/opus-mt-sv-NORWAY',\\n 'Helsinki-NLP/opus-mt-sv-ZH']\\nGROUP_MEMBERS = {\\n 'ZH': ['cmn', 'cn', 'yue', 'ze_zh', 'zh_cn', 'zh_CN', 'zh_HK', 'zh_tw', 'zh_TW', 'zh_yue', 'zhs', 'zht', 'zh'],\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 6193}, page_content=\"'ROMANCE': ['fr', 'fr_BE', 'fr_CA', 'fr_FR', 'wa', 'frp', 'oc', 'ca', 'rm', 'lld', 'fur', 'lij', 'lmo', 'es', 'es_AR', 'es_CL', 'es_CO', 'es_CR', 'es_DO', 'es_EC', 'es_ES', 'es_GT', 'es_HN', 'es_MX', 'es_NI', 'es_PA', 'es_PE', 'es_PR', 'es_SV', 'es_UY', 'es_VE', 'pt', 'pt_br', 'pt_BR', 'pt_PT', 'gl', 'lad', 'an', 'mwl', 'it', 'it_IT', 'co', 'nap', 'scn', 'vec', 'sc', 'ro', 'la'],\\n 'NORTH_EU': ['de', 'nl', 'fy', 'af', 'da', 'fo', 'is', 'no', 'nb', 'nn', 'sv'],\\n 'SCANDINAVIA': ['da', 'fo', 'is', 'no', 'nb', 'nn', 'sv'],\\n 'SAMI': ['se', 'sma', 'smj', 'smn', 'sms'],\\n 'NORWAY': ['nb_NO', 'nb', 'nn_NO', 'nn', 'nog', 'no_nb', 'no'],\\n 'CELTIC': ['ga', 'cy', 'br', 'gd', 'kw', 'gv']\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 6877}, page_content='```\\n\\nExample of translating english to many romance languages, using old-style 2 character language codes\\n\\n\\n```python\\n>>> from transformers import MarianMTModel, MarianTokenizer\\n\\n>>> src_text = [\\n...     \">>fr<< this is a sentence in english that we want to translate to french\",\\n...     \">>pt<< This should go to portuguese\",\\n...     \">>es<< And this to Spanish\",\\n... ]\\n\\n>>> model_name = \"Helsinki-NLP/opus-mt-en-ROMANCE\"\\n>>> tokenizer = MarianTokenizer.from_pretrained(model_name)\\n\\n>>> model = MarianMTModel.from_pretrained(model_name)\\n>>> translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\\n>>> tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\\n[\"c\\'est une phrase en anglais que nous voulons traduire en français\", \\n \\'Isto deve ir para o português.\\',\\n \\'Y esto al español\\']'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/marian.md', 'start_index': 7716}, page_content='```\\n\\n## Resources\\n\\n- [Translation task guide](../tasks/translation)\\n- [Summarization task guide](../tasks/summarization)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n\\n## MarianConfig\\n\\n[[autodoc]] MarianConfig\\n\\n## MarianTokenizer\\n\\n[[autodoc]] MarianTokenizer\\n    - build_inputs_with_special_tokens\\n\\n<frameworkcontent>\\n<pt>\\n\\n## MarianModel\\n\\n[[autodoc]] MarianModel\\n    - forward\\n\\n## MarianMTModel\\n\\n[[autodoc]] MarianMTModel\\n    - forward\\n\\n## MarianForCausalLM\\n\\n[[autodoc]] MarianForCausalLM\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFMarianModel\\n\\n[[autodoc]] TFMarianModel\\n    - call\\n\\n## TFMarianMTModel\\n\\n[[autodoc]] TFMarianMTModel\\n    - call\\n\\n</tf>\\n<jax>\\n\\n## FlaxMarianModel\\n\\n[[autodoc]] FlaxMarianModel\\n    - __call__\\n\\n## FlaxMarianMTModel\\n\\n[[autodoc]] FlaxMarianMTModel\\n    - __call__\\n\\n</jax>\\n</frameworkcontent>'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md', 'start_index': 0}, page_content='--\\n# For reference on dataset card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/datasetcard.md?plain=1\\n# Doc / guide: https://huggingface.co/docs/hub/datasets-cards\\n{{ card_data }}\\n---\\n\\n# Dataset Card for {{ pretty_name | default(\"Dataset Name\", true) }}\\n\\n<!-- Provide a quick summary of the dataset. -->\\n\\n{{ dataset_summary | default(\"\", true) }}\\n\\n## Dataset Details\\n\\n### Dataset Description\\n\\n<!-- Provide a longer summary of what this dataset is. -->\\n\\n{{ dataset_description | default(\"\", true) }}\\n\\n- **Curated by:** {{ curators | default(\"[More Information Needed]\", true)}}\\n- **Funded by [optional]:** {{ funded_by | default(\"[More Information Needed]\", true)}}\\n- **Shared by [optional]:** {{ shared_by | default(\"[More Information Needed]\", true)}}\\n- **Language(s) (NLP):** {{ language | default(\"[More Information Needed]\", true)}}\\n- **License:** {{ license | default(\"[More Information Needed]\", true)}}\\n\\n### Dataset Sources [optional]\\n\\n<!-- Provide the basic links for the dataset. -->\\n\\n- **Repository:** {{ repo | default(\"[More Information Needed]\", true)}}\\n- **Paper [optional]:** {{ paper | default(\"[More Information Needed]\", true)}}\\n- **Demo [optional]:** {{ demo | default(\"[More Information Needed]\", true)}}\\n\\n## Uses\\n\\n<!-- Address questions around how the dataset is intended to be used. -->\\n\\n### Direct Use\\n\\n<!-- This section describes suitable use cases for the dataset. -->\\n\\n{{ direct_use | default(\"[More Information Needed]\", true)}}\\n\\n### Out-of-Scope Use'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md', 'start_index': -1}, page_content='<!-- This section describes suitable use cases for the dataset. -->\\n\\n{{ direct_use | default(\"[More Information Needed]\", true)}}\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->\\n\\n{{ out_of_scope_use | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Structure\\n\\n<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->\\n\\n{{ dataset_structure | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Creation\\n\\n### Curation Rationale\\n\\n<!-- Motivation for the creation of this dataset. -->\\n\\n{{ curation_rationale_section | default(\"[More Information Needed]\", true)}}\\n\\n### Source Data\\n\\n<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->\\n\\n#### Data Collection and Processing\\n\\n<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->\\n\\n{{ data_collection_and_processing_section | default(\"[More Information Needed]\", true)}}\\n\\n#### Who are the source data producers?\\n\\n<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->\\n\\n{{ source_data_producers_section | default(\"[More Information Needed]\", true)}}\\n\\n### Annotations [optional]\\n\\n<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->\\n\\n#### Annotation process\\n\\n<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md', 'start_index': 3377}, page_content='{{ annotation_process_section | default(\"[More Information Needed]\", true)}}\\n\\n#### Who are the annotators?\\n\\n<!-- This section describes the people or systems who created the annotations. -->\\n\\n{{ who_are_annotators_section | default(\"[More Information Needed]\", true)}}\\n\\n#### Personal and Sensitive Information\\n\\n<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->\\n\\n{{ personal_and_sensitive_information | default(\"[More Information Needed]\", true)}}\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n{{ bias_risks_limitations | default(\"[More Information Needed]\", true)}}\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\n{{ bias_recommendations | default(\"Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.\", true)}}\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n{{ citation_bibtex | default(\"[More Information Needed]\", true)}}\\n\\n**APA:**\\n\\n{{ citation_apa | default(\"[More Information Needed]\", true)}}\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->\\n\\n{{ glossary | default(\"[More Information Needed]\", true)}}\\n\\n## More Information [optional]\\n\\n{{ more_information | default(\"[More Information Needed]\", true)}}'),\n",
       " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md', 'start_index': -1}, page_content='{{ glossary | default(\"[More Information Needed]\", true)}}\\n\\n## More Information [optional]\\n\\n{{ more_information | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Card Authors [optional]\\n\\n{{ dataset_card_authors | default(\"[More Information Needed]\", true)}}\\n\\n## Dataset Card Contact\\n\\n{{ dataset_card_contact | default(\"[More Information Needed]\", true)}}'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/wav2vec2-conformer.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Wav2Vec2-Conformer\\n\\n## Overview\\n\\nThe Wav2Vec2-Conformer was added to an updated version of [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.\\n\\nThe official results of the model can be found in Table 3 and Table 4 of the paper.\\n\\nThe Wav2Vec2-Conformer weights were released by the Meta AI team within the [Fairseq library](https://github.com/pytorch/fairseq/blob/main/examples/wav2vec/README.md#pre-trained-models).\\n\\nThis model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\\nThe original code can be found [here](https://github.com/pytorch/fairseq/tree/main/examples/wav2vec).\\n\\n## Usage tips'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/wav2vec2-conformer.md', 'start_index': 1492}, page_content='## Usage tips\\n\\n- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Attention*-block with a *Conformer*-block\\n  as introduced in [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100).\\n- For the same number of layers, Wav2Vec2-Conformer requires more parameters than Wav2Vec2, but also yields \\nan improved word error rate.\\n- Wav2Vec2-Conformer uses the same tokenizer and feature extractor as Wav2Vec2.\\n- Wav2Vec2-Conformer can use either no relative position embeddings, Transformer-XL-like position embeddings, or\\n  rotary position embeddings by setting the correct `config.position_embeddings_type`.\\n\\n## Resources\\n\\n- [Audio classification task guide](../tasks/audio_classification)\\n- [Automatic speech recognition task guide](../tasks/asr)\\n\\n## Wav2Vec2ConformerConfig\\n\\n[[autodoc]] Wav2Vec2ConformerConfig\\n\\n## Wav2Vec2Conformer specific outputs\\n\\n[[autodoc]] models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForPreTrainingOutput\\n\\n## Wav2Vec2ConformerModel\\n\\n[[autodoc]] Wav2Vec2ConformerModel\\n    - forward\\n\\n## Wav2Vec2ConformerForCTC\\n\\n[[autodoc]] Wav2Vec2ConformerForCTC\\n    - forward\\n\\n## Wav2Vec2ConformerForSequenceClassification\\n\\n[[autodoc]] Wav2Vec2ConformerForSequenceClassification\\n    - forward\\n\\n## Wav2Vec2ConformerForAudioFrameClassification\\n\\n[[autodoc]] Wav2Vec2ConformerForAudioFrameClassification\\n    - forward\\n\\n## Wav2Vec2ConformerForXVector'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/wav2vec2-conformer.md', 'start_index': -1}, page_content='[[autodoc]] Wav2Vec2ConformerForAudioFrameClassification\\n    - forward\\n\\n## Wav2Vec2ConformerForXVector\\n\\n[[autodoc]] Wav2Vec2ConformerForXVector\\n    - forward\\n\\n## Wav2Vec2ConformerForPreTraining\\n\\n[[autodoc]] Wav2Vec2ConformerForPreTraining\\n    - forward'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 0}, page_content='--\\ntitle: \"Supercharged Customer Service with Machine Learning\"\\nthumbnail: /blog/assets/61_supercharged_customer_service_with_nlp/thumbnail.png\\nauthors:\\n- user: patrickvonplaten\\n---\\n\\n# Supercharged Customer Service with Machine Learning\\n\\n\\n<a target=\"_blank\" href=\"https://github.com/patrickvonplaten/notebooks/blob/master/Using_%F0%9F%A4%97_Transformers_and_%F0%9F%A4%97_Datasets_filter_customer_feedback_filtering.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n\\nIn this blog post, we will simulate a real-world customer service use case and use tools machine learning tools of the Hugging Face ecosystem to address it.\\n\\nWe strongly recommend using this notebook as a template/example to solve **your** real-world use case.\\n\\n\\n## Defining Task, Dataset & Model\\n\\nBefore jumping into the actual coding part, it\\'s important to have a clear definition of the use case that you would like to automate or partly automate.\\nA clear definition of the use case helps identify the most suitable task, dataset to use, and model to apply for your use case.\\n\\n\\n### Defining your NLP task\\n\\nAlright, let\\'s dive into a hypothetical problem we wish to solve using models of natural language processing models. Let\\'s assume we are selling a product and our customer support team receives thousands of messages including feedback, complaints, and questions which ideally should all be answered.\\n\\nQuickly, it becomes obvious that customer support is by no means able to reply to every message. Thus, we decide to only respond to the most unsatisfied customers and aim to answer 100% of those messages, as these are likely the most urgent compared to the other neutral and positive messages.\\n\\nAssuming that a) messages of very unsatisfied customers represent only a fraction of all messages and b) that we can filter out unsatisfied messages in an automated way, customer support should be able to reach this goal.\\n\\nTo filter out unsatisfied messages in an automated way, we plan on applying natural language processing technologies.'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': -1}, page_content=\"To filter out unsatisfied messages in an automated way, we plan on applying natural language processing technologies.\\n\\nThe first step is to map our use case - *filtering out unsatisfied messages* - to a machine learning task.\\n\\nThe [tasks page on the Hugging Face Hub](https://huggingface.co/tasks) is a great place to get started to see which task best fits a given scenario. Each task has a detailed description and potential use cases.\\n\\nThe task of finding messages of the most unsatisfied customers can be modeled as a text classification task: Classify a message into one of the following 5 categories: *very unsatisfied*, *unsatisfied*, *neutral*, *satisfied*, **or** *very satisfied*.\\n\\n\\n### Finding suitable datasets\\n\\nHaving decided on the task, next, we should find the data the model will be trained on. This is usually more important for the performance of your use case than picking the right model architecture.\\nKeep in mind that a model is **only as good as the data it has been trained on**. Thus, we should be very careful when curating and/or selecting the dataset.\\n\\nSince we consider the hypothetical use case of *filtering out unsatisfied messages*, let's look into what datasets are available.\\n\\nFor your real-world use case, it is **very likely** that you have internal data that best represents the actual data your NLP system is supposed to handle. Therefore, you should use such internal data to train your NLP system.\\nIt can nevertheless be helpful to also include publicly available data to improve the generalizability of your model.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 3512}, page_content=\"Let's take a look at all available Datasets on the [Hugging Face Hub](https://huggingface.co/datasets). On the left side, you can filter the datasets according to *Task Categories* as well as *Tasks* which are more specific. Our use case corresponds to *Text Classification* -> *Sentiment Analysis* so let's select [these filters](https://huggingface.co/datasets?task_categories=task_categories:text-classification&task_ids=task_ids:sentiment-classification&sort=downloads). We are left with *ca.* 80 datasets at the time of writing this notebook. Two aspects should be evaluated when picking a dataset:\\n\\n-   **Quality**: Is the dataset of high quality? More specifically: Does the data correspond to the data you expect to deal with in your use case? Is the data diverse, unbiased, ...?\\n-   **Size**: How big is the dataset? Usually, one can safely say the bigger the dataset, the better.\\n\\nIt's quite tricky to evaluate whether a dataset is of high quality efficiently, and it's even more challenging to know whether and how the dataset is biased.\\nAn efficient and reasonable heuristic for high quality is to look at the download statistics. The more downloads, the more usage, the higher chance that the dataset is of high quality. The size is easy to evaluate as it can usually be quickly read upon. Let's take a look at the most downloaded datasets:\\n\\n-   [Glue](https://huggingface.co/datasets/glue)\\n-   [Amazon polarity](https://huggingface.co/datasets/amazon_polarity)\\n-   [Tweet eval](https://huggingface.co/datasets/tweet_eval)\\n-   [Yelp review full](https://huggingface.co/datasets/yelp_review_full)\\n-   [Amazon reviews multi](https://huggingface.co/datasets/amazon_reviews_multi)\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 5203}, page_content=\"Now we can inspect those datasets in more detail by reading through the dataset card, which ideally should give all relevant and important information. In addition, the [dataset viewer](https://huggingface.co/datasets/glue/viewer/cola/test) is an incredibly powerful tool to inspect whether the data suits your use case.\\n\\nLet's quickly go over the dataset cards of the models above:\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': -1}, page_content=\"Let's quickly go over the dataset cards of the models above:\\n\\n-   *GLUE* is a collection of small datasets that primarily serve to compare new model architectures for researchers. The datasets are too small and don't correspond enough to our use case.\\n-   *Amazon polarity* is a huge and well-suited dataset for customer feedback since the data deals with customer reviews. However, it only has binary labels (positive/negative), whereas we are looking for more granularity in the sentiment classification.\\n-   *Tweet eval* uses different emojis as labels that cannot easily be mapped to a scale going from unsatisfied to satisfied.\\n-   *Amazon reviews multi* seems to be the most suitable dataset here. We have sentiment labels ranging from 1-5 corresponding to 1-5 stars on Amazon. These labels can be mapped to *very unsatisfied, neutral, satisfied, very satisfied*. We have inspected some examples on [the dataset viewer](https://huggingface.co/datasets/amazon_reviews_multi/viewer/en/train) to verify that the reviews look very similar to actual customer feedback reviews, so this seems like a very good dataset. In addition, each review has a `product_category` label, so we could even go as far as to only use reviews of a product category corresponding to the one we are working in. The dataset is multi-lingual, but we are just interested in the English version for now.\\n-   *Yelp review full* looks like a very suitable dataset. It's large and contains product reviews and sentiment labels from 1 to 5. Sadly, the dataset viewer is not working here, and the dataset card is also relatively sparse, requiring some more time to inspect the dataset. At this point, we should read the paper, but given the time constraint of this blog post, we'll choose to go for *Amazon reviews multi*.\\nAs a conclusion, let's focus on the [*Amazon reviews multi*](https://huggingface.co/datasets/amazon_reviews_multi) dataset considering all training examples.\\n\\nAs a final note, we recommend making use of Hub's dataset functionality even when working with private datasets. The Hugging Face Hub, Transformers, and Datasets are flawlessly integrated, which makes it trivial to use them in combination when training models.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 7740}, page_content=\"In addition, the Hugging Face Hub offers:\\n\\n-   [A dataset viewer for every dataset](https://huggingface.co/datasets/amazon_reviews_multi)\\n-   [Easy demoing of every model using widgets](https://huggingface.co/docs/hub/models-widgets)\\n-   [Private and Public models](https://huggingface.co/docs/hub/repositories-settings)\\n-   [Git version control for repositories](https://huggingface.co/docs/hub/repositories-getting-started)\\n-   [Highest security mechanisms](https://huggingface.co/docs/hub/security)\\n\\n\\n### Finding a suitable model\\n\\nHaving decided on the task and the dataset that best describes our use case, we can now look into choosing a model to be used.\\n\\nMost likely, you will have to fine-tune a pretrained model for your own use case, but it is worth checking whether the hub already has suitable fine-tuned models. In this case, you might reach a higher performance by just continuing to fine-tune such a model on your dataset.\\n\\nLet's take a look at all models that have been fine-tuned on Amazon Reviews Multi. You can find the list of models on the bottom right corner - clicking on *Browse models trained on this dataset* you can see [a list of all models fine-tuned on the dataset that are publicly available](https://huggingface.co/models?dataset=dataset:amazon_reviews_multi). Note that we are only interested in the English version of the dataset because our customer feedback will only be in English. Most of the most downloaded models are trained on the multi-lingual version of the dataset and those that don't seem to be multi-lingual have very little information or poor performance. At this point,\\nit might be more sensible to fine-tune a purely pretrained model instead of using one of the already fine-tuned ones shown in the link above.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 9504}, page_content=\"Alright, the next step now is to find a suitable pretrained model to be used for fine-tuning. This is actually more difficult than it seems given the large amount of pretrained and fine-tuned models that are on the [Hugging Face Hub](https://huggingface.co/models). The best option is usually to simply try out a variety of different models to see which one performs best.\\nWe still haven't found the perfect way of comparing different model checkpoints to each other at Hugging Face, but we provide some resources that are worth looking into:\\n\\n-   The [model summary](https://huggingface.co/docs/transformers/model_summary) gives a short overview of different model architectures.\\n-   A task-specific search on the Hugging Face Hub, *e.g.* [a search on text-classification models](https://huggingface.co/models), shows you the most downloaded checkpoints which is also an indication of how well those checkpoints perform.\\n\\nHowever, both of the above resources are currently suboptimal. The model summary is not always kept up to date by the authors. The speed at which new model architectures are released and old model architectures become outdated makes it extremely difficult to have an up-to-date summary of all model architectures.\\nSimilarly, it doesn't necessarily mean that the most downloaded model checkpoint is the best one. E.g. [`bert-base-cased`](https://huggingface.co/bert-base-uncased) is amongst the most downloaded model checkpoints but is not the best performing checkpoint anymore.\\n\\nThe best approach is to try out various model architectures, stay up to date with new model architectures by following experts in the field, and check well-known leaderboards.\\n\\nFor text-classification, the important benchmarks to look at are [GLUE](https://gluebenchmark.com/leaderboard) and [SuperGLUE](https://super.gluebenchmark.com/leaderboard). Both benchmarks evaluate pretrained models on a variety of text-classification tasks, such as grammatical correctness, natural language inference, Yes/No question answering, etc..., which are quite similar to our target task of sentiment analysis. Thus, it is reasonable to choose one of the leading models of these benchmarks for our task.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 11699}, page_content=\"At the time of writing this blog post, the best performing models are very large models containing more than 10 billion parameters most of which are not open-sourced, *e.g.* *ST-MoE-32B*, *Turing NLR v5*, or\\n*ERNIE 3.0*. One of the top-ranking models that is easily accessible is [DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta). Therefore, let's try out DeBERTa's newest base version - *i.e.* [`microsoft/deberta-v3-base`](https://huggingface.co/microsoft/deberta-v3-base).\\n\\n\\n## Training / Fine-tuning a model with 🤗 Transformers and 🤗 Datasets\\n\\nIn this section, we will jump into the technical details of how to\\nfine-tune a model end-to-end to be able to automatically filter out very unsatisfied customer feedback messages.\\n\\nCool! Let's start by installing all necessary pip packages and setting up our code environment, then look into preprocessing the dataset, and finally start training the model.\\n\\nThe following notebook can be run online in a google colab pro with the GPU runtime environment enabled.\\n\\n\\n### Install all necessary packages\\n\\nTo begin with, let's install [`git-lfs`](https://git-lfs.github.com/) so that we can automatically upload our trained checkpoints to the Hub during training.\\n\\n\\n```bash\\napt install git-lfs\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 12956}, page_content='```\\n\\nAlso, we install the 🤗 Transformers and 🤗 Datasets libraries to run this notebook. Since we will be using [DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta-v2#debertav2) in this blog post, we also need to install the [`sentencepiece`](https://github.com/google/sentencepiece) library for its tokenizer.\\n\\n\\n```bash\\npip install datasets transformers[sentencepiece]\\n```\\n\\n\\nNext, let\\'s login into our [Hugging Face account](https://huggingface.co/join) so that models are uploaded correctly under your name tag.\\n\\n\\n```python\\nfrom huggingface_hub import notebook_login\\n\\nnotebook_login()\\n```\\n\\n**Output:**\\n```\\n    Login successful\\n    Your token has been saved to /root/.huggingface/token\\n    Authenticated through git-credential store but this isn\\'t the helper defined on your machine.\\n    You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\\n\\n    git config --global credential.helper store\\n```\\n\\n\\n\\n### Preprocess the dataset\\n\\nBefore we can start training the model, we should bring the dataset in a format\\nthat is understandable by the model.\\n\\nThankfully, the 🤗 Datasets library makes this extremely easy as you will see in the following cells.\\n\\nThe `load_dataset` function loads the dataset, nicely arranges it into predefined attributes, such as `review_body` and `stars`, and finally saves the newly arranged data using the [arrow format](https://arrow.apache.org/#:~:text=Format,data%20access%20without%20serialization%20overhead.) on disk.\\nThe arrow format allows for fast and memory-efficient data reading and writing.\\n\\nLet\\'s load and prepare the English version of the `amazon_reviews_multi` dataset.\\n\\n\\n```python\\nfrom datasets import load_dataset\\n\\namazon_review = load_dataset(\"amazon_reviews_multi\", \"en\")\\n```\\n\\n**Output:**'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 14810}, page_content='```\\n\\n**Output:**\\n```\\n    Downloading and preparing dataset amazon_reviews_multi/en (download: 82.11 MiB, generated: 58.69 MiB, post-processed: Unknown size, total: 140.79 MiB) to /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609...\\n\\n    Dataset amazon_reviews_multi downloaded and prepared to /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609. Subsequent calls will reuse this data.\\n```\\n\\n\\n\\nGreat, that was fast 🔥. Let\\'s take a look at the structure of the dataset.\\n\\n\\n```python\\nprint(amazon_review)\\n```\\n\\n**Output:**\\n```\\n{.output .execute_result execution_count=\"5\"}\\n    DatasetDict({\\n        train: Dataset({\\n            features: [\\'review_id\\', \\'product_id\\', \\'reviewer_id\\', \\'stars\\', \\'review_body\\', \\'review_title\\', \\'language\\', \\'product_category\\'],\\n            num_rows: 200000\\n        })\\n        validation: Dataset({\\n            features: [\\'review_id\\', \\'product_id\\', \\'reviewer_id\\', \\'stars\\', \\'review_body\\', \\'review_title\\', \\'language\\', \\'product_category\\'],\\n            num_rows: 5000\\n        })\\n        test: Dataset({\\n            features: [\\'review_id\\', \\'product_id\\', \\'reviewer_id\\', \\'stars\\', \\'review_body\\', \\'review_title\\', \\'language\\', \\'product_category\\'],\\n            num_rows: 5000\\n        })\\n    })'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 16167}, page_content='```\\n\\n\\n\\nWe have 200,000 training examples as well as 5000 validation and test examples. This sounds reasonable for training! We\\'re only really interested in the input being the `\"review_body\"` column and the target being the `\"starts\"` column.\\n\\nLet\\'s check out a random example.\\n\\n\\n```python\\nrandom_id = 34\\n\\nprint(\"Stars:\", amazon_review[\"train\"][random_id][\"stars\"])\\nprint(\"Review:\", amazon_review[\"train\"][random_id][\"review_body\"])\\n```\\n\\n**Output:**\\n```\\n    Stars: 1\\n    Review: This product caused severe burning of my skin. I have used other brands with no problems\\n```\\n\\n\\n\\nThe dataset is in a human-readable format, but now we need to transform it into a \"machine-readable\" format. Let\\'s define the model repository which includes all utils necessary to preprocess and fine-tune the checkpoint we decided on.\\n\\n\\n```python\\nmodel_repository = \"microsoft/deberta-v3-base\"\\n```\\n\\n\\nNext, we load the tokenizer of the model repository, which is a [DeBERTa\\'s Tokenizer](https://huggingface.co/docs/transformers/model_doc/deberta-v2#transformers.DebertaV2Tokenizer).\\n\\n\\n```python\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_repository)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 17337}, page_content='```\\n\\n\\n\\n\\nAs mentioned before, we will use the `\"review_body\"` as the model\\'s input and `\"stars\"` as the model\\'s target. Next, we make use of the tokenizer to transform the input into a sequence of token ids that can be understood by the model. The tokenizer does exactly this and can also help you to limit your input data to a certain length to not run into a memory issue. Here, we limit\\nthe maximum length to 128 tokens which in the case of DeBERTa corresponds to roughly 100 words which in turn corresponds to *ca.* 5-7 sentences. Looking at the [dataset viewer](https://huggingface.co/datasets/amazon_reviews_multi/viewer/en/test) again, we can see that this covers pretty much all training examples.\\n**Important**: This doesn\\'t mean that our model cannot handle longer input sequences, it just means that we use a maximum length of 128 for training since it covers 99% of our training and we don\\'t want to waste memory. Transformer models have shown to be very good at generalizing to longer sequences after training.\\n\\nIf you want to learn more about tokenization in general, please have a look at [the Tokenizers docs](https://huggingface.co/course/chapter6/1?fw=pt).\\n\\nThe labels are easy to transform as they already correspond to numbers in their raw form, *i.e.* the range from 1 to 5. Here we just shift the labels into the range 0 to 4 since indexes usually start at 0.\\n\\nGreat, let\\'s pour our thoughts into some code. We will define a `preprocess_function` that we\\'ll apply to each data sample.\\n\\n\\n```python\\ndef preprocess_function(example):\\n    output_dict = tokenizer(example[\"review_body\"], max_length=128, truncation=True)\\n    output_dict[\"labels\"] = [e - 1 for e in example[\"stars\"]]\\n    return output_dict'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 19059}, page_content='```\\n\\n\\nTo apply this function to all data samples in our dataset, we use the [`map`](https://huggingface.co/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.map) method of the `amazon_review` object we created earlier. This will apply the function on all the elements of all the splits in `amazon_review`, so our training, validation, and testing data will be preprocessed in one single command. We run the mapping function in `batched=True` mode to speed up the process and also remove all columns since we don\\'t need them anymore for training.\\n\\n\\n```python\\ntokenized_datasets = amazon_review.map(preprocess_function, batched=True, remove_columns=amazon_review[\"train\"].column_names)\\n```\\n\\n\\nLet\\'s take a look at the new structure.\\n\\n\\n```python\\ntokenized_datasets\\n```\\n\\n**Output:**\\n```\\n    DatasetDict({\\n        train: Dataset({\\n            features: [\\'input_ids\\', \\'token_type_ids\\', \\'attention_mask\\', \\'labels\\'],\\n            num_rows: 200000\\n        })\\n        validation: Dataset({\\n            features: [\\'input_ids\\', \\'token_type_ids\\', \\'attention_mask\\', \\'labels\\'],\\n            num_rows: 5000\\n        })\\n        test: Dataset({\\n            features: [\\'input_ids\\', \\'token_type_ids\\', \\'attention_mask\\', \\'labels\\'],\\n            num_rows: 5000\\n        })\\n    })\\n```\\n\\n\\n\\nWe can see that the outer layer of the structure stayed the same but the naming of the columns has changed.\\nLet\\'s take a look at the same random example we looked at previously only that it\\'s preprocessed now.\\n\\n\\n```python\\nprint(\"Input IDS:\", tokenized_datasets[\"train\"][random_id][\"input_ids\"])\\nprint(\"Labels:\", tokenized_datasets[\"train\"][random_id][\"labels\"])\\n```\\n\\n**Output:**'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 20701}, page_content=\"```\\n\\n**Output:**\\n```\\n    Input IDS: [1, 329, 714, 2044, 3567, 5127, 265, 312, 1158, 260, 273, 286, 427, 340, 3006, 275, 363, 947, 2]\\n    Labels: 0\\n```\\n\\n\\n\\nAlright, the input text is transformed into a sequence of integers which can be transformed to word embeddings by the model, and the label index is simply shifted by -1.\\n\\n\\n### Fine-tune the model\\n\\nHaving preprocessed the dataset, next we can fine-tune the model. We will make use of the popular [Hugging Face Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer) which allows us to start training in just a couple of lines of code. The `Trainer` can be used for more or less all tasks in PyTorch and is extremely convenient by taking care of a lot of boilerplate code needed for training.\\n\\nLet's start by loading the model checkpoint using the convenient [`AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification). Since the checkpoint of the model repository is just a pretrained checkpoint we should define the size of the classification head by passing `num_lables=5` (since we have 5 sentiment classes).\\n\\n\\n```python\\nfrom transformers import AutoModelForSequenceClassification\\n\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_repository, num_labels=5)\\n```\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 22046}, page_content=\"```\\n\\n```\\n    Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight']\\n    - This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\\n    - This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\\n    Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'pooler.dense.weight']\\n    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\\n```\\n\\n\\n\\nNext, we load a data collator. A [data collator](https://huggingface.co/docs/transformers/main_classes/data_collator) is responsible for making sure each batch is correctly padded during training, which should happen dynamically since training samples are reshuffled before each epoch.\\n\\n\\n```python\\nfrom transformers import DataCollatorWithPadding\\n\\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 23880}, page_content='```\\n\\n\\nDuring training, it is important to monitor the performance of the model on a held-out validation set. To do so, we should pass a to define a `compute_metrics` function to the `Trainer` which is then called at each validation step during training.\\n\\nThe simplest metric for the text classification task is *accuracy*, which simply states how much percent of the training samples were correctly classified. Using the *accuracy* metric might be problematic however if the validation or test data is very unbalanced. Let\\'s verify quickly that this is not the case by counting the occurrences of each label.\\n\\n\\n```python\\nfrom collections import Counter\\n\\nprint(\"Validation:\", Counter(tokenized_datasets[\"validation\"][\"labels\"]))\\nprint(\"Test:\", Counter(tokenized_datasets[\"test\"][\"labels\"]))\\n```\\n\\n**Output:**\\n```\\n    Validation: Counter({0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000})\\n    Test: Counter({0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000})\\n```\\n\\n\\nThe validation and test data sets are as balanced as they can be, so we can safely use accuracy here!\\n\\n\\nLet\\'s load the [accuracy metric](https://huggingface.co/metrics/accuracy) via the datasets library.\\n\\n\\n```python\\nfrom datasets import load_metric\\n\\naccuracy = load_metric(\"accuracy\")'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 25118}, page_content='```\\n\\nNext, we define the `compute_metrics` which will be applied to the predicted outputs of the model which is of type [`EvalPrediction`](https://huggingface.co/docs/transformers/main/en/internal/trainer_utils#transformers.EvalPrediction) and therefore exposes the model\\'s predictions and the gold labels.\\nWe compute the predicted label class by taking the `argmax` of the model\\'s prediction before passing it alongside the gold labels to the accuracy metric.\\n\\n\\n```python\\nimport numpy as np\\n\\ndef compute_metrics(pred):\\n    pred_logits = pred.predictions\\n    pred_classes = np.argmax(pred_logits, axis=-1)\\n    labels = np.asarray(pred.label_ids)\\n\\n    acc = accuracy.compute(predictions=pred_classes, references=labels)\\n\\n    return {\"accuracy\": acc[\"accuracy\"]}\\n```\\n\\n\\nGreat, now all components required for training are ready and all that\\'s left to do is to define the hyper-parameters of the `Trainer`. We need to make sure that the model checkpoints are uploaded to the Hugging Face Hub during training. By setting `push_to_hub=True`, this is done automatically at every `save_steps` via the convenient [`push_to_hub`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) method.\\n\\nBesides, we define some standard hyper-parameters such as learning rate, warm-up steps and training epochs. We will log the loss every 500 steps and run evaluation every 5000 steps.\\n\\n\\n```python\\nfrom transformers import TrainingArguments\\n\\ntraining_args = TrainingArguments(\\n    output_dir=\"deberta_amazon_reviews_v1\",\\n    num_train_epochs=2, \\n    learning_rate=2e-5,\\n    warmup_steps=200,\\n    logging_steps=500,\\n    save_steps=5000,\\n    eval_steps=5000,\\n    push_to_hub=True,\\n    evaluation_strategy=\"steps\",\\n)'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 26861}, page_content='```\\n\\n\\nPutting it all together, we can finally instantiate the Trainer by passing all required components. We\\'ll use the `\"validation\"` split as the held-out dataset during training.\\n\\n\\n```python\\nfrom transformers import Trainer\\n\\ntrainer = Trainer(\\n    args=training_args,\\n    compute_metrics=compute_metrics,\\n    model=model,\\n    tokenizer=tokenizer,\\n    data_collator=data_collator,\\n    train_dataset=tokenized_datasets[\"train\"],\\n    eval_dataset=tokenized_datasets[\"validation\"]\\n)\\n```\\n\\n\\nThe trainer is ready to go 🚀 You can start training by calling `trainer.train()`.\\n\\n\\n```python\\ntrain_metrics = trainer.train().metrics\\ntrainer.save_metrics(\"train\", train_metrics)\\n```\\n\\n**Output:**\\n```\\n    ***** Running training *****\\n      Num examples = 200000\\n      Num Epochs = 2\\n      Instantaneous batch size per device = 8\\n      Total train batch size (w. parallel, distributed & accumulation) = 8\\n      Gradient Accumulation steps = 1\\n      Total optimization steps = 50000'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 27835}, page_content='**Output:**\\n<div>\\n<table><p>\\n  <tbody>\\n <tr style=\"text-align: left;\">\\n  <td>Step</td>\\n  <td>Training Loss</td>\\n  <td>Validation Loss</td>\\n  <td>Accuracy</td>\\n </tr>\\n  <tr>\\n    <td>5000</td>\\n    <td>0.931200</td>\\n    <td>0.979602</td>\\n    <td>0.585600</td>\\n  </tr>\\n  <tr>\\n    <td>10000</td>\\n    <td>0.931600</td>\\n    <td>0.933607</td>\\n    <td>0.597400</td>\\n  </tr>\\n  <tr>\\n    <td>15000</td>\\n    <td>0.907600</td>\\n    <td>0.917062</td>\\n    <td>0.602600</td>\\n  </tr>\\n  <tr>\\n    <td>20000</td>\\n    <td>0.902400</td>\\n    <td>0.919414</td>\\n    <td>0.604600</td>\\n  </tr>\\n  <tr>\\n    <td>25000</td>\\n    <td>0.879400</td>\\n    <td>0.910928</td>\\n    <td>0.608400</td>\\n  </tr>\\n  <tr>\\n    <td>30000</td>\\n    <td>0.806700</td>\\n    <td>0.933923</td>\\n    <td>0.609200</td>\\n  </tr>\\n  <tr>\\n    <td>35000</td>\\n    <td>0.826800</td>\\n    <td>0.907260</td>\\n    <td>0.616200</td>\\n  </tr>\\n  <tr>\\n    <td>40000</td>\\n    <td>0.820500</td>\\n    <td>0.904160</td>\\n    <td>0.615800</td>\\n  </tr>\\n  <tr>\\n    <td>45000</td>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': -1}, page_content='<td>0.820500</td>\\n    <td>0.904160</td>\\n    <td>0.615800</td>\\n  </tr>\\n  <tr>\\n    <td>45000</td>\\n    <td>0.795000</td>\\n    <td>0.918947</td>\\n    <td>0.616800</td>\\n  </tr>\\n  <tr>\\n    <td>50000</td>\\n    <td>0.783600</td>\\n    <td>0.907572</td>\\n    <td>0.618400</td>\\n  </tr>\\n  </tbody>\\n</table><p>\\n</div>'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 13564}, page_content='**Output:**'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 29044}, page_content=\"```\\n    ***** Running Evaluation *****\\n      Num examples = 5000\\n      Batch size = 8\\n    Saving model checkpoint to deberta_amazon_reviews_v1/checkpoint-50000\\n    Configuration saved in deberta_amazon_reviews_v1/checkpoint-50000/config.json\\n    Model weights saved in deberta_amazon_reviews_v1/checkpoint-50000/pytorch_model.bin\\n    tokenizer config file saved in deberta_amazon_reviews_v1/checkpoint-50000/tokenizer_config.json\\n    Special tokens file saved in deberta_amazon_reviews_v1/checkpoint-50000/special_tokens_map.json\\n    added tokens file saved in deberta_amazon_reviews_v1/checkpoint-50000/added_tokens.json\\n\\n\\n    Training completed. Do not forget to share your model on huggingface.co/models =)\\n```\\n\\n\\nCool, we see that the model seems to learn something! Training loss and validation loss are going down and the accuracy also ends up being well over random chance (20%). Interestingly, we see an accuracy of around **58.6 %** after only 5000 steps which doesn't improve that much anymore afterward. Choosing a bigger model or training for longer would have probably given better results here, but that's good enough for our hypothetical use case!\\n\\nAlright, finally let's upload the model checkpoint to the Hub.\\n\\n\\n```python\\ntrainer.push_to_hub()\\n```\\n\\n**Output:**\\n```\\n    Saving model checkpoint to deberta_amazon_reviews_v1\\n    Configuration saved in deberta_amazon_reviews_v1/config.json\\n    Model weights saved in deberta_amazon_reviews_v1/pytorch_model.bin\\n    tokenizer config file saved in deberta_amazon_reviews_v1/tokenizer_config.json\\n    Special tokens file saved in deberta_amazon_reviews_v1/special_tokens_map.json\\n    added tokens file saved in deberta_amazon_reviews_v1/added_tokens.json\\n    Several commits (2) will be pushed upstream.\\n    The progress bars may be unreliable.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 30849}, page_content='```\\n\\n### Evaluate / Analyse the model\\n\\nNow that we have fine-tuned the model we need to be very careful about analyzing its performance. \\nNote that canonical metrics, such as *accuracy*, are useful to get a general picture\\nabout your model\\'s performance, but it might not be enough to evaluate how well the model performs on your actual use case.\\nThe better approach is to find a metric that best describes the actual use case of the model and measure exactly this metric during and after training.\\n\\nLet\\'s dive into evaluating the model 🤿.\\n\\n\\nThe model has been uploaded to the Hub under [`deberta_v3_amazon_reviews`](https://huggingface.co/patrickvonplaten/deberta_v3_amazon_reviews) after training, so in a first step, let\\'s download it from there again.\\n\\n\\n```python\\nfrom transformers import AutoModelForSequenceClassification\\n\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"patrickvonplaten/deberta_v3_amazon_reviews\")\\n```\\n\\nThe Trainer is not only an excellent class to train a model, but also to evaluate a model on a dataset. Let\\'s instantiate the trainer with the same instances and functions as before, but this time there is no need to pass a training dataset.\\n\\n\\n```python\\ntrainer = Trainer(\\n    args=training_args,\\n    compute_metrics=compute_metrics,\\n    model=model,\\n    tokenizer=tokenizer,\\n    data_collator=data_collator,\\n)\\n```\\n\\nWe use the Trainer\\'s [`predict`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.predict) function to evaluate the model on the test dataset on the same metric.\\n\\n\\n```python\\nprediction_metrics = trainer.predict(tokenized_datasets[\"test\"]).metrics\\nprediction_metrics\\n```\\n\\n**Output:**\\n```\\n    ***** Running Prediction *****\\n      Num examples = 5000\\n      Batch size = 8\\n```\\n\\n\\n**Output:**'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': -1}, page_content=\"```\\n\\n**Output:**\\n```\\n    ***** Running Prediction *****\\n      Num examples = 5000\\n      Batch size = 8\\n```\\n\\n\\n**Output:**\\n```\\n    {'test_accuracy': 0.608,\\n     'test_loss': 0.9637690186500549,\\n     'test_runtime': 21.9574,\\n     'test_samples_per_second': 227.714,\\n     'test_steps_per_second': 28.464}\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 32813}, page_content='```\\n\\n\\n\\nThe results are very similar to performance on the validation dataset, which is usually a good sign as it shows that the model didn\\'t overfit the test dataset.\\n\\nHowever, 60% accuracy is far from being perfect on a 5-class classification problem, but do we need very high accuracy for all classes?\\n\\nSince we are mostly concerned with very negative customer feedback, let\\'s just focus on how well the model performs on classifying reviews of the most unsatisfied customers. We also decide to help the model a bit - all feedback classified as either **very unsatisfied** or **unsatisfied** will be handled by us - to catch close to 99% of the **very unsatisfied** messages. At the same time, we also measure how many **unsatisfied** messages we can answer this way and how much unnecessary work we do by answering messages of neutral, satisfied, and very satisfied customers.\\n\\nGreat, let\\'s write a new `compute_metrics` function.\\n\\n\\n```python\\nimport numpy as np\\n\\ndef compute_metrics(pred):\\n    pred_logits = pred.predictions\\n    pred_classes = np.argmax(pred_logits, axis=-1)\\n    labels = np.asarray(pred.label_ids)\\n\\n    # First let\\'s compute % of very unsatisfied messages we can catch\\n    very_unsatisfied_label_idx = (labels == 0)\\n    very_unsatisfied_pred = pred_classes[very_unsatisfied_label_idx]\\n\\n    # Now both 0 and 1 labels are 0 labels the rest is > 0\\n    very_unsatisfied_pred = very_unsatisfied_pred * (very_unsatisfied_pred - 1)\\n    \\n    # Let\\'s count how many labels are 0 -> that\\'s the \"very unsatisfied\"-accuracy\\n    true_positives = sum(very_unsatisfied_pred == 0) / len(very_unsatisfied_pred)\\n\\n    # Second let\\'s compute how many satisfied messages we unnecessarily reply to\\n    satisfied_label_idx = (labels > 1)\\n    satisfied_pred = pred_classes[satisfied_label_idx]'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': -1}, page_content='# Second let\\'s compute how many satisfied messages we unnecessarily reply to\\n    satisfied_label_idx = (labels > 1)\\n    satisfied_pred = pred_classes[satisfied_label_idx]\\n\\n    # how many predictions are labeled as unsatisfied over all satisfied messages?\\n    false_positives = sum(satisfied_pred <= 1) / len(satisfied_pred)\\n\\n    return {\"%_unsatisfied_replied\": round(true_positives, 2), \"%_satisfied_incorrectly_labels\": round(false_positives, 2)}'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 34882}, page_content='```\\n\\n\\nWe again instantiate the `Trainer` to easily run the evaluation.\\n\\n\\n```python\\ntrainer = Trainer(\\n    args=training_args,\\n    compute_metrics=compute_metrics,\\n    model=model,\\n    tokenizer=tokenizer,\\n    data_collator=data_collator,\\n)\\n```\\n\\n\\nAnd let\\'s run the evaluation again with our new metric computation which is better suited for our use case.\\n\\n\\n```python\\nprediction_metrics = trainer.predict(tokenized_datasets[\"test\"]).metrics\\nprediction_metrics\\n```\\n\\n\\n\\n**Output:**\\n```\\n    ***** Running Prediction *****\\n      Num examples = 5000\\n      Batch size = 8\\n```\\n\\n**Output:**\\n```\\n    {\\'test_%_satisfied_incorrectly_labels\\': 0.11733333333333333,\\n     \\'test_%_unsatisfied_replied\\': 0.949,\\n     \\'test_loss\\': 0.9637690186500549,\\n     \\'test_runtime\\': 22.8964,\\n     \\'test_samples_per_second\\': 218.375,\\n     \\'test_steps_per_second\\': 27.297}\\n```\\n\\n\\n\\nCool! This already paints a pretty nice picture. We catch around 95% of **very unsatisfied** customers automatically at a cost of wasting our efforts on 10% of satisfied messages.\\n\\nLet\\'s do some quick math. We receive daily around 10,000 messages for which we expect ca. 500 to be very negative. Instead of having to answer to all 10,000 messages, using this automatic filtering, we would only need to look into 500 + 0.12 \\\\* 10,000 = 1700 messages and only reply to 475 messages while incorrectly missing 5% of the messages. Pretty nice - a 83% reduction in human effort at missing only 5% of very unsatisfied customers!\\n\\nObviously, the numbers don\\'t represent the gained value of an actual use case, but we could come close to it with enough high-quality training data of your real-world example!\\n\\n\\nLet\\'s save the results\\n\\n\\n```python\\ntrainer.save_metrics(\"prediction\", prediction_metrics)\\n```\\n\\n\\nand again upload everything on the Hub.\\n\\n\\n```python\\ntrainer.push_to_hub()'),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': -1}, page_content=\"```\\n\\n\\nand again upload everything on the Hub.\\n\\n\\n```python\\ntrainer.push_to_hub()\\n```\\n\\n**Output:**\\n```\\n    Saving model checkpoint to deberta_amazon_reviews_v1\\n    Configuration saved in deberta_amazon_reviews_v1/config.json\\n    Model weights saved in deberta_amazon_reviews_v1/pytorch_model.bin\\n    tokenizer config file saved in deberta_amazon_reviews_v1/tokenizer_config.json\\n    Special tokens file saved in deberta_amazon_reviews_v1/special_tokens_map.json\\n    added tokens file saved in deberta_amazon_reviews_v1/added_tokens.json\\n    To https://huggingface.co/patrickvonplaten/deberta_amazon_reviews_v1\\n       599b891..ad77e6d  main -> main\\n\\n    Dropping the following result as it does not have all the necessary fields:\\n    {'task': {'name': 'Text Classification', 'type': 'text-classification'}}\\n    To https://huggingface.co/patrickvonplaten/deberta_amazon_reviews_v1\\n       ad77e6d..13e5ddd  main -> main\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 37533}, page_content=\"```\\n\\n\\n\\nThe data is now saved [here](https://huggingface.co/patrickvonplaten/deberta_amazon_reviews_v1/blob/main/prediction_results.json).\\n\\nThat's it for today 😎. As a final step, it would also make a lot of sense to try the model out on actual real-world data. This can be done directly on the inference widget on [the model card](https://huggingface.co/patrickvonplaten/deberta_amazon_reviews_v1):\\n\\n![example.png](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/classification_widget.png)\\n\\n\\nIt does seem to generalize quite well to real-world data 🔥\\n\\n\\n## Optimization\\n\\nAs soon as you think the model's performance is good enough for production it's all about making the model as memory efficient and fast as possible.\\n\\nThere are some obvious solutions to this like choosing the best suited accelerated hardware, *e.g.* better GPUs, making sure no gradients are computed during the forward pass, or lowering the precision, *e.g.* to float16.\\n\\nMore advanced optimization methods include using open-source accelerator libraries such as [ONNX Runtime](https://onnxruntime.ai/index.html), [quantization](https://pytorch.org/docs/stable/quantization.html), and inference servers like [Triton](https://developer.nvidia.com/nvidia-triton-inference-server).\\n\\nAt Hugging Face, we have been working a lot to facilitate the optimization of models, especially with our open-source [Optimum library](https://huggingface.co/hardware). Optimum makes it extremely simple to optimize most 🤗 Transformers models.\\n\\nIf you're looking for **highly optimized** solutions which don't require any technical knowledge, you might be interested in the [Inference API](https://huggingface.co/inference-api), a plug & play solution to serve in production a wide variety of machine learning tasks, including sentiment analysis.\"),\n",
       " Document(metadata={'source': 'huggingface/blog/blob/main/supercharge-customer-service-with-machine-learning.md', 'start_index': 39360}, page_content=\"Moreover, if you are searching for **support for your custom use cases**, Hugging Face's team of experts can help accelerate your ML projects! Our team answer questions and find solutions as needed in your machine learning journey from research to production. Visit [hf.co/support](https://huggingface.co/support) to learn more and request a quote.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/tapex.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# TAPEX\\n\\n<Tip warning={true}>\\n\\nThis model is in maintenance mode only, we don\\'t accept any new PRs changing its code.\\n\\nIf you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.\\nYou can do so by running the following command: `pip install -U transformers==4.30.0`.\\n\\n</Tip>\\n\\n## Overview\\n\\nThe TAPEX model was proposed in [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu,\\nBei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART model to solve synthetic SQL queries, after\\nwhich it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/tapex.md', 'start_index': 1516}, page_content='TAPEX has been fine-tuned on several datasets: \\n- [SQA](https://www.microsoft.com/en-us/download/details.aspx?id=54253) (Sequential Question Answering by Microsoft)\\n- [WTQ](https://github.com/ppasupat/WikiTableQuestions) (Wiki Table Questions by Stanford University)\\n- [WikiSQL](https://github.com/salesforce/WikiSQL) (by Salesforce)\\n- [TabFact](https://tabfact.github.io/) (by USCB NLP Lab).\\n\\nThe abstract from the paper is the following:\\n\\n*Recent progress in language model pre-training has achieved a great success via leveraging large-scale unstructured textual data. However, it is\\nstill a challenge to apply pre-training on structured tabular data due to the absence of large-scale high-quality tabular data. In this paper, we\\npropose TAPEX to show that table pre-training can be achieved by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically\\nsynthesizing executable SQL queries and their execution outputs. TAPEX addresses the data scarcity challenge via guiding the language model to mimic a SQL\\nexecutor on the diverse, large-scale and high-quality synthetic corpus. We evaluate TAPEX on four benchmark datasets. Experimental results demonstrate that\\nTAPEX outperforms previous table pre-training approaches by a large margin and achieves new state-of-the-art results on all of them. This includes improvements\\non the weakly-supervised WikiSQL denotation accuracy to 89.5% (+2.3%), the WikiTableQuestions denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy\\nto 74.5% (+3.5%), and the TabFact accuracy to 84.2% (+3.2%). To our knowledge, this is the first work to exploit table pre-training via synthetic executable programs\\nand to achieve new state-of-the-art results on various downstream tasks.*\\n\\n## Usage tips'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/tapex.md', 'start_index': 3277}, page_content='## Usage tips\\n\\n- TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPEX into a BART model. \\n- TAPEX has checkpoints on the hub that are either pre-trained only, or fine-tuned on WTQ, SQA, WikiSQL and TabFact.\\n- Sentences + tables are presented to the model as `sentence + \" \" + linearized table`. The linearized table has the following format: \\n  `col: col1 | col2 | col 3 row 1 : val1 | val2 | val3 row 2 : ...`.\\n- TAPEX has its own tokenizer, that allows to prepare all data for the model easily. One can pass Pandas DataFrames and strings to the tokenizer,\\n  and it will automatically create the `input_ids` and `attention_mask` (as shown in the usage examples below). \\n\\n### Usage: inference\\n\\nBelow, we illustrate how to use TAPEX for table question answering. As one can see, one can directly plug in the weights of TAPEX into a BART model.\\nWe use the [Auto API](auto), which will automatically instantiate the appropriate tokenizer ([`TapexTokenizer`]) and model ([`BartForConditionalGeneration`]) for us,\\nbased on the configuration file of the checkpoint on the hub.\\n\\n```python\\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\\n>>> import pandas as pd\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/tapex-large-finetuned-wtq\")\\n\\n>>> # prepare table + question\\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\\n>>> table = pd.DataFrame.from_dict(data)\\n>>> question = \"how many movies does Leonardo Di Caprio have?\"'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/tapex.md', 'start_index': 4910}, page_content='>>> encoding = tokenizer(table, question, return_tensors=\"pt\")\\n\\n>>> # let the model generate an answer autoregressively\\n>>> outputs = model.generate(**encoding)\\n\\n>>> # decode back to text\\n>>> predicted_answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\\n>>> print(predicted_answer)\\n53'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/tapex.md', 'start_index': 5213}, page_content='```\\n\\nNote that [`TapexTokenizer`] also supports batched inference. Hence, one can provide a batch of different tables/questions, or a batch of a single table\\nand multiple questions, or a batch of a single query and multiple tables. Let\\'s illustrate this:\\n\\n```python\\n>>> # prepare table + question\\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\\n>>> table = pd.DataFrame.from_dict(data)\\n>>> questions = [\\n...     \"how many movies does Leonardo Di Caprio have?\",\\n...     \"which actor has 69 movies?\",\\n...     \"what\\'s the first name of the actor who has 87 movies?\",\\n... ]\\n>>> encoding = tokenizer(table, questions, padding=True, return_tensors=\"pt\")\\n\\n>>> # let the model generate an answer autoregressively\\n>>> outputs = model.generate(**encoding)\\n\\n>>> # decode back to text\\n>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\\n[\\' 53\\', \\' george clooney\\', \\' brad pitt\\']'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/tapex.md', 'start_index': 6158}, page_content='```\\n\\nIn case one wants to do table verification (i.e. the task of determining whether a given sentence is supported or refuted by the contents\\nof a table), one can instantiate a [`BartForSequenceClassification`] model. TAPEX has checkpoints on the hub fine-tuned on TabFact, an important\\nbenchmark for table fact checking (it achieves 84% accuracy). The code example below again leverages the [Auto API](auto).\\n\\n```python\\n>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/tapex-large-finetuned-tabfact\")\\n\\n>>> # prepare table + sentence\\n>>> data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\\n>>> table = pd.DataFrame.from_dict(data)\\n>>> sentence = \"George Clooney has 30 movies\"\\n\\n>>> encoding = tokenizer(table, sentence, return_tensors=\"pt\")\\n\\n>>> # forward pass\\n>>> outputs = model(**encoding)\\n\\n>>> # print prediction\\n>>> predicted_class_idx = outputs.logits[0].argmax(dim=0).item()\\n>>> print(model.config.id2label[predicted_class_idx])\\nRefused\\n```\\n\\n<Tip> \\n\\nTAPEX architecture is the same as BART, except for tokenization. Refer to [BART documentation](bart) for information on \\nconfiguration classes and their parameters. TAPEX-specific tokenizer is documented below.  \\n\\n</Tip>\\n\\n## TapexTokenizer\\n\\n[[autodoc]] TapexTokenizer\\n    - __call__\\n    - save_vocabulary'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Video classification\\n\\n[[open-in-colab]]\\n\\nVideo classification is the task of assigning a label or class to an entire video. Videos are expected to have only one class for each video. Video classification models take a video as input and return a prediction about which class the video belongs to. These models can be used to categorize what a video is all about. A real-world application of video classification is action / activity recognition, which is useful for fitness applications. It is also helpful for vision-impaired individuals, especially when they are commuting.\\n\\nThis guide will show you how to:\\n\\n1. Fine-tune [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae) on a subset of the [UCF101](https://www.crcv.ucf.edu/data/UCF101.php) dataset.\\n2. Use your fine-tuned model for inference.\\n\\n<Tip>\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\\n\\n[TimeSformer](../model_doc/timesformer), [VideoMAE](../model_doc/videomae), [ViViT](../model_doc/vivit)\\n\\n<!--End of the generated tip-->\\n\\n</Tip>\\n\\nBefore you begin, make sure you have all the necessary libraries installed:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': -1}, page_content='<!--End of the generated tip-->\\n\\n</Tip>\\n\\nBefore you begin, make sure you have all the necessary libraries installed:\\n\\n```bash\\npip install -q pytorchvideo transformers evaluate'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 2046}, page_content='```\\n\\nYou will use [PyTorchVideo](https://pytorchvideo.org/) (dubbed `pytorchvideo`) to process and prepare the videos.\\n\\nWe encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:\\n\\n```py\\n>>> from huggingface_hub import notebook_login\\n\\n>>> notebook_login()\\n```\\n\\n## Load UCF101 dataset\\n\\nStart by loading a subset of the [UCF-101 dataset](https://www.crcv.ucf.edu/data/UCF101.php). This will give you a chance to experiment and make sure everything works before spending more time training on the full dataset.\\n\\n```py\\n>>> from huggingface_hub import hf_hub_download\\n\\n>>> hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\\n>>> filename = \"UCF101_subset.tar.gz\"\\n>>> file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")\\n```\\n\\nAfter the subset has been downloaded, you need to extract the compressed archive:\\n\\n```py \\n>>> import tarfile\\n\\n>>> with tarfile.open(file_path) as t:\\n...      t.extractall(\".\")\\n```\\n\\nAt a high level, the dataset is organized like so:\\n\\n```bash\\nUCF101_subset/\\n    train/\\n        BandMarching/\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        Archery\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        ...\\n    val/\\n        BandMarching/\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        Archery\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        ...\\n    test/\\n        BandMarching/\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        Archery\\n            video_1.mp4\\n            video_2.mp4\\n            ...\\n        ...'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 3728}, page_content=\"```\\n\\nThe (`sorted`) video paths appear like so:\\n\\n```bash\\n...\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\\n'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\\n...\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 4127}, page_content='```\\n\\nYou will notice that there are video clips belonging to the same group / scene where group is denoted by `g` in the video file paths. `v_ApplyEyeMakeup_g07_c04.avi` and `v_ApplyEyeMakeup_g07_c06.avi`, for example.\\n\\nFor the validation and evaluation splits, you wouldn\\'t want to have video clips from the same group / scene to prevent [data leakage](https://www.kaggle.com/code/alexisbcook/data-leakage). The subset that you are using in this tutorial takes this information into account.\\n\\nNext up, you will derive the set of labels present in the dataset. Also, create two dictionaries that\\'ll be helpful when initializing the model:\\n\\n* `label2id`: maps the class names to integers.\\n* `id2label`: maps the integers to class names. \\n\\n```py \\n>>> class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\\n>>> label2id = {label: i for i, label in enumerate(class_labels)}\\n>>> id2label = {i: label for label, i in label2id.items()}\\n\\n>>> print(f\"Unique classes: {list(label2id.keys())}.\")\\n\\n# Unique classes: [\\'ApplyEyeMakeup\\', \\'ApplyLipstick\\', \\'Archery\\', \\'BabyCrawling\\', \\'BalanceBeam\\', \\'BandMarching\\', \\'BaseballPitch\\', \\'Basketball\\', \\'BasketballDunk\\', \\'BenchPress\\'].'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 5316}, page_content='```\\n\\nThere are 10 unique classes. For each class, there are 30 videos in the training set.\\n\\n## Load a model to fine-tune\\n\\nInstantiate a video classification model from a pretrained checkpoint and its associated image processor. The model\\'s encoder comes with pre-trained parameters, and the classification head is randomly initialized. The image processor will come in handy when writing the preprocessing pipeline for our dataset.\\n\\n```py \\n>>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\\n\\n>>> model_ckpt = \"MCG-NJU/videomae-base\"\\n>>> image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\\n>>> model = VideoMAEForVideoClassification.from_pretrained(\\n...     model_ckpt,\\n...     label2id=label2id,\\n...     id2label=id2label,\\n...     ignore_mismatched_sizes=True,  # provide this in case you\\'re planning to fine-tune an already fine-tuned checkpoint\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 6219}, page_content=\"```\\n\\nWhile the model is loading, you might notice the following warning:\\n\\n```bash\\nSome weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.key.weight']\\n- This IS expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\\n- This IS NOT expected if you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\\nSome weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 7341}, page_content=\"```\\n\\nThe warning is telling us we are throwing away some weights (e.g. the weights and bias of the `classifier` layer) and randomly initializing some others (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.\\n\\n**Note** that [this checkpoint](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) leads to better performance on this task as the checkpoint was obtained fine-tuning on a similar downstream task having considerable domain overlap. You can check out [this checkpoint](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) which was obtained by fine-tuning `MCG-NJU/videomae-base-finetuned-kinetics`.  \\n\\n## Prepare the datasets for training\\n\\nFor preprocessing the videos, you will leverage the [PyTorchVideo library](https://pytorchvideo.org/). Start by importing the dependencies we need. \\n\\n```py \\n>>> import pytorchvideo.data\\n\\n>>> from pytorchvideo.transforms import (\\n...     ApplyTransformToKey,\\n...     Normalize,\\n...     RandomShortSideScale,\\n...     RemoveKey,\\n...     ShortSideScale,\\n...     UniformTemporalSubsample,\\n... )\\n\\n>>> from torchvision.transforms import (\\n...     Compose,\\n...     Lambda,\\n...     RandomCrop,\\n...     RandomHorizontalFlip,\\n...     Resize,\\n... )\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 8816}, page_content='```\\n\\nFor the training dataset transformations, use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping. For the validation and evaluation dataset transformations, keep the same transformation chain except for random cropping and horizontal flipping. To learn more about the details of these transformations check out the [official documentation of PyTorchVideo](https://pytorchvideo.org).  \\n\\nUse the `image_processor` associated with the pre-trained model to obtain the following information:\\n\\n* Image mean and standard deviation with which the video frame pixels will be normalized.\\n* Spatial resolution to which the video frames will be resized.\\n\\nStart by defining some constants.\\n\\n```py\\n>>> mean = image_processor.image_mean\\n>>> std = image_processor.image_std\\n>>> if \"shortest_edge\" in image_processor.size:\\n...     height = width = image_processor.size[\"shortest_edge\"]\\n>>> else:\\n...     height = image_processor.size[\"height\"]\\n...     width = image_processor.size[\"width\"]\\n>>> resize_to = (height, width)\\n\\n>>> num_frames_to_sample = model.config.num_frames\\n>>> sample_rate = 4\\n>>> fps = 30\\n>>> clip_duration = num_frames_to_sample * sample_rate / fps'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 10037}, page_content='```\\n\\nNow, define the dataset-specific transformations and the datasets respectively. Starting with the training set: \\n\\n```py \\n>>> train_transform = Compose(\\n...     [\\n...         ApplyTransformToKey(\\n...             key=\"video\",\\n...             transform=Compose(\\n...                 [\\n...                     UniformTemporalSubsample(num_frames_to_sample),\\n...                     Lambda(lambda x: x / 255.0),\\n...                     Normalize(mean, std),\\n...                     RandomShortSideScale(min_size=256, max_size=320),\\n...                     RandomCrop(resize_to),\\n...                     RandomHorizontalFlip(p=0.5),\\n...                 ]\\n...             ),\\n...         ),\\n...     ]\\n... )\\n\\n>>> train_dataset = pytorchvideo.data.Ucf101(\\n...     data_path=os.path.join(dataset_root_path, \"train\"),\\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\\n...     decode_audio=False,\\n...     transform=train_transform,\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 10999}, page_content='```\\n\\nThe same sequence of workflow can be applied to the validation and evaluation sets: \\n\\n```py \\n>>> val_transform = Compose(\\n...     [\\n...         ApplyTransformToKey(\\n...             key=\"video\",\\n...             transform=Compose(\\n...                 [\\n...                     UniformTemporalSubsample(num_frames_to_sample),\\n...                     Lambda(lambda x: x / 255.0),\\n...                     Normalize(mean, std),\\n...                     Resize(resize_to),\\n...                 ]\\n...             ),\\n...         ),\\n...     ]\\n... )\\n\\n>>> val_dataset = pytorchvideo.data.Ucf101(\\n...     data_path=os.path.join(dataset_root_path, \"val\"),\\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\\n...     decode_audio=False,\\n...     transform=val_transform,\\n... )\\n\\n>>> test_dataset = pytorchvideo.data.Ucf101(\\n...     data_path=os.path.join(dataset_root_path, \"test\"),\\n...     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\\n...     decode_audio=False,\\n...     transform=val_transform,\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 12051}, page_content=\"```\\n\\n**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https://pytorchvideo.org/docs/tutorial_classification#dataset). We're using the [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) function because it's tailored for the UCF-101 dataset. Under the hood, it returns a [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) object. `LabeledVideoDataset` class is the base class for all things video in the PyTorchVideo dataset. So, if you want to use a custom dataset not supported off-the-shelf by PyTorchVideo, you can extend the `LabeledVideoDataset` class accordingly. Refer to the `data` API [documentation to](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) learn more. Also, if your dataset follows a similar structure (as shown above), then using the `pytorchvideo.data.Ucf101()` should work just fine. \\n\\nYou can access the `num_videos` argument to know the number of videos in the dataset.\\n\\n```py\\n>>> print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)\\n# (300, 30, 75)\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 13301}, page_content='```\\n\\n## Visualize the preprocessed video for better debugging \\n\\n```py \\n>>> import imageio\\n>>> import numpy as np\\n>>> from IPython.display import Image\\n\\n>>> def unnormalize_img(img):\\n...     \"\"\"Un-normalizes the image pixels.\"\"\"\\n...     img = (img * std) + mean\\n...     img = (img * 255).astype(\"uint8\")\\n...     return img.clip(0, 255)\\n\\n>>> def create_gif(video_tensor, filename=\"sample.gif\"):\\n...     \"\"\"Prepares a GIF from a video tensor.\\n...     \\n...     The video tensor is expected to have the following shape:\\n...     (num_frames, num_channels, height, width).\\n...     \"\"\"\\n...     frames = []\\n...     for video_frame in video_tensor:\\n...         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\\n...         frames.append(frame_unnormalized)\\n...     kargs = {\"duration\": 0.25}\\n...     imageio.mimsave(filename, frames, \"GIF\", **kargs)\\n...     return filename\\n\\n>>> def display_gif(video_tensor, gif_name=\"sample.gif\"):\\n...     \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\\n...     video_tensor = video_tensor.permute(1, 0, 2, 3)\\n...     gif_filename = create_gif(video_tensor, gif_name)\\n...     return Image(filename=gif_filename)\\n\\n>>> sample_video = next(iter(train_dataset))\\n>>> video_tensor = sample_video[\"video\"]\\n>>> display_gif(video_tensor)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 14587}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif\" alt=\"Person playing basketball\"/>\\n</div>\\n\\n## Train the model \\n\\nLeverage [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) from  🤗 Transformers for training the model. To instantiate a `Trainer`, you need to define the training configuration and an evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to configure the training. It requires an output folder name, which will be used to save the checkpoints of the model. It also helps sync all the information in the model repository on 🤗 Hub.\\n\\nMost of the training arguments are self-explanatory, but one that is quite important here is `remove_unused_columns=False`. This one will drop any features not used by the model\\'s call function. By default it\\'s `True` because usually it\\'s ideal to drop unused feature columns, making it easier to unpack inputs into the model\\'s call function. But, in this case, you need the unused features (\\'video\\' in particular) in order to create `pixel_values` (which is a mandatory key our model expects in its inputs).\\n\\n\\n```py \\n>>> from transformers import TrainingArguments, Trainer\\n\\n>>> model_name = model_ckpt.split(\"/\")[-1]\\n>>> new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\\n>>> num_epochs = 4'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 16130}, page_content='>>> args = TrainingArguments(\\n...     new_model_name,\\n...     remove_unused_columns=False,\\n...     evaluation_strategy=\"epoch\",\\n...     save_strategy=\"epoch\",\\n...     learning_rate=5e-5,\\n...     per_device_train_batch_size=batch_size,\\n...     per_device_eval_batch_size=batch_size,\\n...     warmup_ratio=0.1,\\n...     logging_steps=10,\\n...     load_best_model_at_end=True,\\n...     metric_for_best_model=\"accuracy\",\\n...     push_to_hub=True,\\n...     max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\\n... )'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 16648}, page_content='```\\n\\nThe dataset returned by `pytorchvideo.data.Ucf101()` doesn\\'t implement the `__len__` method. As such, we must define `max_steps` when instantiating `TrainingArguments`. \\n\\nNext, you need to define a function to compute the metrics from the predictions, which will use the `metric` you\\'ll load now. The only preprocessing you have to do is to take the argmax of our predicted logits:\\n\\n```py\\nimport evaluate\\n\\nmetric = evaluate.load(\"accuracy\")\\n\\n\\ndef compute_metrics(eval_pred):\\n    predictions = np.argmax(eval_pred.predictions, axis=1)\\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\\n```\\n\\n**A note on evaluation**:\\n\\nIn the [VideoMAE paper](https://arxiv.org/abs/2203.12602), the authors use the following evaluation strategy. They evaluate the model on several clips from test videos and apply different crops to those clips and report the aggregate score. However, in the interest of simplicity and brevity, we don\\'t consider that in this tutorial.\\n\\nAlso, define a `collate_fn`, which will be used to batch examples together. Each batch consists of 2 keys, namely `pixel_values` and `labels`.\\n\\n```py \\n>>> def collate_fn(examples):\\n...     # permute to (num_frames, num_channels, height, width)\\n...     pixel_values = torch.stack(\\n...         [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\\n...     )\\n...     labels = torch.tensor([example[\"label\"] for example in examples])\\n...     return {\"pixel_values\": pixel_values, \"labels\": labels}'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 18140}, page_content='```\\n\\nThen you just pass all of this along with the datasets to `Trainer`:\\n\\n```py \\n>>> trainer = Trainer(\\n...     model,\\n...     args,\\n...     train_dataset=train_dataset,\\n...     eval_dataset=val_dataset,\\n...     tokenizer=image_processor,\\n...     compute_metrics=compute_metrics,\\n...     data_collator=collate_fn,\\n... )\\n```\\n\\nYou might wonder why you passed along the `image_processor` as a tokenizer when you preprocessed the data already. This is only to make sure the image processor configuration file (stored as JSON) will also be uploaded to the repo on the Hub.\\n\\nNow fine-tune our model by calling the `train` method:\\n\\n```py \\n>>> train_results = trainer.train()\\n```\\n\\nOnce training is completed, share your model to the Hub with the [`~transformers.Trainer.push_to_hub`] method so everyone can use your model:\\n\\n```py\\n>>> trainer.push_to_hub()\\n```\\n\\n## Inference\\n\\nGreat, now that you have fine-tuned a model, you can use it for inference!\\n\\nLoad a video for inference:\\n\\n```py \\n>>> sample_test_video = next(iter(test_dataset))'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 19169}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif\" alt=\"Teams playing basketball\"/>\\n</div>\\n\\nThe simplest way to try out your fine-tuned model for inference is to use it in a [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline). Instantiate a `pipeline` for video classification with your model, and pass your video to it:\\n\\n```py\\n>>> from transformers import pipeline\\n\\n>>> video_cls = pipeline(model=\"my_awesome_video_cls_model\")\\n>>> video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")\\n[{\\'score\\': 0.9272987842559814, \\'label\\': \\'BasketballDunk\\'},\\n {\\'score\\': 0.017777055501937866, \\'label\\': \\'BabyCrawling\\'},\\n {\\'score\\': 0.01663011871278286, \\'label\\': \\'BalanceBeam\\'},\\n {\\'score\\': 0.009560945443809032, \\'label\\': \\'BandMarching\\'},\\n {\\'score\\': 0.0068979403004050255, \\'label\\': \\'BaseballPitch\\'}]'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/video_classification.md', 'start_index': 20201}, page_content='```\\n\\nYou can also manually replicate the results of the `pipeline` if you\\'d like.\\n\\n\\n```py\\n>>> def run_inference(model, video):\\n...     # (num_frames, num_channels, height, width)\\n...     perumuted_sample_test_video = video.permute(1, 0, 2, 3)\\n...     inputs = {\\n...         \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\\n...         \"labels\": torch.tensor(\\n...             [sample_test_video[\"label\"]]\\n...         ),  # this can be skipped if you don\\'t have labels available.\\n...     }\\n\\n...     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n...     inputs = {k: v.to(device) for k, v in inputs.items()}\\n...     model = model.to(device)\\n\\n...     # forward pass\\n...     with torch.no_grad():\\n...         outputs = model(**inputs)\\n...         logits = outputs.logits\\n\\n...     return logits\\n```\\n\\nNow, pass your input to the model and return the `logits`:\\n\\n```\\n>>> logits = run_inference(trained_model, sample_test_video[\"video\"])\\n```\\n\\nDecoding the `logits`, we get: \\n\\n```py \\n>>> predicted_class_idx = logits.argmax(-1).item()\\n>>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\\n# Predicted class: BasketballDunk\\n```'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# OpenAI GPT\\n\\n<div class=\"flex flex-wrap space-x-1\">\\n<a href=\"https://huggingface.co/models?filter=openai-gpt\">\\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-openai--gpt-blueviolet\">\\n</a>\\n<a href=\"https://huggingface.co/spaces/docs-demos/openai-gpt\">\\n<img alt=\"Spaces\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue\">\\n</a>\\n</div>\\n\\n## Overview\\n\\nOpenAI GPT model was proposed in [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\\nby Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever. It\\'s a causal (unidirectional) transformer\\npre-trained using language modeling on a large corpus will long range dependencies, the Toronto Book Corpus.\\n\\nThe abstract from the paper is the following:'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md', 'start_index': 1585}, page_content=\"The abstract from the paper is the following:\\n\\n*Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering,\\nsemantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant,\\nlabeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to\\nperform adequately. We demonstrate that large gains on these tasks can be realized by generative pretraining of a\\nlanguage model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In\\ncontrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve\\neffective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our\\napproach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms\\ndiscriminatively trained models that use architectures specifically crafted for each task, significantly improving upon\\nthe state of the art in 9 out of the 12 tasks studied.*\\n\\n[Write With Transformer](https://transformer.huggingface.co/doc/gpt) is a webapp created and hosted by Hugging Face\\nshowcasing the generative capabilities of several models. GPT is one of them.\\n\\nThis model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/openai/finetune-transformer-lm).\\n\\n## Usage tips\\n\\n- GPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\\n  the left.\\n- GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next\\n  token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be\\n  observed in the *run_generation.py* example script.\\n\\n\\nNote:\\n\\nIf you want to reproduce the original tokenization process of the *OpenAI GPT* paper, you will need to install `ftfy`\\nand `SpaCy`:\\n\\n```bash\\npip install spacy ftfy==4.4.3\\npython -m spacy download en\"),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md', 'start_index': 3737}, page_content='```\\n\\nIf you don\\'t install `ftfy` and `SpaCy`, the [`OpenAIGPTTokenizer`] will default to tokenize\\nusing BERT\\'s `BasicTokenizer` followed by Byte-Pair Encoding (which should be fine for most usage, don\\'t worry).\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with OpenAI GPT. If you\\'re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\\'ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n<PipelineTag pipeline=\"text-classification\"/>\\n\\n- A blog post on [outperforming OpenAI GPT-3 with SetFit for text-classification](https://www.philschmid.de/getting-started-setfit).\\n- See also: [Text classification task guide](../tasks/sequence_classification)\\n\\n<PipelineTag pipeline=\"text-generation\"/>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md', 'start_index': 4608}, page_content='- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface).\\n- A blog on [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate) with GPT-2.\\n- A blog on [Training CodeParrot 🦜 from Scratch](https://huggingface.co/blog/codeparrot), a large GPT-2 model.\\n- A blog on [Faster Text Generation with TensorFlow and XLA](https://huggingface.co/blog/tf-xla-generate) with GPT-2.\\n- A blog on [How to train a Language Model with Megatron-LM](https://huggingface.co/blog/megatron-training) with a GPT-2 model.\\n- A notebook on how to [finetune GPT2 to generate lyrics in the style of your favorite artist](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb). 🌎\\n- A notebook on how to [finetune GPT2 to generate tweets in the style of your favorite Twitter user](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb). 🌎\\n- [Causal language modeling](https://huggingface.co/course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch) chapter of the 🤗 Hugging Face Course.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md', 'start_index': 5868}, page_content='- [`OpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#gpt-2gpt-and-causal-language-modeling), [text generation example script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb).\\n- [`TFOpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_clmpy) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\\n- See also: [Causal language modeling task guide](../tasks/language_modeling)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/openai-gpt.md', 'start_index': 6749}, page_content='<PipelineTag pipeline=\"token-classification\"/>\\n\\n- A course material on [Byte-Pair Encoding tokenization](https://huggingface.co/course/en/chapter6/5).\\n\\n## OpenAIGPTConfig\\n\\n[[autodoc]] OpenAIGPTConfig\\n\\n## OpenAIGPTTokenizer\\n\\n[[autodoc]] OpenAIGPTTokenizer\\n    - save_vocabulary\\n\\n## OpenAIGPTTokenizerFast\\n\\n[[autodoc]] OpenAIGPTTokenizerFast\\n\\n## OpenAI specific outputs\\n\\n[[autodoc]] models.openai.modeling_openai.OpenAIGPTDoubleHeadsModelOutput\\n\\n[[autodoc]] models.openai.modeling_tf_openai.TFOpenAIGPTDoubleHeadsModelOutput\\n\\n<frameworkcontent>\\n<pt>\\n\\n## OpenAIGPTModel\\n\\n[[autodoc]] OpenAIGPTModel\\n    - forward\\n\\n## OpenAIGPTLMHeadModel\\n\\n[[autodoc]] OpenAIGPTLMHeadModel\\n    - forward\\n\\n## OpenAIGPTDoubleHeadsModel\\n\\n[[autodoc]] OpenAIGPTDoubleHeadsModel\\n    - forward\\n\\n## OpenAIGPTForSequenceClassification\\n\\n[[autodoc]] OpenAIGPTForSequenceClassification\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFOpenAIGPTModel\\n\\n[[autodoc]] TFOpenAIGPTModel\\n    - call\\n\\n## TFOpenAIGPTLMHeadModel\\n\\n[[autodoc]] TFOpenAIGPTLMHeadModel\\n    - call\\n\\n## TFOpenAIGPTDoubleHeadsModel\\n\\n[[autodoc]] TFOpenAIGPTDoubleHeadsModel\\n    - call\\n\\n## TFOpenAIGPTForSequenceClassification\\n\\n[[autodoc]] TFOpenAIGPTForSequenceClassification\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/regnety.mdx', 'start_index': 1}, page_content=\"RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parameters: depth \\\\\\\\( d \\\\\\\\), initial width \\\\\\\\( w\\\\_{0} > 0 \\\\\\\\), and slope \\\\\\\\( w\\\\_{a} > 0 \\\\\\\\), and generates a different block width \\\\\\\\( u\\\\_{j} \\\\\\\\) for each block \\\\\\\\( j < d \\\\\\\\). The key restriction for the RegNet types of model is that there is a linear parameterisation of block widths (the design space only contains models with this linear structure):\\n\\n\\\\\\\\( \\\\\\\\) u\\\\_{j} = w\\\\_{0} + w\\\\_{a}\\\\cdot{j} \\\\\\\\( \\\\\\\\)\\n\\nFor **RegNetX** authors have additional restrictions: we set \\\\\\\\( b = 1 \\\\\\\\) (the bottleneck ratio), \\\\\\\\( 12 \\\\leq d \\\\leq 28 \\\\\\\\), and \\\\\\\\( w\\\\_{m} \\\\geq 2 \\\\\\\\) (the width multiplier).\\n\\nFor **RegNetY** authors make one change, which is to include [Squeeze-and-Excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block).\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model('regnety_002', pretrained=True)\\n>>> model.eval()\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/regnety.mdx', 'start_index': 1006}, page_content='```\\n\\nTo load and preprocess the image:\\n\\n```py\\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/regnety.mdx', 'start_index': 1800}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n>>> # Get imagenet class mappings\\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\\n...     categories = [s.strip() for s in f.readlines()]\\n\\n>>> # Print top categories per image\\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\\n>>> for i in range(top5_prob.size(0)):\\n...     print(categories[top5_catid[i]], top5_prob[i].item())\\n>>> # prints class names and probabilities like:\\n>>> # [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/regnety.mdx', 'start_index': 2593}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('regnety_002', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\\n```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{radosavovic2020designing,\\n      title={Designing Network Design Spaces},\\n      author={Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Dollár},\\n      year={2020},\\n      eprint={2003.13678},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 1}, page_content=\"Archived Changes\\n\\n### Nov 22, 2021\\n* A number of updated weights anew new model defs\\n  * `eca_halonext26ts` - 79.5 @ 256\\n  * `resnet50_gn` (new) - 80.1 @ 224, 81.3 @ 288\\n  * `resnet50` - 80.7 @ 224, 80.9 @ 288 (trained at 176, not replacing current a1 weights as default since these don't scale as well to higher res, [weights](https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet50_a1h2_176-001a1197.pth))\\n  * `resnext50_32x4d` - 81.1 @ 224, 82.0 @ 288\\n  * `sebotnet33ts_256` (new) - 81.2 @ 224\\n  * `lamhalobotnet50ts_256` - 81.5 @ 256\\n  * `halonet50ts` - 81.7 @ 256\\n  * `halo2botnet50ts_256` - 82.0 @ 256\\n  * `resnet101` - 82.0 @ 224, 82.8 @ 288\\n  * `resnetv2_101` (new) - 82.1 @ 224, 83.0 @ 288\\n  * `resnet152` - 82.8 @ 224, 83.5 @ 288\\n  * `regnetz_d8` (new) - 83.5 @ 256, 84.0 @ 320\\n  * `regnetz_e8` (new) - 84.5 @ 256, 85.0 @ 320\\n* `vit_base_patch8_224` (85.8 top-1) & `in21k` variant weights added thanks [Martins Bruveris](https://github.com/martinsbruveris)\\n* Groundwork in for FX feature extraction thanks to [Alexander Soare](https://github.com/alexander-soare)\\n  * models updated for tracing compatibility (almost full support with some distlled transformer exceptions)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 1222}, page_content='### Oct 19, 2021\\n* ResNet strikes back (https://arxiv.org/abs/2110.00476) weights added, plus any extra training components used. Model weights and some more details here (https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-rsb-weights)\\n* BCE loss and Repeated Augmentation support for RSB paper\\n* 4 series of ResNet based attention model experiments being added (implemented across byobnet.py/byoanet.py). These include all sorts of attention, from channel attn like SE, ECA to 2D QKV self-attention layers such as Halo, Bottlneck, Lambda. Details here (https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights)\\n* Working implementations of the following 2D self-attention modules (likely to be differences from paper or eventual official impl):\\n  * Halo (https://arxiv.org/abs/2103.12731)\\n  * Bottleneck Transformer (https://arxiv.org/abs/2101.11605)\\n  * LambdaNetworks (https://arxiv.org/abs/2102.08602)\\n* A RegNetZ series of models with some attention experiments (being added to). These do not follow the paper (https://arxiv.org/abs/2103.06877) in any way other than block architecture, details of official models are not available. See more here (https://github.com/rwightman/pytorch-image-models/releases/tag/v0.1-attn-weights)\\n* ConvMixer (https://openreview.net/forum?id=TVHS5Y4dNvM), CrossVit (https://arxiv.org/abs/2103.14899), and BeiT (https://arxiv.org/abs/2106.08254) architectures + weights added'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 2676}, page_content='* freeze/unfreeze helpers by [Alexander Soare](https://github.com/alexander-soare)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 2760}, page_content=\"### Aug 18, 2021\\n* Optimizer bonanza!\\n  * Add LAMB and LARS optimizers, incl trust ratio clipping options. Tweaked to work properly in PyTorch XLA (tested on TPUs w/ `timm bits` [branch](https://github.com/rwightman/pytorch-image-models/tree/bits_and_tpu/timm/bits))\\n  * Add MADGRAD from FB research w/ a few tweaks (decoupled decay option, step handling that works with PyTorch XLA)\\n  * Some cleanup on all optimizers and factory. No more `.data`, a bit more consistency, unit tests for all!\\n  * SGDP and AdamP still won't work with PyTorch XLA but others should (have yet to test Adabelief, Adafactor, Adahessian myself).\\n* EfficientNet-V2 XL TF ported weights added, but they don't validate well in PyTorch (L is better). The pre-processing for the V2 TF training is a bit diff and the fine-tuned 21k -> 1k weights are very sensitive and less robust than the 1k weights.\\n* Added PyTorch trained EfficientNet-V2 'Tiny' w/ GlobalContext attn weights. Only .1-.2 top-1 better than the SE so more of a curiosity for those interested.\\n\\n### July 12, 2021\\n* Add XCiT models from [official facebook impl](https://github.com/facebookresearch/xcit). Contributed by [Alexander Soare](https://github.com/alexander-soare)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 3973}, page_content=\"### July 5-9, 2021\\n* Add `efficientnetv2_rw_t` weights, a custom 'tiny' 13.6M param variant that is a bit better than (non NoisyStudent) B3 models. Both faster and better accuracy (at same or lower res)\\n  * top-1 82.34 @ 288x288 and 82.54 @ 320x320\\n* Add [SAM pretrained](https://arxiv.org/abs/2106.01548) in1k weight for ViT B/16 (`vit_base_patch16_sam_224`) and B/32 (`vit_base_patch32_sam_224`)  models.\\n* Add 'Aggregating Nested Transformer' (NesT) w/ weights converted from official [Flax impl](https://github.com/google-research/nested-transformer). Contributed by [Alexander Soare](https://github.com/alexander-soare).\\n  * `jx_nest_base` - 83.534, `jx_nest_small` - 83.120, `jx_nest_tiny` - 81.426\\n\\n### June 23, 2021\\n* Reproduce gMLP model training, `gmlp_s16_224` trained to 79.6 top-1, matching [paper](https://arxiv.org/abs/2105.08050). Hparams for this and other recent MLP training [here](https://gist.github.com/rwightman/d6c264a9001f9167e06c209f630b2cc6)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 4943}, page_content=\"### June 20, 2021\\n* Release Vision Transformer 'AugReg' weights from [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270)\\n  * .npz weight loading support added, can load any of the 50K+ weights from the [AugReg series](https://console.cloud.google.com/storage/browser/vit_models/augreg)\\n  * See [example notebook](https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax_augreg.ipynb) from [official impl](https://github.com/google-research/vision_transformer/) for navigating the augreg weights\\n  * Replaced all default weights w/ best AugReg variant (if possible). All AugReg 21k classifiers work.\\n    * Highlights: `vit_large_patch16_384` (87.1 top-1), `vit_large_r50_s32_384` (86.2 top-1), `vit_base_patch16_384` (86.0 top-1)\\n  * `vit_deit_*` renamed to just `deit_*`\\n  * Remove my old small model, replace with DeiT compatible small w/ AugReg weights\\n* Add 1st training of my `gmixer_24_224` MLP /w GLU, 78.1 top-1 w/ 25M params.\\n* Add weights from official ResMLP release (https://github.com/facebookresearch/deit)\\n* Add `eca_nfnet_l2` weights from my 'lightweight' series. 84.7 top-1 at 384x384.\\n* Add distilled BiT 50x1 student and 152x2 Teacher weights from  [Knowledge distillation: A good teacher is patient and consistent](https://arxiv.org/abs/2106.05237)\\n* NFNets and ResNetV2-BiT models work w/ Pytorch XLA now\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': -1}, page_content=\"* NFNets and ResNetV2-BiT models work w/ Pytorch XLA now\\n  * weight standardization uses F.batch_norm instead of std_mean (std_mean wasn't lowered)\\n  * eps values adjusted, will be slight differences but should be quite close\\n* Improve test coverage and classifier interface of non-conv (vision transformer and mlp) models\\n* Cleanup a few classifier / flatten details for models w/ conv classifiers or early global pool\\n* Please report any regressions, this PR touched quite a few models.\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 6809}, page_content='### June 8, 2021\\n* Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w/ my XLA branch. 24 block variant, 79.2 top-1.\\n* Add ResNet51-Q model w/ pretrained weights at 82.36 top-1.\\n  * NFNet inspired block layout with quad layer stem and no maxpool\\n  * Same param count (35.7M) and throughput as ResNetRS-50 but +1.5 top-1 @ 224x224 and +2.5 top-1 at 288x288\\n\\n### May 25, 2021\\n* Add LeViT, Visformer, Convit (PR by Aman Arora), Twins (PR by paper authors) transformer models\\n* Cleanup input_size/img_size override handling and testing for all vision transformer models\\n* Add `efficientnetv2_rw_m` model and weights (started training before official code). 84.8 top-1, 53M params.\\n\\n### May 14, 2021\\n* Add EfficientNet-V2 official model defs w/ ported weights from official [Tensorflow/Keras](https://github.com/google/automl/tree/master/efficientnetv2) impl.\\n  * 1k trained variants: `tf_efficientnetv2_s/m/l`\\n  * 21k trained variants: `tf_efficientnetv2_s/m/l_in21k`\\n  * 21k pretrained -> 1k fine-tuned: `tf_efficientnetv2_s/m/l_in21ft1k`\\n  * v2 models w/ v1 scaling: `tf_efficientnetv2_b0` through `b3`\\n  * Rename my prev V2 guess `efficientnet_v2s` -> `efficientnetv2_rw_s`\\n  * Some blank `efficientnetv2_*` models in-place for future native PyTorch training'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 8077}, page_content=\"### May 5, 2021\\n* Add MLP-Mixer models and port pretrained weights from [Google JAX impl](https://github.com/google-research/vision_transformer/tree/linen)\\n* Add CaiT models and pretrained weights from [FB](https://github.com/facebookresearch/deit)\\n* Add ResNet-RS models and weights from [TF](https://github.com/tensorflow/tpu/tree/master/models/official/resnet/resnet_rs). Thanks [Aman Arora](https://github.com/amaarora)\\n* Add CoaT models and weights. Thanks [Mohammed Rizin](https://github.com/morizin)\\n* Add new ImageNet-21k weights & finetuned weights for TResNet, MobileNet-V3, ViT models. Thanks [mrT](https://github.com/mrT23)\\n* Add GhostNet models and weights. Thanks [Kai Han](https://github.com/iamhankai)\\n* Update ByoaNet attention modles\\n   * Improve SA module inits\\n   * Hack together experimental stand-alone Swin based attn module and `swinnet`\\n   * Consistent '26t' model defs for experiments.\\n* Add improved Efficientnet-V2S (prelim model def) weights. 83.8 top-1.\\n* WandB logging support\\n\\n### April 13, 2021\\n* Add Swin Transformer models and weights from https://github.com/microsoft/Swin-Transformer\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': -1}, page_content='### April 13, 2021\\n* Add Swin Transformer models and weights from https://github.com/microsoft/Swin-Transformer\\n\\n### April 12, 2021\\n* Add ECA-NFNet-L1 (slimmed down F1 w/ SiLU, 41M params) trained with this code. 84% top-1 @ 320x320. Trained at 256x256.\\n* Add EfficientNet-V2S model (unverified model definition) weights. 83.3 top-1 @ 288x288. Only trained single res 224. Working on progressive training.\\n* Add ByoaNet model definition (Bring-your-own-attention) w/ SelfAttention block and corresponding SA/SA-like modules and model defs\\n  * Lambda Networks - https://arxiv.org/abs/2102.08602\\n  * Bottleneck Transformers - https://arxiv.org/abs/2101.11605\\n  * Halo Nets - https://arxiv.org/abs/2103.12731\\n* Adabelief optimizer contributed by Juntang Zhuang'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 9845}, page_content='### April 1, 2021\\n* Add snazzy `benchmark.py` script for bulk `timm` model benchmarking of train and/or inference\\n* Add Pooling-based Vision Transformer (PiT) models (from https://github.com/naver-ai/pit)\\n  * Merged distilled variant into main for torchscript compatibility\\n  * Some `timm` cleanup/style tweaks and weights have hub download support\\n* Cleanup Vision Transformer (ViT) models\\n  * Merge distilled (DeiT) model into main so that torchscript can work\\n  * Support updated weight init (defaults to old still) that closer matches original JAX impl (possibly better training from scratch)\\n  * Separate hybrid model defs into different file and add several new model defs to fiddle with, support patch_size != 1 for hybrids\\n  * Fix fine-tuning num_class changes (PiT and ViT) and pos_embed resizing (Vit) with distilled variants\\n  * nn.Sequential for block stack (does not break downstream compat)\\n* TnT (Transformer-in-Transformer) models contributed by author (from https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT)\\n* Add RegNetY-160 weights from DeiT teacher model\\n* Add new NFNet-L0 w/ SE attn (rename `nfnet_l0b`->`nfnet_l0`) weights 82.75 top-1 @ 288x288\\n* Some fixes/improvements for TFDS dataset wrapper\\n\\n### March 7, 2021\\n* First 0.4.x PyPi release w/ NFNets (& related), ByoB (GPU-Efficient, RepVGG, etc).\\n* Change feature extraction for pre-activation nets (NFNets, ResNetV2) to return features before activation.'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 11305}, page_content=\"### Feb 18, 2021\\n* Add pretrained weights and model variants for NFNet-F* models from [DeepMind Haiku impl](https://github.com/deepmind/deepmind-research/tree/master/nfnets).\\n  * Models are prefixed with `dm_`. They require SAME padding conv, skipinit enabled, and activation gains applied in act fn.\\n  * These models are big, expect to run out of GPU memory. With the GELU activiation + other options, they are roughly 1/2 the inference speed of my SiLU PyTorch optimized `s` variants.\\n  * Original model results are based on pre-processing that is not the same as all other models so you'll see different results in the results csv (once updated).\\n  * Matching the original pre-processing as closely as possible I get these results:\\n    * `dm_nfnet_f6` - 86.352\\n    * `dm_nfnet_f5` - 86.100\\n    * `dm_nfnet_f4` - 85.834\\n    * `dm_nfnet_f3` - 85.676\\n    * `dm_nfnet_f2` - 85.178\\n    * `dm_nfnet_f1` - 84.696\\n    * `dm_nfnet_f0` - 83.464\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 12244}, page_content=\"### Feb 16, 2021\\n* Add Adaptive Gradient Clipping (AGC) as per https://arxiv.org/abs/2102.06171. Integrated w/ PyTorch gradient clipping via mode arg that defaults to prev 'norm' mode. For backward arg compat, clip-grad arg must be specified to enable when using train.py.\\n  * AGC w/ default clipping factor `--clip-grad .01 --clip-mode agc`\\n  * PyTorch global norm of 1.0 (old behaviour, always norm), `--clip-grad 1.0`\\n  * PyTorch value clipping of 10, `--clip-grad 10. --clip-mode value`\\n  * AGC performance is definitely sensitive to the clipping factor. More experimentation needed to determine good values for smaller batch sizes and optimizers besides those in paper. So far I've found .001-.005 is necessary for stable RMSProp training w/ NFNet/NF-ResNet.\\n\\n### Feb 12, 2021\\n* Update Normalization-Free nets to include new NFNet-F (https://arxiv.org/abs/2102.06171) model defs\\n\\n### Feb 10, 2021\\n* More model archs, incl a flexible ByobNet backbone ('Bring-your-own-blocks')\\n  * GPU-Efficient-Networks (https://github.com/idstcv/GPU-Efficient-Networks), impl in `byobnet.py`\\n  * RepVGG (https://github.com/DingXiaoH/RepVGG), impl in `byobnet.py`\\n  * classic VGG (from torchvision, impl in `vgg`)\\n* Refinements to normalizer layer arg handling and normalizer+act layer handling in some models\\n* Default AMP mode changed to native PyTorch AMP instead of APEX. Issues not being fixed with APEX. Native works with `--channels-last` and `--torchscript` model training, APEX does not.\\n* Fix a few bugs introduced since last pypi release\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 13782}, page_content='### Feb 8, 2021\\n* Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320. 269d train @ 256, fine-tune @320, test @ 352.\\n  * `ecaresnet26t` - 79.88 top-1 @ 320x320, 79.08 @ 256x256\\n  * `ecaresnet50t` - 82.35 top-1 @ 320x320, 81.52 @ 256x256\\n  * `ecaresnet269d` - 84.93 top-1 @ 352x352, 84.87 @ 320x320\\n* Remove separate tiered (`t`) vs tiered_narrow (`tn`) ResNet model defs, all `tn` changed to `t` and `t` models removed (`seresnext26t_32x4d` only model w/ weights that was removed).\\n* Support model default_cfgs with separate train vs test resolution `test_input_size` and remove extra `_320` suffix ResNet model defs that were just for test.\\n\\n### Jan 30, 2021\\n* Add initial \"Normalization Free\" NF-RegNet-B* and NF-ResNet model definitions based on [paper](https://arxiv.org/abs/2101.08692)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 14605}, page_content=\"### Jan 25, 2021\\n* Add ResNetV2 Big Transfer (BiT) models w/ ImageNet-1k and 21k weights from https://github.com/google-research/big_transfer\\n* Add official R50+ViT-B/16 hybrid models + weights from https://github.com/google-research/vision_transformer\\n* ImageNet-21k ViT weights are added w/ model defs and representation layer (pre logits) support\\n  * NOTE: ImageNet-21k classifier heads were zero'd in original weights, they are only useful for transfer learning\\n* Add model defs and weights for DeiT Vision Transformer models from https://github.com/facebookresearch/deit\\n* Refactor dataset classes into ImageDataset/IterableImageDataset + dataset specific parser classes\\n* Add Tensorflow-Datasets (TFDS) wrapper to allow use of TFDS image classification sets with train script\\n  * Ex: `train.py /data/tfds --dataset tfds/oxford_iiit_pet --val-split test --model resnet50 -b 256 --amp --num-classes 37 --opt adamw --lr 3e-4 --weight-decay .001 --pretrained -j 2`\\n* Add improved .tar dataset parser that reads images from .tar, folder of .tar files, or .tar within .tar\\n  * Run validation on full ImageNet-21k directly from tar w/ BiT model: `validate.py /data/fall11_whole.tar --model resnetv2_50x1_bitm_in21k --amp`\\n* Models in this update should be stable w/ possible exception of ViT/BiT, possibility of some regressions with train/val scripts and dataset handling\\n\\n### Jan 3, 2021\\n* Add SE-ResNet-152D weights\\n  * 256x256 val, 0.94 crop top-1 - 83.75\\n  * 320x320 val, 1.0 crop - 84.36\\n* Update results files\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 16122}, page_content=\"### Dec 18, 2020\\n* Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256\\n  * 256x256 val, 0.94 crop (top-1) - 101D (82.33), 152D (83.08), 200D (83.25)\\n  * 288x288 val, 1.0 crop - 101D (82.64), 152D (83.48), 200D (83.76)\\n  * 320x320 val, 1.0 crop - 101D (83.00), 152D (83.66), 200D (84.01)\\n\\n### Dec 7, 2020\\n* Simplify EMA module (ModelEmaV2), compatible with fully torchscripted models\\n* Misc fixes for SiLU ONNX export, default_cfg missing from Feature extraction models, Linear layer w/ AMP + torchscript\\n* PyPi release @ 0.3.2 (needed by EfficientDet)\\n\\n\\n### Oct 30, 2020\\n* Test with PyTorch 1.7 and fix a small top-n metric view vs reshape issue.\\n* Convert newly added 224x224 Vision Transformer weights from official JAX repo. 81.8 top-1 for B/16, 83.1 L/16.\\n* Support PyTorch 1.7 optimized, native SiLU (aka Swish) activation. Add mapping to 'silu' name, custom swish will eventually be deprecated.\\n* Fix regression for loading pretrained classifier via direct model entrypoint functions. Didn't impact create_model() factory usage.\\n* PyPi release @ 0.3.0 version!\\n\\n### Oct 26, 2020\\n* Update Vision Transformer models to be compatible with official code release at https://github.com/google-research/vision_transformer\\n* Add Vision Transformer weights (ImageNet-21k pretrain) for 384x384 base and large models converted from official jax impl\\n  * ViT-B/16 - 84.2\\n  * ViT-B/32 - 81.7\\n  * ViT-L/16 - 85.2\\n  * ViT-L/32 - 81.5\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 17568}, page_content=\"### Oct 21, 2020\\n* Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79.35 for 'base'. Thanks to [Christof](https://www.kaggle.com/christofhenkel) for training the base model w/ lots of GPUs.\\n\\n### Oct 13, 2020\\n* Initial impl of Vision Transformer models. Both patch and hybrid (CNN backbone) variants. Currently trying to train...\\n* Adafactor and AdaHessian (FP32 only, no AMP) optimizers\\n* EdgeTPU-M (`efficientnet_em`) model trained in PyTorch, 79.3 top-1\\n* Pip release, doc updates pending a few more changes...\\n\\n### Sept 18, 2020\\n* New ResNet 'D' weights. 72.7 (top-1) ResNet-18-D, 77.1 ResNet-34-D, 80.5 ResNet-50-D\\n* Added a few untrained defs for other ResNet models (66D, 101D, 152D, 200/200D)\\n\\n### Sept 3, 2020\\n* New weights\\n  * Wide-ResNet50 - 81.5 top-1 (vs 78.5 torchvision)\\n  * SEResNeXt50-32x4d - 81.3 top-1 (vs 79.1 cadene)\\n* Support for native Torch AMP and channels_last memory format added to train/validate scripts (`--channels-last`, `--native-amp` vs `--apex-amp`)\\n* Models tested with channels_last on latest NGC 20.08 container. AdaptiveAvgPool in attn layers changed to mean((2,3)) to work around bug with NHWC kernel.\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 18741}, page_content='### Aug 12, 2020\\n* New/updated weights from training experiments\\n  * EfficientNet-B3 - 82.1 top-1 (vs 81.6 for official with AA and 81.9 for AdvProp)\\n  * RegNetY-3.2GF - 82.0 top-1 (78.9 from official ver)\\n  * CSPResNet50 - 79.6 top-1 (76.6 from official ver)\\n* Add CutMix integrated w/ Mixup. See [pull request](https://github.com/rwightman/pytorch-image-models/pull/218) for some usage examples\\n* Some fixes for using pretrained weights with `in_chans` != 3 on several models.'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 19221}, page_content=\"### Aug 5, 2020\\nUniversal feature extraction, new models, new weights, new test sets.\\n* All models support the `features_only=True` argument for `create_model` call to return a network that extracts feature maps from the deepest layer at each stride.\\n* New models\\n  * CSPResNet, CSPResNeXt, CSPDarkNet, DarkNet\\n  * ReXNet\\n  * (Modified Aligned) Xception41/65/71 (a proper port of TF models)\\n* New trained weights\\n  * SEResNet50 - 80.3 top-1\\n  * CSPDarkNet53 - 80.1 top-1\\n  * CSPResNeXt50 - 80.0 top-1\\n  * DPN68b - 79.2 top-1\\n  * EfficientNet-Lite0 (non-TF ver) - 75.5 (submitted by [@hal-314](https://github.com/hal-314))\\n* Add 'real' labels for ImageNet and ImageNet-Renditions test set, see [`results/README.md`](results/README.md)\\n* Test set ranking/top-n diff script by [@KushajveerSingh](https://github.com/KushajveerSingh)\\n* Train script and loader/transform tweaks to punch through more aug arguments\\n* README and documentation overhaul. See initial (WIP) documentation at https://rwightman.github.io/pytorch-image-models/\\n* adamp and sgdp optimizers added by [@hellbell](https://github.com/hellbell)\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 20330}, page_content='### June 11, 2020\\nBunch of changes:\\n* DenseNet models updated with memory efficient addition from torchvision (fixed a bug), blur pooling and deep stem additions\\n* VoVNet V1 and V2 models added, 39 V2 variant (ese_vovnet_39b) trained to 79.3 top-1\\n* Activation factory added along with new activations:\\n   * select act at model creation time for more flexibility in using activations compatible with scripting or tracing (ONNX export)\\n   * hard_mish (experimental) added with memory-efficient grad, along with ME hard_swish\\n   * context mgr for setting exportable/scriptable/no_jit states\\n* Norm + Activation combo layers added with initial trial support in DenseNet and VoVNet along with impl of EvoNorm and InplaceAbn wrapper that fit the interface\\n* Torchscript works for all but two of the model types as long as using Pytorch 1.5+, tests added for this\\n* Some import cleanup and classifier reset changes, all models will have classifier reset to nn.Identity on reset_classifer(0) call\\n* Prep for 0.1.28 pip release\\n\\n### May 12, 2020\\n* Add ResNeSt models (code adapted from https://github.com/zhanghang1989/ResNeSt, paper https://arxiv.org/abs/2004.08955))\\n\\n### May 3, 2020\\n* Pruned EfficientNet B1, B2, and B3 (https://arxiv.org/abs/2002.08258) contributed by [Yonathan Aflalo](https://github.com/yoniaflalo)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 21645}, page_content='### May 1, 2020\\n* Merged a number of execellent contributions in the ResNet model family over the past month\\n  * BlurPool2D and resnetblur models initiated by [Chris Ha](https://github.com/VRandme), I trained resnetblur50 to 79.3.\\n  * TResNet models and SpaceToDepth, AntiAliasDownsampleLayer layers by [mrT23](https://github.com/mrT23)\\n  * ecaresnet (50d, 101d, light) models and two pruned variants using pruning as per (https://arxiv.org/abs/2002.08258) by [Yonathan Aflalo](https://github.com/yoniaflalo)\\n* 200 pretrained models in total now with updated results csv in results folder\\n\\n### April 5, 2020\\n* Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite\\n  * 3.5M param MobileNet-V2 100 @ 73%\\n  * 4.5M param MobileNet-V2 110d @ 75%\\n  * 6.1M param MobileNet-V2 140 @ 76.5%\\n  * 5.8M param MobileNet-V2 120d @ 77.3%\\n\\n### March 18, 2020\\n* Add EfficientNet-Lite models w/ weights ported from [Tensorflow TPU](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite)\\n* Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by [Andrew Lavin](https://github.com/andravin) (see Training section for hparams)'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 22882}, page_content='### April 5, 2020\\n* Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite\\n  * 3.5M param MobileNet-V2 100 @ 73%\\n  * 4.5M param MobileNet-V2 110d @ 75%\\n  * 6.1M param MobileNet-V2 140 @ 76.5%\\n  * 5.8M param MobileNet-V2 120d @ 77.3%\\n\\n### March 18, 2020\\n* Add EfficientNet-Lite models w/ weights ported from [Tensorflow TPU](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite)\\n* Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by [Andrew Lavin](https://github.com/andravin) (see Training section for hparams)\\n\\n### Feb 29, 2020\\n* New MobileNet-V3 Large weights trained from stratch with this code to 75.77% top-1\\n* IMPORTANT CHANGE - default weight init changed for all MobilenetV3 / EfficientNet / related models\\n  * overall results similar to a bit better training from scratch on a few smaller models tried\\n  * performance early in training seems consistently improved but less difference by end\\n  * set `fix_group_fanout=False` in `_init_weight_goog` fn if you need to reproducte past behaviour\\n* Experimental LR noise feature added applies a random perturbation to LR each epoch in specified range of training'),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 24134}, page_content=\"### Feb 18, 2020\\n* Big refactor of model layers and addition of several attention mechanisms. Several additions motivated by 'Compounding the Performance Improvements...' (https://arxiv.org/abs/2001.06268):\\n  * Move layer/module impl into `layers` subfolder/module of `models` and organize in a more granular fashion\\n  * ResNet downsample paths now properly support dilation (output stride != 32) for avg_pool ('D' variant) and 3x3 (SENets) networks\\n  * Add Selective Kernel Nets on top of ResNet base, pretrained weights\\n    * skresnet18 - 73% top-1\\n    * skresnet34 - 76.9% top-1 \\n    * skresnext50_32x4d (equiv to SKNet50) - 80.2% top-1\\n  * ECA and CECA (circular padding) attention layer contributed by [Chris Ha](https://github.com/VRandme)\\n  * CBAM attention experiment (not the best results so far, may remove)\\n  * Attention factory to allow dynamically selecting one of SE, ECA, CBAM in the `.se` position for all ResNets\\n  * Add DropBlock and DropPath (formerly DropConnect for EfficientNet/MobileNetv3) support to all ResNet variants\\n* Full dataset results updated that incl NoisyStudent weights and 2 of the 3 SK weights\\n\\n### Feb 12, 2020\\n* Add EfficientNet-L2 and B0-B7 NoisyStudent weights ported from [Tensorflow TPU](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet)\\n\\n### Feb 6, 2020\\n* Add RandAugment trained EfficientNet-ES (EdgeTPU-Small) weights with 78.1 top-1. Trained by [Andrew Lavin](https://github.com/andravin) (see Training section for hparams)\\n\\n### Feb 1/2, 2020\\n* Port new EfficientNet-B8 (RandAugment) weights, these are different than the B8 AdvProp, different input normalization.\\n* Update results csv files on all models for ImageNet validation and three other test sets\\n* Push PyPi package update\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 25892}, page_content=\"### Jan 31, 2020\\n* Update ResNet50 weights with a new 79.038 result from further JSD / AugMix experiments. Full command line for reproduction in training section below.\\n\\n### Jan 11/12, 2020\\n* Master may be a bit unstable wrt to training, these changes have been tested but not all combos\\n* Implementations of AugMix added to existing RA and AA. Including numerous supporting pieces like JSD loss (Jensen-Shannon divergence + CE), and AugMixDataset\\n* SplitBatchNorm adaptation layer added for implementing Auxiliary BN as per AdvProp paper\\n* ResNet-50 AugMix trained model w/ 79% top-1 added\\n* `seresnext26tn_32x4d` - 77.99 top-1, 93.75 top-5 added to tiered experiment, higher img/s than 't' and 'd'\\n\\n### Jan 3, 2020\\n* Add RandAugment trained EfficientNet-B0 weight with 77.7 top-1. Trained by [Michael Klachko](https://github.com/michaelklachko) with this code and recent hparams (see Training section)\\n* Add `avg_checkpoints.py` script for post training weight averaging and update all scripts with header docstrings and shebangs.\\n\\n### Dec 30, 2019\\n* Merge [Dushyant Mehta's](https://github.com/mehtadushy) PR for SelecSLS (Selective Short and Long Range Skip Connections) networks. Good GPU memory consumption and throughput. Original: https://github.com/mehtadushy/SelecSLS-Pytorch\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 27179}, page_content=\"### Dec 28, 2019\\n* Add new model weights and training hparams (see Training Hparams section)\\n  * `efficientnet_b3` - 81.5 top-1, 95.7 top-5 at default res/crop, 81.9, 95.8 at 320x320 1.0 crop-pct\\n     * trained with RandAugment, ended up with an interesting but less than perfect result (see training section)\\n  * `seresnext26d_32x4d`- 77.6 top-1, 93.6 top-5\\n     * deep stem (32, 32, 64), avgpool downsample\\n     * stem/dowsample from bag-of-tricks paper\\n  * `seresnext26t_32x4d`- 78.0 top-1, 93.7 top-5\\n     * deep tiered stem (24, 48, 64), avgpool downsample (a modified 'D' variant)\\n     * stem sizing mods from Jeremy Howard and fastai devs discussing ResNet architecture experiments\\n\\n### Dec 23, 2019\\n* Add RandAugment trained MixNet-XL weights with 80.48 top-1.\\n* `--dist-bn` argument added to train.py, will distribute BN stats between nodes after each train epoch, before eval\\n\\n### Dec 4, 2019\\n* Added weights from the first training from scratch of an EfficientNet (B2) with my new RandAugment implementation. Much better than my previous B2 and very close to the official AdvProp ones (80.4 top-1, 95.08 top-5).\"),\n",
       " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/archived_changes.md', 'start_index': 28303}, page_content=\"### Nov 29, 2019\\n* Brought EfficientNet and MobileNetV3 up to date with my https://github.com/rwightman/gen-efficientnet-pytorch code. Torchscript and ONNX export compat excluded.\\n  * AdvProp weights added\\n  * Official TF MobileNetv3 weights added\\n* EfficientNet and MobileNetV3 hook based 'feature extraction' classes added. Will serve as basis for using models as backbones in obj detection/segmentation tasks. Lots more to be done here...\\n* HRNet classification models and weights added from https://github.com/HRNet/HRNet-Image-Classification\\n* Consistency in global pooling, `reset_classifer`, and `forward_features` across models\\n  * `forward_features` always returns unpooled feature maps now\\n* Reasonable chance I broke something... let me know\\n\\n### Nov 22, 2019\\n* Add ImageNet training RandAugment implementation alongside AutoAugment. PyTorch Transform compatible format, using PIL. Currently training two EfficientNet models from scratch with promising results... will update.\\n* `drop-connect` cmd line arg finally added to `train.py`, no need to hack model fns. Works for efficientnet/mobilenetv3 based models, ignored otherwise.\"),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# Generating the documentation\\n\\nTo generate the documentation, you first have to build it. Several packages are necessary to build the doc, \\nyou can install them with the following command, at the root of the code repository:\\n\\n```bash\\npip install -e \".[docs]\"\\n```\\n\\nThen you need to install our special tool that builds the documentation:\\n\\n```bash\\npip install git+https://github.com/huggingface/doc-builder\\n```\\n\\n---\\n**NOTE**\\n\\nYou only need to generate the documentation to inspect it locally (if you\\'re planning changes and want to\\ncheck how they look before committing for instance). You don\\'t have to commit to the built documentation.\\n\\n---\\n\\n## Building the documentation\\n\\nOnce you have setup the `doc-builder` and additional packages, you can generate the documentation by \\ntyping the following command:\\n\\n```bash\\ndoc-builder build peft docs/source/ --build_dir ~/tmp/test-build\\n```\\n\\nYou can adapt the `--build_dir` to set any temporary folder you prefer. This command will create it and generate\\nthe MDX files that will be rendered as the documentation on the main website. You can inspect them in your favorite\\nMarkdown editor.\\n\\n## Previewing the documentation\\n\\nTo preview the docs, first install the `watchdog` module with:\\n\\n```bash\\npip install watchdog\\n```\\n\\nThen run the following command:\\n\\n```bash\\ndoc-builder preview {package_name} {path_to_docs}\\n```\\n\\nFor example:\\n\\n```bash\\ndoc-builder preview peft docs/source'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/README.md', 'start_index': -1}, page_content='```\\n\\nThen run the following command:\\n\\n```bash\\ndoc-builder preview {package_name} {path_to_docs}\\n```\\n\\nFor example:\\n\\n```bash\\ndoc-builder preview peft docs/source\\n```\\n\\nThe docs will be viewable at [http://localhost:3000](http://localhost:3000). You can also preview the docs once you have opened a PR. You will see a bot add a comment to a link where the documentation with your changes lives.\\n\\n---\\n**NOTE**\\n\\nThe `preview` command only works with existing doc files. When you add a completely new file, you need to update `_toctree.yml` & restart `preview` command (`ctrl-c` to stop it & call `doc-builder preview ...` again).\\n\\n---\\n\\n## Adding a new element to the navigation bar\\n\\nAccepted files are Markdown (.md or .mdx).\\n\\nCreate a file with its extension and put it in the source directory. You can then link it to the toc-tree by putting\\nthe filename without the extension in the [`_toctree.yml`](https://github.com/huggingface/peft/blob/main/docs/source/_toctree.yml) file.\\n\\n## Renaming section headers and moving sections\\n\\nIt helps to keep the old links working when renaming the section header and/or moving sections from one document to another. This is because the old links are likely to be used in Issues, Forums, and Social media and it\\'d make for a much more superior user experience if users reading those months later could still easily navigate to the originally intended information.\\n\\nTherefore, we simply keep a little map of moved sections at the end of the document where the original section was. The key is to preserve the original anchor.\\n\\nSo if you renamed a section from: \"Section A\" to \"Section B\", then you can add at the end of the file:\\n\\n```\\nSections that were moved:\\n\\n[ <a href=\"#section-b\">Section A</a><a id=\"section-a\"></a> ]\\n```\\nand of course, if you moved it to another file, then:'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/README.md', 'start_index': 3606}, page_content='```\\nand of course, if you moved it to another file, then:\\n\\n```\\nSections that were moved:\\n\\n[ <a href=\"../new-file#section-b\">Section A</a><a id=\"section-a\"></a> ]'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/README.md', 'start_index': 3768}, page_content=\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\n\\n## Writing Documentation - Specification\\n\\nThe `huggingface/peft` documentation follows the\\n[Google documentation](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) style for docstrings,\\nalthough we can write them directly in Markdown.\\n\\n### Adding a new tutorial\\n\\nAdding a new tutorial or section is done in two steps:\\n\\n- Add a new file under `./source`. This file can either be ReStructuredText (.rst) or Markdown (.md).\\n- Link that file in `./source/_toctree.yml` on the correct toc-tree.\\n\\nMake sure to put your new file under the proper section. It's unlikely to go in the first section (*Get Started*), so\\ndepending on the intended targets (beginners, more advanced users, or researchers) it should go into sections two, three, or\\nfour.\\n\\n### Writing source documentation\\n\\nValues that should be put in `code` should either be surrounded by backticks: \\\\`like so\\\\`. Note that argument names\\nand objects like True, None, or any strings should usually be put in `code`.\\n\\nWhen mentioning a class, function, or method, it is recommended to use our syntax for internal links so that our tool\\nadds a link to its documentation with this syntax: \\\\[\\\\`XXXClass\\\\`\\\\] or \\\\[\\\\`function\\\\`\\\\]. This requires the class or \\nfunction to be in the main package.\\n\\nIf you want to create a link to some internal class or function, you need to\\nprovide its path. For instance: \\\\[\\\\`utils.gather\\\\`\\\\]. This will be converted into a link with\\n`utils.gather` in the description. To get rid of the path and only keep the name of the object you are\\nlinking to in the description, add a ~: \\\\[\\\\`~utils.gather\\\\`\\\\] will generate a link with `gather` in the description.\\n\\nThe same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or \\\\[~\\\\`XXXClass.method\\\\`\\\\].\\n\\n#### Defining arguments in a method\"),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/README.md', 'start_index': -1}, page_content='The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or \\\\[~\\\\`XXXClass.method\\\\`\\\\].\\n\\n#### Defining arguments in a method\\n\\nArguments should be defined with the `Args:` (or `Arguments:` or `Parameters:`) prefix, followed by a line return and\\nan indentation. The argument should be followed by its type, with its shape if it is a tensor, a colon, and its\\ndescription:'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/README.md', 'start_index': 5911}, page_content='```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description is too long to fit in one line (more than 119 characters in total), another indentation is necessary \\nbefore writing the description after the argument.\\n\\nFinally, to maintain uniformity if any *one* description is too long to fit on one line, the \\nrest of the parameters should follow suit and have an indention before their description.\\n\\nHere\\'s an example showcasing everything so far:\\n\\n```\\n    Args:\\n        gradient_accumulation_steps (`int`, *optional*, default to 1):\\n            The number of steps that should pass before gradients are accumulated. A number > 1 should be combined with `Accelerator.accumulate`.\\n        cpu (`bool`, *optional*):\\n            Whether or not to force the script to execute on CPU. Will ignore GPU available if set to `True` and force the execution on one process only.\\n```\\n\\nFor optional arguments or arguments with defaults we follow the following syntax: imagine we have a function with the\\nfollowing signature:\\n\\n```\\ndef my_function(x: str = None, a: float = 1):\\n```\\n\\nthen its documentation should look like this:\\n\\n```\\n    Args:\\n        x (`str`, *optional*):\\n            This argument controls ... and has a description longer than 119 chars.\\n        a (`float`, *optional*, defaults to 1):\\n            This argument is used to ... and has a description longer than 119 chars.\\n```\\n\\nNote that we always omit the \"defaults to \\\\`None\\\\`\" when None is the default for any argument. Also note that even\\nif the first line describing your argument type and its default gets long, you can\\'t break it into several lines. You can\\nhowever write as many lines as you want in the indented description (see the example above with `input_ids`).\\n\\n#### Writing a multi-line code block\\n\\nMulti-line code blocks can be useful for displaying examples. They are done between two lines of three backticks as usual in Markdown:\\n\\n\\n````\\n```python\\n# first line of code\\n# second line\\n# etc\\n```\\n`'),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/README.md', 'start_index': -1}, page_content=\"```\\n```python\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\n#### Writing a return block\\n\\nThe return block should be introduced with the `Returns:` prefix, followed by a line return and an indentation.\\nThe first line should be the type of the return, followed by a line return. No need to indent further for the elements\\nbuilding the return.\\n\\nHere's an example of a single value return:\\n\\n```\\n    Returns:\\n        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.\\n```\\n\\nHere's an example of a tuple return, comprising several objects:\\n\\n```\\n    Returns:\\n        `tuple(torch.FloatTensor)` comprising various elements depending on the configuration ([`BertConfig`]) and inputs:\\n        - ** loss** (*optional*, returned when `masked_lm_labels` is provided) `torch.FloatTensor` of shape `(1,)` --\\n          Total loss is the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\\n        - **prediction_scores** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --\\n          Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\\n```\\n\\n## Styling the docstring\\n\\nWe have an automatic script running with the `make style` comment that will make sure that:\\n- the docstrings fully take advantage of the line width\\n- all code examples are formatted using black, like the code of the Transformers library\\n\\nThis script may have some weird failures if you make a syntax mistake or if you uncover a bug. Therefore, it's\\nrecommended to commit your changes before running `make style`, so you can revert the changes done by that script\\neasily.\\n\\n## Writing documentation examples\\n\\nThe syntax, for example, docstrings can look as follows:\"),\n",
       " Document(metadata={'source': 'huggingface/peft/blob/main/docs/README.md', 'start_index': 9652}, page_content='```\\n    Example:\\n\\n    ```python\\n    >>> import time\\n    >>> from accelerate import Accelerator\\n    >>> accelerator = Accelerator()\\n    >>> if accelerator.is_main_process:\\n    ...     time.sleep(2)\\n    >>> else:\\n    ...     print(\"I\\'m waiting for the main process to finish its sleep...\")\\n    >>> accelerator.wait_for_everyone()\\n    >>> # Should print on every process at the same time\\n    >>> print(\"Everyone is here\")\\n    ```\\n```\\n\\nThe docstring should give a minimal, clear example of how the respective function \\nis to be used in inference and also include the expected (ideally sensible)\\noutput.\\nOften, readers will try out the example before even going through the function \\nor class definitions. Therefore, it is of utmost importance that the example \\nworks as expected.'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 0}, page_content='!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n<!---\\nA useful guide for English-Chinese translation of Hugging Face documentation\\n- Add space around English words and numbers when they appear between Chinese characters. E.g., 共 100 多种语言; 使用 transformers 库。\\n- Use square quotes, e.g.,「引用」\\n\\nDictionary\\n\\nHugging Face: 抱抱脸\\ntoken: 词符（并用括号标注原英文）\\ntokenize: 词符化（并用括号标注原英文）\\ntokenizer: 词符化器（并用括号标注原英文）\\ntransformer: transformer（不翻译）\\npipeline: 流水线\\nAPI: API (不翻译）\\ninference: 推理\\nTrainer: 训练器。当作为类名出现时不翻译。\\npretrained/pretrain: 预训练\\nfinetune: 微调\\ncommunity: 社区\\nexample: 当特指仓库中 example 目录时翻译为「用例」\\nPython data structures (e.g., list, set, dict): 翻译为列表，集合，词典，并用括号标注原英文\\nNLP/Natural Language Processing: 以 NLP 出现时不翻译，以 Natural Language Processing 出现时翻译为自然语言处理\\ncheckpoint: 检查点\\n-->'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 1304}, page_content='<p align=\"center\">\\n    <br>\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_logo_name.png\" width=\"400\"/>\\n    <br>\\n</p>\\n<p align=\"center\">\\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\">\\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://huggingface.co/docs/transformers/index\">\\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/releases\">\\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">\\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\\n    </a>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 2522}, page_content='</a>\\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\\n</p>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 2657}, page_content='<h4 align=\"center\">\\n    <p>\\n        <a href=\"https://github.com/huggingface/transformers/\">English</a> |\\n        <b>简体中文</b> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">繁體中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">한국어</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_es.md\">Español</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">日本語</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">हिन्दी</a>\\n        <a href=\"https://github.com/huggingface/transformers//blob/main/README_te.md\">తెలుగు</a> |\\n    </p>\\n</h4>\\n\\n<h3 align=\"center\">\\n    <p>为 Jax、PyTorch 和 TensorFlow 打造的先进的自然语言处理</p>\\n</h3>\\n\\n<h3 align=\"center\">\\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\\n</h3>'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 3637}, page_content='🤗 Transformers 提供了数以千计的预训练模型，支持 100 多种语言的文本分类、信息抽取、问答、摘要、翻译、文本生成。它的宗旨是让最先进的 NLP 技术人人易用。\\n\\n🤗 Transformers 提供了便于快速下载和使用的API，让你可以把预训练模型用在给定文本、在你的数据集上微调然后通过 [model hub](https://huggingface.co/models) 与社区共享。同时，每个定义的 Python 模块均完全独立，方便修改和快速研究实验。\\n\\n🤗 Transformers 支持三个最热门的深度学习库： [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) 以及 [TensorFlow](https://www.tensorflow.org/) — 并与之无缝整合。你可以直接使用一个框架训练你的模型然后用另一个加载和推理。\\n\\n## 在线演示\\n\\n你可以直接在模型页面上测试大多数 [model hub](https://huggingface.co/models) 上的模型。 我们也提供了 [私有模型托管、模型版本管理以及推理API](https://huggingface.co/pricing)。'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 4209}, page_content='这里是一些例子：\\n- [用 BERT 做掩码填词](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\\n- [用 Electra 做命名实体识别](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\\n- [用 GPT-2 做文本生成](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)\\n- [用 RoBERTa 做自然语言推理](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': -1}, page_content='- [用 RoBERTa 做自然语言推理](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\\n- [用 BART 做文本摘要](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 5494}, page_content='- [用 DistilBERT'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 5510}, page_content='做问答](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+Amer'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 5971}, page_content='+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Su'),\n",
       " Document(metadata={'source': 'huggingface/transformers/blob/main/README_zh-hans.md', 'start_index': 6432}, page_content='+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)'),\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e447894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\ronak\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ronak\\anaconda3\\lib\\site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ronak\\anaconda3\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ronak\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a114ea",
   "metadata": {},
   "source": [
    "Building vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    # model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
    ")\n",
    "\n",
    "KNOWLEDGE_VECTOR_DB = FAISS.from_documents(\n",
    "    docs_processed,\n",
    "    embedding_model, \n",
    "    distance_strategy=DistanceStrategy.COSINE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
