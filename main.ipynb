{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "678dadb4",
      "metadata": {
        "id": "678dadb4"
      },
      "source": [
        "Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dc1a9a70",
      "metadata": {
        "id": "dc1a9a70"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a38c290b",
      "metadata": {
        "id": "a38c290b"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "525ea453",
      "metadata": {
        "id": "525ea453"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c4ecf19e",
      "metadata": {
        "id": "c4ecf19e",
        "outputId": "68c454ed-7f74-4bfd-8262-2a2a72023a2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "8daa833a973842d4bcf51d641b525c92",
            "e0b98e5c6fd74f4aba05a765a9cc25c2",
            "20bac1eb87e546fe80b705572e7ce6fb",
            "2749260d715c4408a894549cc15f97aa",
            "40138f5daedb484791ef9db0ee615bf9",
            "24ab44f9a52649ecbf59e40a09f9ec29",
            "8667c1faeaa14f3cb7b7f0f0d4bb4abd",
            "299bf678d127401fbcd81534a396c508",
            "c4f5b60aa5da4a49be4666df1ac26132",
            "c2b9712805314000898bc88f30d264e2",
            "11b52f3a62074587a232e9fa1e30410b",
            "cf0250a9faf7405898110a5c406c2dc4",
            "9323248e19f548c48378f5f479918932",
            "3edfcad451af4d35847e807046b5f4e5",
            "07137072bfbe4464b2c5f330f708081e",
            "8b9e9b9d6ed04d01aca7b0e30561e4c6",
            "b1901fe2c2124c0f9f53d3eade8ae253",
            "fee63f6ccb394cd0925fab6c4e91bf29",
            "9a5dfda493d64501b2af0191b9863610",
            "957f2a92d522436aa7d073ba9462fc87",
            "9e0527e7d6f5483381b0dc2d2ae7ae18",
            "a0b661c6c963431ab30d6dac4c2997f2",
            "53c5c9e241e544cfa0844cefc0338bdf",
            "efef862da9f14a68ad6096f6d1fad96a",
            "fcd723eccb094fa5ba0f60458dbfdb0f",
            "644a5bd9135a472ca6a579a8c84f8d8d",
            "e6bdc3910262419d972dcbedb3f1f10b",
            "7c601c3b75524259b0fdb6dc01d1ec13",
            "62ea973c8c5148d4949ef364f459dcdc",
            "0b4c683e7a4d499981aab71e11f718b5",
            "8244350fb0f84a1a81515438ff17683f",
            "e47fa6b2e958445b8b56e8152270adba",
            "1fea0aa1281c4399ad71a89bdaf7cadd"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8daa833a973842d4bcf51d641b525c92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf0250a9faf7405898110a5c406c2dc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53c5c9e241e544cfa0844cefc0338bdf"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import datasets\n",
        "\n",
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b98c3d38",
      "metadata": {
        "id": "b98c3d38"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3b618103",
      "metadata": {
        "id": "3b618103",
        "outputId": "314c7c0d-017a-4c03-906a-ee5c75cc764e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a7b36c5159044d1c94b1cafcd3424432",
            "ad142b13902049ba9eee833cdc43161c",
            "2398aa7200d44829bab7d82b35342647",
            "eb1f72436f824abbac6d5b37f176536f",
            "bb4f4951df6b489dac57d217401e8803",
            "92a6b77111ea4352865545c0d1af6b85",
            "b52df4b76d2b4058b1ef937ea2cf5669",
            "6ff9c0f0660b43c48a75aba071f9d8bb",
            "d32baef2d4ee4c8bae4a5451b7b67ede",
            "75d2e8c5db494d748a08fc1925cd3f05",
            "10d49d9a7a1a4ac79230af5d73e3aaae"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7b36c5159044d1c94b1cafcd3424432"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"],metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6dc0e9b8",
      "metadata": {
        "id": "6dc0e9b8"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
        "# This list is taken from LangChain's MarkdownTextSplitter class\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "   chunk_size=1000,\n",
        "   chunk_overlap=100,\n",
        "   add_start_index=True, # If `True`, includes chunk's start index in metadata\n",
        "   strip_whitespace=True, # If `True`, strips whitespace from the start and end of every document\n",
        "   separators=MARKDOWN_SEPARATORS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a613748d",
      "metadata": {
        "id": "a613748d"
      },
      "outputs": [],
      "source": [
        "docs_processed = []\n",
        "\n",
        "for doc in RAW_KNOWLEDGE_BASE:\n",
        "    docs_processed += text_splitter.split_documents([doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "208a86f1",
      "metadata": {
        "id": "208a86f1",
        "outputId": "525eb81a-0136-4202-803b-59f4a4112e1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 1}, page_content='Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />'),\n",
              " Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 968}, page_content='## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!'),\n",
              " Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx', 'start_index': 1805}, page_content='## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1}, page_content=\"Choosing a metric for your task\\n\\n**So you've trained your model and want to see how well it’s doing on a dataset of your choice. Where do you start?**\\n\\nThere is no “one size fits all” approach to choosing an evaluation metric, but some good guidelines to keep in mind are:\\n\\n## Categories of metrics\\n\\nThere are 3 high-level categories of metrics:\"),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 275}, page_content=\"## Categories of metrics\\n\\nThere are 3 high-level categories of metrics:\\n\\n1. *Generic metrics*, which can be applied to a variety of situations and datasets, such as precision and accuracy.\\n2. *Task-specific metrics*, which are limited to a given task, such as Machine Translation (often evaluated using metrics [BLEU](https://huggingface.co/metrics/bleu) or [ROUGE](https://huggingface.co/metrics/rouge)) or Named Entity Recognition (often evaluated with [seqeval](https://huggingface.co/metrics/seqeval)).\\n3. *Dataset-specific metrics*, which aim to measure model performance on specific benchmarks: for instance, the [GLUE benchmark](https://huggingface.co/datasets/glue) has a dedicated [evaluation metric](https://huggingface.co/metrics/glue).\\n\\nLet's look at each of these three cases:\\n\\n### Generic metrics\\n\\nMany of the metrics used in the Machine Learning community are quite generic and can be applied in a variety of tasks and datasets.\"),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1220}, page_content='This is the case for metrics like [accuracy](https://huggingface.co/metrics/accuracy) and [precision](https://huggingface.co/metrics/precision), which can be used for evaluating labeled (supervised) datasets, as well as [perplexity](https://huggingface.co/metrics/perplexity), which can be used for evaluating different kinds of (unsupervised) generative tasks.\\n\\nTo see the input structure of a given metric, you can look at its metric card. For example, in the case of [precision](https://huggingface.co/metrics/precision), the format is:'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1760}, page_content='```\\n>>> precision_metric = evaluate.load(\"precision\")\\n>>> results = precision_metric.compute(references=[0, 1], predictions=[0, 1])\\n>>> print(results)\\n{\\'precision\\': 1.0}'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 1930}, page_content='```\\n\\n### Task-specific metrics\\n\\nPopular ML tasks like Machine Translation and Named Entity Recognition have specific metrics that can be used to compare models. For example, a series of different metrics have been proposed for text generation, ranging from [BLEU](https://huggingface.co/metrics/bleu) and its derivatives such as [GoogleBLEU](https://huggingface.co/metrics/google_bleu) and [GLEU](https://huggingface.co/metrics/gleu), but also [ROUGE](https://huggingface.co/metrics/rouge), [MAUVE](https://huggingface.co/metrics/mauve), etc.\\n\\nYou can find the right metric for your task by:'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 2474}, page_content='You can find the right metric for your task by:\\n\\n- **Looking at the [Task pages](https://huggingface.co/tasks)** to see what metrics can be used for evaluating models for a given task.\\n- **Checking out leaderboards** on sites like [Papers With Code](https://paperswithcode.com/) (you can search by task and by dataset).\\n-  **Reading the metric cards** for the relevant metrics and see which ones are a good fit for your use case. For example, see the [BLEU metric card](https://github.com/huggingface/evaluate/tree/main/metrics/bleu) or [SQuaD metric card](https://github.com/huggingface/evaluate/tree/main/metrics/squad).\\n- **Looking at papers and blog posts** published on the topic and see what metrics they report. This can change over time, so try to pick papers from the last couple of years!\\n\\n### Dataset-specific metrics'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 3274}, page_content='### Dataset-specific metrics\\n\\nSome datasets have specific metrics associated with them -- this is especially in the case of popular benchmarks like [GLUE](https://huggingface.co/metrics/glue) and [SQuAD](https://huggingface.co/metrics/squad).\\n\\n<Tip warning={true}>\\n💡\\nGLUE is actually a collection of different subsets on different tasks, so first you need to choose the one that corresponds to the NLI task, such as mnli, which is described as “crowdsourced collection of sentence pairs with textual entailment annotations”\\n</Tip>'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 3807}, page_content='If you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use its dedicated evaluation metric. Make sure you respect the format that they require. For example, to evaluate your model on the [SQuAD](https://huggingface.co/datasets/squad) dataset, you need to feed the `question` and `context` into your model and return the `prediction_text`, which should be compared with the `references` (based on matching the `id` of the question) :'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/docs/source/choosing_a_metric.mdx', 'start_index': 4281}, page_content='```\\n>>> from evaluate import load\\n>>> squad_metric = load(\"squad\")\\n>>> predictions = [{\\'prediction_text\\': \\'1976\\', \\'id\\': \\'56e10a3be3433e1400422b22\\'}]\\n>>> references = [{\\'answers\\': {\\'answer_start\\': [97], \\'text\\': [\\'1976\\']}, \\'id\\': \\'56e10a3be3433e1400422b22\\'}]\\n>>> results = squad_metric.compute(predictions=predictions, references=references)\\n>>> results\\n{\\'exact_match\\': 100.0, \\'f1\\': 100.0}\\n```\\n\\nYou can find examples of dataset structures by consulting the \"Dataset Preview\" function or the dataset card for a given dataset, and you can see how to use its dedicated evaluation function based on the metric card.'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 1}, page_content='主要特点\\n\\n让我们来介绍一下 Gradio 最受欢迎的一些功能！这里是 Gradio 的主要特点：\\n\\n1. [添加示例输入](#example-inputs)\\n2. [传递自定义错误消息](#errors)\\n3. [添加描述内容](#descriptive-content)\\n4. [设置旗标](#flagging)\\n5. [预处理和后处理](#preprocessing-and-postprocessing)\\n6. [样式化演示](#styling)\\n7. [排队用户](#queuing)\\n8. [迭代输出](#iterative-outputs)\\n9. [进度条](#progress-bars)\\n10. [批处理函数](#batch-functions)\\n11. [在协作笔记本上运行](#colab-notebooks)\\n\\n## 示例输入\\n\\n您可以提供用户可以轻松加载到 \"Interface\" 中的示例数据。这对于演示模型期望的输入类型以及演示数据集和模型一起探索的方式非常有帮助。要加载示例数据，您可以将嵌套列表提供给 Interface 构造函数的 `examples=` 关键字参数。外部列表中的每个子列表表示一个数据样本，子列表中的每个元素表示每个输入组件的输入。有关每个组件的示例数据格式在[Docs](https://gradio.app/docs#components)中有说明。\\n\\n$code_calculator\\n$demo_calculator\\n\\n您可以将大型数据集加载到示例中，通过 Gradio 浏览和与数据集进行交互。示例将自动分页（可以通过 Interface 的 `examples_per_page` 参数进行配置）。\\n\\n继续了解示例，请参阅[更多示例](https://gradio.app/more-on-examples)指南。\\n\\n## 错误\\n\\n您希望向用户传递自定义错误消息。为此，with `gr.Error(\"custom message\")` 来显示错误消息。如果在上面的计算器示例中尝试除以零，将显示自定义错误消息的弹出模态窗口。了解有关错误的更多信息，请参阅[文档](https://gradio.app/docs#error)。\\n\\n## 描述性内容'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 950}, page_content=\"## 描述性内容\\n\\n在前面的示例中，您可能已经注意到 Interface 构造函数中的 `title=` 和 `description=` 关键字参数，帮助用户了解您的应用程序。\\n\\nInterface 构造函数中有三个参数用于指定此内容应放置在哪里：\\n\\n- `title`：接受文本，并可以将其显示在界面的顶部，也将成为页面标题。\\n- `description`：接受文本、Markdown 或 HTML，并将其放置在标题正下方。\\n- `article`：也接受文本、Markdown 或 HTML，并将其放置在界面下方。\\n\\n![annotated](/assets/guides/annotated.png)\\n\\n如果您使用的是 `Blocks` API，则可以 with `gr.Markdown(...)` 或 `gr.HTML(...)` 组件在任何位置插入文本、Markdown 或 HTML，其中描述性内容位于 `Component` 构造函数内部。\\n\\n另一个有用的关键字参数是 `label=`，它存在于每个 `Component` 中。这修改了每个 `Component` 顶部的标签文本。还可以为诸如 `Textbox` 或 `Radio` 之类的表单元素添加 `info=` 关键字参数，以提供有关其用法的进一步信息。\\n\\n```python\\ngr.Number(label='年龄', info='以年为单位，必须大于0')\"),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 1572}, page_content='```\\n\\n## 旗标\\n\\n默认情况下，\"Interface\" 将有一个 \"Flag\" 按钮。当用户测试您的 `Interface` 时，如果看到有趣的输出，例如错误或意外的模型行为，他们可以将输入标记为您进行查看。在由 `Interface` 构造函数的 `flagging_dir=` 参数提供的目录中，将记录标记的输入到一个 CSV 文件中。如果界面涉及文件数据，例如图像和音频组件，将创建文件夹来存储这些标记的数据。\\n\\n例如，对于上面显示的计算器界面，我们将在下面的旗标目录中存储标记的数据：\\n\\n```directory\\n+-- calculator.py\\n+-- flagged/\\n|   +-- logs.csv\\n```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nnum1,operation,num2,Output\\n5,add,7,12\\n6,subtract,1.5,4.5\\n```\\n\\n与早期显示的冷色界面相对应，我们将在下面的旗标目录中存储标记的数据：\\n\\n```directory\\n+-- sepia.py\\n+-- flagged/\\n|   +-- logs.csv\\n|   +-- im/\\n|   |   +-- 0.png\\n|   |   +-- 1.png\\n|   +-- Output/\\n|   |   +-- 0.png\\n|   |   +-- 1.png\\n```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 2169}, page_content='```\\n\\n_flagged/logs.csv_\\n\\n```csv\\nim,Output\\nim/0.png,Output/0.png\\nim/1.png,Output/1.png\\n```\\n\\n如果您希望用户提供旗标原因，可以将字符串列表传递给 Interface 的 `flagging_options` 参数。用户在进行旗标时必须选择其中一个字符串，这将作为附加列保存到 CSV 中。\\n\\n## 预处理和后处理 (Preprocessing and Postprocessing)\\n\\n![annotated](/assets/img/dataflow.svg)\\n\\n如您所见，Gradio 包括可以处理各种不同数据类型的组件，例如图像、音频和视频。大多数组件都可以用作输入或输出。\\n\\n当组件用作输入时，Gradio 自动处理*预处理*，将数据从用户浏览器发送的类型（例如网络摄像头快照的 base64 表示）转换为您的函数可以接受的形式（例如 `numpy` 数组）。\\n\\n同样，当组件用作输出时，Gradio 自动处理*后处理*，将数据从函数返回的形式（例如图像路径列表）转换为可以在用户浏览器中显示的形式（例如以 base64 格式显示图像的 `Gallery`）。\\n\\n您可以使用构建图像组件时的参数控制*预处理*。例如，如果您使用以下参数实例化 `Image` 组件，它将将图像转换为 `PIL` 类型，并将其重塑为`(100, 100)`，而不管提交时的原始大小如何：\\n\\n```py\\nimg = gr.Image(shape=(100, 100), type=\"pil\")\\n```\\n\\n相反，这里我们保留图像的原始大小，但在将其转换为 numpy 数组之前反转颜色：\\n\\n```py\\nimg = gr.Image(invert_colors=True, type=\"numpy\")'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 2955}, page_content='```\\n\\n后处理要容易得多！Gradio 自动识别返回数据的格式（例如 `Image` 是 `numpy` 数组还是 `str` 文件路径？），并将其后处理为可以由浏览器显示的格式。\\n\\n请查看[文档](https://gradio.app/docs)，了解每个组件的所有与预处理相关的参数。\\n\\n## 样式 (Styling)\\n\\nGradio 主题是自定义应用程序外观和感觉的最简单方法。您可以选择多种主题或创建自己的主题。要这样做，请将 `theme=` 参数传递给 `Interface` 构造函数。例如：\\n\\n```python\\ndemo = gr.Interface(..., theme=gr.themes.Monochrome())\\n```\\n\\nGradio 带有一组预先构建的主题，您可以从 `gr.themes.*` 加载。您可以扩展这些主题或从头开始创建自己的主题 - 有关更多详细信息，请参阅[主题指南](https://gradio.app/theming-guide)。\\n\\n要增加额外的样式能力，您可以 with `css=` 关键字将任何 CSS 传递给您的应用程序。\\nGradio 应用程序的基类是 `gradio-container`，因此以下是一个更改 Gradio 应用程序背景颜色的示例：\\n\\n```python\\nwith `gr.Interface(css=\".gradio-container {background-color: red}\") as demo:\\n    ...\\n```\\n\\n## 队列 (Queuing)\\n\\n如果您的应用程序预计会有大量流量，请 with `queue()` 方法来控制处理速率。这将排队处理调用，因此一次只处理一定数量的请求。队列使用 Websockets，还可以防止网络超时，因此如果您的函数的推理时间很长（> 1 分钟），应使用队列。\\n\\nwith `Interface`：\\n\\n```python\\ndemo = gr.Interface(...).queue()\\ndemo.launch()\\n```\\n\\nwith `Blocks`：\\n\\n```python\\nwith gr.Blocks() as demo：\\n    #...\\ndemo.queue()\\ndemo.launch()'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 3835}, page_content='```\\n\\nwith `Blocks`：\\n\\n```python\\nwith gr.Blocks() as demo：\\n    #...\\ndemo.queue()\\ndemo.launch()\\n```\\n\\n您可以通过以下方式控制一次处理的请求数量：\\n\\n```python\\ndemo.queue(concurrency_count=3)\\n```\\n\\n查看有关配置其他队列参数的[队列文档](/docs/#queue)。\\n\\n在 Blocks 中指定仅对某些函数进行排队：\\n\\n```python\\nwith gr.Blocks() as demo2：\\n    num1 = gr.Number()\\n    num2 = gr.Number()\\n    output = gr.Number()\\n    gr.Button(\"Add\").click(\\n        lambda a, b: a + b, [num1, num2], output)\\n    gr.Button(\"Multiply\").click(\\n        lambda a, b: a * b, [num1, num2], output, queue=True)\\ndemo2.launch()\\n```\\n\\n## 迭代输出 (Iterative Outputs)\\n\\n在某些情况下，您可能需要传输一系列输出而不是一次显示单个输出。例如，您可能有一个图像生成模型，希望显示生成的每个步骤的图像，直到最终图像。或者您可能有一个聊天机器人，它逐字逐句地流式传输响应，而不是一次返回全部响应。\\n\\n在这种情况下，您可以将**生成器**函数提供给 Gradio，而不是常规函数。在 Python 中创建生成器非常简单：函数不应该有一个单独的 `return` 值，而是应该 with `yield` 连续返回一系列值。通常，`yield` 语句放置在某种循环中。下面是一个简单示例，生成器只是简单计数到给定数字：\\n\\n```python\\ndef my_generator(x):\\n    for i in range(x):\\n        yield i'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 4732}, page_content='```\\n\\n您以与常规函数相同的方式将生成器提供给 Gradio。例如，这是一个（虚拟的）图像生成模型，它在输出图像之前生成数个步骤的噪音：\\n\\n$code_fake_diffusion\\n$demo_fake_diffusion\\n\\n请注意，我们在迭代器中添加了 `time.sleep(1)`，以创建步骤之间的人工暂停，以便您可以观察迭代器的步骤（在真实的图像生成模型中，这可能是不必要的）。\\n\\n将生成器提供给 Gradio **需要**在底层 Interface 或 Blocks 中启用队列（请参阅上面的队列部分）。\\n\\n## 进度条\\n\\nGradio 支持创建自定义进度条，以便您可以自定义和控制向用户显示的进度更新。要启用此功能，只需为方法添加一个默认值为 `gr.Progress` 实例的参数即可。然后，您可以直接调用此实例并传入 0 到 1 之间的浮点数来更新进度级别，或者 with `Progress` 实例的 `tqdm()` 方法来跟踪可迭代对象上的进度，如下所示。必须启用队列以进行进度更新。\\n\\n$code_progress_simple\\n$demo_progress_simple\\n\\n如果您 with `tqdm` 库，并且希望从函数内部的任何 `tqdm.tqdm` 自动报告进度更新，请将默认参数设置为 `gr.Progress(track_tqdm=True)`！\\n\\n## 批处理函数 (Batch Functions)\\n\\nGradio 支持传递*批处理*函数。批处理函数只是接受输入列表并返回预测列表的函数。\\n\\n例如，这是一个批处理函数，它接受两个输入列表（一个单词列表和一个整数列表），并返回修剪过的单词列表作为输出：\\n\\n```python\\nimport time\\n\\ndef trim_words(words, lens):\\n    trimmed_words = []\\n    time.sleep(5)\\n    for w, l in zip(words, lens):\\n        trimmed_words.append(w[:int(l)])\\n    return [trimmed_words]\\n    for w, l in zip(words, lens):'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 5686}, page_content='```\\n\\n使用批处理函数的优点是，如果启用了队列，Gradio 服务器可以自动*批处理*传入的请求并并行处理它们，从而可能加快演示速度。以下是 Gradio 代码的示例（请注意 `batch=True` 和 `max_batch_size=16` - 这两个参数都可以传递给事件触发器或 `Interface` 类）\\n\\nwith `Interface`：\\n\\n```python\\ndemo = gr.Interface(trim_words, [\"textbox\", \"number\"], [\"output\"],\\n                    batch=True, max_batch_size=16)\\ndemo.queue()\\ndemo.launch()\\n```\\n\\nwith `Blocks`：\\n\\n```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        word = gr.Textbox(label=\"word\")\\n        leng = gr.Number(label=\"leng\")\\n        output = gr.Textbox(label=\"Output\")\\n    with gr.Row():\\n        run = gr.Button()\\n\\n    event = run.click(trim_words, [word, leng], output, batch=True, max_batch_size=16)\\n\\ndemo.queue()\\ndemo.launch()'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/01_getting-started/02_key-features.md', 'start_index': 6401}, page_content='```\\n\\n在上面的示例中，可以并行处理 16 个请求（总推理时间为 5 秒），而不是分别处理每个请求（总推理时间为 80 秒）。许多 Hugging Face 的 `transformers` 和 `diffusers` 模型在 Gradio 的批处理模式下自然工作：这是[使用批处理生成图像的示例演示](https://github.com/gradio-app/gradio/blob/main/demo/diffusers_with_batching/run.py)\\n\\n注意：使用 Gradio 的批处理函数 **requires** 在底层 Interface 或 Blocks 中启用队列（请参阅上面的队列部分）。\\n\\n## Gradio 笔记本 (Colab Notebooks)\\n\\nGradio 可以在任何运行 Python 的地方运行，包括本地 Jupyter 笔记本和协作笔记本，如[Google Colab](https://colab.research.google.com/)。对于本地 Jupyter 笔记本和 Google Colab 笔记本，Gradio 在本地服务器上运行，您可以在浏览器中与之交互。（注意：对于 Google Colab，这是通过[服务工作器隧道](https://github.com/tensorflow/tensorboard/blob/master/docs/design/colab_integration.md)实现的，您的浏览器需要启用 cookies。）对于其他远程笔记本，Gradio 也将在服务器上运行，但您需要使用[SSH 隧道](https://coderwall.com/p/ohk6cg/remote-access-to-ipython-notebooks-via-ssh)在本地浏览器中查看应用程序。通常，更简单的选择是使用 Gradio 内置的公共链接，[在下一篇指南中讨论](/sharing-your-app/#sharing-demos)。'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Training on TPU with TensorFlow\\n\\n<Tip>\\n\\nIf you don\\'t need long explanations and just want TPU code samples to get started with, check out [our TPU example notebook!](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb)\\n\\n</Tip>\\n\\n### What is a TPU?'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 950}, page_content='</Tip>\\n\\n### What is a TPU?\\n\\nA TPU is a **Tensor Processing Unit.** They are hardware designed by Google, which are used to greatly speed up the tensor computations within neural networks, much like GPUs. They can be used for both network training and inference. They are generally accessed through Google’s cloud services, but small TPUs can also be accessed directly for free through Google Colab and Kaggle Kernels.\\n\\nBecause [all TensorFlow models in 🤗 Transformers are Keras models](https://huggingface.co/blog/tensorflow-philosophy), most of the methods in this document are generally applicable to TPU training for any Keras model! However, there are a few points that are specific to the HuggingFace ecosystem (hug-o-system?) of Transformers and Datasets, and we’ll make sure to flag them up when we get to them.\\n\\n### What kinds of TPU are available?'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 1770}, page_content='### What kinds of TPU are available?\\n\\nNew users are often very confused by the range of TPUs, and the different ways to access them. The first key distinction to understand is the difference between **TPU Nodes** and **TPU VMs.**\\n\\nWhen you use a **TPU Node**, you are effectively indirectly accessing a remote TPU. You will need a separate VM, which will initialize your network and data pipeline and then forward them to the remote node. When you use a TPU on Google Colab, you are accessing it in the **TPU Node** style.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 2294}, page_content='Using TPU Nodes can have some quite unexpected behaviour for people who aren’t used to them! In particular, because the TPU is located on a physically different system to the machine you’re running your Python code on, your data cannot be local to your machine - any data pipeline that loads from your machine’s internal storage will totally fail! Instead, data must be stored in Google Cloud Storage where your data pipeline can still access it, even when the pipeline is running on the remote TPU node.\\n\\n<Tip>\\n\\nIf you can fit all your data in memory as `np.ndarray` or `tf.Tensor`, then you can `fit()` on that data even when using Colab or a TPU Node, without needing to upload it to Google Cloud Storage.\\n\\n</Tip>\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 3004}, page_content='</Tip>\\n\\n<Tip>\\n\\n**🤗Specific Hugging Face Tip🤗:** The methods `Dataset.to_tf_dataset()` and its higher-level wrapper `model.prepare_tf_dataset()` , which you will see throughout our TF code examples, will both fail on a TPU Node. The reason for this is that even though they create a `tf.data.Dataset` it is not a “pure” `tf.data` pipeline and uses `tf.numpy_function` or `Dataset.from_generator()` to stream data from the underlying HuggingFace `Dataset`. This HuggingFace `Dataset` is backed by data that is on a local disc and which the remote TPU Node will not be able to read.\\n\\n</Tip>\\n\\nThe second way to access a TPU is via a **TPU VM.** When using a TPU VM, you connect directly to the machine that the TPU is attached to, much like training on a GPU VM. TPU VMs are generally easier to work with, particularly when it comes to your data pipeline. All of the above warnings do not apply to TPU VMs!'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 3908}, page_content='This is an opinionated document, so here’s our opinion: **Avoid using TPU Node if possible.** It is more confusing and more difficult to debug than TPU VMs. It is also likely to be unsupported in future - Google’s latest TPU, TPUv4, can only be accessed as a TPU VM, which suggests that TPU Nodes are increasingly going to become a “legacy” access method. However, we understand that the only free TPU access is on Colab and Kaggle Kernels, which uses TPU Node - so we’ll try to explain how to handle it if you have to! Check the [TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) for code samples that explain this in more detail.\\n\\n### What sizes of TPU are available?'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 4618}, page_content='### What sizes of TPU are available?\\n\\nA single TPU (a v2-8/v3-8/v4-8) runs 8 replicas. TPUs exist in **pods** that can run hundreds or thousands of replicas simultaneously. When you use more than a single TPU but less than a whole pod (for example, a v3-32), your TPU fleet is referred to as a **pod slice.**\\n\\nWhen you access a free TPU via Colab, you generally get a single v2-8 TPU.\\n\\n### I keep hearing about this XLA thing. What’s XLA, and how does it relate to TPUs?\\n\\nXLA is an optimizing compiler, used by both TensorFlow and JAX. In JAX it is the only compiler, whereas in TensorFlow it is optional (but mandatory on TPU!). The easiest way to enable it when training a Keras model is to pass the argument `jit_compile=True` to `model.compile()`. If you don’t get any errors and performance is good, that’s a great sign that you’re ready to move to TPU!'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 5478}, page_content='Debugging on TPU is generally a bit harder than on CPU/GPU, so we recommend getting your code running on CPU/GPU with XLA first before trying it on TPU. You don’t have to train for long, of course - just for a few steps to make sure that your model and data pipeline are working like you expect them to.\\n\\n<Tip>\\n\\nXLA compiled code is usually faster - so even if you’re not planning to run on TPU, adding `jit_compile=True` can improve your performance. Be sure to note the caveats below about XLA compatibility, though!\\n\\n</Tip>\\n\\n<Tip warning={true}>\\n\\n**Tip born of painful experience:** Although using `jit_compile=True` is a good way to get a speed boost and test if your CPU/GPU code is XLA-compatible, it can actually cause a lot of problems if you leave it in when actually training on TPU. XLA compilation will happen implicitly on TPU, so remember to remove that line before actually running your code on a TPU!\\n\\n</Tip>\\n\\n### How do I make my model XLA compatible?'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 6396}, page_content='</Tip>\\n\\n### How do I make my model XLA compatible?\\n\\nIn many cases, your code is probably XLA-compatible already! However, there are a few things that work in normal TensorFlow that don’t work in XLA. We’ve distilled them into three core rules below:\\n\\n<Tip>\\n\\n**🤗Specific HuggingFace Tip🤗:** We’ve put a lot of effort into rewriting our TensorFlow models and loss functions to be XLA-compatible. Our models and loss functions generally obey rule #1 and #2 by default, so you can skip over them if you’re using `transformers` models. Don’t forget about these rules when writing your own models and loss functions, though!\\n\\n</Tip>\\n\\n#### XLA Rule #1: Your code cannot have “data-dependent conditionals”\\n\\nWhat that means is that any `if` statement cannot depend on values inside a `tf.Tensor`. For example, this code block cannot be compiled with XLA!\\n\\n```python\\nif tf.reduce_sum(tensor) > 10:\\n    tensor = tensor / 2.0'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 7310}, page_content='```\\n\\nThis might seem very restrictive at first, but most neural net code doesn’t need to do this. You can often get around this restriction by using `tf.cond` (see the documentation [here](https://www.tensorflow.org/api_docs/python/tf/cond)) or by removing the conditional and finding a clever math trick with indicator variables instead, like so:\\n\\n```python\\nsum_over_10 = tf.cast(tf.reduce_sum(tensor) > 10, tf.float32)\\ntensor = tensor / (1.0 + sum_over_10)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 7769}, page_content='```\\n\\nThis code has exactly the same effect as the code above, but by avoiding a conditional, we ensure it will compile with XLA without problems!\\n\\n#### XLA Rule #2: Your code cannot have “data-dependent shapes”\\n\\nWhat this means is that the shape of all of the `tf.Tensor` objects in your code cannot depend on their values. For example, the function `tf.unique` cannot be compiled with XLA, because it returns a `tensor` containing one instance of each unique value in the input. The shape of this output will obviously be different depending on how repetitive the input `Tensor` was, and so XLA refuses to handle it!'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 8388}, page_content='In general, most neural network code obeys rule #2 by default. However, there are a few common cases where it becomes a problem. One very common one is when you use **label masking**, setting your labels to a negative value to indicate that those positions should be ignored when computing the loss. If you look at NumPy or PyTorch loss functions that support label masking, you will often see code like this that uses [boolean indexing](https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing):\\n\\n```python\\nlabel_mask = labels >= 0\\nmasked_outputs = outputs[label_mask]\\nmasked_labels = labels[label_mask]\\nloss = compute_loss(masked_outputs, masked_labels)\\nmean_loss = torch.mean(loss)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 9094}, page_content='```\\n\\nThis code is totally fine in NumPy or PyTorch, but it breaks in XLA! Why? Because the shape of `masked_outputs` and `masked_labels` depends on how many positions are masked - that makes it a **data-dependent shape.** However, just like for rule #1, we can often rewrite this code to yield exactly the same output without any data-dependent shapes.\\n\\n```python\\nlabel_mask = tf.cast(labels >= 0, tf.float32)\\nloss = compute_loss(outputs, labels)\\nloss = loss * label_mask  # Set negative label positions to 0\\nmean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 9663}, page_content='```\\n\\nHere, we avoid data-dependent shapes by computing the loss for every position, but zeroing out the masked positions in both the numerator and denominator when we calculate the mean, which yields exactly the same result as the first block while maintaining XLA compatibility. Note that we use the same trick as in rule #1 - converting a `tf.bool` to `tf.float32` and using it as an indicator variable. This is a really useful trick, so remember it if you need to convert your own code to XLA!\\n\\n#### XLA Rule #3: XLA will need to recompile your model for every different input shape it sees\\n\\nThis is the big one. What this means is that if your input shapes are very variable, XLA will have to recompile your model over and over, which will create huge performance problems. This commonly arises in NLP models, where input texts have variable lengths after tokenization. In other modalities, static shapes are more common and this rule is much less of a problem.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 10630}, page_content='How can you get around rule #3? The key is **padding** - if you pad all your inputs to the same length, and then use an `attention_mask`, you can get the same results as you’d get from variable shapes, but without any XLA issues. However, excessive padding can cause severe slowdown too - if you pad all your samples to the maximum length in the whole dataset, you might end up with batches consisting endless padding tokens, which will waste a lot of compute and memory!\\n\\nThere isn’t a perfect solution to this problem. However, you can try some tricks. One very useful trick is to **pad batches of samples up to a multiple of a number like 32 or 64 tokens.** This often only increases the number of tokens by a small amount, but it hugely reduces the number of unique input shapes, because every input shape now has to be a multiple of 32 or 64. Fewer unique input shapes means fewer XLA compilations!\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 11535}, page_content='<Tip>\\n\\n**🤗Specific HuggingFace Tip🤗:** Our tokenizers and data collators have methods that can help you here. You can use `padding=\"max_length\"` or `padding=\"longest\"` when calling tokenizers to get them to output padded data. Our tokenizers and data collators also have a `pad_to_multiple_of` argument that you can use to reduce the number of unique input shapes you see!\\n\\n</Tip>\\n\\n### How do I actually train my model on TPU?\\n\\nOnce your training is XLA-compatible and (if you’re using TPU Node / Colab) your dataset has been prepared appropriately, running on TPU is surprisingly easy! All you really need to change in your code is to add a few lines to initialize your TPU, and to ensure that your model and dataset are created inside a `TPUStrategy` scope. Take a look at [our TPU example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) to see this in action!\\n\\n### Summary'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 12466}, page_content='### Summary\\n\\nThere was a lot in here, so let’s summarize with a quick checklist you can follow when you want to get your model ready for TPU training:'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/perf_train_tpu_tf.md', 'start_index': 12618}, page_content='- Make sure your code follows the three rules of XLA\\n- Compile your model with `jit_compile=True` on CPU/GPU and confirm that you can train it with XLA\\n- Either load your dataset into memory or use a TPU-compatible dataset loading approach (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Migrate your code either to Colab (with accelerator set to “TPU”) or a TPU VM on Google Cloud\\n- Add TPU initializer code (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Create your `TPUStrategy` and make sure dataset loading and model creation are inside the `strategy.scope()` (see [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb))\\n- Don’t forget to take `jit_compile=True` out again when you move to TPU!\\n- 🙏🙏🙏🥺🥺🥺\\n- Call model.fit()\\n- You did it!'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/blocks_random_slider/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef func(slider_1, slider_2):\\n    return slider_1 * 5 + slider_2\\n\\n\\nwith gr.Blocks() as demo:\\n    slider = gr.Slider(minimum=-10.2, maximum=15, label=\"Random Slider (Static)\", randomize=True)\\n    slider_1 = gr.Slider(minimum=100, maximum=200, label=\"Random Slider (Input 1)\", randomize=True)\\n    slider_2 = gr.Slider(minimum=10, maximum=23.2, label=\"Random Slider (Input 2)\", randomize=True)\\n    slider_3 = gr.Slider(value=3, label=\"Non random slider\")\\n    btn = gr.Button(\"Run\")\\n    btn.click(func, inputs=[slider_1, slider_2], outputs=gr.Number())\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n\\n```'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 1}, page_content='Git over SSH\\n\\nYou can access and write data in repositories on huggingface.co using SSH (Secure Shell Protocol). When you connect via SSH, you authenticate using a private key file on your local machine.\\n\\nSome actions, such as pushing changes, or cloning private repositories, will require you to upload your SSH public key to your account on huggingface.co.\\n\\nYou can use a pre-existing SSH key, or generate a new one specifically for huggingface.co.\\n\\n## Checking for existing SSH keys\\n\\nIf you have an existing SSH key, you can use that key to authenticate Git operations over SSH.\\n\\nSSH keys are usually located under `~/.ssh` on Mac & Linux, and under `C:\\\\\\\\Users\\\\\\\\<username>\\\\\\\\.ssh` on Windows. List files under that directory and look for files of the form:\\n\\n- id_rsa.pub\\n- id_ecdsa.pub\\n- id_ed25519.pub\\n\\nThose files contain your SSH public key.'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 761}, page_content=\"- id_rsa.pub\\n- id_ecdsa.pub\\n- id_ed25519.pub\\n\\nThose files contain your SSH public key.\\n\\nIf you don't have such file under `~/.ssh`, you will have to [generate a new key](#generating-a-new-ssh-keypair). Otherwise, you can [add your existing SSH public key(s) to your huggingface.co account](#add-a-ssh-key-to-your-account).\\n\\n## Generating a new SSH keypair\\n\\nIf you don't have any SSH keys on your machine, you can use `ssh-keygen` to generate a new SSH key pair (public + private keys):\"),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 1248}, page_content='```\\n$ ssh-keygen -t ed25519 -C \"your.email@example.co\"\\n```\\n\\nWe recommend entering a passphrase when you are prompted to. A passphrase is an extra layer of security: it is a password that will be prompted whenever you use your SSH key.\\n\\nOnce your new key is generated, add it to your SSH agent with `ssh-add`:\\n\\n```\\n$ ssh-add ~/.ssh/id_ed25519'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 1590}, page_content='```\\n\\nIf you chose a different location than the default to store your SSH key, you would have to replace `~/.ssh/id_ed25519` with the file location you used.\\n\\n## Add a SSH key to your account\\n\\nTo access private repositories with SSH, or to push changes via SSH, you will need to add your SSH public key to your huggingface.co account. You can manage your SSH keys [in your user settings](https://huggingface.co/settings/keys).\\n\\nTo add a SSH key to your account, click on the \"Add SSH key\" button.\\n\\nThen, enter a name for this key (for example, \"Personal computer\"), and copy and paste the content of your **public** SSH key in the area below. The public key is located in the `~/.ssh/id_XXXX.pub` file you found or generated in the previous steps.\\n\\nClick on \"Add key\", and voilà! You have added a SSH key to your huggingface.co account.\\n\\n\\n## Testing your SSH authentication\\n\\nOnce you have added your SSH key to your huggingface.co account, you can test that the connection works as expected.'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 2583}, page_content='In a terminal, run:'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md', 'start_index': 2603}, page_content='```\\n$ ssh -T git@hf.co\\n```\\n\\nIf you see a message with your username, congrats! Everything went well, you are ready to use git over SSH.\\n\\nOtherwise, if the message states something like the following, make sure your SSH key is actually used by your SSH agent.\\n```\\nHi anonymous, welcome to Hugging Face.\\n```'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# Token classification with LayoutLMv3 (PyTorch version)\\n\\nThis directory contains a script, `run_funsd_cord.py`, that can be used to fine-tune (or evaluate) LayoutLMv3 on form understanding datasets, such as [FUNSD](https://guillaumejaume.github.io/FUNSD/) and [CORD](https://github.com/clovaai/cord).'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 896}, page_content='The script `run_funsd_cord.py` leverages the 🤗 Datasets library and the Trainer API. You can easily customize it to your needs.\\n\\n## Fine-tuning on FUNSD\\n\\nFine-tuning LayoutLMv3 for token classification on [FUNSD](https://guillaumejaume.github.io/FUNSD/) can be done as follows:\\n\\n```bash\\npython run_funsd_cord.py \\\\\\n  --model_name_or_path microsoft/layoutlmv3-base \\\\\\n  --dataset_name funsd \\\\\\n  --output_dir layoutlmv3-test \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_steps 1000 \\\\\\n  --evaluation_strategy steps \\\\\\n  --eval_steps 100 \\\\\\n  --learning_rate 1e-5 \\\\\\n  --load_best_model_at_end \\\\\\n  --metric_for_best_model \"eval_f1\" \\\\\\n  --push_to_hub \\\\\\n  --push_to_hub°model_id layoutlmv3-finetuned-funsd'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 1584}, page_content='```\\n\\n👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd. By specifying the `push_to_hub` flag, the model gets uploaded automatically to the hub (regularly), together with a model card, which includes metrics such as precision, recall and F1. Note that you can easily update the model card, as it\\'s just a README file of the respective repo on the hub.\\n\\nThere\\'s also the \"Training metrics\" [tab](https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd/tensorboard), which shows Tensorboard logs over the course of training. Pretty neat, huh?\\n\\n## Fine-tuning on CORD\\n\\nFine-tuning LayoutLMv3 for token classification on [CORD](https://github.com/clovaai/cord) can be done as follows:'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 2314}, page_content='```bash\\npython run_funsd_cord.py \\\\\\n  --model_name_or_path microsoft/layoutlmv3-base \\\\\\n  --dataset_name cord \\\\\\n  --output_dir layoutlmv3-test \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_steps 1000 \\\\\\n  --evaluation_strategy steps \\\\\\n  --eval_steps 100 \\\\\\n  --learning_rate 5e-5 \\\\\\n  --load_best_model_at_end \\\\\\n  --metric_for_best_model \"eval_f1\" \\\\\\n  --push_to_hub \\\\\\n  --push_to_hub°model_id layoutlmv3-finetuned-cord'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/layoutlmv3/README.md', 'start_index': 2721}, page_content='```\\n\\n👀 The resulting model can be found here: https://huggingface.co/nielsr/layoutlmv3-finetuned-cord. Note that a model card gets generated automatically in case you specify the `push_to_hub` flag.'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/03_building-with-blocks/03_state-in-blocks.md', 'start_index': 1}, page_content=\"State in Blocks\\n\\nWe covered [State in Interfaces](https://gradio.app/interface-state), this guide takes a look at state in Blocks, which works mostly the same.\\n\\n## Global State\\n\\nGlobal state in Blocks works the same as in Interface. Any variable created outside a function call is a reference shared between all users.\\n\\n## Session State\\n\\nGradio supports session **state**, where data persists across multiple submits within a page session, in Blocks apps as well. To reiterate, session data is _not_ shared between different users of your model. To store data in a session state, you need to do three things:\\n\\n1. Create a `gr.State()` object. If there is a default value to this stateful object, pass that into the constructor.\\n2. In the event listener, put the `State` object as an input and output.\\n3. In the event listener function, add the variable to the input parameters and the return value.\\n\\nLet's take a look at a game of hangman.\\n\\n$code_hangman\\n$demo_hangman\"),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/03_building-with-blocks/03_state-in-blocks.md', 'start_index': 901}, page_content=\"Let's take a look at a game of hangman.\\n\\n$code_hangman\\n$demo_hangman\\n\\nLet's see how we do each of the 3 steps listed above in this game:\\n\\n1. We store the used letters in `used_letters_var`. In the constructor of `State`, we set the initial value of this to `[]`, an empty list.\\n2. In `btn.click()`, we have a reference to `used_letters_var` in both the inputs and outputs.\\n3. In `guess_letter`, we pass the value of this `State` to `used_letters`, and then return an updated value of this `State` in the return statement.\\n\\nWith more complex apps, you will likely have many State variables storing session state in a single Blocks app.\\n\\nLearn more about `State` in the [docs](https://gradio.app/docs#state).\"),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 1}, page_content='如何使用地图组件绘制图表\\n\\nRelated spaces:\\nTags: PLOTS, MAPS\\n\\n## 简介\\n\\n本指南介绍如何使用 Gradio 的 `Plot` 组件在地图上绘制地理数据。Gradio 的 `Plot` 组件可以与 Matplotlib、Bokeh 和 Plotly 一起使用。在本指南中，我们将使用 Plotly 进行操作。Plotly 可以让开发人员轻松创建各种地图来展示他们的地理数据。点击[这里](https://plotly.com/python/maps/)查看一些示例。\\n\\n## 概述\\n\\n我们将使用纽约市的 Airbnb 数据集，该数据集托管在 kaggle 上，点击[这里](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)。我已经将其上传到 Hugging Face Hub 作为一个数据集，方便使用和下载，点击[这里](https://huggingface.co/datasets/gradio/NYC-Airbnb-Open-Data)。使用这些数据，我们将在地图上绘制 Airbnb 的位置，并允许基于价格和位置进行筛选。下面是我们将要构建的演示。 ⚡️\\n\\n$demo_map_airbnb\\n\\n## 步骤 1-加载 CSV 数据 💾\\n\\n让我们首先从 Hugging Face Hub 加载纽约市的 Airbnb 数据。\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\\ndf = dataset.to_pandas()'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 677}, page_content='dataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\\ndf = dataset.to_pandas()\\n\\ndef filter_map(min_price, max_price, boroughs):\\n    new_df = df[(df[\\'neighbourhood_group\\'].isin(boroughs)) &\\n            (df[\\'price\\'] > min_price) & (df[\\'price\\'] < max_price)]\\n    names = new_df[\"name\"].tolist()\\n    prices = new_df[\"price\"].tolist()\\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 1092}, page_content='```\\n\\n在上面的代码中，我们先将 CSV 数据加载到一个 pandas dataframe 中。让我们首先定义一个函数，这将作为 gradio 应用程序的预测函数。该函数将接受最低价格、最高价格范围和筛选结果地区的列表作为参数。我们可以使用传入的值 (`min_price`、`max_price` 和地区列表) 来筛选数据框并创建 `new_df`。接下来，我们将创建包含每个 Airbnb 的名称和价格的 `text_list`，以便在地图上使用作为标签。\\n\\n## 步骤 2-地图图表 🌐\\n\\nPlotly 使得处理地图变得很容易。让我们看一下下面的代码，了解如何创建地图图表。\\n\\n```python\\nimport plotly.graph_objects as go\\n\\nfig = go.Figure(go.Scattermapbox(\\n            customdata=text_list,\\n            lat=new_df[\\'latitude\\'].tolist(),\\n            lon=new_df[\\'longitude\\'].tolist(),\\n            mode=\\'markers\\',\\n            marker=go.scattermapbox.Marker(\\n                size=6\\n            ),\\n            hoverinfo=\"text\",\\n            hovertemplate=\\'<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}\\'\\n        ))\\n\\nfig.update_layout(\\n    mapbox_style=\"open-street-map\",\\n    hovermode=\\'closest\\',\\n    mapbox=dict(\\n        bearing=0,\\n        center=go.layout.mapbox.Center(\\n            lat=40.67,\\n            lon=-73.90\\n        ),\\n        pitch=0,\\n        zoom=9\\n    ),\\n)'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 2088}, page_content='```\\n\\n上面的代码中，我们通过传入经纬度列表来创建一个散点图。我们还传入了名称和价格的自定义数据，以便在鼠标悬停在每个标记上时显示额外的信息。接下来，我们使用 `update_layout` 来指定其他地图设置，例如缩放和居中。\\n\\n有关使用 Mapbox 和 Plotly 创建散点图的更多信息，请点击[这里](https://plotly.com/python/scattermapbox/)。\\n\\n## 步骤 3-Gradio 应用程序 ⚡️\\n\\n我们将使用两个 `gr.Number` 组件和一个 `gr.CheckboxGroup` 组件，允许用户指定价格范围和地区位置。然后，我们将使用 `gr.Plot` 组件作为我们之前创建的 Plotly + Mapbox 地图的输出。\\n\\n```python\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\\n        btn = gr.Button(value=\"Update Filter\")\\n        map = gr.Plot()\\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\\n    btn.click(filter_map, [min_price, max_price, boroughs], map)'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/cn/05_tabular-data-science-and-plots/plot-component-for-maps.md', 'start_index': 3014}, page_content='```\\n\\n我们使用 `gr.Column` 和 `gr.Row` 布局这些组件，并为演示加载时和点击 \" 更新筛选 \" 按钮时添加了事件触发器，以触发地图更新新的筛选条件。\\n\\n以下是完整演示代码：\\n\\n$code_map_airbnb\\n\\n## 步骤 4-部署 Deployment 🤗\\n\\n如果你运行上面的代码，你的应用程序将在本地运行。\\n如果要获取临时共享链接，可以将 `share=True` 参数传递给 `launch`。\\n\\n但如果你想要一个永久的部署解决方案呢？\\n让我们将我们的 Gradio 应用程序部署到免费的 HuggingFace Spaces 平台。\\n\\n如果你以前没有使用过 Spaces，请按照之前的指南[这里](/using_hugging_face_integrations)。\\n\\n## 结论 🎉\\n\\n你已经完成了！这是构建地图演示所需的所有代码。\\n\\n链接到演示：[地图演示](https://huggingface.co/spaces/gradio/map_airbnb)和[完整代码](https://huggingface.co/spaces/gradio/map_airbnb/blob/main/run.py)（在 Hugging Face Spaces）'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 1}, page_content=\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) that employs [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block) to enable the network to perform dynamic channel-wise feature recalibration.\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model('seresnet152d', pretrained=True)\\n>>> model.eval()\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 458}, page_content='```\\n\\nTo load and preprocess the image:\\n\\n```py \\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 1253}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n>>> # Get imagenet class mappings\\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\n>>> urllib.request.urlretrieve(url, filename) \\n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\\n...     categories = [s.strip() for s in f.readlines()]\\n\\n>>> # Print top categories per image\\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\\n>>> for i in range(top5_prob.size(0)):\\n...     print(categories[top5_catid[i]], top5_prob[i].item())\\n>>> # prints class names and probabilities like:\\n>>> # [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 2047}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('seresnet152d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 2600}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{hu2019squeezeandexcitation,\\n      title={Squeeze-and-Excitation Networks}, \\n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\\n      year={2019},\\n      eprint={1709.01507},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 3206}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 3211}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Networks\\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\\nModels:\\n- Name: seresnet152d\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 20161904304\\n    Parameters: 66840000\\n    File Size: 268144497\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet152d\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 152\\n    Dropout: 0.2\\n    Crop Pct: '0.94'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '256'\\n    Interpolation: bicubic\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 4118}, page_content=\"Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '256'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1206\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet152d_ra2-04464dd2.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83.74%\\n      Top 5 Accuracy: 96.77%\\n- Name: seresnet50\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 5285062320\\n    Parameters: 28090000\\n    File Size: 112621903\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/se-resnet.mdx', 'start_index': 5006}, page_content=\"- Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet50\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 50\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1180\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet50_ra_224-8efdb4bb.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.26%\\n      Top 5 Accuracy: 95.07%\\n-->\"),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 0}, page_content=\"--\\ntitle: poseval\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  The poseval metric can be used to evaluate POS taggers. Since seqeval does not work well with POS data \\n  that is not in IOB format the poseval is an alternative. It treats each token in the dataset as independant \\n  observation and computes the precision, recall and F1-score irrespective of sentences. It uses scikit-learns's\\n  classification report to compute the scores.\\n---\\n\\n# Metric Card for peqeval\\n\\n## Metric description\"),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 544}, page_content=\"# Metric Card for peqeval\\n\\n## Metric description\\n\\nThe poseval metric can be used to evaluate POS taggers. Since seqeval does not work well with POS data (see e.g. [here](https://stackoverflow.com/questions/71327693/how-to-disable-seqeval-label-formatting-for-pos-tagging)) that is not in IOB format the poseval is an alternative. It treats each token in the dataset as independant observation and computes the precision, recall and F1-score irrespective of sentences. It uses scikit-learns's [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to compute the scores.\\n\\n\\n## How to use \\n\\nPoseval produces labelling scores along with its sufficient statistics from a source against references.\\n\\nIt takes two mandatory arguments:\\n\\n`predictions`: a list of lists of predicted labels, i.e. estimated targets as returned by a tagger.\\n\\n`references`: a list of lists of reference labels, i.e. the ground truth/target values.\"),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 1437}, page_content='`references`: a list of lists of reference labels, i.e. the ground truth/target values.\\n\\nIt can also take several optional arguments:\\n\\n`zero_division`: Which value to substitute as a metric value when encountering zero division. Should be one of [`0`,`1`,`\"warn\"`]. `\"warn\"` acts as `0`, but the warning is raised.\\n\\n\\n```python\\n>>> predictions = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'NOUN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'VERB\\', \\'SYM\\']]\\n>>> references = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'SYM\\']]\\n>>> poseval = evaluate.load(\"poseval\")\\n>>> results = poseval.compute(predictions=predictions, references=references)\\n>>> print(list(results.keys()))\\n[\\'ADP\\', \\'INTJ\\', \\'NOUN\\', \\'PROPN\\', \\'PUNCT\\', \\'SYM\\', \\'VERB\\', \\'accuracy\\', \\'macro avg\\', \\'weighted avg\\']\\n>>> print(results[\"accuracy\"])\\n0.8\\n>>> print(results[\"PROPN\"][\"recall\"])\\n0.5'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 2291}, page_content='```\\n\\n## Output values\\n\\nThis metric returns a a classification report as a dictionary with a summary of scores for overall and per type:\\n\\nOverall (weighted and macro avg):\\n\\n`accuracy`: the average [accuracy](https://huggingface.co/metrics/accuracy), on a scale between 0.0 and 1.0.\\n    \\n`precision`: the average [precision](https://huggingface.co/metrics/precision), on a scale between 0.0 and 1.0.\\n    \\n`recall`: the average [recall](https://huggingface.co/metrics/recall), on a scale between 0.0 and 1.0.\\n\\n`f1`: the average [F1 score](https://huggingface.co/metrics/f1), which is the harmonic mean of the precision and recall. It also has a scale of 0.0 to 1.0.\\n\\nPer type (e.g. `MISC`, `PER`, `LOC`,...):\\n\\n`precision`: the average [precision](https://huggingface.co/metrics/precision), on a scale between 0.0 and 1.0.\\n\\n`recall`: the average [recall](https://huggingface.co/metrics/recall), on a scale between 0.0 and 1.0.'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 3215}, page_content='`f1`: the average [F1 score](https://huggingface.co/metrics/f1), on a scale between 0.0 and 1.0.\\n\\n\\n## Examples \\n\\n```python\\n>>> predictions = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'NOUN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'VERB\\', \\'SYM\\']]\\n>>> references = [[\\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'PUNCT\\', \\'INTJ\\', \\'ADP\\', \\'PROPN\\', \\'PROPN\\', \\'SYM\\']]\\n>>> poseval = evaluate.load(\"poseval\")\\n>>> results = poseval.compute(predictions=predictions, references=references)\\n>>> print(list(results.keys()))\\n[\\'ADP\\', \\'INTJ\\', \\'NOUN\\', \\'PROPN\\', \\'PUNCT\\', \\'SYM\\', \\'VERB\\', \\'accuracy\\', \\'macro avg\\', \\'weighted avg\\']\\n>>> print(results[\"accuracy\"])\\n0.8\\n>>> print(results[\"PROPN\"][\"recall\"])\\n0.5'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 3865}, page_content='```\\n\\n## Limitations and bias\\n\\nIn contrast to [seqeval](https://github.com/chakki-works/seqeval), the poseval metric treats each token independently and computes the classification report over all concatenated sequences..\\n\\n\\n## Citation\\n\\n```bibtex\\n@article{scikit-learn,\\n title={Scikit-learn: Machine Learning in {P}ython},\\n author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n journal={Journal of Machine Learning Research},\\n volume={12},\\n pages={2825--2830},\\n year={2011}\\n}'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/poseval/README.md', 'start_index': 4588}, page_content='```\\n    \\n## Further References \\n- [README for seqeval at GitHub](https://github.com/chakki-works/seqeval)\\n- [Classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) \\n- [Issues with seqeval](https://stackoverflow.com/questions/71327693/how-to-disable-seqeval-label-formatting-for-pos-tagging)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 0}, page_content='--\\ntitle: \"Large Language Models: A New Moore\\'s Law?\"\\nthumbnail: /blog/assets/33_large_language_models/01_model_size.jpg\\nauthors:\\n- user: juliensimon\\n---\\n\\n# Large Language Models: A New Moore\\'s Law?\\n\\n\\n\\nA few days ago, Microsoft and NVIDIA [introduced](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) Megatron-Turing NLG 530B, a Transformer-based model hailed as \"*the world’s largest and most powerful generative language model*.\"\\n \\nThis is an impressive show of Machine Learning engineering, no doubt about it. Yet, should we be excited about this mega-model trend?  I, for one, am not. Here\\'s why.\\n\\n<kbd>\\n  <img src=\"assets/33_large_language_models/01_model_size.jpg\">\\n</kbd>\\n\\n### This is your Brain on Deep Learning'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 805}, page_content=\"### This is your Brain on Deep Learning\\n\\nResearchers estimate that the human brain contains an average of [86 billion neurons](https://pubmed.ncbi.nlm.nih.gov/19226510/) and 100 trillion synapses. It's safe to assume that not all of them are dedicated to language either. Interestingly, GPT-4 is [expected](https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/) to have about 100 trillion parameters... As crude as this analogy is, shouldn't we wonder whether building language models that are about the size of the human brain is the best long-term approach?\\n\\nOf course, our brain is a marvelous device, produced by millions of years of evolution, while Deep Learning models are only a few decades old. Still, our intuition should tell us that something doesn't compute (pun intended).\\n\\n### Deep Learning, Deep Pockets?\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 1609}, page_content='### Deep Learning, Deep Pockets?\\n\\nAs you would expect, training a 530-billion parameter model on humongous text datasets requires a fair bit of infrastructure. In fact, Microsoft and NVIDIA used hundreds of DGX A100 multi-GPU servers. At $199,000 a piece, and factoring in networking equipment, hosting costs, etc., anyone looking to replicate this experiment would have to spend close to $100 million dollars. Want fries with that?\\n\\nSeriously, which organizations have business use cases that would justify spending $100 million on Deep Learning infrastructure? Or even $10 million? Very few. So who are these models for, really?\\n\\n### That Warm Feeling is your GPU Cluster'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 2241}, page_content='### That Warm Feeling is your GPU Cluster\\n\\nFor all its engineering brilliance, training Deep Learning models on GPUs is a brute force technique. According to the spec sheet, each DGX server can consume up to 6.5 kilowatts. Of course, you\\'ll need at least as much cooling power in your datacenter (or your server closet). Unless you\\'re the Starks and need to keep Winterfell warm in winter, that\\'s another problem you\\'ll have to deal with. \\n\\nIn addition, as public awareness grows on climate and social responsibility issues, organizations need to account for their carbon footprint. According to this 2019 [study](https://arxiv.org/pdf/1906.02243.pdf) from the University of Massachusetts, \"*training BERT on GPU is roughly equivalent to a trans-American flight*\".\\n\\nBERT-Large has 340 million parameters. One can only extrapolate what the footprint of Megatron-Turing could be... People who know me wouldn\\'t call me a bleeding-heart environmentalist. Still, some numbers are hard to ignore.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 3233}, page_content=\"### So?\\n\\nAm I excited by Megatron-Turing NLG 530B and whatever beast is coming next? No. Do I think that the (relatively small) benchmark improvement is worth the added cost, complexity and carbon footprint? No. Do I think that building and promoting these huge models is helping organizations understand and adopt Machine Learning ? No.\\n\\nI'm left wondering what's the point of it all. Science for the sake of science? Good old marketing? Technological supremacy? Probably a bit of each. I'll leave them to it, then.\\n\\nInstead, let me focus on pragmatic and actionable techniques that you can all use to build high quality Machine Learning solutions.\\n\\n### Use Pretrained Models\\n\\nIn the vast majority of cases, you won't need a custom model architecture. Maybe you'll *want* a custom one (which is a different thing), but there be dragons. Experts only!\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 4086}, page_content=\"A good starting point is to look for [models](https://huggingface.co/models) that have been pretrained for the task you're trying to solve (say, [summarizing English text](https://huggingface.co/models?language=en&pipeline_tag=summarization&sort=downloads)).\\n\\nThen, you should quickly try out a few models to predict your own data. If metrics tell you that one works well enough, you're done! If you need a little more accuracy, you should consider fine-tuning the model (more on this in a minute).\\n\\n### Use Smaller Models\\n\\nWhen evaluating models, you should pick the smallest one that can deliver the accuracy you need. It will predict faster and require fewer hardware resources for training and inference. Frugality goes a long way.\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 4823}, page_content=\"It's nothing new either. Computer Vision practitioners will remember when [SqueezeNet](https://arxiv.org/abs/1602.07360) came out in 2017, achieving a 50x reduction in model size compared to [AlexNet](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html), while meeting or exceeding its accuracy. How clever that was!\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 5175}, page_content=\"Downsizing efforts are also under way in the Natural Language Processing community, using transfer learning techniques such as [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation). [DistilBERT](https://arxiv.org/abs/1910.01108) is perhaps its most widely known achievement. Compared to the original BERT model, it retains 97% of language understanding while being 40% smaller and 60% faster. You can try it [here](https://huggingface.co/distilbert-base-uncased). The same approach has been applied to other models, such as Facebook's [BART](https://arxiv.org/abs/1910.13461), and you can try DistilBART [here](https://huggingface.co/models?search=distilbart).\\n\\nRecent models from the [Big Science](https://bigscience.huggingface.co/) project are also very impressive. As visible in this graph included in the [research paper](https://arxiv.org/abs/2110.08207), their T0 model outperforms GPT-3 on many tasks while being 16x smaller.\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 6135}, page_content='<kbd>\\n  <img src=\"assets/33_large_language_models/02_t0.png\">\\n</kbd>\\n\\nYou can try T0 [here](https://huggingface.co/bigscience/T0pp). This is the kind of research we need more of!\\n\\n### Fine-Tune Models\\n\\nIf you need to specialize a model, there should be very few reasons to train it from scratch. Instead, you should fine-tune it, that is to say train it only for a few epochs on your own data. If you\\'re short on data, maybe of one these [datasets](https://huggingface.co/datasets) can get you started.\\n\\nYou guessed it, that\\'s another way to do transfer learning, and it\\'ll help you save on everything!\\n \\n* Less data to collect, store, clean and annotate,\\n* Faster experiments and iterations,\\n* Fewer resources required in production.\\n\\nIn other words: save time, save money, save hardware resources, save the world! \\n\\nIf you need a tutorial, the Hugging Face [course](https://huggingface.co/course) will get you started in no time.\\n\\n### Use Cloud-Based Infrastructure'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 7068}, page_content='### Use Cloud-Based Infrastructure\\n\\nLike them or not, cloud companies know how to build efficient infrastructure. Sustainability studies show that cloud-based infrastructure is more energy and carbon efficient than the alternative: see [AWS](https://sustainability.aboutamazon.com/environment/the-cloud), [Azure](https://azure.microsoft.com/en-us/global-infrastructure/sustainability), and [Google](https://cloud.google.com/sustainability). Earth.org [says](https://earth.org/environmental-impact-of-cloud-computing/) that while cloud infrastructure is not perfect, \"[*it\\'s] more energy efficient than the alternative and facilitates environmentally beneficial services and economic growth.*\"'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 7762}, page_content=\"Cloud certainly has a lot going for it when it comes to ease of use, flexibility and pay as you go. It's also a little greener than you probably thought. If you're short on GPUs, why not try fine-tune your Hugging Face models on [Amazon SageMaker](https://aws.amazon.com/sagemaker/), AWS' managed service for Machine Learning? We've got [plenty of examples](https://huggingface.co/docs/sagemaker/train) for you.\\n\\n### Optimize Your Models\\n\\nFrom compilers to virtual machines, software engineers have long used tools that automatically optimize their code for whatever hardware they're running on. \\n\\nHowever, the Machine Learning community is still struggling with this topic, and for good reason. Optimizing models for size and speed is a devilishly complex task, which involves techniques such as:\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 8561}, page_content=\"* Specialized hardware that speeds up training ([Graphcore](https://www.graphcore.ai/), [Habana](https://habana.ai/)) and inference ([Google TPU](https://cloud.google.com/tpu), [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)).\\n* Pruning: remove model parameters that have little or no impact on the predicted outcome.\\n* Fusion: merge model layers (say, convolution and activation).\\n* Quantization: storing model parameters in smaller values (say, 8 bits instead of 32 bits)\\n\\nFortunately, automated tools are starting to appear, such as the [Optimum](https://huggingface.co/hardware) open source library, and [Infinity](https://huggingface.co/infinity), a containerized solution that delivers Transformers accuracy at 1-millisecond latency.\\n\\n### Conclusion \\n\\nLarge language model size has been increasing 10x every year for the last few years. This is starting to look like another [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law).\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/large-language-models.md', 'start_index': 9527}, page_content=\"We've been there before, and we should know that this road leads to diminishing returns, higher cost, more complexity, and new risks. Exponentials tend not to end well. Remember [Meltdown and Spectre](https://meltdownattack.com/)? Do we want to find out what that looks like for AI?\\n\\nInstead of chasing trillion-parameter models (place your bets), wouldn't all be better off if we built practical and efficient solutions that all developers can use to solve real-world problems?\\n\\n*Interested in how Hugging Face can help your organization build and deploy production-grade Machine Learning solutions? Get in touch at [julsimon@huggingface.co](mailto:julsimon@huggingface.co) (no recruiters, no sales pitches, please).*\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md', 'start_index': 0}, page_content='!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# VisionTextDualEncoder\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md', 'start_index': 746}, page_content='-->\\n\\n# VisionTextDualEncoder\\n\\n## Overview\\n\\nThe [`VisionTextDualEncoderModel`] can be used to initialize a vision-text dual encoder model with\\nany pretrained vision autoencoding model as the vision encoder (*e.g.* [ViT](vit), [BEiT](beit), [DeiT](deit)) and any pretrained text autoencoding model as the text encoder (*e.g.* [RoBERTa](roberta), [BERT](bert)). Two projection layers are added on top of both the vision and text encoder to project the output embeddings\\nto a shared latent space. The projection layers are randomly initialized so the model should be fine-tuned on a\\ndownstream task. This model can be used to align the vision-text embeddings using CLIP like contrastive image-text\\ntraining and then can be used for zero-shot vision tasks such image-classification or retrieval.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/vision-text-dual-encoder.md', 'start_index': 1538}, page_content='In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how\\nleveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement on\\nnew zero-shot vision tasks such as image classification or retrieval.\\n\\n## VisionTextDualEncoderConfig\\n\\n[[autodoc]] VisionTextDualEncoderConfig\\n\\n## VisionTextDualEncoderProcessor\\n\\n[[autodoc]] VisionTextDualEncoderProcessor\\n\\n<frameworkcontent>\\n<pt>\\n\\n## VisionTextDualEncoderModel\\n\\n[[autodoc]] VisionTextDualEncoderModel\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## FlaxVisionTextDualEncoderModel\\n\\n[[autodoc]] FlaxVisionTextDualEncoderModel\\n    - __call__\\n\\n</tf>\\n<jax>\\n\\n## TFVisionTextDualEncoderModel\\n\\n[[autodoc]] TFVisionTextDualEncoderModel\\n    - call\\n\\n</jax>\\n</frameworkcontent>'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02d_dynamic-padding.md', 'start_index': 0}, page_content='hat is dynamic padding? In the \"Batching Inputs together\" video, we have seen that to be able to group inputs of different lengths in the same batch, we need to add padding tokens to all the short inputs until they are all of the same length. Here for instance, the longest sentence is the third one, and we need to add 5, 2 and 7 pad tokens to the other to have four sentences of the same lengths. When dealing with a whole dataset, there are various padding strategies we can apply. The most obvious one is to pad all the elements of the dataset to the same length: the length of the longest sample. This will then give us batches that all have the same shape determined by the maximum sequence length. The downside is that batches composed from short sentences will have a lot of padding tokens which introduce more computations in the model we ultimately don\\'t need. To avoid this, another strategy is to pad the elements when we batch them together, to the longest sentence inside the batch.'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02d_dynamic-padding.md', 'start_index': 903}, page_content=\"is to pad the elements when we batch them together, to the longest sentence inside the batch. This way batches composed of short inputs will be smaller than the batch containing the longest sentence in the dataset. This will yield some nice speedup on CPU and GPU. The downside is that all batches will then have different shapes, which slows down training on other accelerators like TPUs. Let's see how to apply both strategies in practice. We have actually seen how to apply fixed padding in the Datasets Overview video, when we preprocessed the MRPC dataset: after loading the dataset and tokenizer, we applied the tokenization to all the dataset with padding and truncation to make all samples of length 128. As a result, if we pass this dataset to a PyTorch DataLoader, we get batches of shape batch size (here 16) by 128. To apply dynamic padding, we must defer the padding to the batch preparation, so we remove that part from our tokenize function. We still leave the truncation part so that\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02d_dynamic-padding.md', 'start_index': 1809}, page_content='so we remove that part from our tokenize function. We still leave the truncation part so that inputs that are bigger than the maximum length accepted by the model (usually 512) get truncated to that length. Then we pad our samples dynamically by using a data collator. Those classes in the Transformers library are responsible for applying all the final processing needed before forming a batch, here DataCollatorWithPadding will pad the samples to the maximum length inside the batch of sentences. We pass it to the PyTorch DataLoader as a collate function, then observe that the batches generated have various lenghs, all way below the 128 from before. Dynamic batching will almost always be faster on CPUs and GPUs, so you should apply it if you can. Remember to switch back to fixed padding however if you run your training script on TPU or need batches of fixed shapes.'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Kandinsky 3'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 584}, page_content=\"# Kandinsky 3\\n\\nKandinsky 3 is created by [Vladimir Arkhipkin](https://github.com/oriBetelgeuse),[Anastasia Maltseva](https://github.com/NastyaMittseva),[Igor Pavlov](https://github.com/boomb0om),[Andrei Filatov](https://github.com/anvilarth),[Arseniy Shakhmatov](https://github.com/cene555),[Andrey Kuznetsov](https://github.com/kuznetsoffandrey),[Denis Dimitrov](https://github.com/denndimitrov), [Zein Shaheen](https://github.com/zeinsh)\\n\\nThe description from it's Github page: \\n\\n*Kandinsky 3.0 is an open-source text-to-image diffusion model built upon the Kandinsky2-x model family. In comparison to its predecessors, enhancements have been made to the text understanding and visual quality of the model, achieved by increasing the size of the text encoder and Diffusion U-Net models, respectively.*\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 1389}, page_content='Its architecture includes 3 main components:\\n1. [FLAN-UL2](https://huggingface.co/google/flan-ul2), which is an encoder decoder model based on the T5 architecture. \\n2. New U-Net architecture featuring BigGAN-deep blocks doubles depth while maintaining the same number of parameters.\\n3. Sber-MoVQGAN is a decoder proven to have superior results in image restoration.\\n\\n\\n\\nThe original codebase can be found at [ai-forever/Kandinsky-3](https://github.com/ai-forever/Kandinsky-3).\\n\\n<Tip>\\n\\nCheck out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\\n\\n</Tip>\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky3.md', 'start_index': 2075}, page_content='</Tip>\\n\\n<Tip>\\n\\nMake sure to check out the schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## Kandinsky3Pipeline\\n\\n[[autodoc]] Kandinsky3Pipeline\\n\\t- all\\n\\t- __call__\\n\\n## Kandinsky3Img2ImgPipeline\\n\\n[[autodoc]] Kandinsky3Img2ImgPipeline\\n\\t- all\\n\\t- __call__'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 1}, page_content='Datasets server - worker\\n\\n> Workers that pre-compute and cache the response to /splits, /first-rows, /parquet, /info and /size.\\n\\n## Configuration\\n\\nUse environment variables to configure the workers. The prefix of each environment variable gives its scope.\\n\\n### Uvicorn\\n\\nThe following environment variables are used to configure the Uvicorn server (`WORKER_UVICORN_` prefix). It is used for the /healthcheck and the /metrics endpoints:\\n\\n- `WORKER_UVICORN_HOSTNAME`: the hostname. Defaults to `\"localhost\"`.\\n- `WORKER_UVICORN_NUM_WORKERS`: the number of uvicorn workers. Defaults to `2`.\\n- `WORKER_UVICORN_PORT`: the port. Defaults to `8000`.\\n\\n### Prometheus\\n\\n- `PROMETHEUS_MULTIPROC_DIR`: the directory where the uvicorn workers share their prometheus metrics. See https://github.com/prometheus/client_python#multiprocess-mode-eg-gunicorn. Defaults to empty, in which case every uvicorn worker manages its own metrics, and the /metrics endpoint returns the metrics of a random worker.'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 986}, page_content='## Worker configuration\\n\\nSet environment variables to configure the worker.'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 1063}, page_content='- `WORKER_CONTENT_MAX_BYTES`: the maximum size in bytes of the response content computed by a worker (to prevent returning big responses in the REST API). Defaults to `10_000_000`.\\n- `WORKER_DIFFICULTY_MAX`: the maximum difficulty of the jobs to process. Defaults to None.\\n- `WORKER_DIFFICULTY_MIN`: the minimum difficulty of the jobs to process. Defaults to None.\\n- `WORKER_HEARTBEAT_INTERVAL_SECONDS`: the time interval between two heartbeats. Each heartbeat updates the job \"last_heartbeat\" field in the queue. Defaults to `60` (1 minute).\\n- `WORKER_JOB_TYPES_BLOCKED`: comma-separated list of job types that will not be processed, e.g. \"dataset-config-names,dataset-split-names\". If empty, no job type is blocked. Defaults to empty.\\n- `WORKER_JOB_TYPES_ONLY`: comma-separated list of the non-blocked job types to process, e.g. \"dataset-config-names,dataset-split-names\". If empty, the worker processes all the non-blocked jobs. Defaults to empty.'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 2014}, page_content='- `WORKER_KILL_LONG_JOB_INTERVAL_SECONDS`: the time interval at which the worker looks for long jobs to kill them. Defaults to `60` (1 minute).\\n- `WORKER_KILL_ZOMBIES_INTERVAL_SECONDS`: the time interval at which the worker looks for zombie jobs to kill them. Defaults to `600` (10 minutes).\\n- `WORKER_MAX_DISK_USAGE_PCT`: maximum disk usage of every storage disk in the list (in percentage) to allow a job to start. Set to 0 to disable the test. Defaults to 90.\\n- `WORKER_MAX_JOB_DURATION_SECONDS`: the maximum duration allowed for a job to run. If the job runs longer, it is killed (see `WORKER_KILL_LONG_JOB_INTERVAL_SECONDS`). Defaults to `1200` (20 minutes).\\n- `WORKER_MAX_LOAD_PCT`: maximum load of the machine (in percentage: the max between the 1m load and the 5m load divided by the number of CPUs \\\\*100) allowed to start a job. Set to 0 to disable the test. Defaults to 70.'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 2898}, page_content=\"- `WORKER_MAX_MEMORY_PCT`: maximum memory (RAM + SWAP) usage of the machine (in percentage) allowed to start a job. Set to 0 to disable the test. Defaults to 80.\\n- `WORKER_MAX_MISSING_HEARTBEATS`: the number of hearbeats a job must have missed to be considered a zombie job. Defaults to `5`.\\n- `WORKER_SLEEP_SECONDS`: wait duration in seconds at each loop iteration before checking if resources are available and processing a job if any is available. Note that the loop doesn't wait just after finishing a job: the next job is immediately processed. Defaults to `15`.\\n- `WORKER_STORAGE_PATHS`: comma-separated list of paths to check for disk usage. Defaults to empty.\"),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 3567}, page_content=\"Also, it's possible to force the parent directory in which the temporary files (as the current job state file and its associated lock file) will be created by setting `TMPDIR` to a writable directory. If not set, the worker will use the default temporary directory of the system, as described in https://docs.python.org/3/library/tempfile.html#tempfile.gettempdir.\\n\\n### Datasets based worker\\n\\nSet environment variables to configure the datasets-based worker (`DATASETS_BASED_` prefix):\\n\\n- `DATASETS_BASED_HF_DATASETS_CACHE`: directory where the `datasets` library will store the cached datasets' data. If not set, the datasets library will choose the default location. Defaults to None.\\n\\nAlso, set the modules cache configuration for the datasets-based worker. See [../../libs/libcommon/README.md](../../libs/libcommon/README.md). Note that this variable has no `DATASETS_BASED_` prefix:\"),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 4456}, page_content='- `HF_MODULES_CACHE`: directory where the `datasets` library will store the cached dataset scripts. If not set, the datasets library will choose the default location. Defaults to None.\\n\\nNote that both directories will be appended to `WORKER_STORAGE_PATHS` (see [../../libs/libcommon/README.md](../../libs/libcommon/README.md)) to hold the workers when the disk is full.\\n\\n### Numba library\\n\\nNumba requires setting the `NUMBA_CACHE_DIR` environment variable to a writable directory to cache the compiled functions. Required on cloud infrastructure (see https://stackoverflow.com/a/63367171/7351594):\\n\\n- `NUMBA_CACHE_DIR`: directory where the `numba` decorators (used by `librosa`) can write cache.\\n\\nNote that this directory will be appended to `WORKER_STORAGE_PATHS` (see [../../libs/libcommon/README.md](../../libs/libcommon/README.md)) to hold the workers when the disk is full.\\n\\n### Huggingface_hub library'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 5336}, page_content='### Huggingface_hub library\\n\\nIf the Hub is not https://huggingface.co (i.e., if you set the `COMMON_HF_ENDPOINT` environment variable), you must set the `HF_ENDPOINT` environment variable to the same value. See https://github.com/huggingface/datasets/pull/5196#issuecomment-1322191411 for more details:\\n\\n- `HF_ENDPOINT`: the URL of the Hub. Defaults to `https://huggingface.co`.\\n\\n### First rows worker\\n\\nSet environment variables to configure the `first-rows` worker (`FIRST_ROWS_` prefix):'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 5739}, page_content='Set environment variables to configure the `first-rows` worker (`FIRST_ROWS_` prefix):\\n\\n- `FIRST_ROWS_MAX_BYTES`: the max size of the /first-rows response in bytes. Defaults to `1_000_000` (1 MB).\\n- `FIRST_ROWS_MAX_NUMBER`: the max number of rows fetched by the worker for the split and provided in the /first-rows response. Defaults to `100`.\\n- `FIRST_ROWS_MIN_CELL_BYTES`: the minimum size in bytes of a cell when truncating the content of a row (see `FIRST_ROWS_ROWS_MAX_BYTES`). Below this limit, the cell content will not be truncated. Defaults to `100`.\\n- `FIRST_ROWS_MIN_NUMBER`: the min number of rows fetched by the worker for the split and provided in the /first-rows response. Defaults to `10`.\\n- `FIRST_ROWS_COLUMNS_MAX_NUMBER`: the max number of columns (features) provided in the /first-rows response. If the number of columns is greater than the limit, an error is returned. Defaults to `1_000`.'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 6651}, page_content='Also, set the assets-related configuration for the first-rows worker. See [../../libs/libcommon/README.md](../../libs/libcommon/README.md).\\n\\n### Parquet and info worker\\n\\nSet environment variables to configure the `parquet-and-info` worker (`PARQUET_AND_INFO_` prefix):'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 6921}, page_content='- `PARQUET_AND_INFO_COMMIT_MESSAGE`: the git commit message when the worker uploads the parquet files to the Hub. Defaults to `Update parquet files`.\\n- `PARQUET_AND_INFO_COMMITTER_HF_TOKEN`: the HuggingFace token to commit the parquet files to the Hub. The token must be an app token associated with a user that has the right to 1. create the `refs/convert/parquet` branch (see `PARQUET_AND_INFO_TARGET_REVISION`) and 2. push commits to it on any dataset. [Datasets maintainers](https://huggingface.co/datasets-maintainers) members have these rights. The token must have permission to write. If not set, the worker will fail. Defaults to None.\\n- `PARQUET_AND_INFO_MAX_DATASET_SIZE_BYTES`: the maximum size in bytes of the dataset to pre-compute the parquet files. Bigger datasets, or datasets without that information, are partially streamed to get parquet files up to this value. Defaults to `100_000_000`.'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 7829}, page_content='- `PARQUET_AND_INFO_MAX_EXTERNAL_DATA_FILES`: the maximum number of external files of the datasets. Bigger datasets, or datasets without that information, are partially streamed to get parquet files up to `PARQUET_AND_INFO_MAX_DATASET_SIZE_BYTES` bytes. Defaults to `10_000`.\\n- `PARQUET_AND_INFO_MAX_ROW_GROUP_BYTE_SIZE_FOR_COPY`: the maximum size in bytes of the row groups of parquet datasets that are copied to the target revision. Bigger datasets, or datasets without that information, are partially streamed to get parquet files up to `PARQUET_AND_INFO_MAX_DATASET_SIZE_BYTES` bytes. Defaults to `100_000_000`.\\n- `PARQUET_AND_INFO_SOURCE_REVISION`: the git revision of the dataset to use to prepare the parquet files. Defaults to `main`.\\n- `PARQUET_AND_INFO_TARGET_REVISION`: the git revision of the dataset where to store the parquet files. Make sure the committer token (`PARQUET_AND_INFO_COMMITTER_HF_TOKEN`) has the permission to write there. Defaults to `refs/convert/parquet`.'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 8817}, page_content='- `PARQUET_AND_INFO_URL_TEMPLATE`: the URL template to build the parquet file URLs. Defaults to `/datasets/%s/resolve/%s/%s`.'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 8944}, page_content='### Duckdb Index worker\\n\\nSet environment variables to configure the `duckdb-index` worker (`DUCKDB_INDEX_` prefix):'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 9061}, page_content=\"- `DUCKDB_INDEX_CACHE_DIRECTORY`: directory where the temporal duckdb index files are stored. Defaults to empty.\\n- `DUCKDB_INDEX_COMMIT_MESSAGE`: the git commit message when the worker uploads the duckdb index file to the Hub. Defaults to `Update duckdb index file`.\\n- `DUCKDB_INDEX_COMMITTER_HF_TOKEN`: the HuggingFace token to commit the duckdb index file to the Hub. The token must be an app token associated with a user that has the right to 1. create the `refs/convert/parquet` branch (see `DUCKDB_INDEX_TARGET_REVISION`) and 2. push commits to it on any dataset. [Datasets maintainers](https://huggingface.co/datasets-maintainers) members have these rights. The token must have permission to write. If not set, the worker will fail. Defaults to None.\\n- `DUCKDB_INDEX_MAX_DATASET_SIZE_BYTES`: the maximum size in bytes of the dataset's parquet files to index. Datasets with bigger size are ignored. Defaults to `100_000_000`.\"),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 9992}, page_content='- `DUCKDB_INDEX_TARGET_REVISION`: the git revision of the dataset where to store the duckdb index file. Make sure the committer token (`DUCKDB_INDEX_COMMITTER_HF_TOKEN`) has the permission to write there. Defaults to `refs/convert/parquet`.\\n- `DUCKDB_INDEX_URL_TEMPLATE`: the URL template to build the duckdb index file URL. Defaults to `/datasets/%s/resolve/%s/%s`.\\n- `DUCKDB_INDEX_EXTENSIONS_DIRECTORY`: directory where the duckdb extensions will be downloaded. Defaults to empty.'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 10476}, page_content=\"### Descriptive statistics worker\\n\\nSet environment variables to configure the `descriptive-statistics` worker (`DESCRIPTIVE_STATISTICS_` prefix):\\n\\n- `DESCRIPTIVE_STATISTICS_CACHE_DIRECTORY`: directory to which a dataset in parquet format is downloaded. Defaults to empty.\\n- `DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS`: number of histogram bins (see examples below for more info).\\n- `DESCRIPTIVE_STATISTICS_MAX_PARQUET_SIZE_BYTES`: maximum size in bytes of the dataset's parquet files to compute statistics. Datasets with bigger size are ignored. Defaults to `100_000_000`.\\n\\n#### How descriptive statistics are computed \\n\\nDescriptive statistics are currently computed for the following data types: strings, floats, and ints (including `ClassLabel` int). \\nResponse has two fields: `num_examples` and `statistics`. `statistics` field is a list of dicts with three keys: `column_name`, `column_type`, and `column_statistics`.\"),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 11399}, page_content='`column_type` is one of the following values:\\n* `class_label` - for `datasets.ClassLabel` feature\\n* `float` - for float dtypes (\"float16\", \"float32\", \"float64\")\\n* `int` - for integer dtypes (\"int8\", \"int16\", \"int32\", \"int64\", \"uint8\", \"uint16\", \"uint32\", \"uint64\")\\n* `string_label` - for string dtypes (\"string\", \"large_string\") - if there are less than or equal to `MAX_NUM_STRING_LABELS` unique values (hardcoded in worker\\'s code, for now it\\'s 30)\\n* `string_text` - for string dtypes (\"string\", \"large_string\") - if there are more than `MAX_NUM_STRING_LABELS` unique values\\n* `bool` - for boolean dtype (\"bool\")\\n\\n`column_statistics` content depends on the feature type, see examples below.\\n##### class_label\\n\\n<details><summary>example: </summary>\\n<p>'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 12110}, page_content='<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    \"column_name\": \"class_col\",\\n    \"column_type\": \"class_label\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"no_label_count\": 0,  # number of -1 values - special value of the `datasets` lib to encode `no label` \\n        \"no_label_proportion\": 0.0,\\n        \"n_unique\": 5,  # number of unique values (excluding `no label` and nan)\\n        \"frequencies\": {   # mapping value -> its count\\n            \"this\": 19834,\\n            \"are\": 20159,\\n            \"random\": 20109,\\n            \"words\": 20172,\\n            \"test\": 19726\\n        }\\n    }\\n}'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 12752}, page_content='```\\n</p>\\n</details> \\n\\n##### float\\n\\nBin size for histogram is counted as `(max_value - min_value) / DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS`\\n\\n<details><summary>example: </summary>\\n<p>'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 12895}, page_content='<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    \"column_name\": \"delay\",\\n    \"column_type\": \"float\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": -10.206,\\n        \"max\": 8.48053,\\n        \"mean\": 2.10174,\\n        \"median\": 3.4012,\\n        \"std\": 3.12487,\\n        \"histogram\": {\\n            \"hist\": [\\n                2,\\n                34,\\n                256,\\n                15198,\\n                9037,\\n                2342,\\n                12743,\\n                45114,\\n                14904,\\n                370\\n            ],\\n            \"bin_edges\": [\\n                -10.206,\\n                -8.33734,\\n                -6.46869,\\n                -4.60004,\\n                -2.73139,\\n                -0.86273,\\n                1.00592,\\n                2.87457,\\n                4.74322,\\n                6.61188,\\n                8.48053  # includes maximum value, so len is always len(hist) + 1\\n            ]\\n        }\\n    }\\n}'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 13883}, page_content=\"```\\n</p>\\n</details> \\n\\n##### int\\n\\nAs bin edges for integer values also must be integers, bin size is counted as `np.ceil((max_value - min_value + 1) / DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS)`. Rounding up means that there might be smaller number of bins in response then provided `DESCRIPTIVE_STATISTICS_HISTOGRAM_NUM_BINS`. The last bin's size might be smaller than that of the others if the feature's range is not divisible by the rounded bin size. \\n\\n<details><summary>examples: </summary>\\n<p>\"),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 14382}, page_content='```python\\n{\\n    \"column_name\": \"direction\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 0,\\n        \"max\": 1,\\n        \"mean\": 0.49925,\\n        \"median\": 0.0,\\n        \"std\": 0.5,\\n        \"histogram\": {\\n            \"hist\": [\\n                50075,\\n                49925\\n            ],\\n            \"bin_edges\": [\\n                0,\\n                1,\\n                1  # if the last value is equal to the last but one, that means that this bin includes only this value\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"hour\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 0,\\n        \"max\": 23,\\n        \"mean\": 13.44402,\\n        \"median\": 14.0,\\n        \"std\": 5.49455,\\n        \"histogram\": {\\n            \"hist\": [\\n                2694,\\n                2292,\\n                16785,\\n                16326,\\n                16346,'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 15291}, page_content='2292,\\n                16785,\\n                16326,\\n                16346,\\n                17809,\\n                16546,\\n                11202\\n            ],\\n            \"bin_edges\": [\\n                0,\\n                3,\\n                6,\\n                9,\\n                12,\\n                15,\\n                18,\\n                21,\\n                23\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"humidity\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 54,\\n        \"max\": 99,\\n        \"mean\": 83.89878,\\n        \"median\": 85.0,\\n        \"std\": 8.65174,\\n        \"histogram\": {\\n            \"hist\": [\\n                554,\\n                1662,\\n                3823,\\n                6532,\\n                12512,\\n                17536,\\n                23871,\\n                20355,\\n                12896,\\n                259\\n            ],\\n            \"bin_edges\": [\\n                54,'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 16200}, page_content='259\\n            ],\\n            \"bin_edges\": [\\n                54,\\n                59,\\n                64,\\n                69,\\n                74,\\n                79,\\n                84,\\n                89,\\n                94,\\n                99,\\n                99\\n            ]\\n        }\\n    }\\n},\\n{\\n    \"column_name\": \"weekday\",\\n    \"column_type\": \"int\",\\n    \"column_statistics\": {\\n        \"nan_count\": 0,\\n        \"nan_proportion\": 0.0,\\n        \"min\": 0,\\n        \"max\": 6,\\n        \"mean\": 3.08063,\\n        \"median\": 3.0,\\n        \"std\": 1.90347,\\n        \"histogram\": {\\n            \"hist\": [\\n                10282,\\n                15416,\\n                15291,\\n                15201,\\n                15586,\\n                15226,\\n                12998\\n            ],\\n            \"bin_edges\": [\\n                0,\\n                1,\\n                2,\\n                3,\\n                4,\\n                5,\\n                6,\\n                6\\n            ]\\n        }\\n    }\\n}'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 17176}, page_content='```\\n\\n</p>\\n</details>\\n\\n##### string_label\\n\\nIf the number of unique values in a column (within requested split) is <= `MAX_NUM_STRING_LABELS` (currently 30), the column is considered to be a category and the categories counts are computed.\\n\\n<details><summary>examples: </summary>\\n<p>\\n\\n```python\\n{\\n    \\'column_name\\': \\'string_col\\',\\n    \\'column_type\\': \\'string_label\\',\\n    \\'column_statistics\\': \\n        {\\n            \"nan_count\": 0,\\n            \"nan_proportion\": 0.0,\\n            \"n_unique\": 5,  # number of unique values (excluding nan)\\n            \"frequencies\": {   # mapping value -> its count\\n                \"this\": 19834,\\n                \"are\": 20159,\\n                \"random\": 20109,\\n                \"words\": 20172,\\n                \"test\": 19726\\n        }\\n    }\\n}'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 17942}, page_content='```\\n</p>\\n</details>\\n\\n##### string_text\\n\\nIf the number of unique values in a column (within requested split) is > `MAX_NUM_STRING_LABELS` (currently 30), the column is considered to be text and the distribution of text **lengths** is computed.\\n\\n<details><summary>example: </summary>\\n<p>'),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 18186}, page_content=\"<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    'column_name': 'text_col',\\n    'column_type': 'string_text',\\n    'column_statistics': {\\n        'max': 296,\\n        'mean': 97.46649,\\n        'median': 88.0,\\n        'min': 11,\\n        'nan_count': 0,\\n        'nan_proportion': 0.0,\\n        'std': 55.82714,\\n        'histogram': {\\n            'bin_edges': [\\n                11,\\n                40,\\n                69,\\n                98,\\n                127,\\n                156,\\n                185,\\n                214,\\n                243,\\n                272,\\n                296\\n            ],\\n            'hist': [\\n                171,\\n                224,\\n                235,\\n                180,\\n                102,\\n                99,\\n                53,\\n                28,\\n                10,\\n                2\\n               ]\\n             },\\n    }\\n}\"),\n",
              " Document(metadata={'source': 'huggingface/datasets-server/blob/main/services/worker/README.md', 'start_index': 19059}, page_content=\"```\\n</p>\\n</details>\\n\\n##### bool\\n\\n<details><summary>example: </summary>\\n<p>\\n\\n```python\\n{\\n    'column_name': 'bool__nan_column', \\n    'column_type': 'bool', \\n    'column_statistics': \\n        {\\n            'nan_count': 3, \\n            'nan_proportion': 0.15, \\n            'frequencies': {\\n                'False': 7, \\n                'True': 10\\n            }\\n        }\\n}\\n```\\n</p>\\n</details>\\n\\n\\n\\n### Splits worker\\n\\nThe `splits` worker does not need any additional configuration.\\n\\n### Common\\n\\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\"),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 1}, page_content='Differences between Dataset and IterableDataset\\n\\nThere are two types of dataset objects, a [`Dataset`] and an [`IterableDataset`].\\nWhichever type of dataset you choose to use or create depends on the size of the dataset.\\nIn general, an [`IterableDataset`] is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and speed advantages, while a [`Dataset`] is great for everything else.\\nThis page will compare the differences between a [`Dataset`] and an [`IterableDataset`] to help you pick the right dataset object for you.\\n\\n## Downloading and streaming\\n\\nWhen you have a regular [`Dataset`], you can access it using `my_dataset[0]`. This provides random access to the rows.\\nSuch datasets are also called \"map-style\" datasets.\\nFor example you can download ImageNet-1k like this and access any row:\\n\\n```python\\nfrom datasets import load_dataset\\n\\nimagenet = load_dataset(\"imagenet-1k\", split=\"train\")  # downloads the full dataset\\nprint(imagenet[0])'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 966}, page_content='```\\n\\nBut one caveat is that you must have the entire dataset stored on your disk or in memory, which blocks you from accessing datasets bigger than the disk.\\nBecause it can become inconvenient for big datasets, there exists another type of dataset, the [`IterableDataset`].\\nWhen you have an `IterableDataset`, you can access it using a `for` loop to load the data progressively as you iterate over the dataset.\\nThis way, only a small fraction of examples is loaded in memory, and you don\\'t write anything on disk.\\n\\nFor example, you can stream the ImageNet-1k dataset without downloading it on disk:\\n\\n```python\\nfrom datasets import load_dataset\\n\\nimagenet = load_dataset(\"imagenet-1k\", split=\"train\", streaming=True)  # will start loading the data when iterated over\\nfor example in imagenet:\\n    print(example)\\n    break'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 1785}, page_content='```\\n\\nStreaming can read online data without writing any file to disk.\\nFor example, you can stream datasets made out of multiple shards, each of which is hundreds of gigabytes like [C4](https://huggingface.co/datasets/c4), [OSCAR](https://huggingface.co/datasets/oscar) or [LAION-2B](https://huggingface.co/datasets/laion/laion2B-en).\\nLearn more about how to stream a dataset in the [Dataset Streaming Guide](./stream).\\n\\nThis is not the only difference though, because the \"lazy\" behavior of an `IterableDataset` is also present when it comes to dataset creation and processing.\\n\\n## Creating map-style datasets and iterable datasets\\n\\nYou can create a [`Dataset`] using lists or dictionaries, and the data is entirely converted to Arrow so you can easily access any row:\\n```python\\nmy_dataset = Dataset.from_dict({\"col_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]})\\nprint(my_dataset[0])'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 2659}, page_content='```\\n\\nTo create an `IterableDataset` on the other hand, you must provide a \"lazy\" way to load the data.\\nIn Python, we generally use generator functions. These functions `yield` one example at a time, which means you can\\'t access a row by slicing it like a regular `Dataset`:\\n```python\\ndef my_generator(n):\\n    for i in range(n):\\n        yield {\"col_1\": i}\\n\\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs={\"n\": 10})\\nfor example in my_iterable_dataset:\\n    print(example)\\n    break\\n```\\n\\n## Loading local files entirely and progressively\\n\\nIt is possible to convert local or remote data files to an Arrow [`Dataset`] using [`load_dataset`]:\\n```python\\ndata_files = {\"train\": [\"path/to/data.csv\"]}\\nmy_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\")\\nprint(my_dataset[0])'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 3473}, page_content='```\\n\\nHowever, this requires a conversion step from CSV to Arrow format, which takes time and disk space if your dataset is big.\\n\\nTo save disk space and skip the conversion step, you can define an `IterableDataset` by streaming from the local files directly.\\nThis way, the data is read progressively from the local files as you iterate over the dataset:\\n\\n```python\\ndata_files = {\"train\": [\"path/to/data.csv\"]}\\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\\nfor example in my_iterable_dataset:  # this reads the CSV file progressively as you iterate over the dataset\\n    print(example)\\n    break'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 4116}, page_content='```\\n\\nMany file formats are supported, like CSV, JSONL, and Parquet, as well as image and audio files.\\nYou can find more information in the corresponding guides for loading [tabular](./tabular_load), [text](./nlp_load), [vision](./image_load), and [audio](./audio_load]) datasets.\\n\\n## Eager data processing and lazy data processing\\n\\nWhen you process a [`Dataset`] object using [`Dataset.map`], the entire dataset is processed immediately and returned.\\nThis is similar to how `pandas` works for example.\\n\\n```python\\nmy_dataset = my_dataset.map(process_fn)  # process_fn is applied on all the examples of the dataset\\nprint(my_dataset[0])'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 4750}, page_content='```\\n\\nOn the other hand, due to the \"lazy\" nature of an `IterableDataset`, calling [`IterableDataset.map`] does not apply your `map` function over the full dataset.\\nInstead, your `map` function is applied on-the-fly.\\n\\nBecause of that, you can chain multiple processing steps and they will all run at once when you start iterating over the dataset:\\n\\n```python\\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_1)\\nmy_iterable_dataset = my_iterable_dataset.filter(filter_fn)\\nmy_iterable_dataset = my_iterable_dataset.map(process_fn_2)\\n\\n# process_fn_1, filter_fn and process_fn_2 are applied on-the-fly when iterating over the dataset\\nfor example in my_iterable_dataset:  \\n    print(example)\\n    break'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 5454}, page_content='```\\n\\n## Exact and fast approximate shuffling\\n\\nWhen you shuffle a [`Dataset`] using [`Dataset.shuffle`], you apply an exact shuffling of the dataset.\\nIt works by taking a list of indices `[0, 1, 2, ... len(my_dataset) - 1]` and shuffling this list.\\nThen, accessing `my_dataset[0]` returns the row and index defined by the first element of the indices mapping that has been shuffled:\\n```python\\nmy_dataset = my_dataset.shuffle(seed=42)\\nprint(my_dataset[0])'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 5908}, page_content=\"```\\n\\nSince we don't have random access to the rows in the case of an `IterableDataset`, we can't use a shuffled list of indices and access a row at an arbitrary position.\\nThis prevents the use of exact shuffling.\\nInstead, a fast approximate shuffling is used in [`IterableDataset.shuffle`].\\nIt uses a shuffle buffer to sample random examples iteratively from the dataset.\\nSince the dataset is still read iteratively, it provides excellent speed performance:\\n```python\\nmy_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\\nfor example in my_iterable_dataset:\\n    print(example)\\n    break\"),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 6517}, page_content='```\\n\\nBut using a shuffle buffer is not enough to provide a satisfactory shuffling for machine learning model training. So [`IterableDataset.shuffle`] also shuffles the dataset shards if your dataset is made of multiple files or sources:\\n\\n```python\\n# Stream from the internet\\nmy_iterable_dataset = load_dataset(\"deepmind/code_contests\", split=\"train\", streaming=True)\\nmy_iterable_dataset.n_shards  # 39\\n\\n# Stream from local files\\ndata_files = {\"train\": [f\"path/to/data_{i}.csv\" for i in range(1024)]}\\nmy_iterable_dataset = load_dataset(\"csv\", data_files=data_files, split=\"train\", streaming=True)\\nmy_iterable_dataset.n_shards  # 1024\\n\\n# From a generator function\\ndef my_generator(n, sources):\\n    for source in sources:\\n        for example_id_for_current_source in range(n):\\n            yield {\"example_id\": f\"{source}_{example_id_for_current_source}\"}'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 7370}, page_content='gen_kwargs = {\"n\": 10, \"sources\": [f\"path/to/data_{i}\" for i in range(1024)]}\\nmy_iterable_dataset = IterableDataset.from_generator(my_generator, gen_kwargs=gen_kwargs)\\nmy_iterable_dataset.n_shards  # 1024'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 7575}, page_content=\"```\\n\\n## Speed differences\\n\\nRegular [`Dataset`] objects are based on Arrow which provides fast random access to the rows.\\nThanks to memory mapping and the fact that Arrow is an in-memory format, reading data from disk doesn't do expensive system calls and deserialization.\\nIt provides even faster data loading when iterating using a `for` loop by iterating on contiguous Arrow record batches.\\n\\nHowever as soon as your [`Dataset`] has an indices mapping (via [`Dataset.shuffle`] for example), the speed can become 10x slower.\\nThis is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you aren't reading contiguous chunks of data anymore.\\nTo restore the speed, you'd need to rewrite the entire dataset on your disk again using [`Dataset.flatten_indices`], which removes the indices mapping.\\nThis may take a lot of time depending of the size of your dataset though:\"),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 8497}, page_content='```python\\nmy_dataset[0]  # fast\\nmy_dataset = my_dataset.shuffle(seed=42)\\nmy_dataset[0]  # up to 10x slower\\nmy_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\\nmy_dataset[0]  # fast again'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 8743}, page_content='```\\n\\n\\nIn this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approximate shuffling method [`IterableDataset.shuffle`].\\nIt only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal.\\nYou can also reshuffle the dataset easily:\\n\\n```python\\nfor example in enumerate(my_iterable_dataset):  # fast\\n    pass\\n\\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\\n\\nfor example in enumerate(shuffled_iterable_dataset):  # as fast as before\\n    pass\\n\\nshuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=1337, buffer_size=100)  # reshuffling using another seed is instantaneous\\n\\nfor example in enumerate(shuffled_iterable_dataset):  # still as fast as before\\n    pass'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/docs/source/about_mapstyle_vs_iterable.mdx', 'start_index': 9529}, page_content='```\\n\\nIf you\\'re using your dataset on multiple epochs, the effective seed to shuffle the shards order in the shuffle buffer is `seed + epoch`.\\nIt makes it easy to reshuffle a dataset between epochs:\\n```python\\nfor epoch in range(n_epochs):\\n    my_iterable_dataset.set_epoch(epoch)\\n    for example in my_iterable_dataset:  # fast + reshuffled at each epoch using `effective_seed = seed + epoch`\\n        pass\\n```\\n\\n## Switch from map-style to iterable\\n\\nIf you want to benefit from the \"lazy\" behavior of an [`IterableDataset`] or their speed advantages, you can switch your map-style [`Dataset`] to an [`IterableDataset`]:\\n```python\\nmy_iterable_dataset = my_dataset.to_iterable_dataset()\\n```\\n\\nIf you want to shuffle your dataset or [use it with a PyTorch DataLoader](./use_with_pytorch#stream-data), we recommend generating a sharded [`IterableDataset`]:\\n```python\\nmy_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=1024)\\nmy_iterable_dataset.n_shards  # 1024\\n```'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/q-learning-recap.mdx', 'start_index': 1}, page_content='Q-Learning Recap [[q-learning-recap]]\\n\\n\\n*Q-Learning* **is the RL algorithm that** :\\n\\n- Trains a *Q-function*, an **action-value function** encoded, in internal memory, by a *Q-table* **containing all the state-action pair values.**\\n\\n- Given a state and action, our Q-function **will search its Q-table for the corresponding value.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\\n\\n- When the training is done, **we have an optimal Q-function, or, equivalently, an optimal Q-table.**\\n\\n- And if we **have an optimal Q-function**, we\\nhave an optimal policy, since we **know, for each state, the best action to take.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/q-learning-recap.mdx', 'start_index': 896}, page_content='But, in the beginning,\\xa0our **Q-table is useless since it gives arbitrary values for each state-action pair\\xa0(most of the time we initialize the Q-table to 0 values)**. But, as we\\xa0explore the environment and update our Q-table it will give us a better and better approximation.\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\\n\\nThis is the Q-Learning pseudocode:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Zero-shot object detection\\n\\n[[open-in-colab]]\\n\\nTraditionally, models used for [object detection](object_detection) require labeled image datasets for training,\\nand are limited to detecting the set of classes from the training data.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 986}, page_content='Zero-shot object detection is supported by the [OWL-ViT](../model_doc/owlvit) model which uses a different approach. OWL-ViT\\nis an open-vocabulary object detector. It means that it can detect objects in images based on free-text queries without\\nthe need to fine-tune the model on labeled datasets.\\n\\nOWL-ViT leverages multi-modal representations to perform open-vocabulary detection. It combines [CLIP](../model_doc/clip) with\\nlightweight object classification and localization heads. Open-vocabulary detection is achieved by embedding free-text queries with the text encoder of CLIP and using them as input to the object classification and localization heads.\\nassociate images and their corresponding textual descriptions, and ViT processes image patches as inputs. The authors\\nof OWL-ViT first trained CLIP from scratch and then fine-tuned OWL-ViT end to end on standard object detection datasets using\\na bipartite matching loss.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 1918}, page_content='With this approach, the model can detect objects based on textual descriptions without prior training on labeled datasets.\\n\\nIn this guide, you will learn how to use OWL-ViT:\\n- to detect objects based on text prompts\\n- for batch object detection\\n- for image-guided object detection\\n\\nBefore you begin, make sure you have all the necessary libraries installed:\\n\\n```bash\\npip install -q transformers'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 2313}, page_content='```\\n\\n## Zero-shot object detection pipeline\\n\\nThe simplest way to try out inference with OWL-ViT is to use it in a [`pipeline`]. Instantiate a pipeline\\nfor zero-shot object detection from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit):\\n\\n```python\\n>>> from transformers import pipeline\\n\\n>>> checkpoint = \"google/owlvit-base-patch32\"\\n>>> detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\\n```\\n\\nNext, choose an image you\\'d like to detect objects in. Here we\\'ll use the image of astronaut Eileen Collins that is\\na part of the [NASA](https://www.nasa.gov/multimedia/imagegallery/index.html) Great Images dataset.\\n\\n```py\\n>>> import skimage\\n>>> import numpy as np\\n>>> from PIL import Image\\n\\n>>> image = skimage.data.astronaut()\\n>>> image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\\n\\n>>> image'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 3162}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\" alt=\"Astronaut Eileen Collins\"/>\\n</div>\\n\\nPass the image and the candidate object labels to look for to the pipeline.\\nHere we pass the image directly; other suitable options include a local path to an image or an image url. We also pass text descriptions for all items we want to query the image for.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 3644}, page_content='```py\\n>>> predictions = detector(\\n...     image,\\n...     candidate_labels=[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\\n... )\\n>>> predictions\\n[{\\'score\\': 0.3571370542049408,\\n  \\'label\\': \\'human face\\',\\n  \\'box\\': {\\'xmin\\': 180, \\'ymin\\': 71, \\'xmax\\': 271, \\'ymax\\': 178}},\\n {\\'score\\': 0.28099656105041504,\\n  \\'label\\': \\'nasa badge\\',\\n  \\'box\\': {\\'xmin\\': 129, \\'ymin\\': 348, \\'xmax\\': 206, \\'ymax\\': 427}},\\n {\\'score\\': 0.2110239565372467,\\n  \\'label\\': \\'rocket\\',\\n  \\'box\\': {\\'xmin\\': 350, \\'ymin\\': -1, \\'xmax\\': 468, \\'ymax\\': 288}},\\n {\\'score\\': 0.13790413737297058,\\n  \\'label\\': \\'star-spangled banner\\',\\n  \\'box\\': {\\'xmin\\': 1, \\'ymin\\': 1, \\'xmax\\': 105, \\'ymax\\': 509}},\\n {\\'score\\': 0.11950037628412247,\\n  \\'label\\': \\'nasa badge\\',\\n  \\'box\\': {\\'xmin\\': 277, \\'ymin\\': 338, \\'xmax\\': 327, \\'ymax\\': 380}},\\n {\\'score\\': 0.10649408400058746,\\n  \\'label\\': \\'rocket\\',\\n  \\'box\\': {\\'xmin\\': 358, \\'ymin\\': 64, \\'xmax\\': 424, \\'ymax\\': 280}}]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 4523}, page_content='```\\n\\nLet\\'s visualize the predictions:\\n\\n```py\\n>>> from PIL import ImageDraw\\n\\n>>> draw = ImageDraw.Draw(image)\\n\\n>>> for prediction in predictions:\\n...     box = prediction[\"box\"]\\n...     label = prediction[\"label\"]\\n...     score = prediction[\"score\"]\\n\\n...     xmin, ymin, xmax, ymax = box.values()\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\\n\\n>>> image'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 4979}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_2.png\" alt=\"Visualized predictions on NASA image\"/>\\n</div>\\n\\n## Text-prompted zero-shot object detection by hand\\n\\nNow that you\\'ve seen how to use the zero-shot object detection pipeline, let\\'s replicate the same\\nresult manually.\\n\\nStart by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit).\\nHere we\\'ll use the same checkpoint as before:\\n\\n```py\\n>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\\n\\n>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\\n>>> processor = AutoProcessor.from_pretrained(checkpoint)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 5790}, page_content='```\\n\\nLet\\'s take a different image to switch things up.\\n\\n```py\\n>>> import requests\\n\\n>>> url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\\n>>> im = Image.open(requests.get(url, stream=True).raw)\\n>>> im\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/>\\n</div>\\n\\nUse the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\\nimage for the model by resizing and normalizing it, and a [`CLIPTokenizer`] that takes care of the text inputs.\\n\\n```py\\n>>> text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\\n>>> inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 6658}, page_content='```\\n\\nPass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\\nfeeding them to the model, you need to use the [`~OwlViTImageProcessor.post_process_object_detection`] method to make sure the predicted bounding\\nboxes have the correct coordinates relative to the original image:\\n\\n```py\\n>>> import torch\\n\\n>>> with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = torch.tensor([im.size[::-1]])\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\\n\\n>>> draw = ImageDraw.Draw(im)\\n\\n>>> scores = results[\"scores\"].tolist()\\n>>> labels = results[\"labels\"].tolist()\\n>>> boxes = results[\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{text_queries[label]}: {round(score,2)}\", fill=\"white\")'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 7659}, page_content='>>> im'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 7666}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\\n</div>\\n\\n## Batch processing\\n\\nYou can pass multiple sets of images and text queries to search for different (or same) objects in several images.\\nLet\\'s use both an astronaut image and the beach image together.\\nFor batch processing, you should pass text queries as a nested list to the processor and images as lists of PIL images,\\nPyTorch tensors, or NumPy arrays.\\n\\n```py\\n>>> images = [image, im]\\n>>> text_queries = [\\n...     [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\\n...     [\"hat\", \"book\", \"sunglasses\", \"camera\"],\\n... ]\\n>>> inputs = processor(text=text_queries, images=images, return_tensors=\"pt\")'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 8508}, page_content='```\\n\\nPreviously for post-processing you passed the single image\\'s size as a tensor, but you can also pass a tuple, or, in case\\nof several images, a list of tuples. Let\\'s create predictions for the two examples, and visualize the second one (`image_idx = 1`).\\n\\n```py\\n>>> with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = [x.size[::-1] for x in images]\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\\n\\n>>> image_idx = 1\\n>>> draw = ImageDraw.Draw(images[image_idx])\\n\\n>>> scores = results[image_idx][\"scores\"].tolist()\\n>>> labels = results[image_idx][\"labels\"].tolist()\\n>>> boxes = results[image_idx][\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\\n...     draw.text((xmin, ymin), f\"{text_queries[image_idx][label]}: {round(score,2)}\", fill=\"white\")\\n\\n>>> images[image_idx]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 9505}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\\n</div>\\n\\n## Image-guided object detection\\n\\nIn addition to zero-shot object detection with text queries, OWL-ViT offers image-guided object detection. This means\\nyou can use an image query to find similar objects in the target image.\\nUnlike text queries, only a single example image is allowed.\\n\\nLet\\'s take an image with two cats on a couch as a target image, and an image of a single cat\\nas a query:\\n\\n```py\\n>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\\n>>> image_target = Image.open(requests.get(url, stream=True).raw)\\n\\n>>> query_url = \"http://images.cocodataset.org/val2017/000000524280.jpg\"\\n>>> query_image = Image.open(requests.get(query_url, stream=True).raw)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 10411}, page_content='```\\n\\nLet\\'s take a quick look at the images:\\n\\n```py\\n>>> import matplotlib.pyplot as plt\\n\\n>>> fig, ax = plt.subplots(1, 2)\\n>>> ax[0].imshow(image_target)\\n>>> ax[1].imshow(query_image)\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_5.png\" alt=\"Cats\"/>\\n</div>\\n\\nIn the preprocessing step, instead of text queries, you now need to use `query_images`:\\n\\n```py\\n>>> inputs = processor(images=image_target, query_images=query_image, return_tensors=\"pt\")'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 10980}, page_content='```\\n\\nFor predictions, instead of passing the inputs to the model, pass them to [`~OwlViTForObjectDetection.image_guided_detection`]. Draw the predictions\\nas before except now there are no labels.\\n\\n```py\\n>>> with torch.no_grad():\\n...     outputs = model.image_guided_detection(**inputs)\\n...     target_sizes = torch.tensor([image_target.size[::-1]])\\n...     results = processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]\\n\\n>>> draw = ImageDraw.Draw(image_target)\\n\\n>>> scores = results[\"scores\"].tolist()\\n>>> boxes = results[\"boxes\"].tolist()\\n\\n>>> for box, score, label in zip(boxes, scores, labels):\\n...     xmin, ymin, xmax, ymax = box\\n...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"white\", width=4)\\n\\n>>> image_target'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md', 'start_index': 11748}, page_content='```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_6.png\" alt=\"Cats with bounding boxes\"/>\\n</div>\\n\\nIf you\\'d like to interactively try out inference with OWL-ViT, check out this demo:\\n\\n<iframe\\n\\tsrc=\"https://adirik-owl-vit.hf.space\"\\n\\tframeborder=\"0\"\\n\\twidth=\"850\"\\n\\theight=\"450\"\\n></iframe>'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 1}, page_content='Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.\\n\\n\\n### Q1: Which of the following interpretations of bias-variance tradeoff is the most accurate in the field of Reinforcement Learning?'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 394}, page_content='<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"The bias-variance tradeoff reflects how my model is able to generalize the knowledge to previously tagged data we give to the model during training time.\",\\n\\t\\t\\texplain: \"This is the traditional bias-variance tradeoff in Machine Learning. In our specific case of Reinforcement Learning, we don\\'t have previously tagged data, but only a reward signal.\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n   \\t\\t{\\n\\t\\t\\ttext: \"The bias-variance tradeoff reflects how well the reinforcement signal reflects the true reward the agent should get from the enviromment\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\t\\t\\n\\t]}\\n/>\\n\\n### Q2: Which of the following statements are true, when talking about models with bias and/or variance in RL?'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 1134}, page_content='<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"An unbiased reward signal returns rewards similar to the real / expected ones from the environment\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"A biased reward signal returns rewards similar to the real / expected ones from the environment\",\\n\\t\\t\\texplain: \"If a reward signal is biased, it means the reward signal we get differs from the real reward we should be getting from an environment\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"A reward signal with high variance has much noise in it and gets affected by, for example, stochastic (non constant) elements in the environment\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\t\\t\\n    \\t\\t{\\n\\t\\t\\ttext: \"A reward signal with low variance has much noise in it and gets affected by, for example, stochastic (non constant) elements in the environment\",'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 1990}, page_content='explain: \"If a reward signal has low variance, then it\\'s less affected by the noise of the environment and produce similar values regardless the random elements in the environment\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n\\t]}\\n/>'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 2211}, page_content='### Q3: Which of the following statements are true about Monte Carlo method?\\n\\n<Question\\n\\tchoices={[\\n\\t\\t{\\n\\t\\t\\ttext: \"It\\'s a sampling mechanism, which means we don\\'t analyze all the possible states, but a sample of those\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"It\\'s very resistant to stochasticity (random elements in the trajectory)\",\\n\\t\\t\\texplain: \"Monte Carlo randomly estimates everytime a sample of trajectories. However, even same trajectories can have different reward values if they contain stochastic elements\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"To reduce the impact of stochastic elements in Monte Carlo, we take `n` strategies and average them, reducing their individual impact\",\\n\\t\\t\\texplain: \"\",\\n\\t\\t\\tcorrect: true,\\n\\t\\t},\\t\\t    \\n\\t]}\\n/>\\n\\n### Q4: How would you describe, with your own words, the Actor-Critic Method (A2C)?\\n\\n<details>\\n<summary>Solution</summary>'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 3073}, page_content='<details>\\n<summary>Solution</summary>\\n\\nThe idea behind Actor-Critic is that we learn two function approximations:\\n1. A `policy` that controls how our agent acts (π)\\n2. A `value` function to assist the policy update by measuring how good the action taken is (q)\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/step2.jpg\" alt=\"Actor-Critic, step 2\"/>\\n\\n</details>\\n\\n### Q5: Which of the following statements are true about the Actor-Critic Method?'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 3481}, page_content='</details>\\n\\n### Q5: Which of the following statements are true about the Actor-Critic Method?\\n\\n<Question\\n\\tchoices={[\\n   \\t\\t {\\n\\t\\t\\ttext: \"The Critic does not learn any function during the training process\",\\n\\t\\t\\texplain: \"Both the Actor and the Critic function parameters are updated during training time\",\\n      \\t\\t\\tcorrect: false,\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\ttext: \"The Actor learns a policy function, while the Critic learns a value function\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\n    \\t\\t{\\n\\t\\t\\ttext: \"It adds resistance to stochasticity and reduces high variance\",\\n\\t\\t\\texplain: \"\",\\n      \\t\\t\\tcorrect: true,\\n\\t\\t},\\t    \\n\\t]}\\n/>\\n\\n\\n\\n### Q6: What is `Advantage` in the A2C method?\\n\\n<details>\\n<summary>Solution</summary>\\n\\nInstead of using directly the Action-Value function of the Critic as it is, we could use an `Advantage` function. The idea behind an `Advantage` function is that we calculate the relative advantage of an action compared to the others possible at a state, averaging them.'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit6/quiz.mdx', 'start_index': 4452}, page_content='In other words: how taking that action at a state is better compared to the average value of the state\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/advantage1.jpg\" alt=\"Advantage in A2C\"/>\\n\\n</details>\\n\\nCongrats on finishing this Quiz 🥳, if you missed some elements, take time to read the chapter again to reinforce (😏) your knowledge.'),\n",
              " Document(metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx', 'start_index': 1}, page_content='Access and read Logs\\n\\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the “Logs” tab of your Endpoint. \\n\\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\\n\\nThe Container Logs are only available when your Endpoint is in the “Running” state. \\n\\n_Note: If your Endpoint creation is in the “Failed” state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\\n\\n**Build Logs:**\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_build_logs.png\" alt=\"build logs\" />\\n\\n**Container Logs:**\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_logs.png\" alt=\"container logs\" />'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/examples_component/run.ipynb', 'start_index': 1}, page_content=\"Gradio Demo: examples_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo repo\\nimport os\\nos.mkdir('images')\\n!wget -q -O images/cheetah1.jpg https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/cheetah1.jpg\\n!wget -q -O images/lion.jpg https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/lion.jpg\\n!wget -q -O images/lion.webp https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/lion.webp\\n!wget -q -O images/logo.png https://github.com/gradio-app/gradio/raw/main/demo/examples_component/images/logo.png\\n```\"),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/examples_component/run.ipynb', 'start_index': 607}, page_content='```\\n\\n\\n```\\nimport gradio as gr\\nimport os\\n\\n\\ndef flip(i):\\n    return i.rotate(180)\\n\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        with gr.Column():\\n            img_i = gr.Image(label=\"Input Image\", type=\"pil\")\\n        with gr.Column():\\n            img_o = gr.Image(label=\"Output Image\")\\n    with gr.Row():\\n        btn = gr.Button(value=\"Flip Image\")\\n    btn.click(flip, inputs=[img_i], outputs=[img_o])\\n\\n    gr.Examples(\\n        [\\n            os.path.join(os.path.abspath(\\'\\'), \"images/cheetah1.jpg\"),\\n            os.path.join(os.path.abspath(\\'\\'), \"images/lion.jpg\"),\\n        ],\\n        img_i,\\n        img_o,\\n        flip,\\n    )\\n\\ndemo.launch()\\n\\n```'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/additional-readings.mdx', 'start_index': 1}, page_content='Additional Readings\\n\\nThese are **optional readings** if you want to go deeper.\\n\\n\\n## Introduction to Policy Optimization\\n\\n- [Part 3: Intro to Policy Optimization - Spinning Up documentation](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)\\n\\n\\n## Policy Gradient\\n\\n- [https://johnwlambert.github.io/policy-gradients/](https://johnwlambert.github.io/policy-gradients/)\\n- [RL - Policy Gradient Explained](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146)\\n- [Chapter 13, Policy Gradient Methods;  Reinforcement Learning, an introduction by Richard Sutton and Andrew G. Barto](http://incompleteideas.net/book/RLbook2020.pdf)\\n\\n## Implementation\\n\\n- [PyTorch Reinforce implementation](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\\n- [Implementations from DDPG to PPO](https://github.com/MrSyee/pg-is-all-you-need)'),\n",
              " Document(metadata={'source': 'huggingface/optimum/blob/main/docs/source/onnxruntime/package_reference/quantization.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Quantization\\n\\n## ORTQuantizer\\n\\n[[autodoc]] onnxruntime.quantization.ORTQuantizer\\n    - all'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/number_component/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: number_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Blocks() as demo:\\n    gr.Number()\\n\\ndemo.launch()\\n```'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: map_airbnb\\n### Display an interactive map of AirBnB locations with Plotly. Data is hosted on HuggingFace Datasets. \\n        \\n\\n\\n```\\n!pip install -q gradio plotly\\n```'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 181}, page_content='```\\nimport gradio as gr\\nimport plotly.graph_objects as go\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"gradio/NYC-Airbnb-Open-Data\", split=\"train\")\\ndf = dataset.to_pandas()\\n\\ndef filter_map(min_price, max_price, boroughs):\\n\\n    filtered_df = df[(df[\\'neighbourhood_group\\'].isin(boroughs)) & \\n          (df[\\'price\\'] > min_price) & (df[\\'price\\'] < max_price)]\\n    names = filtered_df[\"name\"].tolist()\\n    prices = filtered_df[\"price\"].tolist()\\n    text_list = [(names[i], prices[i]) for i in range(0, len(names))]\\n    fig = go.Figure(go.Scattermapbox(\\n            customdata=text_list,\\n            lat=filtered_df[\\'latitude\\'].tolist(),\\n            lon=filtered_df[\\'longitude\\'].tolist(),\\n            mode=\\'markers\\',\\n            marker=go.scattermapbox.Marker(\\n                size=6\\n            ),\\n            hoverinfo=\"text\",\\n            hovertemplate=\\'<b>Name</b>: %{customdata[0]}<br><b>Price</b>: $%{customdata[1]}\\'\\n        ))'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 1126}, page_content='fig.update_layout(\\n        mapbox_style=\"open-street-map\",\\n        hovermode=\\'closest\\',\\n        mapbox=dict(\\n            bearing=0,\\n            center=go.layout.mapbox.Center(\\n                lat=40.67,\\n                lon=-73.90\\n            ),\\n            pitch=0,\\n            zoom=9\\n        ),\\n    )\\n\\n    return fig\\n\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_price = gr.Number(value=250, label=\"Minimum Price\")\\n            max_price = gr.Number(value=1000, label=\"Maximum Price\")\\n        boroughs = gr.CheckboxGroup(choices=[\"Queens\", \"Brooklyn\", \"Manhattan\", \"Bronx\", \"Staten Island\"], value=[\"Queens\", \"Brooklyn\"], label=\"Select Boroughs:\")\\n        btn = gr.Button(value=\"Update Filter\")\\n        map = gr.Plot()\\n    demo.load(filter_map, [min_price, max_price, boroughs], map)\\n    btn.click(filter_map, [min_price, max_price, boroughs], map)\\n\\nif __name__ == \"__main__\":\\n    demo.launch()'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/map_airbnb/run.ipynb', 'start_index': 2063}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 1}, page_content=\"Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2Net Blocks](https://paperswithcode.com/method/res2net-block). The motivation is to be able to represent features at multiple scales. This is achieved through a novel building block for CNNs that constructs hierarchical residual-like connections within one single residual block. This represents multi-scale features at a granular level and increases the range of receptive fields for each network layer.\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model('res2net101_26w_4s', pretrained=True)\\n>>> model.eval()\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 683}, page_content='```\\n\\nTo load and preprocess the image:\\n\\n```py \\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 1478}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n>>> # Get imagenet class mappings\\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\n>>> urllib.request.urlretrieve(url, filename) \\n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\\n...     categories = [s.strip() for s in f.readlines()]\\n\\n>>> # Print top categories per image\\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\\n>>> for i in range(top5_prob.size(0)):\\n...     print(categories[top5_catid[i]], top5_prob[i].item())\\n>>> # prints class names and probabilities like:\\n>>> # [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 2272}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('res2net101_26w_4s', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 2835}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@article{Gao_2021,\\n   title={Res2Net: A New Multi-Scale Backbone Architecture},\\n   volume={43},\\n   ISSN={1939-3539},\\n   url={http://dx.doi.org/10.1109/TPAMI.2019.2938758},\\n   DOI={10.1109/tpami.2019.2938758},\\n   number={2},\\n   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},\\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\\n   author={Gao, Shang-Hua and Cheng, Ming-Ming and Zhao, Kai and Zhang, Xin-Yu and Yang, Ming-Hsuan and Torr, Philip},\\n   year={2021},\\n   month={Feb},\\n   pages={652–662}\\n}\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 3721}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 3726}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: Res2Net\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale Backbone Architecture'\\n    URL: https://paperswithcode.com/paper/res2net-a-new-multi-scale-backbone\\nModels:\\n- Name: res2net101_26w_4s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 10415881200\\n    Parameters: 45210000\\n    File Size: 181456059\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net101_26w_4s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L152\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 4696}, page_content=\"Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net101_26w_4s-02a759a1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.19%\\n      Top 5 Accuracy: 94.43%\\n- Name: res2net50_14w_8s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 5403546768\\n    Parameters: 25060000\\n    File Size: 100638543\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_14w_8s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 5486}, page_content=\"Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L196\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_14w_8s-6527dddc.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.14%\\n      Top 5 Accuracy: 93.86%\\n- Name: res2net50_26w_4s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 5499974064\\n    Parameters: 25700000\\n    File Size: 103110087\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_26w_4s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 6407}, page_content=\"ID: res2net50_26w_4s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L141\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_4s-06e79181.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.99%\\n      Top 5 Accuracy: 93.85%\\n- Name: res2net50_26w_6s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 8130156528\\n    Parameters: 37050000\\n    File Size: 148603239\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 7303}, page_content=\"- SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_26w_6s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L163\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_6s-19041792.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78.57%\\n      Top 5 Accuracy: 94.12%\\n- Name: res2net50_26w_8s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 10760338992\\n    Parameters: 48400000\\n    File Size: 194085165\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 8195}, page_content=\"- Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_26w_8s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L174\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_8s-2c7c9f12.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79.19%\\n      Top 5 Accuracy: 94.37%\\n- Name: res2net50_48w_2s\\n  In Collection: Res2Net\\n  Metadata:\\n    FLOPs: 5375291520\\n    Parameters: 25290000\\n    File Size: 101421406\\n    Architecture:\\n    - Batch Normalization\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/res2net.mdx', 'start_index': 9097}, page_content=\"Parameters: 25290000\\n    File Size: 101421406\\n    Architecture:\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - ReLU\\n    - Res2Net Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 4x Titan Xp GPUs\\n    ID: res2net50_48w_2s\\n    LR: 0.1\\n    Epochs: 100\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 256\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/res2net.py#L185\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_48w_2s-afed724a.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77.53%\\n      Top 5 Accuracy: 93.56%\\n-->\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': 0}, page_content=\"ow to slice and dice a dataset. Most of the time, the data you work with won’t be perfectly prepared for training models. In this video we’ll explore various features that Datasets provides to clean up your datasets. The Datasets library provides several built-in methods that allow you to wrangle your data. In this video we'll see how you can shuffle and split your data, select the rows you're interested in, tweak the columns, and apply processing functions with the map() method. Let's start with shuffling. It is generally a good idea to apply shuffling to the training set so that your model doesn't learn any artificial ordering in the data. If you want to shuffle the whole dataset, you can apply the appropriately named shuffle() method to your dataset. You can see an example of this method in action here, where we've downloaded the training split of the SQUAD dataset and shuffled all the rows randomly.Another way to shuffle the data is to create random train and test splits. This can\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': 902}, page_content=\"rows randomly.Another way to shuffle the data is to create random train and test splits. This can be useful if you have to create your own test splits from raw data. To do this, you just apply the train_test_split method and specify how large the test split should be. In this example, we've specified that the test set should be 10% of the total dataset size. You can see that the output of train_test_split is a DatasetDict object, whose keys correspond to the new splits. Now that we know how to shuffle a dataset, let's take a look at returning the rows we're interested in. The most common way to do this is with the select method. This method expects a list or generator of the dataset's indices, and will then return a new Dataset object containing just those rows. If you want to create a random sample of rows, you can do this by chaining the shuffle and select methods together. In this example, we've created a sample of 5 elements from the SQuAD dataset. The last way to pick out\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': 1794}, page_content='this example, we\\'ve created a sample of 5 elements from the SQuAD dataset. The last way to pick out specific rows in a dataset is by applying the filter method. This method checks whether each rows fulfills some condition or not. For example, here we\\'ve created a small lambda function that checks whether the title starts with the letter \"L\". Once we apply this function with the filter method, we get a subset of the data consisting of just these titles. So far we\\'ve been talking about the rows of a dataset, but what about the columns? The Datasets library has two main methods for transforming columns: a rename_column method to change the name of a column, and a remove_columns method to delete them. You can see examples of both these method here. Some datasets have nested columns and you can expand these by applying the flatten method. For example in the SQUAD dataset, the answers column contains a text and answer_start field. If we want to promote them to their own separate columns, we'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter5/03a_slice-and-dice.md', 'start_index': 2702}, page_content='a text and answer_start field. If we want to promote them to their own separate columns, we can apply flatten as shown here. Of course, no discussion of the Datasets library would be complete without mentioning the famous map method. This method applies a custom processing function to each row in the dataset. For example,here we first define a lowercase_title function that simply lowercases the text in the title column and then we feed that to the map method and voila! we now have lowercase titles. The map method can also be used to feed batches of rows to the processing function. This is especially useful for tokenization, where the tokenizers are backed by the Tokenizers library can use fast multithreading to process batches in parallel.'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: question-answering\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 87}, page_content='```\\nimport gradio as gr\\n\\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\\n\\nmodel_name = \"deepset/roberta-base-squad2\"\\n\\nnlp = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 237}, page_content='nlp = pipeline(\"question-answering\", model=model_name, tokenizer=model_name)\\n\\ncontext = \"The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.\"\\nquestion = \"Which continent is the Amazon rainforest in?\"'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 1219}, page_content='def predict(context, question):\\n    res = nlp({\"question\": question, \"context\": context})\\n    return res[\"answer\"], res[\"score\"]\\n\\n\\ngr.Interface(\\n    predict,\\n    inputs=[\\n        gr.Textbox(lines=7, value=context, label=\"Context Paragraph\"),\\n        gr.Textbox(lines=2, value=question, label=\"Question\"),\\n    ],\\n    outputs=[gr.Textbox(label=\"Answer\"), gr.Textbox(label=\"Score\")],\\n).launch()'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/question-answering/run.ipynb', 'start_index': 1612}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/loaders/ip_adapter.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# IP-Adapter\\n\\n[IP-Adapter](https://hf.co/papers/2308.06721) is a lightweight adapter that enables prompting a diffusion model with an image. This method decouples the cross-attention layers of the image and text features. The image features are generated from an image encoder. Files generated from IP-Adapter are only ~100MBs.\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/loaders/ip_adapter.md', 'start_index': 916}, page_content='<Tip>\\n\\nLearn how to load an IP-Adapter checkpoint and image in the [IP-Adapter](../../using-diffusers/loading_adapters#ip-adapter) loading guide.\\n\\n</Tip>\\n\\n## IPAdapterMixin\\n\\n[[autodoc]] loaders.ip_adapter.IPAdapterMixin'),\n",
              " Document(metadata={'source': 'huggingface/peft/blob/main/docs/source/package_reference/config.md', 'start_index': 0}, page_content='!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Configuration\\n\\n[`PeftConfigMixin`] is the base configuration class for storing the adapter configuration of a [`PeftModel`], and [`PromptLearningConfig`] is the base configuration class for soft prompt methods (p-tuning, prefix tuning, and prompt tuning). These base classes contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads.\\n\\n## PeftConfigMixin\\n\\n[[autodoc]] config.PeftConfigMixin\\n    - all\\n\\n## PeftConfig\\n\\n[[autodoc]] PeftConfig\\n    - all\\n\\n## PromptLearningConfig\\n\\n[[autodoc]] PromptLearningConfig\\n    - all'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 593}, page_content='<p align=\"center\">\\n  <picture>\\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\\n  </picture>\\n  <br/>\\n  <br/>\\n</p>'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 1188}, page_content='<p align=\"center\">\\n    <a href=\"https://circleci.com/gh/huggingface/transformers\">\\n        <img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\">\\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\">\\n    </a>\\n    <a href=\"https://huggingface.co/docs/transformers/index\">\\n        <img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/releases\">\\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\">\\n    </a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 2008}, page_content='</a>\\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\">\\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\">\\n    </a>\\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\\n</p>'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 2365}, page_content='<h4 align=\"center\">\\n    <p>\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README.md\">English</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hans.md\">简体中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_zh-hant.md\">繁體中文</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ko.md\">한국어</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_es.md\">Español</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_ja.md\">日本語</a> |\\n        <a href=\"https://github.com/huggingface/transformers/blob/main/README_hd.md\">हिन्दी</a> |\\n        <b>Русский</b>\\n        <a href=\"https://github.com/huggingface/transformers//blob/main/README_te.md\">తెలుగు</a> |\\n    <p>\\n</h4>\\n\\n<h3 align=\"center\">\\n    <p>Современное машинное обучение для JAX, PyTorch и TensorFlow</p>\\n</h3>'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 3215}, page_content='<h3 align=\"center\">\\n    <p>Современное машинное обучение для JAX, PyTorch и TensorFlow</p>\\n</h3>\\n\\n<h3 align=\"center\">\\n    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\\n</h3>\\n\\n🤗 Transformers предоставляет тысячи предварительно обученных моделей для выполнения различных задач, таких как текст, зрение и аудио.\\n\\nЭти модели могут быть применены к:\\n\\n* 📝 Тексту для таких задач, как классификация текстов, извлечение информации, ответы на вопросы, обобщение, перевод, генерация текстов на более чем 100 языках.\\n* 🖼️ Изображениям для задач классификации изображений, обнаружения объектов и сегментации.\\n* 🗣️ Аудио для задач распознавания речи и классификации аудио.\\n\\nМодели transformers также могут выполнять несколько задач, такие как ответы на табличные вопросы, распознавание оптических символов, извлечение информации из отсканированных документов, классификация видео и ответы на визуальные вопросы.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 4212}, page_content='🤗 Transformers предоставляет API для быстрой загрузки и использования предварительно обученных моделей, их тонкой настройки на собственных датасетах и последующего взаимодействия ими с сообществом на нашем [сайте](https://huggingface.co/models). В то же время каждый python модуль, определяющий архитектуру, полностью автономен и может быть модифицирован для проведения быстрых исследовательских экспериментов.\\n\\n🤗 Transformers опирается на три самые популярные библиотеки глубокого обучения - [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) и [TensorFlow](https://www.tensorflow.org/) - и легко интегрируется между ними. Это позволяет легко обучать модели с помощью одной из них, а затем загружать их для выводов с помощью другой.\\n\\n## Онлайн демонстрация'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 4974}, page_content='## Онлайн демонстрация\\n\\nБольшинство наших моделей можно протестировать непосредственно на их страницах с [сайта](https://huggingface.co/models). Мы также предлагаем [привтаный хостинг моделей, контроль версий и API для выводов](https://huggingface.co/pricing) для публичных и частных моделей.\\n\\nВот несколько примеров:'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 5293}, page_content='В области NLP ( Обработка текстов на естественном языке ):\\n- [Маскированное заполнение слов с помощью BERT](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\\n- [Распознавание сущностей с помощью Electra](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\\n- [Генерация текста с помощью GPT-2](https://huggingface.co/gpt2?text=A+long+time+ago%2C+)\\n- [Выводы на естественном языке с помощью RoBERTa](https://huggingface.co/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 5891}, page_content='- [Обобщение с помощью BART](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 6744}, page_content='- [Ответы на вопросы с помощью'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 6775}, page_content='DistilBERT](https://huggingface.co/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+G'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 7674}, page_content='0%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 8102}, page_content='- [Перевод с помощью T5](https://huggingface.co/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 8206}, page_content='В области компьютерного зрения:\\n- [Классификация изображений с помощью ViT](https://huggingface.co/google/vit-base-patch16-224)\\n- [Обнаружение объектов с помощью DETR](https://huggingface.co/facebook/detr-resnet-50)\\n- [Семантическая сегментация с помощью SegFormer](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\\n- [Сегментация паноптикума с помощью MaskFormer](https://huggingface.co/facebook/maskformer-swin-small-coco)\\n- [Оценка глубины с помощью DPT](https://huggingface.co/docs/transformers/model_doc/dpt)\\n- [Классификация видео с помощью VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)\\n- [Универсальная сегментация с помощью OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_dinat_large)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 8953}, page_content='В области звука:\\n- [Автоматическое распознавание речи с помощью Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)\\n- [Поиск ключевых слов с помощью Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\\n- [Классификация аудиоданных с помощью траснформера аудиоспектрограмм](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)\\n\\nВ мультимодальных задачах:\\n- [Ответы на вопросы по таблице с помощью TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)\\n- [Визуальные ответы на вопросы с помощью ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)\\n- [Zero-shot классификация изображений с помощью CLIP](https://huggingface.co/openai/clip-vit-large-patch14)\\n- [Ответы на вопросы по документам с помощью LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)\\n- [Zero-shot классификация видео с помощью X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)\\n\\n\\n## 100 проектов, использующих Transformers'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 9876}, page_content='## 100 проектов, использующих Transformers\\n\\nTransformers - это не просто набор инструментов для использования предварительно обученных моделей: это сообщество проектов, созданное на его основе, и\\nHugging Face Hub. Мы хотим, чтобы Transformers позволил разработчикам, исследователям, студентам, профессорам, инженерам и всем желающим\\nсоздавать проекты своей мечты.\\n\\nЧтобы отпраздновать 100 тысяч звезд Transformers, мы решили сделать акцент на сообществе, и создали страницу [awesome-transformers](./awesome-transformers.md), на которой перечислены 100\\nневероятных проектов, созданных с помощью transformers.\\n\\nЕсли вы являетесь владельцем или пользователем проекта, который, по вашему мнению, должен быть включен в этот список, пожалуйста, откройте PR для его добавления!\\n\\n## Если вы хотите получить индивидуальную поддержку от команды Hugging Face'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 10648}, page_content='## Если вы хотите получить индивидуальную поддержку от команды Hugging Face\\n\\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\\n</a><br>\\n\\n## Быстрый гайд\\n\\nДля использования модели на заданном входе (текст, изображение, звук, ...) мы предоставляем API `pipeline`. Конвейеры объединяют предварительно обученную модель с препроцессингом, который использовался при ее обучении. Вот как можно быстро использовать конвейер для классификации положительных и отрицательных текстов:\\n\\n```python\\n>>> from transformers import pipeline'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 11388}, page_content=\"```python\\n>>> from transformers import pipeline\\n\\n# Выделение конвейера для анализа настроений\\n>>> classifier = pipeline('sentiment-analysis')\\n>>> classifier('Мы очень рады представить конвейер в transformers.')\\n[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 11652}, page_content='```\\n\\nВторая строка кода загружает и кэширует предварительно обученную модель, используемую конвейером, а третья оценивает ее на заданном тексте. Здесь ответ \"POSITIVE\" с уверенностью 99,97%.\\n\\nВо многих задачах, как в НЛП, так и в компьютерном зрении и речи, уже есть готовый `pipeline`. Например, мы можем легко извлечь обнаруженные объекты на изображении:\\n\\n``` python\\n>>> import requests\\n>>> from PIL import Image\\n>>> from transformers import pipeline\\n\\n# Скачиваем изображение с милыми котиками\\n>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\\n>>> image_data = requests.get(url, stream=True).raw\\n>>> image = Image.open(image_data)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 12342}, page_content=\"# Выделение конвейера для обнаружения объектов\\n>>> object_detector = pipeline('object-detection')\\n>>> object_detector(image)\\n[{'score': 0.9982201457023621,\\n  'label': 'remote',\\n  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\\n {'score': 0.9960021376609802,\\n  'label': 'remote',\\n  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\\n {'score': 0.9954745173454285,\\n  'label': 'couch',\\n  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\\n {'score': 0.9988006353378296,\\n  'label': 'cat',\\n  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\\n {'score': 0.9986783862113953,\\n  'label': 'cat',\\n  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 13030}, page_content='```\\n\\nЗдесь мы получаем список объектов, обнаруженных на изображении, с рамкой вокруг объекта и оценкой достоверности. Слева - исходное изображение, справа прогнозы:\\n\\n<h3 align=\"center\">\\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\" width=\"400\"></a>\\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample_post_processed.png\" width=\"400\"></a>\\n</h3>\\n\\nПодробнее о задачах, поддерживаемых API `pipeline`, можно узнать в [этом учебном пособии](https://huggingface.co/docs/transformers/task_sum)\\n\\nВ дополнение к `pipeline`, для загрузки и использования любой из предварительно обученных моделей в заданной задаче достаточно трех строк кода. Вот версия для PyTorch:\\n```python\\n>>> from transformers import AutoTokenizer, AutoModel\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = AutoModel.from_pretrained(\"bert-base-uncased\")'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 13998}, page_content='>>> inputs = tokenizer(\"Привет мир!\", return_tensors=\"pt\")\\n>>> outputs = model(**inputs)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 14087}, page_content='```\\n\\nА вот эквивалентный код для TensorFlow:\\n```python\\n>>> from transformers import AutoTokenizer, TFAutoModel\\n\\n>>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n>>> model = TFAutoModel.from_pretrained(\"bert-base-uncased\")\\n\\n>>> inputs = tokenizer(\"Привет мир!\", return_tensors=\"tf\")\\n>>> outputs = model(**inputs)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 14417}, page_content='```\\n\\nТокенизатор отвечает за всю предварительную обработку, которую ожидает предварительно обученная модель, и может быть вызван непосредственно с помощью одной строки (как в приведенных выше примерах) или на списке. В результате будет получен словарь, который можно использовать в последующем коде или просто напрямую передать в модель с помощью оператора распаковки аргументов **.\\n\\nСама модель представляет собой обычный [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) или [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) (в зависимости от используемого бэкенда), который можно использовать как обычно. [В этом руководстве](https://huggingface.co/docs/transformers/training) рассказывается, как интегрировать такую модель в классический цикл обучения PyTorch или TensorFlow, или как использовать наш API `Trainer` для быстрой тонкой настройки на новом датасете.\\n\\n## Почему необходимо использовать transformers?'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 15353}, page_content='## Почему необходимо использовать transformers?\\n\\n1. Простые в использовании современные модели:\\n    - Высокая производительность в задачах понимания и генерации естественного языка, компьютерного зрения и аудио.\\n    - Низкий входной барьер для преподавателей и практиков.\\n    - Небольшое количество абстракций для пользователя и всего три класса для изучения.\\n    - Единый API для использования всех наших предварительно обученных моделей.\\n\\n1. Более низкие вычислительные затраты, меньший \"углеродный след\":\\n    - Исследователи могут обмениваться обученными моделями вместо того, чтобы постоянно их переобучать.\\n    - Практики могут сократить время вычислений и производственные затраты.\\n    - Десятки архитектур с более чем 60 000 предварительно обученных моделей для всех модальностей.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 16142}, page_content='1. Выбор подходящего фреймворка для каждого этапа жизни модели:\\n    - Обучение самых современных моделей за 3 строки кода.\\n    - Перемещайте одну модель между фреймворками TF2.0/PyTorch/JAX по своему усмотрению.\\n    - Беспрепятственный выбор подходящего фреймворка для обучения, оценки и производства.\\n\\n1. Легко настроить модель или пример под свои нужды:\\n    - Мы предоставляем примеры для каждой архитектуры, чтобы воспроизвести результаты, опубликованные их авторами.\\n    - Внутренние компоненты модели раскрываются максимально последовательно.\\n    - Файлы моделей можно использовать независимо от библиотеки для проведения быстрых экспериментов.\\n\\n## Почему я не должен использовать transformers?'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 16843}, page_content='- Данная библиотека не является модульным набором строительных блоков для нейронных сетей. Код в файлах моделей специально не рефакторится дополнительными абстракциями, чтобы исследователи могли быстро итеративно работать с каждой из моделей, не погружаясь в дополнительные абстракции/файлы.\\n- API обучения не предназначен для работы с любой моделью, а оптимизирован для работы с моделями, предоставляемыми библиотекой. Для работы с общими циклами машинного обучения следует использовать другую библиотеку (возможно, [Accelerate](https://huggingface.co/docs/accelerate)).\\n- Несмотря на то, что мы стремимся представить как можно больше примеров использования, скрипты в нашей папке [примеров](https://github.com/huggingface/transformers/tree/main/examples) являются именно примерами. Предполагается, что они не будут работать \"из коробки\" для решения вашей конкретной задачи, и вам придется изменить несколько строк кода, чтобы адаптировать их под свои нужды.\\n\\n## Установка\\n\\n### С помощью pip'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 17804}, page_content='## Установка\\n\\n### С помощью pip\\n\\nДанный репозиторий протестирован на Python 3.8+, Flax 0.4.1+, PyTorch 1.10+ и TensorFlow 2.6+.\\n\\nУстанавливать 🤗 Transformers следует в [виртуальной среде](https://docs.python.org/3/library/venv.html). Если вы не знакомы с виртуальными средами Python, ознакомьтесь с [руководством пользователя](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\\n\\nСначала создайте виртуальную среду с той версией Python, которую вы собираетесь использовать, и активируйте ее.\\n\\nЗатем необходимо установить хотя бы один бекенд из Flax, PyTorch или TensorFlow.\\nПожалуйста, обратитесь к страницам [TensorFlow установочная страница](https://www.tensorflow.org/install/), [PyTorch установочная страница](https://pytorch.org/get-started/locally/#start-locally) и/или [Flax](https://github.com/google/flax#quick-install) и [Jax](https://github.com/google/jax#installation), где описаны команды установки для вашей платформы.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 18771}, page_content='После установки одного из этих бэкендов 🤗 Transformers может быть установлен с помощью pip следующим образом:\\n\\n```bash\\npip install transformers'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 18915}, page_content='```\\n\\nЕсли вы хотите поиграть с примерами или вам нужен самый современный код и вы не можете ждать нового релиза, вы должны [установить библиотеку из исходного кода](https://huggingface.co/docs/transformers/installation#installing-from-source).\\n\\n### С помощью conda\\n\\nНачиная с версии Transformers v4.0.0, у нас появилсась поддержка conda: `huggingface`.\\n\\nУстановить Transformers с помощью conda можно следующим образом:\\n\\n```bash\\nconda install -c huggingface transformers'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 19385}, page_content='```\\n\\nО том, как установить Flax, PyTorch или TensorFlow с помощью conda, читайте на страницах, посвященных их установке.\\n\\n> **_ЗАМЕТКА:_** В операционной системе Windows вам может быть предложено активировать режим разработчика, чтобы воспользоваться преимуществами кэширования. Если для вас это невозможно, сообщите нам об этом [здесь](https://github.com/huggingface/huggingface_hub/issues/1062).\\n\\n## Модельные архитектуры\\n\\n**[Все контрольные точки моделей](https://huggingface.co/models)**, предоставляемые 🤗 Transformers, беспрепятственно интегрируются с huggingface.co [model hub](https://huggingface.co/models), куда они загружаются непосредственно [пользователями](https://huggingface.co/users) и [организациями](https://huggingface.co/organizations).\\n\\nТекущее количество контрольных точек: ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 20284}, page_content='🤗 В настоящее время Transformers предоставляет следующие архитектуры (подробное описание каждой из них см. [здесь](https://huggingface.co/docs/transformers/model_summary)):'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 20458}, page_content='1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\\n1. **[ALIGN](https://huggingface.co/docs/transformers/model_doc/align)** (from Google Research) released with the paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 21205}, page_content='1. **[AltCLIP](https://huggingface.co/docs/transformers/model_doc/altclip)** (from BAAI) released with the paper [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679) by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.\\n1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\\n1. **[Autoformer](https://huggingface.co/docs/transformers/model_doc/autoformer)** (from Tsinghua University) released with the paper [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 22122}, page_content='1. **[Bark](https://huggingface.co/docs/transformers/model_doc/bark)** (from Suno) released in the repository [suno-ai/bark](https://github.com/suno-ai/bark) by Suno AI team.\\n1. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\\n1. **[BARThez](https://huggingface.co/docs/transformers/model_doc/barthez)** (from École polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 22984}, page_content='1. **[BARTpho](https://huggingface.co/docs/transformers/model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\\n1. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\\n1. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 23786}, page_content='1. **[BERT For Sequence Generation](https://huggingface.co/docs/transformers/model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\\n1. **[BERTweet](https://huggingface.co/docs/transformers/model_doc/bertweet)** (from VinAI Research) released with the paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.\\n1. **[BigBird-Pegasus](https://huggingface.co/docs/transformers/model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 24743}, page_content='1. **[BigBird-RoBERTa](https://huggingface.co/docs/transformers/model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\\n1. **[BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt)** (from Microsoft Research AI4Science) released with the paper [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 25559}, page_content='1. **[BiT](https://huggingface.co/docs/transformers/model_doc/bit)** (from Google AI) released with the paper [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.\\n1. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 26237}, page_content='1. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\\n1. **[BLIP](https://huggingface.co/docs/transformers/model_doc/blip)** (from Salesforce) released with the paper [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\\n1. **[BLIP-2](https://huggingface.co/docs/transformers/model_doc/blip-2)** (from Salesforce) released with the paper [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 27225}, page_content='1. **[BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).\\n1. **[BORT](https://huggingface.co/docs/transformers/model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) by Adrian de Wynter and Daniel J. Perry.\\n1. **[BridgeTower](https://huggingface.co/docs/transformers/model_doc/bridgetower)** (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 28016}, page_content='1. **[BROS](https://huggingface.co/docs/transformers/model_doc/bros)** (from NAVER CLOVA) released with the paper [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.\\n1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 28704}, page_content='1. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, Éric Villemonte de la Clergerie, Djamé Seddah and Benoît Sagot.\\n1. **[CANINE](https://huggingface.co/docs/transformers/model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\\n1. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 29697}, page_content='1. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.\\n1. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\\n1. **[CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)** (from University of Göttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lüddecke and Alexander Ecker.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 30694}, page_content='1. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.\\n1. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (from MetaAI) released with the paper [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 31647}, page_content='1. **[Conditional DETR](https://huggingface.co/docs/transformers/model_doc/conditional_detr)** (from Microsoft Research Asia) released with the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.\\n1. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\\n1. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 32555}, page_content='1. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\\n1. **[CPM](https://huggingface.co/docs/transformers/model_doc/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\\n1. **[CPM-Ant](https://huggingface.co/docs/transformers/model_doc/cpmant)** (from OpenBMB) released by the [OpenBMB](https://www.openbmb.org/).'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 33554}, page_content='1. **[CTRL](https://huggingface.co/docs/transformers/model_doc/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\\n1. **[CvT](https://huggingface.co/docs/transformers/model_doc/cvt)** (from Microsoft) released with the paper [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\\n1. **[Data2Vec](https://huggingface.co/docs/transformers/model_doc/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 34483}, page_content='1. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\\n1. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\\n1. **[Decision Transformer](https://huggingface.co/docs/transformers/model_doc/decision_transformer)** (from Berkeley/Facebook/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 35429}, page_content='1. **[Deformable DETR](https://huggingface.co/docs/transformers/model_doc/deformable_detr)** (from SenseTime Research) released with the paper [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.\\n1. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 36077}, page_content='1. **[DePlot](https://huggingface.co/docs/transformers/model_doc/deplot)** (from Google AI) released with the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) by Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.\\n1. **[DETA](https://huggingface.co/docs/transformers/model_doc/deta)** (from The University of Texas at Austin) released with the paper [NMS Strikes Back](https://arxiv.org/abs/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krähenbühl.\\n1. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 37025}, page_content='1. **[DialoGPT](https://huggingface.co/docs/transformers/model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\\n1. **[DiNAT](https://huggingface.co/docs/transformers/model_doc/dinat)** (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 37623}, page_content='1. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (from Meta AI) released with the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 38232}, page_content='1. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) and a German version of DistilBERT.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 38987}, page_content='1. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\\n1. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\\n1. **[DPR](https://huggingface.co/docs/transformers/model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 39942}, page_content='1. **[DPT](https://huggingface.co/docs/transformers/master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by René Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\\n1. **[EfficientFormer](https://huggingface.co/docs/transformers/model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.\\n1. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google Brain) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) by Mingxing Tan, Quoc V. Le.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 40790}, page_content='1. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\\n1. **[EnCodec](https://huggingface.co/docs/transformers/model_doc/encodec)** (from Meta AI) released with the paper [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Défossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\\n1. **[EncoderDecoder](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 41663}, page_content='1. **[ERNIE](https://huggingface.co/docs/transformers/model_doc/ernie)** (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.\\n1. **[ErnieM](https://huggingface.co/docs/transformers/model_doc/ernie_m)** (from Baidu) released with the paper [ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674) by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 42321}, page_content='1. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (from Meta AI) are transformer protein language models.  **ESM-1b** was released with the paper [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. **ESM-1v** was released with the paper [Language models enable zero-shot prediction of the effects of mutations on protein function](https://doi.org/10.1101/2021.07.09.450648) by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. **ESM-2 and ESMFold** were released with the paper [Language models of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu,'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 43257}, page_content='by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 43420}, page_content='1. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)** (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 43820}, page_content='1. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 44475}, page_content='1. **[FLAN-UL2](https://huggingface.co/docs/transformers/model_doc/flan-ul2)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 45133}, page_content='1. **[FlauBERT](https://huggingface.co/docs/transformers/model_doc/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, Didier Schwab.\\n1. **[FLAVA](https://huggingface.co/docs/transformers/model_doc/flava)** (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.\\n1. **[FNet](https://huggingface.co/docs/transformers/model_doc/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 46102}, page_content='1. **[FocalNet](https://huggingface.co/docs/transformers/model_doc/focalnet)** (from Microsoft Research) released with the paper [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.\\n1. **[Funnel Transformer](https://huggingface.co/docs/transformers/model_doc/funnel)** (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\\n1. **[Fuyu](https://huggingface.co/docs/transformers/model_doc/fuyu)** (from ADEPT) Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar. Released with the paper [blog post](https://www.adept.ai/blog/fuyu-8b)'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 46933}, page_content='1. **[GIT](https://huggingface.co/docs/transformers/model_doc/git)** (from Microsoft Research) released with the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.\\n1. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\\n1. **[GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://blog.openai.com/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 47867}, page_content='1. **[GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.\\n1. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\\n1. **[GPT NeoX Japanese](https://huggingface.co/docs/transformers/model_doc/gpt_neox_japanese)** (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 48771}, page_content='1. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\\n1. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.\\n1. **[GPT-Sw3](https://huggingface.co/docs/transformers/model_doc/gpt-sw3)** (from AI-Sweden) released with the paper [Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf) by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey Öhman, Fredrik Carlsson, Magnus Sahlgren.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 49746}, page_content=\"1. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) released with the paper [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 50603}, page_content='1. **[GPTSAN-japanese](https://huggingface.co/docs/transformers/model_doc/gptsan-japanese)** released in the repository [tanreinama/GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md) by Toshiyuki Sakamoto(tanreinama).\\n1. **[Graphormer](https://huggingface.co/docs/transformers/model_doc/graphormer)** (from Microsoft) released with the paper [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.\\n1. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 51486}, page_content='1. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.\\n1. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\\n1. **[I-BERT](https://huggingface.co/docs/transformers/model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 52458}, page_content='1. **[IDEFICS](https://huggingface.co/docs/transformers/model_doc/idefics)** (from HuggingFace) released with the paper [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://huggingface.co/papers/2306.16527) by Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, Victor Sanh.\\n1. **[ImageGPT](https://huggingface.co/docs/transformers/model_doc/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 53175}, page_content='1. **[Informer](https://huggingface.co/docs/transformers/model_doc/informer)** (from Beihang University, UC Berkeley, Rutgers University, SEDD Company) released with the paper [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\\n1. **[InstructBLIP](https://huggingface.co/docs/transformers/model_doc/instructblip)** (from Salesforce) released with the paper [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 53944}, page_content='1. **[Jukebox](https://huggingface.co/docs/transformers/model_doc/jukebox)** (from OpenAI) released with the paper [Jukebox: A Generative Model for Music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.\\n1. **[LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 54549}, page_content='1. **[LayoutLMv2](https://huggingface.co/docs/transformers/model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\\n1. **[LayoutLMv3](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)** (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 55256}, page_content=\"1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\\n1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\\n1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 56174}, page_content='1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.\\n1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 56945}, page_content='1. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (from The FAIR team of Meta AI) released with the paper [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX) by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 57847}, page_content='Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 58257}, page_content='1. **[Longformer](https://huggingface.co/docs/transformers/model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\\n1. **[LongT5](https://huggingface.co/docs/transformers/model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.\\n1. **[LUKE](https://huggingface.co/docs/transformers/model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 59130}, page_content='1. **[LXMERT](https://huggingface.co/docs/transformers/model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) by Hao Tan and Mohit Bansal.\\n1. **[M-CTC-T](https://huggingface.co/docs/transformers/model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 59718}, page_content='1. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\\n1. **[MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)** (from Google) released with the paper [MADLAD-400: A Multilingual And Document-Level Large Audited Dataset](https://arxiv.org/abs/2309.04662) by Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, Orhan Firat.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 60589}, page_content='1. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)** Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by Jörg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.\\n1. **[MarkupLM](https://huggingface.co/docs/transformers/model_doc/markuplm)** (from Microsoft Research Asia) released with the paper [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.\\n1. **[Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former)** (from FAIR and UIUC) released with the paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 61493}, page_content='1. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\\n1. **[MatCha](https://huggingface.co/docs/transformers/model_doc/matcha)** (from Google AI) released with the paper [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662) by Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 62174}, page_content='1. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\\n1. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\\n1. **[MEGA](https://huggingface.co/docs/transformers/model_doc/mega)** (from Meta/USC/CMU/SJTU) released with the paper [Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 63163}, page_content='1. **[Megatron-BERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n1. **[Megatron-GPT2](https://huggingface.co/docs/transformers/model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\\n1. **[MGP-STR](https://huggingface.co/docs/transformers/model_doc/mgp-str)** (from Alibaba Research) released with the paper [Multi-Granularity Prediction for Scene Text Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang, Cheng Da, and Cong Yao.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 64120}, page_content='1. **[mLUKE](https://huggingface.co/docs/transformers/model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\\n1. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (from Facebook) released with the paper [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516) by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 64836}, page_content='1. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\\n1. **[MobileNetV1](https://huggingface.co/docs/transformers/model_doc/mobilenet_v1)** (from Google Inc.) released with the paper [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\\n1. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)** (from Google Inc.) released with the paper [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 65830}, page_content='1. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari.\\n1. **[MobileViTV2](https://huggingface.co/docs/transformers/model_doc/mobilevitv2)** (from Apple) released with the paper [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) by Sachin Mehta and Mohammad Rastegari.\\n1. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 66644}, page_content='1. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)** (from MosaiML) released with the repository [llm-foundry](https://github.com/mosaicml/llm-foundry/) by the MosaicML NLP Team.\\n1. **[MRA](https://huggingface.co/docs/transformers/model_doc/mra)** (from the University of Wisconsin - Madison) released with the paper [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, Vikas Singh.\\n1. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 67477}, page_content='1. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)** (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Défossez.\\n1. **[MVP](https://huggingface.co/docs/transformers/model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\\n1. **[NAT](https://huggingface.co/docs/transformers/model_doc/nat)** (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 68309}, page_content='1. **[Nezha](https://huggingface.co/docs/transformers/model_doc/nezha)** (from Huawei Noah’s Ark Lab) released with the paper [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.\\n1. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.\\n1. **[NLLB-MOE](https://huggingface.co/docs/transformers/model_doc/nllb-moe)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 69142}, page_content='1. **[Nyströmformer](https://huggingface.co/docs/transformers/model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\\n1. **[OneFormer](https://huggingface.co/docs/transformers/model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\\n1. **[OpenLlama](https://huggingface.co/docs/transformers/model_doc/open-llama)** (from [s-JoL](https://huggingface.co/s-JoL)) released on GitHub (now removed).'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 69977}, page_content='1. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\\n1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 70716}, page_content='1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.\\n1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (from Google) released with the paper [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) by Jason Phang, Yao Zhao, and Peter J. Liu.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 71287}, page_content='1. **[Perceiver IO](https://huggingface.co/docs/transformers/model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hénaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, João Carreira.\\n1. **[Persimmon](https://huggingface.co/docs/transformers/main/model_doc/persimmon)** (from ADEPT) released in a [blog post](https://www.adept.ai/blog/persimmon-8b) by Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, Arushi Somani.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 72052}, page_content='1. **[Phi](https://huggingface.co/docs/main/transformers/model_doc/phi)** (from Microsoft Research) released with the papers - [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.\\n1. **[PhoBERT](https://huggingface.co/docs/transformers/model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 73014}, page_content='1. **[Pix2Struct](https://huggingface.co/docs/transformers/model_doc/pix2struct)** (from Google) released with the paper [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.\\n1. **[PLBart](https://huggingface.co/docs/transformers/model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 73688}, page_content='1. **[PoolFormer](https://huggingface.co/docs/transformers/model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.\\n1. **[Pop2Piano](https://huggingface.co/docs/transformers/model_doc/pop2piano)** released with the paper [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi and Kyogu Lee.\\n1. **[ProphetNet](https://huggingface.co/docs/transformers/model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 74595}, page_content='1. **[PVT](https://huggingface.co/docs/transformers/model_doc/pvt)** (from Nanjing University, The University of Hong Kong etc.) released with the paper [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/pdf/2102.12122.pdf) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.\\n1. **[QDQBert](https://huggingface.co/docs/transformers/model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 75302}, page_content='1. **[RAG](https://huggingface.co/docs/transformers/model_doc/rag)** (from Facebook) released with the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela.\\n1. **[REALM](https://huggingface.co/docs/transformers/model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.\\n1. **[Reformer](https://huggingface.co/docs/transformers/model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 76241}, page_content='1. **[RegNet](https://huggingface.co/docs/transformers/model_doc/regnet)** (from META Platforms) released with the paper [Designing Network Design Space](https://arxiv.org/abs/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár.\\n1. **[RemBERT](https://huggingface.co/docs/transformers/model_doc/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Févry, Henry Tsai, M. Johnson, Sebastian Ruder.\\n1. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 77072}, page_content='1. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\\n1. **[RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/model_doc/roberta-prelayernorm)** (from Facebook) released with the paper [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 77771}, page_content='1. **[RoCBert](https://huggingface.co/docs/transformers/model_doc/roc_bert)** (from WeChatAI) released with the paper [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.\\n1. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\\n1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (from Bo Peng), released on [this repo](https://github.com/BlinkDL/RWKV-LM) by Bo Peng.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 78542}, page_content='1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\\n1. **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)** (from Meta AI) released with the paper [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\\n1. **[SEW](https://huggingface.co/docs/transformers/model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 79527}, page_content='1. **[SEW-D](https://huggingface.co/docs/transformers/model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\\n1. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 80235}, page_content='1. **[SpeechToTextTransformer](https://huggingface.co/docs/transformers/model_doc/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\\n1. **[SpeechToTextTransformer2](https://huggingface.co/docs/transformers/model_doc/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\\n1. **[Splinter](https://huggingface.co/docs/transformers/model_doc/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 81199}, page_content='1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\\n1. **[SwiftFormer](https://huggingface.co/docs/transformers/model_doc/swiftformer)** (from MBZUAI) released with the paper [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 81880}, page_content='1. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\\n1. **[Swin Transformer V2](https://huggingface.co/docs/transformers/model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\\n1. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)** (from University of Würzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 82868}, page_content='1. **[SwitchTransformers](https://huggingface.co/docs/transformers/model_doc/switch_transformers)** (from Google) released with the paper [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\\n1. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 83555}, page_content='1. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\\n1. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 84296}, page_content='1. **[TAPAS](https://huggingface.co/docs/transformers/model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno and Julian Martin Eisenschlos.\\n1. **[TAPEX](https://huggingface.co/docs/transformers/model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\\n1. **[Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer)** (from HuggingFace).'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 85048}, page_content='1. **[TimeSformer](https://huggingface.co/docs/transformers/model_doc/timesformer)** (from Facebook) released with the paper [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Gedas Bertasius, Heng Wang, Lorenzo Torresani.\\n1. **[Trajectory Transformer](https://huggingface.co/docs/transformers/model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine\\n1. **[Transformer-XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 85980}, page_content='1. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\\n1. **[TVLT](https://huggingface.co/docs/transformers/model_doc/tvlt)** (from UNC Chapel Hill) released with the paper [TVLT: Textless Vision-Language Transformer](https://arxiv.org/abs/2209.14156) by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal.\\n1. **[UL2](https://huggingface.co/docs/transformers/model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 86893}, page_content='1. **[UMT5](https://huggingface.co/docs/transformers/model_doc/umt5)** (from Google Research) released with the paper [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\\n1. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 87603}, page_content='1. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\\n1. **[UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet)** (from Peking University) released with the paper [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221) by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.\\n1. **[VAN](https://huggingface.co/docs/transformers/model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 88551}, page_content='1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\\n1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 89189}, page_content='1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\\n1. **[VisualBERT](https://huggingface.co/docs/transformers/model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 89942}, page_content='1. **[ViT Hybrid](https://huggingface.co/docs/transformers/model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\\n1. **[VitDet](https://huggingface.co/docs/transformers/model_doc/vitdet)** (from Meta AI) released with the paper [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.\\n1. **[ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 90936}, page_content='1. **[ViTMatte](https://huggingface.co/docs/transformers/main/model_doc/vitmatte)** (from HUST-VL) rreleased with the paper [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.\\n1. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)** (from Meta AI) released with the paper [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.\\n1. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)** (from Kakao Enterprise) released with the paper [Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) by Jaehyeon Kim, Jungil Kong, Juhee Son.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 91869}, page_content='1. **[ViViT](https://huggingface.co/docs/transformers/model_doc/vivit)** (from Google Research) released with the paper [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, Cordelia Schmid.\\n1. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\\n1. **[Wav2Vec2-Conformer](https://huggingface.co/docs/transformers/model_doc/wav2vec2-conformer)** (from Facebook AI) released with the paper [FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 92775}, page_content='1. **[Wav2Vec2Phoneme](https://huggingface.co/docs/transformers/model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\\n1. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 93516}, page_content='1. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\\n1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (from Microsoft Research) released with the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\\n1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)** (from Meta AI) released with the paper [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 94503}, page_content=\"1. **[XGLM](https://huggingface.co/docs/transformers/model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\\n1. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 95255}, page_content='1. **[XLM-ProphetNet](https://huggingface.co/docs/transformers/model_doc/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\\n1. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 96017}, page_content='1. **[XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\\n1. **[XLM-V](https://huggingface.co/docs/transformers/model_doc/xlm-v)** (from Meta AI) released with the paper [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472) by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.\\n1. **[XLNet](https://huggingface.co/docs/transformers/model_doc/xlnet)** (from Google/CMU) released with the paper [\\u200bXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 97005}, page_content='1. **[XLS-R](https://huggingface.co/docs/transformers/model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\\n1. **[XLSR-Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 97759}, page_content='1. **[YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\\n1. **[YOSO](https://huggingface.co/docs/transformers/model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 98482}, page_content='1. Want to contribute a new model? We have added a **detailed guide and templates** to guide you in the process of adding a new model. You can find them in the [`templates`](./templates) folder of the repository. Be sure to check the [contributing guidelines](./CONTRIBUTING.md) and contact the maintainers or open an issue to collect feedbacks before starting your PR.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 98853}, page_content='Чтобы проверить, есть ли у каждой модели реализация на Flax, PyTorch или TensorFlow, или связанный с ней токенизатор, поддерживаемый библиотекой 🤗 Tokenizers, обратитесь к [этой таблице](https://huggingface.co/docs/transformers/index#supported-frameworks).\\n\\nЭти реализации были протестированы на нескольких наборах данных (см. примеры скриптов) и должны соответствовать производительности оригинальных реализаций. Более подробную информацию о производительности можно найти в разделе \"Примеры\" [документации](https://github.com/huggingface/transformers/tree/main/examples).\\n\\n\\n## Изучи больше'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 99429}, page_content='## Изучи больше\\n\\n| Секция | Описание |\\n|-|-|\\n| [Документация](https://huggingface.co/docs/transformers/) | Полная документация по API и гайды |\\n| [Краткие описания задач](https://huggingface.co/docs/transformers/task_summary) | Задачи поддерживаются 🤗 Transformers |\\n| [Пособие по предварительной обработке](https://huggingface.co/docs/transformers/preprocessing) | Использование класса `Tokenizer` для подготовки данных для моделей |\\n| [Обучение и доработка](https://huggingface.co/docs/transformers/training) | Использование моделей, предоставляемых 🤗 Transformers, в цикле обучения PyTorch/TensorFlow и API `Trainer`. |\\n| [Быстрый тур: Тонкая настройка/скрипты использования](https://github.com/huggingface/transformers/tree/main/examples) | Примеры скриптов для тонкой настройки моделей на широком спектре задач |\\n| [Совместное использование и загрузка моделей](https://huggingface.co/docs/transformers/model_sharing) | Загружайте и делитесь с сообществом своими доработанными моделями |'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 100422}, page_content='## Цитирование'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 100438}, page_content='Теперь у нас есть [статья](https://www.aclweb.org/anthology/2020.emnlp-demos.6/), которую можно цитировать для библиотеки 🤗 Transformers:\\n```bibtex\\n@inproceedings{wolf-etal-2020-transformers,\\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\\n    month = oct,\\n    year = \"2020\",\\n    address = \"Online\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 101359}, page_content='url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\\n    pages = \"38--45\"\\n}'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/README_ru.md', 'start_index': 101443}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Share a model'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 746}, page_content='-->\\n\\n# Share a model\\n\\nThe last two tutorials showed how you can fine-tune a model with PyTorch, Keras, and 🤗 Accelerate for distributed setups. The next step is to share your model with the community! At Hugging Face, we believe in openly sharing knowledge and resources to democratize artificial intelligence for everyone. We encourage you to consider sharing your model with the community to help others save time and resources.\\n\\nIn this tutorial, you will learn two methods for sharing a trained or fine-tuned model on the [Model Hub](https://huggingface.co/models):\\n\\n- Programmatically push your files to the Hub.\\n- Drag-and-drop your files to the Hub with the web interface.\\n\\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XvSGPZFEjDY\" title=\"YouTube video player\"\\nframeborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;\\npicture-in-picture\" allowfullscreen></iframe>\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 1677}, page_content='<Tip>\\n\\nTo share a model with the community, you need an account on [huggingface.co](https://huggingface.co/join). You can also join an existing organization or create a new one.\\n\\n</Tip>\\n\\n## Repository features\\n\\nEach repository on the Model Hub behaves like a typical GitHub repository. Our repositories offer versioning, commit history, and the ability to visualize differences.\\n\\nThe Model Hub\\'s built-in versioning is based on git and [git-lfs](https://git-lfs.github.com/). In other words, you can treat one model as one repository, enabling greater access control and scalability. Version control allows *revisions*, a method for pinning a specific version of a model with a commit hash, tag or branch.\\n\\nAs a result, you can load a specific model version with the `revision` parameter:\\n\\n```py\\n>>> model = AutoModel.from_pretrained(\\n...     \"julien-c/EsperBERTo-small\", revision=\"v2.0.1\"  # tag name, or branch name, or commit hash\\n... )'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 2617}, page_content='```\\n\\nFiles are also easily edited in a repository, and you can view the commit history as well as the difference:\\n\\n![vis_diff](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis_diff.png)\\n\\n## Setup\\n\\nBefore sharing a model to the Hub, you will need your Hugging Face credentials. If you have access to a terminal, run the following command in the virtual environment where 🤗 Transformers is installed. This will store your access token in your Hugging Face cache folder (`~/.cache/` by default):\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nIf you are using a notebook like Jupyter or Colaboratory, make sure you have the [`huggingface_hub`](https://huggingface.co/docs/hub/adding-a-library) library installed. This library allows you to programmatically interact with the Hub.\\n\\n```bash\\npip install huggingface_hub'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 3453}, page_content='```\\n\\nThen use `notebook_login` to sign-in to the Hub, and follow the link [here](https://huggingface.co/settings/token) to generate a token to login with:\\n\\n```py\\n>>> from huggingface_hub import notebook_login\\n\\n>>> notebook_login()'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 3684}, page_content='```\\n\\n## Convert a model for all frameworks\\n\\nTo ensure your model can be used by someone working with a different framework, we recommend you convert and upload your model with both PyTorch and TensorFlow checkpoints. While users are still able to load your model from a different framework if you skip this step, it will be slower because 🤗 Transformers will need to convert the checkpoint on-the-fly.\\n\\nConverting a checkpoint for another framework is easy. Make sure you have PyTorch and TensorFlow installed (see [here](installation) for installation instructions), and then find the specific model for your task in the other framework. \\n\\n<frameworkcontent>\\n<pt>\\nSpecify `from_tf=True` to convert a checkpoint from TensorFlow to PyTorch:\\n\\n```py\\n>>> pt_model = DistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_tf=True)\\n>>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 4611}, page_content='```\\n</pt>\\n<tf>\\nSpecify `from_pt=True` to convert a checkpoint from PyTorch to TensorFlow:\\n\\n```py\\n>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\\n```\\n\\nThen you can save your new TensorFlow model with its new checkpoint:\\n\\n```py\\n>>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\\n```\\n</tf>\\n<jax>\\nIf a model is available in Flax, you can also convert a checkpoint from PyTorch to Flax:\\n\\n```py\\n>>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\\n...     \"path/to/awesome-name-you-picked\", from_pt=True\\n... )'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 5219}, page_content='```\\n</jax>\\n</frameworkcontent>\\n\\n## Push a model during training\\n\\n<frameworkcontent>\\n<pt>\\n<Youtube id=\"Z1-XMy-GNLQ\"/>\\n\\nSharing a model to the Hub is as simple as adding an extra parameter or callback. Remember from the [fine-tuning tutorial](training), the [`TrainingArguments`] class is where you specify hyperparameters and additional training options. One of these training options includes the ability to push a model directly to the Hub. Set `push_to_hub=True` in your [`TrainingArguments`]:\\n\\n```py\\n>>> training_args = TrainingArguments(output_dir=\"my-awesome-model\", push_to_hub=True)\\n```\\n\\nPass your training arguments as usual to [`Trainer`]:\\n\\n```py\\n>>> trainer = Trainer(\\n...     model=model,\\n...     args=training_args,\\n...     train_dataset=small_train_dataset,\\n...     eval_dataset=small_eval_dataset,\\n...     compute_metrics=compute_metrics,\\n... )'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 6078}, page_content='```\\n\\nAfter you fine-tune your model, call [`~transformers.Trainer.push_to_hub`] on [`Trainer`] to push the trained model to the Hub. 🤗 Transformers will even automatically add training hyperparameters, training results and framework versions to your model card!\\n\\n```py\\n>>> trainer.push_to_hub()\\n```\\n</pt>\\n<tf>\\nShare a model to the Hub with [`PushToHubCallback`]. In the [`PushToHubCallback`] function, add:\\n\\n- An output directory for your model.\\n- A tokenizer.\\n- The `hub_model_id`, which is your Hub username and model name.\\n\\n```py\\n>>> from transformers import PushToHubCallback\\n\\n>>> push_to_hub_callback = PushToHubCallback(\\n...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\\n... )'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 6823}, page_content='```\\n\\nAdd the callback to [`fit`](https://keras.io/api/models/model_training_apis/), and 🤗 Transformers will push the trained model to the Hub:\\n\\n```py\\n>>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\\n```\\n</tf>\\n</frameworkcontent>\\n\\n## Use the `push_to_hub` function\\n\\nYou can also call `push_to_hub` directly on your model to upload it to the Hub.\\n\\nSpecify your model name in `push_to_hub`:\\n\\n```py\\n>>> pt_model.push_to_hub(\"my-awesome-model\")\\n```\\n\\nThis creates a repository under your username with the model name `my-awesome-model`. Users can now load your model with the `from_pretrained` function:\\n\\n```py\\n>>> from transformers import AutoModel\\n\\n>>> model = AutoModel.from_pretrained(\"your_username/my-awesome-model\")\\n```\\n\\nIf you belong to an organization and want to push your model under the organization name instead, just add it to the `repo_id`:\\n\\n```py\\n>>> pt_model.push_to_hub(\"my-awesome-org/my-awesome-model\")'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 7804}, page_content='```\\n\\nThe `push_to_hub` function can also be used to add other files to a model repository. For example, add a tokenizer to a model repository:\\n\\n```py\\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\\n```\\n\\nOr perhaps you\\'d like to add the TensorFlow version of your fine-tuned PyTorch model:\\n\\n```py\\n>>> tf_model.push_to_hub(\"my-awesome-model\")'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 8143}, page_content=\"```\\n\\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\\n\\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\\n\\n## Upload with the web interface\\n\\nUsers who prefer a no-code approach are able to upload a model through the Hub's web interface. Visit [huggingface.co/new](https://huggingface.co/new) to create a new repository:\\n\\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\\n\\nFrom here, add some information about your model:\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 8833}, page_content=\"From here, add some information about your model:\\n\\n- Select the **owner** of the repository. This can be yourself or any of the organizations you belong to.\\n- Pick a name for your model, which will also be the repository name.\\n- Choose whether your model is public or private.\\n- Specify the license usage for your model.\\n\\nNow click on the **Files** tab and click on the **Add file** button to upload a new file to your repository. Then drag-and-drop a file to upload and add a commit message.\\n\\n![upload_file](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/upload_file.png)\\n\\n## Add a model card\\n\\nTo make sure users understand your model's capabilities, limitations, potential biases and ethical considerations, please add a model card to your repository. The model card is defined in the `README.md` file. You can add a model card by:\\n\\n* Manually creating and uploading a `README.md` file.\\n* Clicking on the **Edit model card** button in your model repository.\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_sharing.md', 'start_index': 9825}, page_content=\"Take a look at the DistilBert [model card](https://huggingface.co/distilbert-base-uncased) for a good example of the type of information a model card should include. For more details about other options you can control in the `README.md` file such as a model's carbon footprint or widget examples, refer to the documentation [here](https://huggingface.co/docs/hub/models-cards).\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# LoRA\\n\\n<Tip warning={true}>\\n\\nThis is experimental and the API may change in the future.\\n\\n</Tip>'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 587}, page_content='# LoRA\\n\\n<Tip warning={true}>\\n\\nThis is experimental and the API may change in the future.\\n\\n</Tip>\\n\\n[LoRA (Low-Rank Adaptation of Large Language Models)](https://hf.co/papers/2106.09685) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights into the model and only these are trained. This makes training with LoRA much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), which are easier to store and share. LoRA can also be combined with other training techniques like DreamBooth to speedup training.\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 1229}, page_content='<Tip>\\n\\nLoRA is very versatile and supported for [DreamBooth](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py), [Kandinsky 2.2](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_lora_decoder.py), [Stable Diffusion XL](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py), [text-to-image](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py), and [Wuerstchen](https://github.com/huggingface/diffusers/blob/main/examples/wuerstchen/text_to_image/train_text_to_image_lora_prior.py).\\n\\n</Tip>\\n\\nThis guide will explore the [train_text_to_image_lora.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora.py) script to help you become more familiar with it, and how you can adapt it for your own use-case.'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 2186}, page_content='Before running the script, make sure you install the library from source:\\n\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install .'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 2347}, page_content='```\\n\\nNavigate to the example folder with the training script and install the required dependencies for the script you\\'re using:\\n\\n<hfoptions id=\"installation\">\\n<hfoption id=\"PyTorch\">\\n\\n```bash\\ncd examples/text_to_image\\npip install -r requirements.txt\\n```\\n\\n</hfoption>\\n<hfoption id=\"Flax\">\\n\\n```bash\\ncd examples/text_to_image\\npip install -r requirements_flax.txt\\n```\\n\\n</hfoption>\\n</hfoptions>\\n\\n<Tip>\\n\\n🤗 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It\\'ll automatically configure your training setup based on your hardware and environment. Take a look at the 🤗 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\\n\\n</Tip>\\n\\nInitialize an 🤗 Accelerate environment:\\n\\n```bash\\naccelerate config\\n```\\n\\nTo setup a default 🤗 Accelerate environment without choosing any configurations:\\n\\n```bash\\naccelerate config default'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 3237}, page_content=\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```bash\\nfrom accelerate.utils import write_basic_config\\n\\nwrite_basic_config()\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 3412}, page_content=\"```\\n\\nLastly, if you want to train a model on your own dataset, take a look at the [Create a dataset for training](create_dataset) guide to learn how to create a dataset that works with the training script.\\n\\n<Tip>\\n\\nThe following sections highlight parts of the training script that are important for understanding how to modify it, but it doesn't cover every aspect of the script in detail. If you're interested in learning more, feel free to read through the [script](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/text_to_image_lora.py) and let us know if you have any questions or concerns.\\n\\n</Tip>\\n\\n## Script parameters\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 4033}, page_content=\"</Tip>\\n\\n## Script parameters\\n\\nThe training script has many parameters to help you customize your training run. All of the parameters and their descriptions are found in the [`parse_args()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L85) function. Default values are provided for most parameters that work pretty well, but you can also set your own values in the training command if you'd like.\\n\\nFor example, to increase the number of epochs to train:\\n\\n```bash\\naccelerate launch train_text_to_image_lora.py \\\\\\n  --num_train_epochs=150 \\\\\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 4664}, page_content=\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#script-parameters) training guide, so this guide just focuses on the LoRA relevant parameters:\\n\\n- `--rank`: the number of low-rank matrices to train\\n- `--learning_rate`: the default learning rate is 1e-4, but with LoRA, you can use a higher learning rate\\n\\n## Training script\\n\\nThe dataset preprocessing code and training loop are found in the [`main()`](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L371) function, and if you need to adapt the training script, this is where you'll make your changes.\\n\\nAs with the script parameters, a walkthrough of the training script is provided in the [Text-to-image](text2image#training-script) training guide. Instead, this guide takes a look at the LoRA relevant parts of the script.\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 5577}, page_content='The script begins by adding the [new LoRA weights](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L447) to the attention layers. This involves correctly configuring the weight size for each block in the UNet. You\\'ll see the `rank` parameter is used to create the [`~models.attention_processor.LoRAAttnProcessor`]:\\n\\n```py\\nlora_attn_procs = {}\\nfor name in unet.attn_processors.keys():\\n    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\\n    if name.startswith(\"mid_block\"):\\n        hidden_size = unet.config.block_out_channels[-1]\\n    elif name.startswith(\"up_blocks\"):\\n        block_id = int(name[len(\"up_blocks.\")])\\n        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\\n    elif name.startswith(\"down_blocks\"):\\n        block_id = int(name[len(\"down_blocks.\")])\\n        hidden_size = unet.config.block_out_channels[block_id]'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 6574}, page_content='lora_attn_procs[name] = LoRAAttnProcessor(\\n        hidden_size=hidden_size,\\n        cross_attention_dim=cross_attention_dim,\\n        rank=args.rank,\\n    )\\n\\nunet.set_attn_processor(lora_attn_procs)\\nlora_layers = AttnProcsLayers(unet.attn_processors)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 6823}, page_content=\"```\\n\\nThe [optimizer](https://github.com/huggingface/diffusers/blob/dd9a5caf61f04d11c0fa9f3947b69ab0010c9a0f/examples/text_to_image/train_text_to_image_lora.py#L519) is initialized with the `lora_layers` because these are the only weights that'll be optimized:\\n\\n```py\\noptimizer = optimizer_cls(\\n    lora_layers.parameters(),\\n    lr=args.learning_rate,\\n    betas=(args.adam_beta1, args.adam_beta2),\\n    weight_decay=args.adam_weight_decay,\\n    eps=args.adam_epsilon,\\n)\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 7290}, page_content=\"```\\n\\nAside from setting up the LoRA layers, the training script is more or less the same as train_text_to_image.py!\\n\\n## Launch the script\\n\\nOnce you've made all your changes or you're okay with the default configuration, you're ready to launch the training script! 🚀\\n\\nLet's train on the [Pokémon BLIP captions](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions) dataset to generate our yown Pokémon. Set the environment variables `MODEL_NAME` and `DATASET_NAME` to the model and dataset respectively. You should also specify where to save the model in `OUTPUT_DIR`, and the name of the model to save to on the Hub with `HUB_MODEL_ID`. The script creates and saves the following files to your repository:\\n\\n- saved model checkpoints\\n- `pytorch_lora_weights.safetensors` (the trained LoRA weights)\\n\\nIf you're training on more than one GPU, add the `--multi_gpu` parameter to the `accelerate launch` command.\\n\\n<Tip warning={true}>\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 8210}, page_content='<Tip warning={true}>\\n\\nA full training run takes ~5 hours on a 2080 Ti GPU with 11GB of VRAM.\\n\\n</Tip>\\n\\n```bash\\nexport MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\\nexport OUTPUT_DIR=\"/sddata/finetune/lora/pokemon\"\\nexport HUB_MODEL_ID=\"pokemon-lora\"\\nexport DATASET_NAME=\"lambdalabs/pokemon-blip-captions\"\\n\\naccelerate launch --mixed_precision=\"fp16\"  train_text_to_image_lora.py \\\\\\n  --pretrained_model_name_or_path=$MODEL_NAME \\\\\\n  --dataset_name=$DATASET_NAME \\\\\\n  --dataloader_num_workers=8 \\\\\\n  --resolution=512 \\\\\\n  --center_crop \\\\\\n  --random_flip \\\\\\n  --train_batch_size=1 \\\\\\n  --gradient_accumulation_steps=4 \\\\\\n  --max_train_steps=15000 \\\\\\n  --learning_rate=1e-04 \\\\\\n  --max_grad_norm=1 \\\\\\n  --lr_scheduler=\"cosine\" \\\\\\n  --lr_warmup_steps=0 \\\\\\n  --output_dir=${OUTPUT_DIR} \\\\\\n  --push_to_hub \\\\\\n  --hub_model_id=${HUB_MODEL_ID} \\\\\\n  --report_to=wandb \\\\\\n  --checkpointing_steps=500 \\\\\\n  --validation_prompt=\"A pokemon with blue eyes.\" \\\\\\n  --seed=1337'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/training/lora.md', 'start_index': 9146}, page_content='```\\n\\nOnce training has been completed, you can use your model for inference:\\n\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(\"cuda\")\\npipeline.load_lora_weights(\"path/to/lora/model\", weight_name=\"pytorch_lora_weights.safetensors\")\\nimage = pipeline(\"A pokemon with blue eyes\").images[0]\\n```\\n\\n## Next steps\\n\\nCongratulations on training a new model with LoRA! To learn more about how to use your new model, the following guides may be helpful:\\n\\n- Learn how to [load different LoRA formats](../using-diffusers/loading_adapters#LoRA) trained using community trainers like Kohya and TheLastBen.\\n- Learn how to use and [combine multiple LoRA\\'s](../tutorials/using_peft_for_inference) with PEFT for inference.'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 0}, page_content='--\\ntitle: MAPE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  Mean Absolute Percentage Error (MAPE) is the mean percentage error difference between the predicted and actual\\n  values.\\n---\\n\\n# Metric Card for MAPE\\n\\n\\n## Metric Description\\n\\nMean Absolute Error (MAPE) is the mean of the percentage error of difference between the predicted $x_i$ and actual $y_i$ numeric values:\\n![image](https://user-images.githubusercontent.com/8100/200005316-c3975d32-8978-40f3-b541-c2ef57ec7c5b.png)\\n\\n## How to Use\\n\\nAt minimum, this metric requires predictions and references as inputs.\\n\\n```python\\n>>> mape_metric = evaluate.load(\"mape\")\\n>>> predictions = [2.5, 0.0, 2, 8]\\n>>> references = [3, -0.5, 2, 7]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 869}, page_content='```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (`n_samples`, `n_outputs`), representing the estimated target values.\\n- `references`: numeric array-like of shape (`n_samples,`) or (`n_samples`, `n_outputs`), representing the ground truth (correct) target values.\\n\\nOptional arguments:\\n- `sample_weight`: numeric array-like of shape (`n_samples,`) representing sample weights. The default is `None`.\\n- `multioutput`: `raw_values`, `uniform_average` or numeric array-like of shape (`n_outputs,`), which defines the aggregation of multiple output values. The default value is `uniform_average`.\\n  - `raw_values` returns a full set of errors in case of multioutput input.\\n  - `uniform_average` means that the errors of all outputs are averaged with uniform weight. \\n  - the array-like value defines weights used to average errors.'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 1747}, page_content=\"### Output Values\\nThis metric outputs a dictionary, containing the mean absolute error score, which is of type:\\n- `float`: if multioutput is `uniform_average` or an ndarray of weights, then the weighted average of all output errors is returned.\\n- numeric array-like of shape (`n_outputs,`): if multioutput is `raw_values`, then the score is returned for each output separately. \\n\\nEach MAPE `float` value is postive with the best value being 0.0.\\n\\nOutput Example(s):\\n```python\\n{'mape': 0.5}\"),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 2237}, page_content='```\\n\\nIf `multioutput=\"raw_values\"`:\\n```python\\n{\\'mape\\': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\n\\nExample with the `uniform_average` config:\\n```python\\n>>> mape_metric = evaluate.load(\"mape\")\\n>>> predictions = [2.5, 0.0, 2, 8]\\n>>> references = [3, -0.5, 2, 7]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)\\n>>> print(results)\\n{\\'mape\\': 0.3273...}\\n```\\n\\nExample with multi-dimensional lists, and the `raw_values` config:\\n```python\\n>>> mape_metric = evaluate.load(\"mape\", \"multilist\")\\n>>> predictions = [[0.5, 1], [-1, 1], [7, -6]]\\n>>> references = [[0.1, 2], [-1, 2], [8, -5]]\\n>>> results = mape_metric.compute(predictions=predictions, references=references)\\n>>> print(results)\\n{\\'mape\\': 0.8874...}\\n>>> results = mape_metric.compute(predictions=predictions, references=references, multioutput=\\'raw_values\\')\\n>>> print(results)\\n{\\'mape\\': array([1.3749..., 0.4])}'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 3156}, page_content='```\\n\\n## Limitations and Bias\\nOne limitation of MAPE is that it cannot be used if the ground truth is zero or close to zero. This metric is also asymmetric in that it puts a heavier penalty on predictions less than the ground truth and a smaller penalty on predictions bigger than the ground truth and thus can lead to a bias of methods being select which under-predict if selected via this metric.\\n\\n## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n  title={Scikit-learn: Machine Learning in {P}ython},\\n  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n  journal={Journal of Machine Learning Research},\\n  volume={12},\\n  pages={2825--2830},\\n  year={2011}\\n}'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/mape/README.md', 'start_index': 4063}, page_content='```\\n\\n```bibtex\\n@article{DEMYTTENAERE201638,\\n    title = {Mean Absolute Percentage Error for regression models},\\n    journal = {Neurocomputing},\\n    volume = {192},\\n    pages = {38--48},\\n    year = {2016},\\n    note = {Advances in artificial neural networks, machine learning and computational intelligence},\\n    issn = {0925-2312},\\n    doi = {https://doi.org/10.1016/j.neucom.2015.12.114},\\n    url = {https://www.sciencedirect.com/science/article/pii/S0925231216003325},\\n    author = {Arnaud {de Myttenaere} and Boris Golden and Bénédicte {Le Grand} and Fabrice Rossi},\\n}\\n```\\n\\n## Further References\\n- [Mean absolute percentage error - Wikipedia](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 1}, page_content=\"# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural architecture that builds on the Inception family of architectures but incorporates [residual connections](https://paperswithcode.com/method/residual-connection) (replacing the filter concatenation stage of the Inception architecture).\\n\\nThis particular model was trained for study of adversarial examples (adversarial training).\\n\\nThe weights from this model were ported from [Tensorflow/Models](https://github.com/tensorflow/models).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True)\\nmodel.eval()\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 707}, page_content='```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 1444}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 2197}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model('ens_adv_inception_resnet_v2', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 2825}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 3202}, page_content='```BibTeX\\n@article{DBLP:journals/corr/abs-1804-00097,\\n  author    = {Alexey Kurakin and\\n               Ian J. Goodfellow and\\n               Samy Bengio and\\n               Yinpeng Dong and\\n               Fangzhou Liao and\\n               Ming Liang and\\n               Tianyu Pang and\\n               Jun Zhu and\\n               Xiaolin Hu and\\n               Cihang Xie and\\n               Jianyu Wang and\\n               Zhishuai Zhang and\\n               Zhou Ren and\\n               Alan L. Yuille and\\n               Sangxia Huang and\\n               Yao Zhao and\\n               Yuzhe Zhao and\\n               Zhonglin Han and\\n               Junjiajia Long and\\n               Yerkebulan Berdibekov and\\n               Takuya Akiba and\\n               Seiya Tokui and\\n               Motoki Abe},\\n  title     = {Adversarial Attacks and Defences Competition},\\n  journal   = {CoRR},\\n  volume    = {abs/1804.00097},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1804.00097},\\n  archivePrefix = {arXiv},'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 4105}, page_content='year      = {2018},\\n  url       = {http://arxiv.org/abs/1804.00097},\\n  archivePrefix = {arXiv},\\n  eprint    = {1804.00097},\\n  timestamp = {Thu, 31 Oct 2019 16:31:22 +0100},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-00097.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 4420}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 4425}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: Ensemble Adversarial\\n  Paper:\\n    Title: Adversarial Attacks and Defences Competition\\n    URL: https://paperswithcode.com/paper/adversarial-attacks-and-defences-competition\\nModels:\\n- Name: ens_adv_inception_resnet_v2\\n  In Collection: Ensemble Adversarial\\n  Metadata:\\n    FLOPs: 16959133120\\n    Parameters: 55850000\\n    File Size: 223774238\\n    Architecture:\\n    - 1x1 Convolution\\n    - Auxiliary Classifier\\n    - Average Pooling\\n    - Average Pooling\\n    - Batch Normalization\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inception-v3 Module\\n    - Max Pooling\\n    - ReLU\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Data:\\n    - ImageNet\\n    ID: ens_adv_inception_resnet_v2\\n    Crop Pct: '0.897'\\n    Image Size: '299'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/inception_resnet_v2.py#L351\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/ensemble-adversarial.md', 'start_index': 5396}, page_content='Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ens_adv_inception_resnet_v2-2592a550.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 1.0%\\n      Top 5 Accuracy: 17.32%\\n-->'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': 1}, page_content='Using Flair at Hugging Face\\n\\n[Flair](https://github.com/flairNLP/flair) is a very simple framework for state-of-the-art NLP.\\nDeveloped by [Humboldt University of Berlin](https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en/) and friends.\\n\\n## Exploring Flair in the Hub\\n\\nYou can find `flair` models by filtering at the left of the [models page](https://huggingface.co/models?library=flair).\\n\\nAll models on the Hub come with these useful features:\\n\\n1. An automatically generated model card with a brief description.\\n2. An interactive widget you can use to play with the model directly in the browser.\\n3. An Inference API that allows you to make inference requests.\\n\\n## Installation\\n\\nTo get started, you can follow the [Flair installation guide](https://github.com/flairNLP/flair?tab=readme-ov-file#requirements-and-installation).\\nYou can also use the following one-line install through pip:\\n\\n```\\n$ pip install -U flair'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': 904}, page_content='```\\n$ pip install -U flair\\n```\\n\\n## Using existing models\\n\\nAll `flair` models can easily be loaded from the Hub:\\n\\n```py\\nfrom flair.data import Sentence\\nfrom flair.models import SequenceTagger\\n\\n# load tagger\\ntagger = SequenceTagger.load(\"flair/ner-multi\")\\n```\\n\\nOnce loaded, you can use `predict()` to perform inference:\\n\\n```py\\nsentence = Sentence(\"George Washington ging nach Washington.\")\\ntagger.predict(sentence)\\n\\n# print sentence\\nprint(sentence)\\n```\\n\\nIt outputs the following:\\n\\n```text\\nSentence[6]: \"George Washington ging nach Washington.\" → [\"George Washington\"/PER, \"Washington\"/LOC]'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': 1492}, page_content='```\\n\\nIf you want to load a specific Flair model, you can click `Use in Flair` in the model card and you will be given a working snippet!\\n\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet1.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet1-dark.png\"/>\\n</div>\\n<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet2.png\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-flair_snippet2-dark.png\"/>\\n</div>\\n\\n## Additional resources'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/flair.md', 'start_index': 2323}, page_content='## Additional resources\\n\\n* Flair [repository](https://github.com/flairNLP/flair)\\n* Flair [docs](https://flairnlp.github.io/docs/intro)\\n* Official Flair [models](https://huggingface.co/flair) on the Hub (mainly trained by [@alanakbik](https://huggingface.co/alanakbik) and [@stefan-it](https://huggingface.co/stefan-it))'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/accordion/README.md', 'start_index': 1}, page_content='`@gradio/button`\\n\\n```html\\n<script>\\n\\timport { Button } from \"@gradio/button\";\\n</script>\\n\\n<button type=\"primary|secondary\" href=\"string\" on:click=\"{e.detail === href}\">\\n\\tcontent\\n</button>\\n```'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': 0}, page_content='he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library that provides an API to quickly download many public datasets and preprocess them. In this video we will explore how to do that. The downloading part is easy: with the load_dataset function, you can directly download and cache a dataset from its identifier on the Dataset hub. Here we fetch the MRPC dataset from the GLUE benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which is a sort of dictionary containing each split of our dataset. We can access each split by indexing with its name. This split is then an instance of the Dataset class, with columns (here sentence1, sentence2. label and idx) and rows. We can access a given element by its index. The amazing thing about the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': 903}, page_content='the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which means that even if your dataset is huge you won\\'t get out of RAM: only the elements you request are loaded in memory. Accessing a slice of your dataset is as easy as one element. The result is then a dictionary with list of values for each keys (here the list of labels, the list of first sentences and the list of second sentences). The features attribute of a Dataset gives us more information about its columns. In particular, we can see here it gives us the correspondence between the integers and names for the labels. 0 stands for not equivalent and 1 for equivalent. To preprocess all the elements of our dataset, we need to tokenize them. Have a look at the video \"Preprocess sentence pairs\" for a refresher, but you just have to send the two sentences to the tokenizer with some additional keyword arguments. Here we indicate a maximum length of 128 and pad inputs shorter than this length,'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': 1806}, page_content=\"arguments. Here we indicate a maximum length of 128 and pad inputs shorter than this length, truncate inputs that are longer. We put all of this in a tokenize_function that we can directly apply to all the splits in our dataset with the map method. As long as the function returns a dictionary-like object, the map method will add new columns as needed or update existing ones. To speed up preprocessing and take advantage of the fact our tokenizer is backed by Rust thanks to the Hugging Face Tokenizers library, we can process several elements at the same time to our tokenize function, using the batched=True argument. Since the tokenizer can handle list of first/second sentences, the tokenize_function does not need to change for this. You can also use multiprocessing with the map method, check out its documentation! Once this is done, we are almost ready for training: we just remove the columns we don't need anymore with the remove_columns method, rename label to labels (since the models\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02a_datasets-overview-pt.md', 'start_index': 2710}, page_content=\"we don't need anymore with the remove_columns method, rename label to labels (since the models from Hugging Face Transformers expect that) and set the output format to our desired backend: torch, tensorflow or numpy. If needed, we can also generate a short sample of a dataset using the select method.\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 0}, page_content='--\\ntitle: \"Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too\"\\nthumbnail: /blog/assets/78_ml_director_insights/mantis1.png\\nauthors:\\n- user: mattupson\\n  guest: true\\n---\\n\\n# Why we’re switching to Hugging Face Inference Endpoints, and maybe you should too\\n\\n\\n\\nHugging Face recently launched [Inference Endpoints](https://huggingface.co/inference-endpoints); which as they put it: solves transformers in production. Inference Endpoints is a managed service that allows you to:\\n\\n- Deploy (almost) any model on Hugging Face Hub\\n- To any cloud (AWS, and Azure, GCP on the way)\\n- On a range of instance types (including GPU)\\n- We’re switching some of our Machine Learning (ML) models that do inference on a CPU to this new service. This blog is about why, and why you might also want to consider it.\\n\\n## What were we doing?'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 823}, page_content='## What were we doing?\\n\\nThe models that we have switched over to Inference Endpoints were previously managed internally and were running on AWS [Elastic Container Service](https://aws.amazon.com/ecs/) (ECS) backed by [AWS Fargate](https://aws.amazon.com/fargate/). This gives you a serverless cluster which can run container based tasks. Our process was as follows:\\n\\n- Train model on a GPU instance (provisioned by [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))\\n- Upload to [Hugging Face Hub](https://huggingface.co/models)\\n- Build API to serve model [(FastAPI)](https://fastapi.tiangolo.com/)\\n- Wrap API in container [(Docker)](https://www.docker.com/)\\n- Upload container to AWS [Elastic Container Repository](https://aws.amazon.com/ecr/) (ECR)\\n- Deploy model to ECS Cluster'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 1655}, page_content='Now, you can reasonably argue that ECS was not the best approach to serving ML models, but it served us up until now, and also allowed ML models to sit alongside other container based services, so it reduced cognitive load.\\n\\n## What do we do now?\\n\\nWith Inference Endpoints, our flow looks like this:\\n\\n- Train model on a GPU instance (provisioned by  [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))\\n- Upload to [Hugging Face Hub](https://huggingface.co/models)\\n- Deploy using Hugging Face Inference Endpoints.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 2219}, page_content='So this is significantly easier. We could also use another managed service such as [SageMaker](https://aws.amazon.com/es/sagemaker/), [Seldon](https://www.seldon.io/), or [Bento ML](https://www.bentoml.com/), etc., but since we are already uploading our model to Hugging Face hub to act as a model registry, and we’re pretty invested in Hugging Face’s other tools (like transformers, and [AutoTrain](https://huggingface.co/autotrain)) using Inference Endpoints makes a lot of sense for us.\\n\\n\\n## What about Latency and Stability?\\n\\nBefore switching to Inference Endpoints we tested different CPU endpoints types using [ab](https://httpd.apache.org/docs/2.4/programs/ab.html).\\n\\nFor ECS we didn’t test so extensively, but we know that a large container had a latency of about ~200ms from an instance in the same region. The tests we did for Inference Endpoints we based on text classification model fine tuned on [RoBERTa](https://huggingface.co/roberta-base) with the following test parameters:'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 3212}, page_content='- Requester region: eu-east-1\\n- Requester instance size: t3-medium\\n- Inference endpoint region: eu-east-1\\n- Endpoint Replicas: 1\\n- Concurrent connections: 1\\n- Requests: 1000 (1000 requests in 1–2 minutes even from a single connection would represent very heavy use for this particular application)\\n\\nThe following table shows latency (ms ± standard deviation and time to complete test in seconds) for four Intel Ice Lake equipped CPU endpoints.\\n\\n```bash\\nsize   |  vCPU (cores) |   Memory (GB)  |  ECS (ms) |  🤗 (ms)\\n----------------------------------------------------------------------\\nsmall  |  1            |  2             |   _       | ~ 296   \\nmedium |  2            |  4             |   _       | 156 ± 51 (158s)  \\nlarge  |  4            |   8            |   ~200    | 80 ± 30 (80s)   \\nxlarge |  8            | 16             |  _        | 43 ± 31 (43s)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 4076}, page_content='```\\nWhat we see from these results is pretty encouraging. The application that will consume these endpoints serves requests in real time, so we need as low latency as possible. We can see that the vanilla Hugging Face container was more than twice as fast as our bespoke container run on ECS — the slowest response we received from the large Inference Endpoint was just 108ms.\\n\\n## What about the cost?\\n\\nSo how much does this all cost? The table below shows a price comparison for what we were doing previously (ECS + Fargate) and using Inference Endpoints.\\n\\n```bash\\nsize   |  vCPU         |   Memory (GB)  |  ECS      |  🤗       |  % diff\\n----------------------------------------------------------------------\\nsmall  |  1            |  2             |  $ 33.18  | $ 43.80   |  0.24\\nmedium |  2            |  4             |  $ 60.38  | $ 87.61   |  0.31 \\nlarge  |  4            |  8             |  $ 114.78 | $ 175.22  |  0.34\\nxlarge |  8            | 16             |  $ 223.59 | $ 350.44  | 0.5'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 5074}, page_content='```\\n\\nWe can say a couple of things about this. Firstly, we want a managed solution to deployment, we don’t have a dedicated MLOPs team (yet), so we’re looking for a solution that helps us minimize the time we spend on deploying models, even if it costs a little more than handling the deployments ourselves.\\n\\nInference Endpoints are more expensive that what we were doing before, there’s an increased cost of between 24% and 50%. At the scale we’re currently operating, this additional cost, a difference of ~$60 a month for a large CPU instance is nothing compared to the time and cognitive load we are saving by not having to worry about APIs, and containers. If we were deploying 100s of ML microservices we would probably want to think again, but that is probably true of many approaches to hosting.\\n\\n## Some notes and caveats:'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 5879}, page_content='## Some notes and caveats:\\n\\n- You can find pricing for Inference Endpoints [here](https://huggingface.co/pricing#endpoints), but a different number is displayed when you deploy a new endpoint from the [GUI](https://ui.endpoints.huggingface.co/new). I’ve used the latter, which is higher.\\n- The values that I present in the table for ECS + Fargate are an underestimate, but probably not by much. I extracted them from the [fargate pricing page](https://aws.amazon.com/fargate/pricing/) and it includes just the cost of hosting the instance. I’m not including the data ingress/egress (probably the biggest thing is downloading the model from Hugging Face hub), nor have I included the costs related to ECR.\\n\\n## Other considerations\\n\\n### Deployment Options'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 6585}, page_content='## Other considerations\\n\\n### Deployment Options\\n\\nCurrently you can deploy an Inference Endpoint from the [GUI](https://ui.endpoints.huggingface.co/new) or using a [RESTful API](https://huggingface.co/docs/inference-endpoints/api_reference). You can also make use of our command line tool [hugie](https://github.com/MantisAI/hfie) (which will be the subject of a future blog) to launch Inference Endpoints in one line of code by passing a configuration, it’s really this simple:\\n\\n```bash\\nhugie endpoint create example/development.json'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 7119}, page_content='```\\n\\nFor me, what’s lacking is a [custom terraform provider](https://www.hashicorp.com/blog/writing-custom-terraform-providers). It’s all well and good deploying an inference endpoint from a [GitHub action](https://github.com/features/actions) using hugie, as we do, but it would be better if we could use the awesome state machine that is terraform to keep track of these. I’m pretty sure that someone (if not Hugging Face) will write one soon enough — if not, we will.\\n\\n### Hosting multiple models on a single endpoint\\n\\nPhilipp Schmid posted a really nice blog about how to write a custom [Endpoint Handler](https://www.philschmid.de/multi-model-inference-endpoints) class to allow you to host multiple models on a single endpoint, potentially saving you quite a bit of money. His blog was about GPU inference, and the only real limitation is how many models you can fit into the GPU memory. I assume this will also work for CPU instances, though I’ve not tried yet.\\n\\n## To conclude…'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/mantis-case-study.md', 'start_index': 8089}, page_content='## To conclude…\\n\\nWe find Hugging Face Inference Endpoints to be a very simple and convenient way to deploy transformer (and [sklearn](https://huggingface.co/scikit-learn)) models into an endpoint so they can be consumed by an application. Whilst they cost a little more than the ECS approach we were using before, it’s well worth it because it saves us time on thinking about deployment, we can concentrate on the thing we want to: building NLP solutions for our clients to help solve their problems.\\n\\n_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!_\\n\\n_This article was originally published on February 15, 2023 [in Medium](https://medium.com/mantisnlp/why-were-switching-to-hugging-face-inference-endpoints-and-maybe-you-should-too-829371dcd330)._'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 0}, page_content='--\\ntitle: ROUGE\\nemoji: 🤗 \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\npinned: false\\ntags:\\n- evaluate\\n- metric\\ndescription: >-\\n  ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for\\n  evaluating automatic summarization and machine translation software in natural language processing.\\n  The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\\n  \\n  Note that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\\n  \\n  This metrics is a wrapper around Google Research reimplementation of ROUGE:\\n  https://github.com/google-research/google-research/tree/master/rouge\\n---\\n\\n# Metric Card for ROUGE'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 811}, page_content='# Metric Card for ROUGE\\n\\n## Metric Description\\nROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation.\\n\\nNote that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\\n\\nThis metrics is a wrapper around the [Google Research reimplementation of ROUGE](https://github.com/google-research/google-research/tree/master/rouge)'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 1495}, page_content='## How to Use\\nAt minimum, this metric takes as input a list of predictions and a list of references:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello there\", \"general kenobi\"]\\n>>> references = [\"hello there\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references)\\n>>> print(results)\\n{\\'rouge1\\': 1.0, \\'rouge2\\': 1.0, \\'rougeL\\': 1.0, \\'rougeLsum\\': 1.0}'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 1931}, page_content='```\\n\\nOne can also pass a custom tokenizer which is especially useful for non-latin languages.\\n```python\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n                            tokenizer=lambda x: x.split())\\n>>> print(results)\\n{\\'rouge1\\': 1.0, \\'rouge2\\': 1.0, \\'rougeL\\': 1.0, \\'rougeLsum\\': 1.0}\\n```\\n\\nIt can also deal with lists of references for each predictions:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello there\", \"general kenobi\"]\\n>>> references = [[\"hello\", \"there\"], [\"general kenobi\", \"general yoda\"]]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references)\\n>>> print(results)\\n{\\'rouge1\\': 0.8333, \\'rouge2\\': 0.5, \\'rougeL\\': 0.8333, \\'rougeLsum\\': 0.8333}```'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 2716}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 2725}, page_content='### Inputs\\n- **predictions** (`list`): list of predictions to score. Each prediction\\n        should be a string with tokens separated by spaces.\\n- **references** (`list` or `list[list]`): list of reference for each prediction or a list of several references per prediction. Each\\n        reference should be a string with tokens separated by spaces.\\n- **rouge_types** (`list`): A list of rouge types to calculate. Defaults to `[\\'rouge1\\', \\'rouge2\\', \\'rougeL\\', \\'rougeLsum\\']`.\\n    - Valid rouge types:\\n        - `\"rouge1\"`: unigram (1-gram) based scoring\\n        - `\"rouge2\"`: bigram (2-gram) based scoring\\n        - `\"rougeL\"`: Longest common subsequence based scoring.\\n        - `\"rougeLSum\"`: splits text using `\"\\\\n\"`\\n        - See [here](https://github.com/huggingface/datasets/issues/617) for more information\\n- **use_aggregator** (`boolean`): If True, returns aggregates. Defaults to `True`.'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 3535}, page_content='- **use_aggregator** (`boolean`): If True, returns aggregates. Defaults to `True`.\\n- **use_stemmer** (`boolean`): If `True`, uses Porter stemmer to strip word suffixes. Defaults to `False`.'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 3726}, page_content=\"### Output Values\\nThe output is a dictionary with one entry for each rouge type in the input list `rouge_types`. If `use_aggregator=False`, each dictionary entry is a list of scores, with one score for each sentence. E.g. if `rouge_types=['rouge1', 'rouge2']` and `use_aggregator=False`, the output is:\\n\\n```python\\n{'rouge1': [0.6666666666666666, 1.0], 'rouge2': [0.0, 1.0]}\"),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 4100}, page_content='```\\n\\nIf `rouge_types=[\\'rouge1\\', \\'rouge2\\']` and `use_aggregator=True`, the output is of the following format:\\n```python\\n{\\'rouge1\\': 1.0, \\'rouge2\\': 1.0}\\n```\\n\\nThe ROUGE values are in the range of 0 to 1.\\n\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nAn example without aggregation:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello goodbye\", \"ankh morpork\"]\\n>>> references = [\"goodbye\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n...                         use_aggregator=False)\\n>>> print(list(results.keys()))\\n[\\'rouge1\\', \\'rouge2\\', \\'rougeL\\', \\'rougeLsum\\']\\n>>> print(results[\"rouge1\"])\\n[0.5, 0.0]'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 4795}, page_content='```\\n\\nThe same example, but with aggregation:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello goodbye\", \"ankh morpork\"]\\n>>> references = [\"goodbye\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n...                         use_aggregator=True)\\n>>> print(list(results.keys()))\\n[\\'rouge1\\', \\'rouge2\\', \\'rougeL\\', \\'rougeLsum\\']\\n>>> print(results[\"rouge1\"])\\n0.25\\n```\\n\\nThe same example, but only calculating `rouge_1`:\\n```python\\n>>> rouge = evaluate.load(\\'rouge\\')\\n>>> predictions = [\"hello goodbye\", \"ankh morpork\"]\\n>>> references = [\"goodbye\", \"general kenobi\"]\\n>>> results = rouge.compute(predictions=predictions,\\n...                         references=references,\\n...                         rouge_types=[\\'rouge_1\\'],\\n...                         use_aggregator=True)\\n>>> print(list(results.keys()))\\n[\\'rouge1\\']\\n>>> print(results[\"rouge1\"])\\n0.25'),\n",
              " Document(metadata={'source': 'huggingface/evaluate/blob/main/metrics/rouge/README.md', 'start_index': 5729}, page_content='```\\n\\n## Limitations and Bias\\nSee [Schluter (2017)](https://aclanthology.org/E17-2007/) for an in-depth discussion of many of ROUGE\\'s limits.\\n\\n## Citation\\n```bibtex\\n@inproceedings{lin-2004-rouge,\\n    title = \"{ROUGE}: A Package for Automatic Evaluation of Summaries\",\\n    author = \"Lin, Chin-Yew\",\\n    booktitle = \"Text Summarization Branches Out\",\\n    month = jul,\\n    year = \"2004\",\\n    address = \"Barcelona, Spain\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://www.aclweb.org/anthology/W04-1013\",\\n    pages = \"74--81\",\\n}\\n```\\n\\n## Further References\\n- This metrics is a wrapper around the [Google Research reimplementation of ROUGE](https://github.com/google-research/google-research/tree/master/rouge)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# AudioLDM'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 587}, page_content='# AudioLDM\\n\\nAudioLDM was proposed in [AudioLDM: Text-to-Audio Generation with Latent Diffusion Models](https://huggingface.co/papers/2301.12503) by Haohe Liu et al. Inspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview), AudioLDM\\nis a text-to-audio _latent diffusion model (LDM)_ that learns continuous audio representations from [CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap)\\nlatents. AudioLDM takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional\\nsound effects, human speech and music.\\n\\nThe abstract from the paper is:'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 1232}, page_content='*Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 2136}, page_content='frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at [this https URL](https://audioldm.github.io/).*'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 2393}, page_content='The original codebase can be found at [haoheliu/AudioLDM](https://github.com/haoheliu/AudioLDM).\\n\\n## Tips\\n\\nWhen constructing a prompt, keep in mind:\\n\\n* Descriptive prompt inputs work best; you can use adjectives to describe the sound (for example, \"high quality\" or \"clear\") and make the prompt context specific (for example, \"water stream in a forest\" instead of \"stream\").\\n* It\\'s best to use general terms like \"cat\" or \"dog\" instead of specific names or abstract objects the model may not be familiar with.\\n\\nDuring inference:\\n\\n* The _quality_ of the predicted audio sample can be controlled by the `num_inference_steps` argument; higher steps give higher quality audio at the expense of slower inference.\\n* The _length_ of the predicted audio sample can be controlled by varying the `audio_length_in_s` argument.\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/audioldm.md', 'start_index': 3210}, page_content='<Tip>\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## AudioLDMPipeline\\n[[autodoc]] AudioLDMPipeline\\n\\t- all\\n\\t- __call__\\n\\n## AudioPipelineOutput\\n[[autodoc]] pipelines.AudioPipelineOutput'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter2/04c_character-based-tokenizers.md', 'start_index': 0}, page_content=\"efore diving in character-based tokenization, understanding why this kind of tokenization is interesting requires understanding the flaws of word-based tokenization. If you haven't seen the first video on word-based tokenization we recommend you check it out before looking at this video. Let's take a look at character-based tokenization. We now split our text into individual characters, rather than words. There are generally a lot of different words in languages, while the number of characters stays low. Here for example, for the English language that has an estimated 170,000 different words, we would need a very large vocabulary to encompass all words. With a character-based vocabulary, we can get by with only 256 characters! Even languages with a lot of different characters like the Chinese languages have dictionaries with ~20,000 different characters but more than 375,000 different words. Character-based vocabularies let us fewer different tokens than the word-based tokenization\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter2/04c_character-based-tokenizers.md', 'start_index': 898}, page_content='words. Character-based vocabularies let us fewer different tokens than the word-based tokenization dictionaries we would otherwise use. These vocabularies are also more complete than their word-based vocabularies counterparts. As our vocabulary contains all characters used in a language, even words unseen during the tokenizer training can still be tokenized, so out-of-vocabulary tokens will be less frequent. This includes the ability to correctly tokenize misspelled words, rather than discarding them as unknown straight away. However, this algorithm isn\\'t perfect either! Intuitively, characters do not hold as much information individually as a word would hold. For example, \"Let\\'s\" holds more information than \"l\". Of course, this is not true for all languages, as some languages like ideogram-based languages have a lot of information held in single characters, but for others like roman-based languages, the model will have to make sense of multiple tokens at a time to get the information'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter2/04c_character-based-tokenizers.md', 'start_index': 1801}, page_content='languages, the model will have to make sense of multiple tokens at a time to get the information held in a single word. This leads to another issue with character-based tokenizers: their sequences are translated into very large amount of tokens to be processed by the model. This can have an impact on the size of the context the model will carry around, and will reduce the size of the text we can use as input for our model. This tokenization, while it has some issues, has seen some very good results in the past and should be considered when approaching a new problem as it solves some issues encountered in the word-based algorithm.'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 1}, page_content=\"Hands-on\\n\\nNow that you learned the basics of multi-agents, you're ready to train your first agents in a multi-agent system: **a 2vs2 soccer team that needs to beat the opponent team**.\\n\\nAnd you’re going to participate in AI vs. AI challenges where your trained agent will compete against other classmates’ **agents every day and be ranked on a new leaderboard.**\\n\\nTo validate this hands-on for the certification process, you just need to push a trained model. There **are no minimal results to attain to validate it.**\\n\\nFor more information about the certification process, check this section 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)\"),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 764}, page_content=\"This hands-on will be different since to get correct results **you need to train your agents from 4 hours to 8 hours**. And given the risk of timeout in Colab, we advise you to train on your computer. You don’t need a supercomputer: a simple laptop is good enough for this exercise.\\n\\nLet's get started! 🔥\\n\\n## What is AI vs. AI?\\n\\nAI vs. AI is an open-source tool we developed at Hugging Face to compete agents on the Hub against one another in a multi-agent setting. These models are then ranked in a leaderboard.\\n\\nThe idea of this tool is to have a robust evaluation tool: **by evaluating your agent with a lot of others, you’ll get a good idea of the quality of your policy.**\\n\\nMore precisely, AI vs. AI is three tools:\"),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 1443}, page_content='More precisely, AI vs. AI is three tools:\\n\\n- A *matchmaking process* defining the matches (which model against which) and running the model fights using a background task in the Space.\\n- A *leaderboard* getting the match history results and displaying the models’ ELO ratings: [https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos](https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos)\\n- A *Space demo* to visualize your agents playing against others: [https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos)\\n\\nIn addition to these three tools, your classmate cyllum created a 🤗 SoccerTwos Challenge Analytics where you can check the detailed match results of a model: [https://huggingface.co/spaces/cyllum/soccertwos-analytics](https://huggingface.co/spaces/cyllum/soccertwos-analytics)'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 2323}, page_content=\"We're [wrote a blog post to explain this AI vs. AI tool in detail](https://huggingface.co/blog/aivsai), but to give you the big picture it works this way:\\n\\n- Every four hours, our algorithm **fetches all the available models for a given environment (in our case ML-Agents-SoccerTwos).**\\n- It creates a **queue of matches with the matchmaking algorithm.**\\n- We simulate the match in a Unity headless process and **gather the match result** (1 if the first model won, 0.5 if it’s a draw, 0 if the second model won) in a Dataset.\\n- Then, when all matches from the matches queue are done, **we update the ELO score for each model and update the leaderboard.**\\n\\n### Competition Rules\\n\\nThis first AI vs. AI competition **is an experiment**: the goal is to improve the tool in the future with your feedback. So some **breakups can happen during the challenge**. But don't worry\\n**all the results are saved in a dataset so we can always restart the calculation correctly without losing information**.\"),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 3317}, page_content=\"In order for your model to get correctly evaluated against others you need to follow these rules:\\n\\n1. **You can't change the observation space or action space of the agent.** By doing that your model will not work during evaluation.\\n2. You **can't use a custom trainer for now,** you need to use the Unity MLAgents ones.\\n3. We provide executables to train your agents. You can also use the Unity Editor if you prefer **, but to avoid bugs, we advise that you use our executables**.\\n\\nWhat will make the difference during this challenge are **the hyperparameters you choose**.\\n\\nWe're constantly trying to improve our tutorials, so\\xa0**if you find some issues in this notebook**, please\\xa0[open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).\\n\\n### Chat with your classmates, share advice and ask questions on Discord\"),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 4089}, page_content='### Chat with your classmates, share advice and ask questions on Discord\\n\\n- We created a new channel called `ai-vs-ai-challenge` to exchange advice and ask questions.\\n- If you didn’t join the discord server yet, you can [join here](https://discord.gg/ydHrjt3WP5)\\n\\n## Step 0: Install MLAgents and download the correct executable\\n\\nWe advise you to use [conda](https://docs.conda.io/en/latest/) as a package manager and create a new environment.\\n\\nWith conda, we create a new environment called rl with **Python 3.10.12**:\\n\\n```bash\\nconda create --name rl python=3.10.12\\nconda activate rl'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 4673}, page_content='```\\n\\nTo be able to train our agents correctly and push to the Hub, we need to install ML-Agents\\n\\n```bash\\ngit clone https://github.com/Unity-Technologies/ml-agents\\n```\\n\\nWhen the cloning is done (it takes 2.63 GB), we go inside the repository and install the package\\n\\n```bash\\ncd ml-agents\\npip install -e ./ml-agents-envs\\npip install -e ./ml-agents'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 5019}, page_content='```\\n\\nFinally, you need to install git-lfs: https://git-lfs.com/\\n\\nNow that it’s installed, we need to add the environment training executable. Based on your operating system you need to download one of them, unzip it and place it in a new folder inside `ml-agents` that you call `training-envs-executables`\\n\\nAt the end your executable should be in `ml-agents/training-envs-executables/SoccerTwos`\\n\\nWindows: Download [this executable](https://drive.google.com/file/d/1sqFxbEdTMubjVktnV4C6ICjp89wLhUcP/view?usp=sharing)\\n\\nLinux (Ubuntu): Download [this executable](https://drive.google.com/file/d/1KuqBKYiXiIcU4kNMqEzhgypuFP5_45CL/view?usp=sharing)\\n\\nMac: Download [this executable](https://drive.google.com/drive/folders/1h7YB0qwjoxxghApQdEUQmk95ZwIDxrPG?usp=share_link)\\n⚠ For Mac you need also to call this `xattr -cr training-envs-executables/SoccerTwos/SoccerTwos.app` to be able to run SoccerTwos\\n\\n## Step 1: Understand the environment'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 5917}, page_content='## Step 1: Understand the environment\\n\\nThe environment is called `SoccerTwos`. The Unity MLAgents Team made it. You can find its documentation [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md#soccer-twos)\\n\\nThe goal in this environment **is to get the ball into the opponent\\'s goal while preventing the ball from entering your own goal.**\\n\\n<figure>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif\" alt=\"SoccerTwos\"/>\\n\\n<figcaption>This environment was made by the <a href=\"https://github.com/Unity-Technologies/ml-agents\"> Unity MLAgents Team</a></figcaption>\\n\\n</figure>\\n\\n### The reward function\\n\\nThe reward function is:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccerreward.png\" alt=\"SoccerTwos Reward\"/>\\n\\n### The observation space\\n\\nThe observation space is composed of vectors of size 336:'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 6818}, page_content='### The observation space\\n\\nThe observation space is composed of vectors of size 336:\\n\\n- 11 ray-casts forward distributed over 120 degrees (264 state dimensions)\\n- 3 ray-casts backward distributed over 90 degrees (72 state dimensions)\\n- Both of these ray-casts can detect 6 objects:\\n    - Ball\\n    - Blue Goal\\n    - Purple Goal\\n    - Wall\\n    - Blue Agent\\n    - Purple Agent\\n\\n### The action space\\n\\nThe action space is three discrete branches:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/socceraction.png\" alt=\"SoccerTwos Action\"/>\\n\\n## Step 2: Understand MA-POCA\\n\\nWe know how to train agents to play against others: **we can use self-play.** This is a perfect technique for a 1vs1.\\n\\nBut in our case we’re 2vs2, and each team has 2 agents. How then can we **train cooperative behavior for groups of agents?**'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 7688}, page_content='As explained in the [Unity Blog](https://blog.unity.com/technology/ml-agents-v20-release-now-supports-training-complex-cooperative-behaviors), agents typically receive a reward as a group (+1 - penalty) when the team scores a goal. This implies that **every agent on the team is rewarded even if each agent didn’t contribute the same to the win**, which makes it difficult to learn what to do independently.\\n\\nThe Unity MLAgents team developed the solution in a new multi-agent trainer called *MA-POCA (Multi-Agent POsthumous Credit Assignment)*.\\n\\nThe idea is simple but powerful: a centralized critic **processes the states of all agents in the team to estimate how well each agent is doing**. Think of this critic as a coach.\\n\\nThis allows each agent to **make decisions based only on what it perceives locally**, and **simultaneously evaluate how good its behavior is in the context of the whole group**.'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 8596}, page_content='<figure>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/mapoca.png\" alt=\"MA POCA\"/>\\n\\n<figcaption>This illustrates MA-POCA’s centralized learning and decentralized execution. Source: <a href=\"https://blog.unity.com/technology/ml-agents-plays-dodgeball\">MLAgents Plays Dodgeball</a>\\n</figcaption>\\n\\n</figure>\\n\\nThe solution then is to use Self-Play with an MA-POCA trainer (called poca). The poca trainer will help us to train cooperative behavior and self-play to win against an opponent team.\\n\\nIf you want to dive deeper into this MA-POCA algorithm, you need to read the paper they published [here](https://arxiv.org/pdf/2111.05992.pdf) and the sources we put on the additional readings section.\\n\\n## Step 3: Define the config file\\n\\nWe already learned in [Unit 5](https://huggingface.co/deep-rl-course/unit5/introduction) that in ML-Agents, you define **the training hyperparameters in `config.yaml` files.**'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 9562}, page_content='There are multiple hyperparameters. To understand them better, you should read the explanations for each of them in\\xa0**[the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)**\\n\\nThe config file we’re going to use here is in  `./config/poca/SoccerTwos.yaml`. It looks like this:'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 9910}, page_content='```csharp\\nbehaviors:\\n  SoccerTwos:\\n    trainer_type: poca\\n    hyperparameters:\\n      batch_size: 2048\\n      buffer_size: 20480\\n      learning_rate: 0.0003\\n      beta: 0.005\\n      epsilon: 0.2\\n      lambd: 0.95\\n      num_epoch: 3\\n      learning_rate_schedule: constant\\n    network_settings:\\n      normalize: false\\n      hidden_units: 512\\n      num_layers: 2\\n      vis_encode_type: simple\\n    reward_signals:\\n      extrinsic:\\n        gamma: 0.99\\n        strength: 1.0\\n    keep_checkpoints: 5\\n    max_steps: 5000000\\n    time_horizon: 1000\\n    summary_freq: 10000\\n    self_play:\\n      save_steps: 50000\\n      team_change: 200000\\n      swap_steps: 2000\\n      window: 10\\n      play_against_latest_model_ratio: 0.5\\n      initial_elo: 1200.0'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 10644}, page_content='```\\n\\nCompared to Pyramids or SnowballTarget, we have new hyperparameters with a self-play part. How you modify them can be critical in getting good results.\\n\\nThe advice I can give you here is to check the explanation and recommended value for each parameters (especially self-play ones) against\\xa0**[the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md).**\\n\\nNow that you’ve modified our config file, you’re ready to train your agents.\\n\\n## Step 4: Start the training\\n\\nTo train the agents, we need to\\xa0**launch mlagents-learn and select the executable containing the environment.**\\n\\nWe define four parameters:\\n\\n1. `mlagents-learn <config>`: the path where the hyperparameter config file is.\\n2. `-env`: where the environment executable is.\\n3. `-run_id`: the name you want to give to your training run id.\\n4. `-no-graphics`: to not launch the visualization during the training.'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 11587}, page_content='Depending on your hardware, 5M timesteps (the recommended value, but you can also try 10M) will take 5 to 8 hours of training. You can continue using your computer in the meantime, but I advise deactivating the computer standby mode to prevent the training from being stopped.\\n\\nDepending on the executable you use (windows, ubuntu, mac) the training command will look like this (your executable path can be different so don’t hesitate to check before running).\\n\\n```bash\\nmlagents-learn ./config/poca/SoccerTwos.yaml --env=./training-envs-executables/SoccerTwos.exe --run-id=\"SoccerTwos\" --no-graphics'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 12187}, page_content='```\\n\\nThe executable contains 8 copies of SoccerTwos.\\n\\n⚠️ It’s normal if you don’t see a big increase of ELO score (and even a decrease below 1200) before 2M timesteps, since your agents will spend most of their time moving randomly on the field before being able to goal.\\n\\n⚠️ You can stop the training with Ctrl + C but beware of typing this command only once to stop the training since MLAgents needs to generate a final .onnx file before closing the run.\\n\\n## Step 5: **Push the agent to the Hugging Face Hub**\\n\\nNow that we trained our agents, we’re\\xa0**ready to push them to the Hub to be able to participate in the AI vs. AI challenge and visualize them playing on your browser🔥.**\\n\\nTo be able to share your model with the community, there are three more steps to follow:\\n\\n1️⃣ (If it’s not already done) create an account to HF ➡\\xa0[https://huggingface.co/join](https://huggingface.co/join)\\n\\n2️⃣ Sign in and store your authentication token from the Hugging Face website.'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 13078}, page_content='2️⃣ Sign in and store your authentication token from the Hugging Face website.\\n\\nCreate a new token (https://huggingface.co/settings/tokens)\\xa0**with write role**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\\n\\nCopy the token, run this, and paste the token\\n\\n```bash\\nhuggingface-cli login'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 13467}, page_content='```\\n\\nThen, we need to run `mlagents-push-to-hf`.\\n\\nAnd we define four parameters:\\n\\n1. `-run-id`: the name of the training run id.\\n2. `-local-dir`: where the agent was saved, it’s results/<run_id name>, so in my case results/First Training.\\n3. `-repo-id`: the name of the Hugging Face repo you want to create or update. It’s always <your huggingface username>/<the repo name>\\nIf the repo does not exist **it will be created automatically**\\n4. `--commit-message`: since HF repos are git repositories you need to give a commit message.\\n\\nIn my case\\n\\n```bash\\nmlagents-push-to-hf  --run-id=\"SoccerTwos\" --local-dir=\"./results/SoccerTwos\" --repo-id=\"ThomasSimonini/poca-SoccerTwos\" --commit-message=\"First Push\"`\\n```\\n\\n```bash\\nmlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id --commit-message=\"First Push\"'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 14322}, page_content=\"```\\n\\nIf everything worked you should see this at the end of the process (but with a different url 😆) :\\n\\nYour model is pushed to the Hub. You can view your model here: https://huggingface.co/ThomasSimonini/poca-SoccerTwos\\n\\nIt's the link to your model. It contains a model card that explains how to use it, your Tensorboard, and your config file. **What's awesome is that it's a git repository, which means you can have different commits, update your repository with a new push, etc.**\\n\\n## Step 6: Verify that your model is ready for AI vs AI Challenge\\n\\nNow that your model is pushed to the Hub, **it’s going to be added automatically to the AI vs AI Challenge model pool.** It can take a little bit of time before your model is added to the leaderboard given we do a run of matches every 4h.\\n\\nBut to ensure that everything works perfectly you need to check:\"),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 15114}, page_content='But to ensure that everything works perfectly you need to check:\\n\\n1. That you have this tag in your model: ML-Agents-SoccerTwos. This is the tag we use to select models to be added to the challenge pool. To do that go to your model and check the tags\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify1.png\" alt=\"Verify\"/>\\n\\n\\nIf it’s not the case you just need to modify the readme and add it\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify2.png\" alt=\"Verify\"/>\\n\\n2. That you have a `SoccerTwos.onnx` file\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify3.png\" alt=\"Verify\"/>\\n\\nWe strongly suggest that you create a new model when you push to the Hub if you want to train it again or train a new version.\\n\\n## Step 7: Visualize some match in our demo'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit7/hands-on.mdx', 'start_index': 16011}, page_content=\"## Step 7: Visualize some match in our demo\\n\\nNow that your model is part of AI vs AI Challenge, **you can visualize how good it is compared to others**: https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos\\n\\nIn order to do that, you just need to go to this demo:\\n\\n- Select your model as team blue (or team purple if you prefer) and another model to compete against. The best opponents to compare your model to are either whoever is on top of the leaderboard or the [baseline model](https://huggingface.co/unity/MLAgents-SoccerTwos)\\n\\nThe matches you see live are not used in the calculation of your result **but they are a good way to visualize how good your agent is**.\\n\\nAnd don't hesitate to share the best score your agent gets on discord in the #rl-i-made-this channel 🔥\"),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/sales_projections/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: sales_projections\\n\\n\\n```\\n!pip install -q gradio pandas numpy matplotlib\\n```'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/sales_projections/run.ipynb', 'start_index': 91}, page_content='```\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nimport gradio as gr\\n\\n\\ndef sales_projections(employee_data):\\n    sales_data = employee_data.iloc[:, 1:4].astype(\"int\").to_numpy()\\n    regression_values = np.apply_along_axis(\\n        lambda row: np.array(np.poly1d(np.polyfit([0, 1, 2], row, 2))), 0, sales_data\\n    )\\n    projected_months = np.repeat(\\n        np.expand_dims(np.arange(3, 12), 0), len(sales_data), axis=0\\n    )\\n    projected_values = np.array(\\n        [\\n            month * month * regression[0] + month * regression[1] + regression[2]\\n            for month, regression in zip(projected_months, regression_values)\\n        ]\\n    )\\n    plt.plot(projected_values.T)\\n    plt.legend(employee_data[\"Name\"])\\n    return employee_data, plt.gcf(), regression_values'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/sales_projections/run.ipynb', 'start_index': 869}, page_content='demo = gr.Interface(\\n    sales_projections,\\n    gr.Dataframe(\\n        headers=[\"Name\", \"Jan Sales\", \"Feb Sales\", \"Mar Sales\"],\\n        value=[[\"Jon\", 12, 14, 18], [\"Alice\", 14, 17, 2], [\"Sana\", 8, 9.5, 12]],\\n    ),\\n    [\"dataframe\", \"plot\", \"numpy\"],\\n    description=\"Enter sales figures for employees to predict sales trajectory over year.\",\\n)\\nif __name__ == \"__main__\":\\n    demo.launch()'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/sales_projections/run.ipynb', 'start_index': 1260}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 1}, page_content='Metric Card for F1\\n\\n\\n## Metric Description\\n\\nThe F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\\nF1 = 2 * (precision * recall) / (precision + recall)\\n\\n\\n## How to Use\\n\\nAt minimum, this metric requires predictions and references as input\\n\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(predictions=[0, 1], references=[0, 1])\\n>>> print(results)\\n[\"{\\'f1\\': 1.0}\"]'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 445}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 451}, page_content=\"### Inputs\\n- **predictions** (`list` of `int`): Predicted labels.\\n- **references** (`list` of `int`): Ground truth labels.\\n- **labels** (`list` of `int`): The set of labels to include when `average` is not set to `'binary'`, and the order of the labels if `average` is `None`. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class. Labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in `predictions` and `references` are used in sorted order. Defaults to None.\\n- **pos_label** (`int`): The class to be considered the positive class, in the case where `average` is set to `binary`. Defaults to 1.\"),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 1220}, page_content=\"- **average** (`string`): This parameter is required for multiclass/multilabel targets. If set to `None`, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data. Defaults to `'binary'`.\\n    - 'binary': Only report results for the class specified by `pos_label`. This is applicable only if the classes found in `predictions` and `references` are binary.\\n    - 'micro': Calculate metrics globally by counting the total true positives, false negatives and false positives.\\n    - 'macro': Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\\n    - 'weighted': Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters `'macro'` to account for label imbalance. This option can result in an F-score that is not between precision and recall.\"),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 2155}, page_content=\"- 'samples': Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).\\n- **sample_weight** (`list` of `float`): Sample weights Defaults to None.\"),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 2353}, page_content=\"### Output Values\\n- **f1**(`float` or `array` of `float`): F1 score or list of f1 scores, depending on the value passed to `average`. Minimum possible value is 0. Maximum possible value is 1. Higher f1 scores are better.\\n\\nOutput Example(s):\\n```python\\n{'f1': 0.26666666666666666}\"),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 2632}, page_content='```\\n```python\\n{\\'f1\\': array([0.8, 0.0, 0.0])}\\n```\\n\\nThis metric outputs a dictionary, with either a single f1 score, of type `float`, or an array of f1 scores, with entries of type `float`.\\n\\n\\n#### Values from Popular Papers\\n\\n\\n\\n\\n### Examples\\n\\nExample 1-A simple binary example\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0])\\n>>> print(results)\\n{\\'f1\\': 0.5}\\n```\\n\\nExample 2-The same simple binary example as in Example 1, but with `pos_label` set to `0`.\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], pos_label=0)\\n>>> print(round(results[\\'f1\\'], 2))\\n0.67'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 3370}, page_content='```\\n\\nExample 3-The same simple binary example as in Example 1, but with `sample_weight` included.\\n```python\\n>>> f1_metric = datasets.load_metric(\"f1\")\\n>>> results = f1_metric.compute(references=[0, 1, 0, 1, 0], predictions=[0, 0, 1, 1, 0], sample_weight=[0.9, 0.5, 3.9, 1.2, 0.3])\\n>>> print(round(results[\\'f1\\'], 2))\\n0.35'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 3691}, page_content='```\\n\\nExample 4-A multiclass example, with different values for the `average` input.\\n```python\\n>>> predictions = [0, 2, 1, 0, 0, 1]\\n>>> references = [0, 1, 2, 0, 1, 2]\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=\"macro\")\\n>>> print(round(results[\\'f1\\'], 2))\\n0.27\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=\"micro\")\\n>>> print(round(results[\\'f1\\'], 2))\\n0.33\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=\"weighted\")\\n>>> print(round(results[\\'f1\\'], 2))\\n0.27\\n>>> results = f1_metric.compute(predictions=predictions, references=references, average=None)\\n>>> print(results)\\n{\\'f1\\': array([0.8, 0. , 0. ])}'),\n",
              " Document(metadata={'source': 'huggingface/datasets/blob/main/metrics/f1/README.md', 'start_index': 4416}, page_content='```\\n\\n\\n## Limitations and Bias\\n\\n\\n\\n## Citation(s)\\n```bibtex\\n@article{scikit-learn,\\n    title={Scikit-learn: Machine Learning in {P}ython},\\n    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n           and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n           and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n           Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n    journal={Journal of Machine Learning Research},\\n    volume={12},\\n    pages={2825--2830},\\n    year={2011}\\n}\\n```\\n\\n\\n## Further References'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# TimeSformer\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 746}, page_content='-->\\n\\n# TimeSformer\\n\\n## Overview\\n\\nThe TimeSformer model was proposed in [TimeSformer: Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Facebook Research.\\nThis work is a milestone in action-recognition field being the first video transformer. It inspired many transformer based video understanding and classification papers.\\n\\nThe abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 1168}, page_content='*We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named \"TimeSformer,\" adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that \"divided attention,\" where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 2069}, page_content='(at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: [this https URL](https://github.com/facebookresearch/TimeSformer).*'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/timesformer.md', 'start_index': 2281}, page_content='This model was contributed by [fcakyon](https://huggingface.co/fcakyon).\\nThe original code can be found [here](https://github.com/facebookresearch/TimeSformer).\\n\\n## Usage tips\\n\\nThere are many pretrained variants. Select your pretrained model based on the dataset it is trained on. Moreover,\\nthe number of input frames per clip changes based on the model size so you should consider this parameter while selecting your pretrained model.\\n\\n## Resources\\n\\n- [Video classification task guide](../tasks/video_classification)\\n\\n## TimesformerConfig\\n\\n[[autodoc]] TimesformerConfig\\n\\n## TimesformerModel\\n\\n[[autodoc]] TimesformerModel\\n    - forward\\n\\n## TimesformerForVideoClassification\\n\\n[[autodoc]] TimesformerForVideoClassification\\n    - forward'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Swin Transformer V2\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 746}, page_content='-->\\n\\n# Swin Transformer V2\\n\\n## Overview\\n\\nThe Swin Transformer V2 model was proposed in [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\\n\\nThe abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 1106}, page_content='*Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 2003}, page_content=\"successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time.*\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 2583}, page_content='This model was contributed by [nandwalritik](https://huggingface.co/nandwalritik).\\nThe original code can be found [here](https://github.com/microsoft/Swin-Transformer).\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Swin Transformer v2.\\n\\n<PipelineTag pipeline=\"image-classification\"/>\\n\\n- [`Swinv2ForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\\n- See also: [Image classification task guide](../tasks/image_classification)\\n\\nBesides that:\\n\\n- [`Swinv2ForMaskedImageModeling`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/swinv2.md', 'start_index': 3489}, page_content=\"If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n## Swinv2Config\\n\\n[[autodoc]] Swinv2Config\\n\\n## Swinv2Model\\n\\n[[autodoc]] Swinv2Model\\n    - forward\\n\\n## Swinv2ForMaskedImageModeling\\n\\n[[autodoc]] Swinv2ForMaskedImageModeling\\n    - forward\\n\\n## Swinv2ForImageClassification\\n\\n[[autodoc]] transformers.Swinv2ForImageClassification\\n    - forward\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# RemBERT\\n\\n## Overview\\n\\nThe RemBERT model was proposed in [Rethinking Embedding Coupling in Pre-trained Language Models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Févry, Henry Tsai, Melvin Johnson, Sebastian Ruder.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 988}, page_content='The abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 1035}, page_content=\"*We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art\\npre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to\\nsignificantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By\\nreallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on\\nstandard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that\\nallocating additional capacity to the output embedding provides benefits to the model that persist through the\\nfine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger\\noutput embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 1950}, page_content='Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these\\nfindings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the\\nnumber of parameters at the fine-tuning stage.*'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 2232}, page_content='## Usage tips\\n\\nFor fine-tuning, RemBERT can be thought of as a bigger version of mBERT with an ALBERT-like factorization of the\\nembedding layer. The embeddings are not tied in pre-training, in contrast with BERT, which enables smaller input\\nembeddings (preserved during fine-tuning) and bigger output embeddings (discarded at fine-tuning). The tokenizer is\\nalso similar to the Albert one rather than the BERT one.\\n\\n## Resources\\n\\n- [Text classification task guide](../tasks/sequence_classification)\\n- [Token classification task guide](../tasks/token_classification)\\n- [Question answering task guide](../tasks/question_answering)\\n- [Causal language modeling task guide](../tasks/language_modeling)\\n- [Masked language modeling task guide](../tasks/masked_language_modeling)\\n- [Multiple choice task guide](../tasks/multiple_choice)\\n\\n## RemBertConfig\\n\\n[[autodoc]] RemBertConfig\\n\\n## RemBertTokenizer'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 3061}, page_content='## RemBertConfig\\n\\n[[autodoc]] RemBertConfig\\n\\n## RemBertTokenizer\\n\\n[[autodoc]] RemBertTokenizer\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n## RemBertTokenizerFast\\n\\n[[autodoc]] RemBertTokenizerFast\\n    - build_inputs_with_special_tokens\\n    - get_special_tokens_mask\\n    - create_token_type_ids_from_sequences\\n    - save_vocabulary\\n\\n<frameworkcontent>\\n<pt>\\n\\n## RemBertModel\\n\\n[[autodoc]] RemBertModel\\n    - forward\\n\\n## RemBertForCausalLM\\n\\n[[autodoc]] RemBertForCausalLM\\n    - forward\\n\\n## RemBertForMaskedLM\\n\\n[[autodoc]] RemBertForMaskedLM\\n    - forward\\n\\n## RemBertForSequenceClassification\\n\\n[[autodoc]] RemBertForSequenceClassification\\n    - forward\\n\\n## RemBertForMultipleChoice\\n\\n[[autodoc]] RemBertForMultipleChoice\\n    - forward\\n\\n## RemBertForTokenClassification\\n\\n[[autodoc]] RemBertForTokenClassification\\n    - forward\\n\\n## RemBertForQuestionAnswering\\n\\n[[autodoc]] RemBertForQuestionAnswering\\n    - forward'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/rembert.md', 'start_index': 3973}, page_content='## RemBertForQuestionAnswering\\n\\n[[autodoc]] RemBertForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFRemBertModel\\n\\n[[autodoc]] TFRemBertModel\\n    - call\\n\\n## TFRemBertForMaskedLM\\n\\n[[autodoc]] TFRemBertForMaskedLM\\n    - call\\n\\n## TFRemBertForCausalLM\\n\\n[[autodoc]] TFRemBertForCausalLM\\n    - call\\n\\n## TFRemBertForSequenceClassification\\n\\n[[autodoc]] TFRemBertForSequenceClassification\\n    - call\\n\\n## TFRemBertForMultipleChoice\\n\\n[[autodoc]] TFRemBertForMultipleChoice\\n    - call\\n\\n## TFRemBertForTokenClassification\\n\\n[[autodoc]] TFRemBertForTokenClassification\\n    - call\\n\\n## TFRemBertForQuestionAnswering\\n\\n[[autodoc]] TFRemBertForQuestionAnswering\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n[[open-in-colab]]\\n\\n# Performing inference with LCM-LoRA\\n\\nLatent Consistency Models (LCM) enable quality image generation in typically 2-4 steps making it possible to use diffusion models in almost real-time settings. \\n\\nFrom the [official website](https://latent-consistency-models.github.io/):'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 806}, page_content='From the [official website](https://latent-consistency-models.github.io/):\\n\\n> LCMs can be distilled from any pre-trained Stable Diffusion (SD) in only 4,000 training steps (~32 A100 GPU Hours) for generating high quality 768 x 768 resolution images in 2~4 steps or even one step, significantly accelerating text-to-image generation. We employ LCM to distill the Dreamshaper-V7 version of SD in just 4,000 training iterations.\\n\\nFor a more technical overview of LCMs, refer to [the paper](https://huggingface.co/papers/2310.04378).'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 1337}, page_content=\"However, each model needs to be distilled separately for latent consistency distillation. The core idea with LCM-LoRA is to train just a few adapter layers, the adapter being LoRA in this case. \\nThis way, we don't have to train the full model and keep the number of trainable parameters manageable. The resulting LoRAs can then be applied to any fine-tuned version of the model without distilling them separately.\\nAdditionally, the LoRAs can be applied to image-to-image, ControlNet/T2I-Adapter, inpainting, AnimateDiff etc. \\nThe LCM-LoRA can also be combined with other LoRAs to generate styled images in very few steps (4-8).\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 1966}, page_content=\"LCM-LoRAs are available for [stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5), [stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), and the [SSD-1B](https://huggingface.co/segmind/SSD-1B) model. All the checkpoints can be found in this [collection](https://huggingface.co/collections/latent-consistency/latent-consistency-models-loras-654cdd24e111e16f0865fba6).\\n\\nFor more details about LCM-LoRA, refer to [the technical report](https://huggingface.co/papers/2311.05556).\\n\\nThis guide shows how to perform inference with LCM-LoRAs for \\n- text-to-image\\n- image-to-image\\n- combined with styled LoRAs\\n- ControlNet/T2I-Adapter\\n- inpainting\\n- AnimateDiff\\n\\nBefore going through this guide, we'll take a look at the general workflow for performing inference with LCM-LoRAs.\\nLCM-LoRAs are similar to other Stable Diffusion LoRAs so they can be used with any [`DiffusionPipeline`] that supports LoRAs.\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 2931}, page_content=\"- Load the task specific pipeline and model.\\n- Set the scheduler to [`LCMScheduler`].\\n- Load the LCM-LoRA weights for the model.\\n- Reduce the `guidance_scale` between `[1.0, 2.0]` and set the `num_inference_steps` between [4, 8].\\n- Perform inference with the pipeline with the usual parameters.\\n\\nLet's look at how we can perform inference with LCM-LoRAs for different tasks.\\n\\nFirst, make sure you have [peft](https://github.com/huggingface/peft) installed, for better LoRA support.\\n\\n```bash\\npip install -U peft\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 3442}, page_content='```\\n\\n## Text-to-image\\n\\nYou\\'ll use the [`StableDiffusionXLPipeline`] with the scheduler: [`LCMScheduler`] and then load the LCM-LoRA. Together with the LCM-LoRA and the scheduler, the pipeline enables a fast inference workflow overcoming the slow iterative nature of diffusion models.\\n\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\\n    variant=\"fp16\",\\n    torch_dtype=torch.float16\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\\n\\nprompt = \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\"\\n\\ngenerator = torch.manual_seed(42)\\nimage = pipe(\\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\\n).images[0]'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 4339}, page_content=\"```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2i.png)\\n\\nNotice that we use only 4 steps for generation which is way less than what's typically used for standard SDXL.\\n\\n<Tip>\\n\\nYou may have noticed that we set `guidance_scale=1.0`, which disables classifer-free-guidance. This is because the LCM-LoRA is trained with guidance, so the batch size does not have to be doubled in this case. This leads to a faster inference time, with the drawback that negative prompts don't have any effect on the denoising process.\\n\\nYou can also use guidance with LCM-LoRA, but due to the nature of training the model is very sensitve to the `guidance_scale` values, high values can lead to artifacts in the generated images. In our experiments, we found that the best values are in the range of [1.0, 2.0].\\n\\n</Tip>\\n\\n### Inference with a fine-tuned model\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 5192}, page_content='</Tip>\\n\\n### Inference with a fine-tuned model\\n\\nAs mentioned above, the LCM-LoRA can be applied to any fine-tuned version of the model without having to distill them separately. Let\\'s look at how we can perform inference with a fine-tuned model. In this example, we\\'ll use the [animagine-xl](https://huggingface.co/Linaqruf/animagine-xl) model, which is a fine-tuned version of the SDXL model for generating anime.\\n\\n```python\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretrained(\\n    \"Linaqruf/animagine-xl\",\\n    variant=\"fp16\",\\n    torch_dtype=torch.float16\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\\n\\nprompt = \"face focus, cute, masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, night, turtleneck\"'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 6117}, page_content='generator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt, num_inference_steps=4, generator=generator, guidance_scale=1.0\\n).images[0]'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 6258}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2i_finetuned.png)\\n\\n\\n## Image-to-image\\n\\nLCM-LoRA can be applied to image-to-image tasks too. Let\\'s look at how we can perform image-to-image generation with LCMs. For this example we\\'ll use the [dreamshaper-7](https://huggingface.co/Lykon/dreamshaper-7) model and the LCM-LoRA for `stable-diffusion-v1-5 `.\\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForImage2Image, LCMScheduler\\nfrom diffusers.utils import make_image_grid, load_image\\n\\npipe = AutoPipelineForImage2Image.from_pretrained(\\n    \"Lykon/dreamshaper-7\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 7044}, page_content='# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\\n\\n# prepare image\\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png\"\\ninit_image = load_image(url)\\nprompt = \"Astronauts in a jungle, cold color palette, muted colors, detailed, 8k\"\\n\\n# pass prompt and image to pipeline\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt,\\n    image=init_image,\\n    num_inference_steps=4,\\n    guidance_scale=1,\\n    strength=0.6,\\n    generator=generator\\n).images[0]\\nmake_image_grid([init_image, image], rows=1, cols=2)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 7636}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_i2i.png)\\n\\n\\n<Tip>\\n\\nYou can get different results based on your prompt and the image you provide. To get the best results, we recommend trying different values for `num_inference_steps`, `strength`, and `guidance_scale` parameters and choose the best one.\\n\\n</Tip>\\n\\n\\n## Combine with styled LoRAs\\n\\nLCM-LoRA can be combined with other LoRAs to generate styled-images in very few steps (4-8). In the following example, we\\'ll use the LCM-LoRA with the [papercut LoRA](TheLastBen/Papercut_SDXL). \\nTo learn more about how to combine LoRAs, refer to [this guide](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#combine-multiple-adapters).\\n\\n```python\\nimport torch\\nfrom diffusers import DiffusionPipeline, LCMScheduler\\n\\npipe = DiffusionPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\\n    variant=\"fp16\",\\n    torch_dtype=torch.float16\\n).to(\"cuda\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 8635}, page_content='# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LoRAs\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\", adapter_name=\"lcm\")\\npipe.load_lora_weights(\"TheLastBen/Papercut_SDXL\", weight_name=\"papercut.safetensors\", adapter_name=\"papercut\")\\n\\n# Combine LoRAs\\npipe.set_adapters([\"lcm\", \"papercut\"], adapter_weights=[1.0, 0.8])\\n\\nprompt = \"papercut, a cute fox\"\\ngenerator = torch.manual_seed(0)\\nimage = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]\\nimage'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 9170}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdx_lora_mix.png)\\n\\n\\n## ControlNet/T2I-Adapter\\n\\nLet\\'s look at how we can perform inference with ControlNet/T2I-Adapter and LCM-LoRA. \\n\\n### ControlNet\\nFor this example, we\\'ll use the SD-v1-5 model and the LCM-LoRA for SD-v1-5 with canny ControlNet.\\n\\n```python\\nimport torch\\nimport cv2\\nimport numpy as np\\nfrom PIL import Image\\n\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, LCMScheduler\\nfrom diffusers.utils import load_image\\n\\nimage = load_image(\\n    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\\n).resize((512, 512))\\n\\nimage = np.array(image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 10118}, page_content='controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    controlnet=controlnet,\\n    torch_dtype=torch.float16,\\n    safety_checker=None,\\n    variant=\"fp16\"\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\\n\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    \"the mona lisa\",\\n    image=canny_image,\\n    num_inference_steps=4,\\n    guidance_scale=1.5,\\n    controlnet_conditioning_scale=0.8,\\n    cross_attention_kwargs={\"scale\": 1},\\n    generator=generator,\\n).images[0]\\nmake_image_grid([canny_image, image], rows=1, cols=2)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 10909}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_controlnet.png)\\n\\n\\n<Tip>\\nThe inference parameters in this example might not work for all examples, so we recommend you to try different values for `num_inference_steps`, `guidance_scale`, `controlnet_conditioning_scale` and `cross_attention_kwargs` parameters and choose the best one. \\n</Tip>\\n\\n### T2I-Adapter\\n\\nThis example shows how to use the LCM-LoRA with the [Canny T2I-Adapter](TencentARC/t2i-adapter-canny-sdxl-1.0) and SDXL.\\n\\n```python\\nimport torch\\nimport cv2\\nimport numpy as np\\nfrom PIL import Image\\n\\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, LCMScheduler\\nfrom diffusers.utils import load_image, make_image_grid\\n\\n# Prepare image\\n# Detect the canny map in low resolution to avoid high-frequency details\\nimage = load_image(\\n    \"https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg\"\\n).resize((384, 384))'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 11887}, page_content='image = np.array(image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image).resize((1024, 1024))\\n\\n# load adapter\\nadapter = T2IAdapter.from_pretrained(\"TencentARC/t2i-adapter-canny-sdxl-1.0\", torch_dtype=torch.float16, varient=\"fp16\").to(\"cuda\")\\n\\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\\n    \"stabilityai/stable-diffusion-xl-base-1.0\", \\n    adapter=adapter,\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\", \\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdxl\")\\n\\nprompt = \"Mystical fairy in real, magic, 4k picture, high quality\"\\nnegative_prompt = \"extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured\"'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 12844}, page_content='generator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt,\\n    negative_prompt=negative_prompt,\\n    image=canny_image,\\n    num_inference_steps=4,\\n    guidance_scale=1.5, \\n    adapter_conditioning_scale=0.8, \\n    adapter_conditioning_factor=1,\\n    generator=generator,\\n).images[0]\\nmake_image_grid([canny_image, image], rows=1, cols=2)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 13185}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2iadapter.png)\\n\\n\\n## Inpainting\\n\\nLCM-LoRA can be used for inpainting as well. \\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\\nfrom diffusers.utils import load_image, make_image_grid\\n\\npipe = AutoPipelineForInpainting.from_pretrained(\\n    \"runwayml/stable-diffusion-inpainting\",\\n    torch_dtype=torch.float16,\\n    variant=\"fp16\",\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\")\\n\\n# load base and mask image\\ninit_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png\")\\nmask_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 14123}, page_content='# generator = torch.Generator(\"cuda\").manual_seed(92)\\nprompt = \"concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k\"\\ngenerator = torch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt,\\n    image=init_image,\\n    mask_image=mask_image,\\n    generator=generator,\\n    num_inference_steps=4,\\n    guidance_scale=4, \\n).images[0]\\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 14555}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_inpainting.png)\\n\\n\\n## AnimateDiff\\n\\n[`AnimateDiff`] allows you to animate images using Stable Diffusion models. To get good results, we need to generate multiple frames (16-24), and doing this with standard SD models can be very slow. \\nLCM-LoRA can be used to speed up the process significantly, as you just need to do 4-8 steps for each frame. Let\\'s look at how we can perform animation with LCM-LoRA and AnimateDiff.\\n\\n```python\\nimport torch\\nfrom diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler, LCMScheduler\\nfrom diffusers.utils import export_to_gif\\n\\nadapter = MotionAdapter.from_pretrained(\"diffusers/animatediff-motion-adapter-v1-5\")\\npipe = AnimateDiffPipeline.from_pretrained(\\n    \"frankjoshua/toonyou_beta6\",\\n    motion_adapter=adapter,\\n).to(\"cuda\")\\n\\n# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 15441}, page_content='# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npipe.load_lora_weights(\"latent-consistency/lcm-lora-sdv1-5\", adapter_name=\"lcm\")\\npipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", weight_name=\"diffusion_pytorch_model.safetensors\", adapter_name=\"motion-lora\")\\n\\npipe.set_adapters([\"lcm\", \"motion-lora\"], adapter_weights=[0.55, 1.2])\\n\\nprompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\\ngenerator = torch.manual_seed(0)\\nframes = pipe(\\n    prompt=prompt,\\n    num_inference_steps=5,\\n    guidance_scale=1.25,\\n    cross_attention_kwargs={\"scale\": 1},\\n    num_frames=24,\\n    generator=generator\\n).frames[0]\\nexport_to_gif(frames, \"animation.gif\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/using-diffusers/inference_with_lcm_lora.md', 'start_index': 16206}, page_content='```\\n\\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdv1-5_animatediff.gif)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 1}, page_content=\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https://arxiv.org/abs/2310.04378) is a method to distill a latent diffusion model to enable swift inference with minimal steps. This example demonstrates how to use latent consistency distillation to distill SDXL for inference with few timesteps.\\n\\n## Full model distillation\\n\\n### Running locally with PyTorch\\n\\n#### Installing the dependencies\\n\\nBefore running the scripts, make sure to install the library's training dependencies:\\n\\n**Important**\\n\\nTo make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install -e .\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 939}, page_content=\"```\\n\\nThen cd in the example folder and run\\n```bash\\npip install -r requirements.txt\\n```\\n\\nAnd initialize an [🤗 Accelerate](https://github.com/huggingface/accelerate/) environment with:\\n\\n```bash\\naccelerate config\\n```\\n\\nOr for a default accelerate configuration without answering questions about your environment\\n\\n```bash\\naccelerate config default\\n```\\n\\nOr if your environment doesn't support an interactive shell e.g. a notebook\\n\\n```python\\nfrom accelerate.utils import write_basic_config\\nwrite_basic_config()\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 1443}, page_content='```\\n\\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramatic speedups.\\n\\n\\n#### Example\\n\\nThe following uses the [Conceptual Captions 12M (CC12M) dataset](https://github.com/google-research-datasets/conceptual-12m) as an example, and for illustrative purposes only. For best results you may consider large and high-quality text-image datasets such as [LAION](https://laion.ai/blog/laion-400-open-dataset/). You may also need to search the hyperparameter space according to the dataset you use.\\n\\n```bash\\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\\nexport OUTPUT_DIR=\"path/to/saved/model\"'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 2086}, page_content='accelerate launch train_lcm_distill_sdxl_wds.py \\\\\\n    --pretrained_teacher_model=$MODEL_NAME \\\\\\n    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\\\\n    --output_dir=$OUTPUT_DIR \\\\\\n    --mixed_precision=fp16 \\\\\\n    --resolution=1024 \\\\\\n    --learning_rate=1e-6 --loss_type=\"huber\" --use_fix_crop_and_size --ema_decay=0.95 --adam_weight_decay=0.0 \\\\\\n    --max_train_steps=1000 \\\\\\n    --max_train_samples=4000000 \\\\\\n    --dataloader_num_workers=8 \\\\\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\\\\n    --validation_steps=200 \\\\\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\\\\n    --train_batch_size=12 \\\\\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\\\\n    --gradient_accumulation_steps=1 \\\\\\n    --use_8bit_adam \\\\\\n    --resume_from_checkpoint=latest \\\\\\n    --report_to=wandb \\\\\\n    --seed=453645634 \\\\\\n    --push_to_hub \\\\'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 3079}, page_content='```\\n\\n## LCM-LoRA\\n\\nInstead of fine-tuning the full model, we can also just train a LoRA that can be injected into any SDXL model.\\n\\n### Example\\n\\nThe following uses the [Conceptual Captions 12M (CC12M) dataset](https://github.com/google-research-datasets/conceptual-12m) as an example. For best results you may consider large and high-quality text-image datasets such as [LAION](https://laion.ai/blog/laion-400-open-dataset/).\\n\\n```bash\\nexport MODEL_NAME=\"stabilityai/stable-diffusion-xl-base-1.0\"\\nexport OUTPUT_DIR=\"path/to/saved/model\"'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 3614}, page_content='accelerate launch train_lcm_distill_lora_sdxl_wds.py \\\\\\n    --pretrained_teacher_model=$MODEL_DIR \\\\\\n    --pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix \\\\\\n    --output_dir=$OUTPUT_DIR \\\\\\n    --mixed_precision=fp16 \\\\\\n    --resolution=1024 \\\\\\n    --lora_rank=64 \\\\\\n    --learning_rate=1e-6 --loss_type=\"huber\" --use_fix_crop_and_size --adam_weight_decay=0.0 \\\\\\n    --max_train_steps=1000 \\\\\\n    --max_train_samples=4000000 \\\\\\n    --dataloader_num_workers=8 \\\\\\n    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\\\\n    --validation_steps=200 \\\\\\n    --checkpointing_steps=200 --checkpoints_total_limit=10 \\\\\\n    --train_batch_size=12 \\\\\\n    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\\\\n    --gradient_accumulation_steps=1 \\\\\\n    --use_8bit_adam \\\\\\n    --resume_from_checkpoint=latest \\\\\\n    --report_to=wandb \\\\\\n    --seed=453645634 \\\\'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/consistency_distillation/README_sdxl.md', 'start_index': 4514}, page_content='--resume_from_checkpoint=latest \\\\\\n    --report_to=wandb \\\\\\n    --seed=453645634 \\\\\\n    --push_to_hub \\\\\\n```'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Autoformer\\n\\n## Overview\\n\\nThe Autoformer model was proposed in [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 1000}, page_content='This model augments the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.\\n\\nThe abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 1222}, page_content='*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 2122}, page_content='progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.*'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/autoformer.md', 'start_index': 2696}, page_content=\"This model was contributed by [elisim](https://huggingface.co/elisim) and [kashif](https://huggingface.co/kashif).\\nThe original code can be found [here](https://github.com/thuml/Autoformer).\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\\n\\n- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)](https://huggingface.co/blog/autoformer)\\n\\n## AutoformerConfig\\n\\n[[autodoc]] AutoformerConfig\\n\\n## AutoformerModel\\n\\n[[autodoc]] AutoformerModel\\n    - forward\\n\\n## AutoformerForPrediction\\n\\n[[autodoc]] AutoformerForPrediction\\n    - forward\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 0}, page_content='--\\ntitle: \"DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub\" \\nthumbnail: /blog/assets/hub_duckdb/hub_duckdb.png\\nauthors:\\n- user: stevhliu\\n- user: lhoestq\\n- user: severo\\n---\\n\\n# DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub\\n\\n\\nThe Hugging Face Hub is dedicated to providing open access to datasets for everyone and giving users the tools to explore and understand them. You can find many of the datasets used to train popular large language models (LLMs) like [Falcon](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [MPT](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf), and [StarCoder](https://huggingface.co/datasets/bigcode/the-stack). There are tools for addressing fairness and bias in datasets like [Disaggregators](https://huggingface.co/spaces/society-ethics/disaggregators), and tools for previewing examples inside a dataset like the Dataset Viewer.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 988}, page_content='<div class=\"flex justify-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets-server/oasst1_light.png\"/>\\n</div>\\n<small>A preview of the OpenAssistant dataset with the Dataset Viewer.</small>\\n\\nWe are happy to share that we recently added another feature to help you analyze datasets on the Hub; you can run SQL queries with DuckDB on any dataset stored on the Hub! According to the 2022 [StackOverflow Developer Survey](https://survey.stackoverflow.co/2022/#section-most-popular-technologies-programming-scripting-and-markup-languages), SQL is the 3rd most popular programming language. We also wanted a fast database management system (DBMS) designed for running analytical queries, which is why we’re excited about integrating with [DuckDB](https://duckdb.org/). We hope this allows even more users to access and analyze datasets on the Hub!\\n\\n## TLDR'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 1887}, page_content='## TLDR\\n\\n[Datasets Server](https://huggingface.co/docs/datasets-server/index) **automatically converts all public datasets on the Hub to Parquet files**, that you can see by clicking on the \"Auto-converted to Parquet\" button at the top of a dataset page. You can also access the list of the Parquet files URLs with a simple HTTP call.\\n\\n```py\\nr = requests.get(\"https://datasets-server.huggingface.co/parquet?dataset=blog_authorship_corpus\")\\nj = r.json()\\nurls = [f[\\'url\\'] for f in j[\\'parquet_files\\'] if f[\\'split\\'] == \\'train\\']\\nurls\\n[\\'https://huggingface.co/datasets/blog_authorship_corpus/resolve/refs%2Fconvert%2Fparquet/blog_authorship_corpus/blog_authorship_corpus-train-00000-of-00002.parquet\\',\\n \\'https://huggingface.co/datasets/blog_authorship_corpus/resolve/refs%2Fconvert%2Fparquet/blog_authorship_corpus/blog_authorship_corpus-train-00001-of-00002.parquet\\']'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 2750}, page_content='```\\n\\nCreate a connection to DuckDB and install and load the `httpfs` extension to allow reading and writing remote files:\\n\\n```py\\nimport duckdb\\n\\nurl = \"https://huggingface.co/datasets/blog_authorship_corpus/resolve/refs%2Fconvert%2Fparquet/blog_authorship_corpus/blog_authorship_corpus-train-00000-of-00002.parquet\"\\n\\ncon = duckdb.connect()\\ncon.execute(\"INSTALL httpfs;\")\\ncon.execute(\"LOAD httpfs;\")\\n```\\n\\nOnce you’re connected, you can start writing SQL queries!\\n\\n```sql\\ncon.sql(f\"\"\"SELECT horoscope, \\n\\tcount(*), \\n\\tAVG(LENGTH(text)) AS avg_blog_length \\n\\tFROM \\'{url}\\' \\n\\tGROUP BY horoscope \\n\\tORDER BY avg_blog_length \\n\\tDESC LIMIT(5)\"\"\"\\n)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 3384}, page_content=\"```\\n\\nTo learn more, check out the [documentation](https://huggingface.co/docs/datasets-server/parquet_process).\\n\\n## From dataset to Parquet\\n\\n[Parquet](https://parquet.apache.org/docs/) files are columnar, making them more efficient to store, load and analyze. This is especially important when you're working with large datasets, which we’re seeing more and more of in the LLM era. To support this, Datasets Server automatically converts and publishes any public dataset on the Hub as Parquet files. The URL to the Parquet files can be retrieved with the [`/parquet`](https://huggingface.co/docs/datasets-server/quick_start#access-parquet-files) endpoint.\\n\\n## Analyze with DuckDB\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/hub-duckdb.md', 'start_index': 4041}, page_content='## Analyze with DuckDB\\n\\nDuckDB offers super impressive performance for running complex analytical queries. It is able to execute a SQL query directly on a remote Parquet file without any overhead. With the [`httpfs`](https://duckdb.org/docs/extensions/httpfs) extension, DuckDB is able to query remote files such as datasets stored on the Hub using the URL provided from the `/parquet` endpoint. DuckDB also supports querying multiple Parquet files which is really convenient because Datasets Server shards big datasets into smaller 500MB chunks.\\n\\n## Looking forward\\n\\nKnowing what’s inside a dataset is important for developing models because it can impact model quality in all sorts of ways! By allowing users to write and execute any SQL query on Hub datasets, this is another way for us to enable open access to datasets and help users be more aware of the datasets contents. We are excited for you to try this out, and we’re looking forward to what kind of insights your analysis uncovers!'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 1}, page_content='Gradio and W&B Integration\\n\\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\\nTags: WANDB, SPACES\\nContributed by Gradio team\\n\\n## Introduction\\n\\nIn this Guide, we\\'ll walk you through:\\n\\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\\n\\n\\n## What is Wandb?\\n\\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:\\n\\n<img alt=\"Screen Shot 2022-08-01 at 5 54 59 PM\" src=\"https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png\">\\n\\n## What are Hugging Face Spaces & Gradio?\\n\\n### Gradio'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 918}, page_content=\"## What are Hugging Face Spaces & Gradio?\\n\\n### Gradio\\n\\nGradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.\\n\\nGet started [here](https://gradio.app/getting_started)\\n\\n### Hugging Face Spaces\\n\\nHugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).\\n\\n## Setting up a Gradio Demo for JoJoGAN\"),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 1760}, page_content=\"## Setting up a Gradio Demo for JoJoGAN\\n\\nNow, let's walk you through how to do this on your own. We'll make the assumption that you're new to W&B and Gradio for the purposes of this tutorial.\\n\\nLet's get started!\\n\\n1. Create a W&B account\\n\\n   Follow [these quick instructions](https://app.wandb.ai/login) to create your free account if you don’t have one already. It shouldn't take more than a couple minutes. Once you're done (or if you've already got an account), next, we'll run a quick colab.\\n\\n2. Open Colab Install Gradio and W&B\\n\\n   We'll be following along with the colab provided in the JoJoGAN repo with some minor modifications to use Wandb and Gradio more effectively.\\n\\n   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb)\\n\\n   Install Gradio and Wandb at the top:\\n\\n```sh\\n\\npip install gradio wandb\"),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 2675}, page_content='```\\n\\n3. Finetune StyleGAN and W&B experiment tracking\\n\\n   This next step will open a W&B dashboard to track your experiments and a gradio panel showing pretrained models to choose from a drop down menu from a Gradio Demo hosted on Huggingface Spaces. Here\\'s the code you need for that:\\n\\n   ```python\\n\\n   alpha =  1.0\\n   alpha = 1-alpha\\n\\n   preserve_color = True\\n   num_iter = 100\\n   log_interval = 50\\n\\n\\n   samples = []\\n   column_names = [\"Reference (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\\n\\n   wandb.init(project=\"JoJoGAN\")\\n   config = wandb.config\\n   config.num_iter = num_iter\\n   config.preserve_color = preserve_color\\n   wandb.log(\\n   {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\\n   step=0)\\n\\n   # load discriminator for perceptual loss\\n   discriminator = Discriminator(1024, 2).eval().to(device)\\n   ckpt = torch.load(\\'models/stylegan2-ffhq-config-f.pt\\', map_location=lambda storage, loc: storage)\\n   discriminator.load_state_dict(ckpt[\"d\"], strict=False)'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 3668}, page_content='# reset generator\\n   del generator\\n   generator = deepcopy(original_generator)\\n\\n   g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\\n\\n   # Which layers to swap for generating a family of plausible real images -> fake image\\n   if preserve_color:\\n       id_swap = [9,11,15,16,17]\\n   else:\\n       id_swap = list(range(7, generator.n_latent))\\n\\n   for idx in tqdm(range(num_iter)):\\n       mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\\n       in_latent = latents.clone()\\n       in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\\n\\n       img = generator(in_latent, input_is_latent=True)\\n\\n       with torch.no_grad():\\n           real_feat = discriminator(targets)\\n       fake_feat = discriminator(img)\\n\\n       loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 4506}, page_content='loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\\n\\n\\n       wandb.log({\"loss\": loss}, step=idx)\\n       if idx % log_interval == 0:\\n           generator.eval()\\n           my_sample = generator(my_w, input_is_latent=True)\\n           generator.train()\\n           my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\\n           wandb.log(\\n           {\"Current stylization\": [wandb.Image(my_sample)]},\\n           step=idx)\\n       table_data = [\\n               wandb.Image(transforms.ToPILImage()(target_im)),\\n               wandb.Image(img),\\n               wandb.Image(my_sample),\\n           ]\\n       samples.append(table_data)\\n\\n       g_optim.zero_grad()\\n       loss.backward()\\n       g_optim.step()\\n\\n   out_table = wandb.Table(data=samples, columns=column_names)\\n   wandb.log({\"Current Samples\": out_table})'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 5388}, page_content='```\\n\\nalpha = 1.0\\nalpha = 1-alpha\\n\\npreserve_color = True\\nnum_iter = 100\\nlog_interval = 50\\n\\nsamples = []\\ncolumn_names = [\"Referece (y)\", \"Style Code(w)\", \"Real Face Image(x)\"]\\n\\nwandb.init(project=\"JoJoGAN\")\\nconfig = wandb.config\\nconfig.num_iter = num_iter\\nconfig.preserve_color = preserve_color\\nwandb.log(\\n{\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\\nstep=0)\\n\\n# load discriminator for perceptual loss\\n\\ndiscriminator = Discriminator(1024, 2).eval().to(device)\\nckpt = torch.load(\\'models/stylegan2-ffhq-config-f.pt\\', map_location=lambda storage, loc: storage)\\ndiscriminator.load_state_dict(ckpt[\"d\"], strict=False)\\n\\n# reset generator\\n\\ndel generator\\ngenerator = deepcopy(original_generator)\\n\\ng_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\\n\\n# Which layers to swap for generating a family of plausible real images -> fake image\\n\\nif preserve_color:\\nid_swap = [9,11,15,16,17]\\nelse:\\nid_swap = list(range(7, generator.n_latent))'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 6260}, page_content='if preserve_color:\\nid_swap = [9,11,15,16,17]\\nelse:\\nid_swap = list(range(7, generator.n_latent))\\n\\nfor idx in tqdm(range(num_iter)):\\nmean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\\nin_latent = latents.clone()\\nin_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\\n\\n    img = generator(in_latent, input_is_latent=True)\\n\\n    with torch.no_grad():\\n        real_feat = discriminator(targets)\\n    fake_feat = discriminator(img)\\n\\n    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 6795}, page_content='loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\\n\\n\\n    wandb.log({\"loss\": loss}, step=idx)\\n    if idx % log_interval == 0:\\n        generator.eval()\\n        my_sample = generator(my_w, input_is_latent=True)\\n        generator.train()\\n        my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\\n        wandb.log(\\n        {\"Current stylization\": [wandb.Image(my_sample)]},\\n        step=idx)\\n    table_data = [\\n            wandb.Image(transforms.ToPILImage()(target_im)),\\n            wandb.Image(img),\\n            wandb.Image(my_sample),\\n        ]\\n    samples.append(table_data)\\n\\n    g_optim.zero_grad()\\n    loss.backward()\\n    g_optim.step()\\n\\nout_table = wandb.Table(data=samples, columns=column_names)\\nwandb.log({\"Current Samples\": out_table})\\n\\n`'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 7616}, page_content='```\\n\\n4. Save, Download, and Load Model\\n\\n    Here\\'s how to save and download your model.\\n\\n```python\\n\\nfrom PIL import Image\\nimport torch\\ntorch.backends.cudnn.benchmark = True\\nfrom torchvision import transforms, utils\\nfrom util import *\\nimport math\\nimport random\\nimport numpy as np\\nfrom torch import nn, autograd, optim\\nfrom torch.nn import functional as F\\nfrom tqdm import tqdm\\nimport lpips\\nfrom model import *\\nfrom e4e_projection import projection as e4e_projection\\n\\nfrom copy import deepcopy\\nimport imageio\\n\\nimport os\\nimport sys\\nimport torchvision.transforms as transforms\\nfrom argparse import Namespace\\nfrom e4e.models.psp import pSp\\nfrom util import *\\nfrom huggingface_hub import hf_hub_download\\nfrom google.colab import files\\n\\ntorch.save({\"g\": generator.state_dict()}, \"your-model-name.pt\")\\n\\nfiles.download(\\'your-model-name.pt\\')'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 8411}, page_content='files.download(\\'your-model-name.pt\\')\\n\\nlatent_dim = 512\\ndevice=\"cuda\"\\nmodel_path_s = hf_hub_download(repo_id=\"akhaliq/jojogan-stylegan2-ffhq-config-f\", filename=\"stylegan2-ffhq-config-f.pt\")\\noriginal_generator = Generator(1024, latent_dim, 8, 2).to(device)\\nckpt = torch.load(model_path_s, map_location=lambda storage, loc: storage)\\noriginal_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\\nmean_latent = original_generator.mean_latent(10000)\\n\\ngenerator = deepcopy(original_generator)\\n\\nckpt = torch.load(\"/content/JoJoGAN/your-model-name.pt\", map_location=lambda storage, loc: storage)\\ngenerator.load_state_dict(ckpt[\"g\"], strict=False)\\ngenerator.eval()\\n\\nplt.rcParams[\\'figure.dpi\\'] = 150\\n\\n\\n\\ntransform = transforms.Compose(\\n    [\\n        transforms.Resize((1024, 1024)),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\\n    ]\\n)\\n\\n\\ndef inference(img):\\n    img.save(\\'out.jpg\\')\\n    aligned_face = align_face(\\'out.jpg\\')'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 9290}, page_content='def inference(img):\\n    img.save(\\'out.jpg\\')\\n    aligned_face = align_face(\\'out.jpg\\')\\n\\n    my_w = e4e_projection(aligned_face, \"out.pt\", device).unsqueeze(0)\\n    with torch.no_grad():\\n        my_sample = generator(my_w, input_is_latent=True)\\n\\n\\n    npimage = my_sample[0].cpu().permute(1, 2, 0).detach().numpy()\\n    imageio.imwrite(\\'filename.jpeg\\', npimage)\\n    return \\'filename.jpeg\\'\\n`'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 9674}, page_content='```\\n\\n5. Build a Gradio Demo\\n\\n```python\\n\\nimport gradio as gr\\n\\ntitle = \"JoJoGAN\"\\ndescription = \"Gradio Demo for JoJoGAN: One Shot Face Stylization. To use it, simply upload your image, or click one of the examples to load them. Read more at the links below.\"\\n\\ndemo = gr.Interface(\\n    inference,\\n    gr.Image(type=\"pil\"),\\n    gr.Image(type=\"file\"),\\n    title=title,\\n    description=description\\n)\\n\\ndemo.launch(share=True)\\n```\\n\\n6. Integrate Gradio into your W&B Dashboard\\n\\n   The last step—integrating your Gradio demo with your W&B dashboard—is just one extra line:\\n\\n```python\\n\\ndemo.integrate(wandb=wandb)\\n```\\n\\n    Once you call integrate, a demo will be created and you can integrate it into your dashboard or report\\n\\n    Outside of W&B with Web components, using the gradio-app tags allows anyone can embed Gradio demos on HF spaces directly into their blogs, websites, documentation, etc.:\\n\\n```html\\n<gradio-app space=\"akhaliq/JoJoGAN\"> </gradio-app>'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 10624}, page_content='```\\n\\n7. (Optional) Embed W&B plots in your Gradio App\\n\\n   It\\'s also possible to embed W&B plots within Gradio apps. To do so, you can create a W&B Report of your plots and\\n   embed them within your Gradio app within a `gr.HTML` block.\\n\\n   The Report will need to be public and you will need to wrap the URL within an iFrame like this:\\n\\n```python\\n\\nimport gradio as gr\\n\\ndef wandb_report(url):\\n    iframe = f\\'<iframe src={url} style=\"border:none;height:1024px;width:100%\">\\'\\n    return gr.HTML(iframe)\\n\\nwith gr.Blocks() as demo:\\n    report_url = \\'https://wandb.ai/_scott/pytorch-sweeps-demo/reports/loss-22-10-07-16-00-17---VmlldzoyNzU2NzAx\\'\\n    report = wandb_report(report_url)\\n\\ndemo.launch(share=True)'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 11325}, page_content='```\\n\\n## Conclusion\\n\\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! Thanks for making it to the end. To recap:\\n\\n- Only one single reference image is needed for fine-tuning JoJoGAN which usually takes about 1 minute on a GPU in colab. After training, style can be applied to any input image. Read more in the paper.\\n\\n- W&B tracks experiments with just a few lines of code added to a colab and you can visualize, sort, and understand your experiments in a single, centralized dashboard.\\n\\n- Gradio, meanwhile, demos the model in a user friendly interface to share anywhere on the web.\\n\\n## How to contribute Gradio demos on HF spaces on the Wandb organization'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md', 'start_index': 11940}, page_content='## How to contribute Gradio demos on HF spaces on the Wandb organization\\n\\n- Create an account on Hugging Face [here](https://huggingface.co/join).\\n- Add Gradio Demo under your username, see this [course](https://huggingface.co/course/chapter9/4?fw=pt) for setting up Gradio Demo on Hugging Face.\\n- Request to join wandb organization [here](https://huggingface.co/wandb).\\n- Once approved transfer model from your username to Wandb organization'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/duplicatebutton_component/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: duplicatebutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr.Blocks() as demo:\\n    gr.DuplicateButton()\\n\\ndemo.launch()\\n```'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 1}, page_content='Widget Examples\\n\\nNote that each widget example can also optionally describe the corresponding model output, directly in the `output` property. See [the spec](./models-widgets#example-outputs) for more details.\\n\\n## Natural Language Processing\\n\\n### Fill-Mask\\n\\n```yaml\\nwidget:\\n- text: \"Paris is the <mask> of France.\"\\n  example_title: \"Capital\"\\n- text: \"The goal of life is <mask>.\"\\n  example_title: \"Philosophy\"\\n```\\n\\n### Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"What\\'s my name?\"\\n  context: \"My name is Clara and I live in Berkeley.\"\\n  example_title: \"Name\"\\n- text: \"Where do I live?\"\\n  context: \"My name is Sarah and I live in London\"\\n  example_title: \"Location\"'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 666}, page_content='```\\n\\n### Summarization'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 690}, page_content='```yaml\\nwidget:\\n- text: \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\\n  example_title: \"Eiffel Tower\"'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 1462}, page_content='example_title: \"Eiffel Tower\"\\n- text: \"Laika, a dog that was the first living creature to be launched into Earth orbit, on board the Soviet artificial satellite Sputnik 2, on November 3, 1957. It was always understood that Laika would not survive the mission, but her actual fate was misrepresented for decades. Laika was a small (13 pounds [6 kg]), even-tempered, mixed-breed dog about two years of age. She was one of a number of stray dogs that were taken into the Soviet spaceflight program after being rescued from the streets. Only female dogs were used because they were considered to be anatomically better suited than males for close confinement.\"\\n  example_title: \"First in Space\"'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 2153}, page_content='```\\n\\n### Table Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"How many stars does the transformers repository have?\"\\n  table:\\n    Repository:\\n      - \"Transformers\"\\n      - \"Datasets\"\\n      - \"Tokenizers\"\\n    Stars:\\n      - 36542\\n      - 4512\\n      - 3934\\n    Contributors:\\n      - 651\\n      - 77\\n      - 34\\n    Programming language:\\n      - \"Python\"\\n      - \"Python\"\\n      - \"Rust, Python and NodeJS\"\\n  example_title: \"Github stars\"\\n```\\n\\n### Text Classification\\n\\n```yaml\\nwidget:\\n- text: \"I love football so much\"\\n  example_title: \"Positive\"\\n- text: \"I don\\'t really like this type of food\"\\n  example_title: \"Negative\"\\n```\\n\\n### Text Generation\\n\\n```yaml\\nwidget:\\n- text: \"My name is Julien and I like to\"\\n  example_title: \"Julien\"\\n- text: \"My name is Merve and my favorite\"\\n  example_title: \"Merve\"\\n```\\n\\n### Text2Text Generation\\n\\n```yaml\\nwidget:\\n- text: \"My name is Julien and I like to\"\\n  example_title: \"Julien\"\\n- text: \"My name is Merve and my favorite\"\\n  example_title: \"Merve\"'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 3130}, page_content='```\\n\\n### Token Classification\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n### Translation\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n### Zero-Shot Classification\\n\\n```yaml\\nwidget:\\n- text: \"I have a problem with my car that needs to be resolved asap!!\"\\n  candidate_labels: \"urgent, not urgent, phone, tablet, computer\"\\n  multi_class: true\\n  example_title: \"Car problem\"\\n- text: \"Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\"\\n  candidate_labels: \"mobile, website, billing, account access\"\\n  multi_class: false\\n  example_title: \"Phone issue\"'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 3997}, page_content='```\\n### Sentence Similarity\\n\\n```yaml\\nwidget:\\n- source_sentence: \"That is a happy person\"\\n  sentences:\\n    - \"That is a happy dog\"\\n    - \"That is a very happy person\"\\n    - \"Today is a sunny day\"\\n  example_title: \"Happy\"\\n```\\n\\n### Conversational\\n\\n```yaml\\nwidget:\\n- text: \"Hey my name is Julien! How are you?\"\\n  example_title: \"Julien\"\\n- text: \"Hey my name is Clara! How are you?\"\\n  example_title: \"Clara\"\\n```\\n\\n### Feature Extraction\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"\\n```\\n\\n## Audio\\n\\n### Text-to-Speech\\n\\n```yaml\\nwidget:\\n- text: \"My name is Sylvain and I live in Paris\"\\n  example_title: \"Parisian\"\\n- text: \"My name is Sarah and I live in London\"\\n  example_title: \"Londoner\"'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 4802}, page_content='```\\n\\n### Automatic Speech Recognition\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2\\n```\\n\\n### Audio-to-Audio\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2\\n```\\n\\n### Audio Classification\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 5581}, page_content='```\\n\\n### Voice Activity Detection\\n\\n```yaml\\nwidget:\\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\\n  example_title: Librispeech sample 1\\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\\n  example_title: Librispeech sample 2\\n```\\n\\n## Computer Vision\\n\\n### Image Classification\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\\n  example_title: Tiger\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\\n  example_title: Teapot\\n```\\n\\n### Object Detection\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\\n  example_title: Football Match\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\\n  example_title: Airport'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 6402}, page_content='```\\n\\n### Image Segmentation\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\\n  example_title: Football Match\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\\n  example_title: Airport\\n```\\n\\n### Image-to-Image\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/canny-edge.jpg\\n  prompt: Girl with Pearl Earring # `prompt` field is optional in case the underlying model supports text guidance\\n```\\n\\n### Text-to-Image\\n\\n```yaml\\nwidget:\\n- text: \"A cat playing with a ball\"\\n  example_title: \"Cat\"\\n- text: \"A dog jumping over a fence\"\\n  example_title: \"Dog\"'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 7084}, page_content='```\\n\\n### Document Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"What is the invoice number?\"\\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\\n- text: \"What is the purchase amount?\"\\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg\"\\n```\\n\\n### Visual Question Answering\\n\\n```yaml\\nwidget:\\n- text: \"What animal is it?\"\\n  src: \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\"\\n- text: \"Where is it?\"\\n  src: \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\"\\n```\\n\\n### Zero-Shot Image Classification\\n\\n```yaml\\nwidget:\\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\\n  candidate_labels: playing music, playing sports\\n  example_title: Cat & Dog'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md', 'start_index': 7949}, page_content='```\\n\\n## Other\\n\\n### Structured Data Classification\\n\\n```yaml\\nwidget:\\n- structured_data:\\n    fixed_acidity:\\n      - 7.4\\n      - 7.8\\n      - 10.3\\n    volatile_acidity:\\n      - 0.7\\n      - 0.88\\n      - 0.32\\n    citric_acid:\\n      - 0\\n      - 0\\n      - 0.45\\n    residual_sugar:\\n      - 1.9\\n      - 2.6\\n      - 6.4\\n    chlorides:\\n      - 0.076\\n      - 0.098\\n      - 0.073\\n    free_sulfur_dioxide:\\n      - 11\\n      - 25\\n      - 5\\n    total_sulfur_dioxide:\\n      - 34\\n      - 67\\n      - 13\\n    density:\\n      - 0.9978\\n      - 0.9968\\n      - 0.9976\\n    pH:\\n      - 3.51\\n      - 3.2\\n      - 3.23\\n    sulphates:\\n      - 0.56\\n      - 0.68\\n      - 0.82\\n    alcohol:\\n      - 9.4\\n      - 9.8\\n      - 12.6\\n  example_title: \"Wine\"\\n```'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# UNet1DModel'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 587}, page_content=\"# UNet1DModel\\n\\nThe [UNet](https://huggingface.co/papers/1505.04597) model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in 🤗 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in 🤗 Diffusers, depending on it's number of dimensions and whether it is a conditional model or not. This is a 1D UNet model.\\n\\nThe abstract from the paper is:\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 1174}, page_content='*There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 2074}, page_content='image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.*'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/models/unet.md', 'start_index': 2268}, page_content='## UNet1DModel\\n[[autodoc]] UNet1DModel\\n\\n## UNet1DOutput\\n[[autodoc]] models.unet_1d.UNet1DOutput'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 0}, page_content='--\\ntitle: \"Building an AI WebTV\"\\nthumbnail: /blog/assets/156_ai_webtv/thumbnail.gif\\nauthors:\\n- user: jbilcke-hf\\n---\\n\\n# Building an AI WebTV\\n\\n\\nThe AI WebTV is an experimental demo to showcase the latest advancements in automatic video and music synthesis.\\n\\n👉 Watch the stream now by going to the [AI WebTV Space](https://huggingface.co/spaces/jbilcke-hf/AI-WebTV).\\n\\nIf you are using a mobile device, you can view the stream from the [Twitch mirror](https://www.twitch.tv/ai_webtv).\\n\\n![thumbnail.gif](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/thumbnail.gif)\\n\\n# Concept\\n\\nThe motivation for the AI WebTV is to demo videos generated with open-source [text-to-video models](https://huggingface.co/tasks/text-to-video) such as Zeroscope and MusicGen, in an entertaining and accessible way.\\n\\nYou can find those open-source models on the Hugging Face hub:'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 838}, page_content='You can find those open-source models on the Hugging Face hub:\\n\\n- For video: [zeroscope_v2_576](https://huggingface.co/cerspense/zeroscope_v2_576w) and [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL)\\n- For music: [musicgen-melody](https://huggingface.co/facebook/musicgen-melody)\\n\\nThe individual video sequences are purposely made to be short, meaning the WebTV should be seen as a tech demo/showreel rather than an actual show (with an art direction or programming).\\n\\n# Architecture\\n\\nThe AI WebTV works by taking a sequence of [video shot](https://en.wikipedia.org/wiki/Shot_(filmmaking)) prompts and passing them to a [text-to-video model](https://huggingface.co/tasks/text-to-video) to generate a sequence of [takes](https://en.wikipedia.org/wiki/Take). \\n\\nAdditionally, a base theme and idea (written by a human) are passed through a LLM (in this case, ChatGPT), in order to generate a variety of individual prompts for each video clip.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 1799}, page_content=\"Here's a diagram of the current architecture of the AI WebTV:\\n\\n![diagram.jpg](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/diagram.jpg)\\n\\n# Implementing the pipeline\\n\\nThe WebTV is implemented in NodeJS and TypeScript, and uses various services hosted on Hugging Face.\\n\\n## The text-to-video model\\n\\nThe central video model is Zeroscope V2, a model based on [ModelScope](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis).\\n\\nZeroscope is comprised of two parts that can be chained together:\\n\\n- A first pass with [zeroscope_v2_576](https://huggingface.co/cerspense/zeroscope_v2_576w), to generate a 576x320 video clip\\n- An optional second pass with [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) to upscale the video to 1024x576\\n\\n👉\\xa0 You will need to use the same prompt for both the generation and upscaling.\\n\\n## Calling the video chain\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 2699}, page_content='## Calling the video chain\\n\\nTo make a quick prototype, the WebTV runs Zeroscope from two duplicated Hugging Face Spaces running [Gradio](https://github.com/gradio-app/gradio/), which are called using the [@gradio/client](https://www.npmjs.com/package/@gradio/client) NPM package. You can find the original spaces here:\\n\\n- [zeroscope-v2](https://huggingface.co/spaces/hysts/zeroscope-v2/tree/main) by @hysts\\n- [Zeroscope XL](https://huggingface.co/spaces/fffiloni/zeroscope-XL) by @fffiloni\\n\\nOther spaces deployed by the community can also be found if you [search for Zeroscope on the Hub](https://huggingface.co/spaces?search=zeroscope).\\n\\n👉\\xa0 Public Spaces may become overcrowded and paused at any time. If you intend to deploy your own system, please duplicate those Spaces and run them under your own account.\\n\\n## Using a model hosted on a Space'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 3511}, page_content='## Using a model hosted on a Space\\n\\nSpaces using Gradio have the ability to [expose a REST API](https://www.gradio.app/guides/sharing-your-app#api-page), which can then be called from Node using the [@gradio/client](https://www.npmjs.com/package/@gradio/client) module.\\n\\nHere is an example:\\n\\n```typescript\\nimport { client } from \"@gradio/client\"\\n\\nexport const generateVideo = async (prompt: string) => {\\n  const api = await client(\"*** URL OF THE SPACE ***\")\\n\\n  // call the \"run()\" function with an array of parameters\\n  const { data } = await api.predict(\"/run\", [\\t\\t\\n    prompt,\\n    42,\\t// seed\\t\\n    24, // nbFrames\\n    35 // nbSteps\\n  ])\\n  \\n  const { orig_name } = data[0][0]\\n\\n  const remoteUrl = `${instance}/file=${orig_name}`\\n\\n  // the file can then be downloaded and stored locally\\n}'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 4301}, page_content='```\\n\\n\\n## Post-processing\\n\\nOnce an individual take (a video clip) is upscaled, it is then passed to FILM (Frame Interpolation for Large Motion), a frame interpolation algorithm:\\n\\n- Original links: [website](https://film-net.github.io/), [source code](https://github.com/google-research/frame-interpolation)\\n- Model on Hugging Face: [/frame-interpolation-film-style](https://huggingface.co/akhaliq/frame-interpolation-film-style)\\n- A Hugging Face Space you can duplicate: [video_frame_interpolation](https://huggingface.co/spaces/fffiloni/video_frame_interpolation/blob/main/app.py) by @fffiloni\\n\\nDuring post-processing, we also add music generated with MusicGen:\\n\\n- Original links: [website](https://ai.honu.io/papers/musicgen/), [source code](https://github.com/facebookresearch/audiocraft)\\n- Hugging Face Space you can duplicate: [MusicGen](https://huggingface.co/spaces/facebook/MusicGen)\\n\\n\\n## Broadcasting the stream'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 5194}, page_content='## Broadcasting the stream\\n\\nNote: there are multiple tools you can use to create a video stream. The AI WebTV currently uses [FFmpeg](https://ffmpeg.org/documentation.html) to read a playlist made of mp4 videos files and m4a audio files.\\n\\nHere is an example of creating such a playlist:\\n\\n```typescript\\nimport { promises as fs } from \"fs\"\\nimport path from \"path\"\\n\\nconst allFiles = await fs.readdir(\"** PATH TO VIDEO FOLDER **\")\\nconst allVideos = allFiles\\n  .map(file => path.join(dir, file))\\n  .filter(filePath => filePath.endsWith(\\'.mp4\\'))\\n\\nlet playlist = \\'ffconcat version 1.0\\\\n\\'\\nallFilePaths.forEach(filePath => {\\n  playlist += `file \\'${filePath}\\'\\\\n`\\n})\\nawait fs.promises.writeFile(\"playlist.txt\", playlist)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 5904}, page_content=\"```\\n\\nThis will generate the following playlist content:\\n\\n```bash\\nffconcat version 1.0\\nfile 'video1.mp4'\\nfile 'video2.mp4'\\n...\\n```\\n\\nFFmpeg is then used again to read this playlist and send a [FLV stream](https://en.wikipedia.org/wiki/Flash_Video) to a [RTMP server](https://en.wikipedia.org/wiki/Real-Time_Messaging_Protocol). FLV is an old format but still popular in the world of real-time streaming due to its low latency.\\n\\n```bash\\nffmpeg -y -nostdin \\\\\\n  -re \\\\\\n  -f concat \\\\\\n  -safe 0 -i channel_random.txt -stream_loop -1 \\\\\\n  -loglevel error \\\\\\n  -c:v libx264 -preset veryfast -tune zerolatency \\\\\\n  -shortest \\\\\\n  -f flv rtmp://<SERVER>\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 6542}, page_content='```\\n\\nThere are many different configuration options for FFmpeg, for more information in the [official documentation](http://trac.ffmpeg.org/wiki/StreamingGuide).\\n\\nFor the RTMP server, you can find [open-source implementations on GitHub](https://github.com/topics/rtmp-server), such as the [NGINX-RTMP module](https://github.com/arut/nginx-rtmp-module).\\n\\nThe AI WebTV itself uses [node-media-server](https://github.com/illuspas/Node-Media-Server).\\n\\n💡 You can also directly stream to [one of the Twitch RTMP entrypoints](https://help.twitch.tv/s/twitch-ingest-recommendation?language=en_US). Check out the Twitch documentation for more details.\\n\\n# Observations and examples\\n\\nHere are some examples of the generated content.\\n\\nThe first thing we notice is that applying the second pass of Zeroscope XL significantly improves the quality of the image. The impact of frame interpolation is also clearly visible.\\n\\n## Characters and scene composition'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 7449}, page_content='## Characters and scene composition\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo4.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo4.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Photorealistic movie of a <strong>llama acting as a programmer, wearing glasses and a hoodie</strong>, intensely <strong>staring at a screen</strong> with lines of code, in a cozy, <strong>dimly lit room</strong>, Canon EOS, ambient lighting, high details, cinematic, trending on artstation</i></figcaption>\\n</figure>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 8143}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo5.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo5.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>3D rendered animation showing a group of food characters <strong>forming a pyramid</strong>, with a <strong>banana</strong> standing triumphantly on top. In a city with <strong>cotton candy clouds</strong> and <strong>chocolate road</strong>, Pixar\\'s style, CGI, ambient lighting, direct sunlight, rich color scheme, ultra realistic, cinematic, photorealistic.</i></figcaption>\\n</figure>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 8870}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo7.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo7.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Intimate <strong>close-up of a red fox, gazing into the camera with sharp eyes</strong>, ambient lighting creating a high contrast silhouette, IMAX camera, <strong>high detail</strong>, <strong>cinematic effect</strong>, golden hour, film grain.</i></figcaption>\\n</figure>\\n\\n## Simulation of dynamic scenes\\n\\nSomething truly fascinating about text-to-video models is their ability to emulate real-life phenomena they have been trained on.\\n\\nWe\\'ve seen it with large language models and their ability to synthesize convincing content that mimics human responses, but this takes things to a whole new dimension when applied to video.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 9838}, page_content='A video model predicts the next frames of a scene, which might include objects in motion such as fluids, people, animals, or vehicles. Today, this emulation isn\\'t perfect, but it will be interesting to evaluate future models (trained on larger or specialized datasets, such as animal locomotion) for their accuracy when reproducing physical phenomena, and also their ability to simulate the behavior of agents.\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo17.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo17.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Cinematic movie shot of <strong>bees energetically buzzing around a flower</strong>, sun rays illuminating the scene, captured in 4k IMAX with a soft bokeh background.</i></figcaption>\\n</figure>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 10786}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo8.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo8.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i><strong>Dynamic footage of a grizzly bear catching a salmon in a rushing river</strong>, ambient lighting highlighting the splashing water, low angle, IMAX camera, 4K movie quality, golden hour, film grain.</i></figcaption>\\n</figure>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 11359}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo18.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo18.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Aerial footage of a quiet morning at the coast of California, with <strong>waves gently crashing against the rocky shore</strong>. A startling sunrise illuminates the coast with vibrant colors, captured beautifully with a DJI Phantom 4 Pro. Colors and textures of the landscape come alive under the soft morning light. Film grain, cinematic, imax, movie</i></figcaption>\\n</figure>\\n\\n💡 It will be interesting to see these capabilities explored more in the future, for instance by training video models on larger video datasets covering more phenomena.\\n\\n## Styling and effects'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 12250}, page_content='## Styling and effects\\n\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo0.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo0.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>\\n<strong>3D rendered video</strong> of a friendly broccoli character wearing a hat, walking in a candy-filled city street with gingerbread houses, under a <strong>bright sun and blue skies</strong>, <strong>Pixar\\'s style</strong>, cinematic, photorealistic, movie, <strong>ambient lighting</strong>, natural lighting, <strong>CGI</strong>, wide-angle view, daytime, ultra realistic.</i>\\n</figcaption>\\n</figure>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 13025}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo2.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo2.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i><strong>Cinematic movie</strong>, shot of an astronaut and a llama at dawn, the mountain landscape bathed in <strong>soft muted colors</strong>, early morning fog, dew glistening on fur, craggy peaks, vintage NASA suit, Canon EOS, high detailed skin, epic composition, high quality, 4K, trending on artstation, beautiful</i>\\n</figcaption>\\n</figure>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 13713}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"demo1.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/demo1.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Panda and black cat <strong>navigating down the flowing river</strong> in a small boat, Studio Ghibli style &gt; Cinematic, beautiful composition &gt; IMAX <strong>camera panning following the boat</strong> &gt; High quality, cinematic, movie, mist effect, film grain, trending on Artstation</i>\\n</figcaption>\\n</figure>\\n\\n\\n\\n## Failure cases\\n\\n**Wrong direction:** the model sometimes has trouble with movement and direction. For instance, here the clip seems to be played in reverse. Also the modifier keyword ***green*** was not taken into account.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 14600}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"fail1.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/fail1.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Movie showing a <strong>green pumpkin</strong> falling into a bed of nails, slow-mo explosion with chunks flying all over, ambient fog adding to the dramatic lighting, filmed with IMAX camera, 8k ultra high definition, high quality, trending on artstation.</i>\\n</figcaption>\\n</figure>\\n\\n\\n**Rendering errors on realistic scenes:** sometimes we can see artifacts such as moving vertical lines or waves. It is unclear what causes this, but it may be due to the combination of keywords used.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 15426}, page_content='<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"fail2.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/fail2.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Film shot of a captivating flight above the Grand Canyon, ledges and plateaus etched in orange and red. <strong>Deep shadows contrast</strong> with the fiery landscape under the midday sun, shot with DJI Phantom 4 Pro. The camera rotates to capture the vastness, <strong>textures</strong> and colors, in imax quality. Film <strong>grain</strong>, cinematic, movie.</i>\\n</figcaption>\\n</figure>\\n\\n\\n**Text or objects inserted into the image:** the model sometimes injects words from the prompt into the scene, such as \"IMAX\". Mentioning \"Canon EOS\" or \"Drone footage\" in the prompt can also make those objects appear in the video.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 16392}, page_content='In the following example, we notice the word \"llama\" inserts a llama but also two occurrences of the word llama in flames.\\n\\n<figure class=\"image flex flex-col items-center text-center m-0 w-full\">\\n   <video\\n      alt=\"fail3.mp4\"\\n      autoplay loop autobuffer muted playsinline\\n    >\\n    <source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/156_ai_webtv/fail3.mp4\" type=\"video/mp4\">\\n  </video>\\n  <figcaption>Prompt: <i>Movie scene of a <strong>llama</strong> acting as a firefighter, in firefighter uniform, dramatically spraying water at <strong>roaring flames</strong>, amidst a chaotic urban scene, Canon EOS, ambient lighting, high quality, award winning, highly detailed fur, cinematic, trending on artstation.</i>\\n</figcaption>\\n</figure>\\n\\n# Recommendations\\n\\nHere are some early recommendations that can be made from the previous observations:\\n\\n## Using video-specific prompt keywords'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 17285}, page_content='## Using video-specific prompt keywords\\n\\nYou may already know that if you don’t prompt a specific aspect of the image with Stable Diffusion, things like the color of clothes or the time of the day might become random, or be assigned a generic value such as a neutral mid-day light.\\n\\nThe same is true for video models: you will want to be specific about things. Examples include camera and character movement, their orientation, speed and direction. You can leave it unspecified for creative purposes (idea generation), but this might not always give you the results you want (e.g., entities animated in reverse).\\n\\n## Maintaining consistency between scenes\\n\\nIf you plan to create sequences of multiple videos, you will want to make sure you add as many details as possible in each prompt, otherwise you may lose important details from one sequence to another, such as the color.\\n\\n💡 This will also improve the quality of the image since the prompt is used for the upscaling part with Zeroscope XL.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 18282}, page_content='## Leverage frame interpolation\\n\\nFrame interpolation is a powerful tool which can repair small rendering errors and turn many defects into features, especially in scenes with a lot of animation, or where a cartoon effect is acceptable. The [FILM algorithm](https://film-net.github.io/) will smoothen out elements of a frame with previous and following events in the video clip.\\n\\nThis works great to displace the background when the camera is panning or rotating, and will also give you creative freedom, such as control over the number of frames after the generation, to make slow-motion effects.\\n\\n# Future work\\n\\nWe hope you enjoyed watching the AI WebTV stream and that it will inspire you to build more in this space.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/ai-webtv.md', 'start_index': 19003}, page_content='As this was a first trial, a lot of things were not the focus of the tech demo: generating longer and more varied sequences, adding audio (sound effects, dialogue), generating and orchestrating complex scenarios, or letting a language model agent have more control over the pipeline.\\n\\nSome of these ideas may make their way into future updates to the AI WebTV, but we also can’t wait to see what the community of researchers, engineers and builders will come up with!'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 1}, page_content='Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](https://stability.ai/blog/stable-diffusion-announcement) by Patrick Esser and Robin Rombach and the Stability AI team.\\n\\nThe summary of the model is the following:'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 217}, page_content='The summary of the model is the following:\\n\\n*Stable Diffusion is a text-to-image model that will empower billions of people to create stunning art within seconds. It is a breakthrough in speed and quality meaning that it can run on consumer GPUs. You can see some of the amazing output that has been created by this model without pre or post-processing on this page. The model itself builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all.*\\n\\n## Tips:'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 1018}, page_content=\"## Tips:\\n\\n- Stable Diffusion has the same architecture as [Latent Diffusion](https://arxiv.org/abs/2112.10752) but uses a frozen CLIP Text Encoder instead of training the text encoder jointly with the diffusion model.\\n- An in-detail explanation of the Stable Diffusion model can be found under [Stable Diffusion with 🧨 Diffusers](https://huggingface.co/blog/stable_diffusion).\\n- If you don't want to rely on the Hugging Face Hub and having to pass a authentication token, you can\\ndownload the weights with `git lfs install; git clone https://huggingface.co/runwayml/stable-diffusion-v1-5` and instead pass the local path to the cloned folder to `from_pretrained` as shown below.\\n- Stable Diffusion can work with a variety of different samplers as is shown below.\\n\\n## Available Pipelines:\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 1807}, page_content='| Pipeline | Tasks | Colab\\n|---|---|:---:|\\n| [pipeline_stable_diffusion.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py) | *Text-to-Image Generation* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)\\n| [pipeline_stable_diffusion_img2img](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py) | *Image-to-Image Text-Guided Generation* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/image_2_image_using_diffusers.ipynb)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 2629}, page_content='| [pipeline_stable_diffusion_inpaint](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py) | *Text-Guided Image Inpainting* | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/in_painting_with_stable_diffusion_using_diffusers.ipynb)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 3050}, page_content='## Examples:\\n\\n### Using Stable Diffusion without being logged into the Hub.\\n\\nIf you want to download the model weights using a single Python line, you need to be logged in via `huggingface-cli login`.\\n\\n```python\\nfrom diffusers import DiffusionPipeline\\n\\npipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 3382}, page_content='```\\n\\nThis however can make it difficult to build applications on top of `diffusers` as you will always have to pass the token around. A potential way to solve this issue is by downloading the weights to a local path `\"./stable-diffusion-v1-5\"`:\\n\\n```\\ngit lfs install\\ngit clone https://huggingface.co/runwayml/stable-diffusion-v1-5\\n```\\n\\nand simply passing the local path to `from_pretrained`:\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\n\\npipe = StableDiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\")\\n```\\n\\n### Text-to-Image with default PLMS scheduler\\n\\n```python\\n# make sure you\\'re logged in with `huggingface-cli login`\\nfrom diffusers import StableDiffusionPipeline\\n\\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\\npipe = pipe.to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\"\\nimage = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 4307}, page_content='```\\n\\n### Text-to-Image with DDIM scheduler\\n\\n```python\\n# make sure you\\'re logged in with `huggingface-cli login`\\nfrom diffusers import StableDiffusionPipeline, DDIMScheduler\\n\\nscheduler =  DDIMScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\\n\\npipe = StableDiffusionPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    scheduler=scheduler,\\n).to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\"\\nimage = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 4836}, page_content='```\\n\\n### Text-to-Image with K-LMS scheduler\\n\\n```python\\n# make sure you\\'re logged in with `huggingface-cli login`\\nfrom diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\\n\\nlms = LMSDiscreteScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\\n\\npipe = StableDiffusionPipeline.from_pretrained(\\n    \"runwayml/stable-diffusion-v1-5\",\\n    scheduler=lms,\\n).to(\"cuda\")\\n\\nprompt = \"a photo of an astronaut riding a horse on mars\"\\nimage = pipe(prompt).images[0]\\n\\nimage.save(\"astronaut_rides_horse.png\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 5367}, page_content='```\\n\\n### CycleDiffusion using Stable Diffusion and DDIM scheduler\\n\\n```python\\nimport requests\\nimport torch\\nfrom PIL import Image\\nfrom io import BytesIO\\n\\nfrom diffusers import CycleDiffusionPipeline, DDIMScheduler\\n\\n\\n# load the scheduler. CycleDiffusion only supports stochastic schedulers.\\n\\n# load the pipeline\\n# make sure you\\'re logged in with `huggingface-cli login`\\nmodel_id_or_path = \"CompVis/stable-diffusion-v1-4\"\\nscheduler = DDIMScheduler.from_pretrained(model_id_or_path, subfolder=\"scheduler\")\\npipe = CycleDiffusionPipeline.from_pretrained(model_id_or_path, scheduler=scheduler).to(\"cuda\")\\n\\n# let\\'s download an initial image\\nurl = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/An%20astronaut%20riding%20a%20horse.png\"\\nresponse = requests.get(url)\\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\\ninit_image = init_image.resize((512, 512))\\ninit_image.save(\"horse.png\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 6291}, page_content='# let\\'s specify a prompt\\nsource_prompt = \"An astronaut riding a horse\"\\nprompt = \"An astronaut riding an elephant\"\\n\\n# call the pipeline\\nimage = pipe(\\n    prompt=prompt,\\n    source_prompt=source_prompt,\\n    image=init_image,\\n    num_inference_steps=100,\\n    eta=0.1,\\n    strength=0.8,\\n    guidance_scale=2,\\n    source_guidance_scale=1,\\n).images[0]\\n\\nimage.save(\"horse_to_elephant.png\")\\n\\n# let\\'s try another example\\n# See more samples at the original repo: https://github.com/ChenWu98/cycle-diffusion\\nurl = \"https://raw.githubusercontent.com/ChenWu98/cycle-diffusion/main/data/dalle2/A%20black%20colored%20car.png\"\\nresponse = requests.get(url)\\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\\ninit_image = init_image.resize((512, 512))\\ninit_image.save(\"black.png\")\\n\\nsource_prompt = \"A black colored car\"\\nprompt = \"A blue colored car\"'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 7070}, page_content='source_prompt = \"A black colored car\"\\nprompt = \"A blue colored car\"\\n\\n# call the pipeline\\ntorch.manual_seed(0)\\nimage = pipe(\\n    prompt=prompt,\\n    source_prompt=source_prompt,\\n    image=init_image,\\n    num_inference_steps=100,\\n    eta=0.1,\\n    strength=0.85,\\n    guidance_scale=3,\\n    source_guidance_scale=1,\\n).images[0]\\n\\nimage.save(\"black_to_blue.png\")'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/README.md', 'start_index': 7425}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 0}, page_content='p align=\"center\">\\n  <br/>\\n    <img alt=\"huggingface_hub library logo\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingface_hub.svg\" width=\"376\" height=\"59\" style=\"max-width: 100%;\">\\n  <br/>\\n</p>\\n\\n<p align=\"center\">\\n    <i>Huggingface Hub के लिए आधिकारिक पायथन क्लाइंट।</i>\\n</p>'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 317}, page_content='<p align=\"center\">\\n    <a href=\"https://huggingface.co/docs/huggingface_hub/ko/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/huggingface_hub/index.svg?down_color=red&down_message=offline&up_message=online&label=doc\"></a>\\n    <a href=\"https://github.com/huggingface/huggingface_hub/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/huggingface_hub.svg\"></a>\\n    <a href=\"https://github.com/huggingface/huggingface_hub\"><img alt=\"PyPi version\" src=\"https://img.shields.io/pypi/pyversions/huggingface_hub.svg\"></a>\\n    <a href=\"https://pypi.org/project/huggingface-hub\"><img alt=\"downloads\" src=\"https://static.pepy.tech/badge/huggingface_hub/month\"></a>\\n    <a href=\"https://codecov.io/gh/huggingface/huggingface_hub\"><img alt=\"Code coverage\" src=\"https://codecov.io/gh/huggingface/huggingface_hub/branch/main/graph/badge.svg?token=RXP95LE2XL\"></a>\\n</p>'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 1258}, page_content='<h4 align=\"center\">\\n    <p>\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README.md\">English</a> |\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_de.md\">Deutsch</a> |\\n        <b>हिंदी</b>  |\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_ko.md\">한국어</a> |\\n        <a href=\"https://github.com/huggingface/huggingface_hub/blob/main/README_cn.md\">中文（简体）</a> \\n    <p>\\n</h4>\\n\\n---\\n\\n**दस्तावेज़ीकरण**: <a href=\"https://hf.co/docs/huggingface_hub\" target=\"_blank\">https://hf.co/docs/huggingface_hub</a>\\n\\n**सोर्स कोड**: <a href=\"https://github.com/huggingface/huggingface_hub\" target=\"_blank\">https://github.com/huggingface/huggingface_hub</a>\\n\\n---\\n\\n## huggingface_hub लाइब्रेरी में आपका स्वागत है'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 1989}, page_content='---\\n\\n## huggingface_hub लाइब्रेरी में आपका स्वागत है\\n\\n`huggingface_hub` लाइब्रेरी आपको [हगिंग फेस हब](https://huggingface.co/) के साथ बातचीत करने की अनुमति देती है, जो रचनाकारों और सहयोगियों के लिए ओपन-सोर्स मशीन लर्निंग का लोकतंत्रीकरण करने वाला एक मंच है। अपनी परियोजनाओं के लिए पूर्व-प्रशिक्षित मॉडल और डेटासेट खोजें या हब पर होस्ट किए गए हजारों मशीन लर्निंग ऐप्स के साथ खेलें। आप समुदाय के साथ अपने स्वयं के मॉडल, डेटासेट और डेमो भी बना और साझा कर सकते हैं। `huggingface_hub` लाइब्रेरी पायथन के साथ इन सभी चीजों को करने का एक आसान तरीका प्रदान करती है।\\n\\n## प्रमुख विशेषताऐं'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 2547}, page_content='## प्रमुख विशेषताऐं\\n\\n- [फ़ाइलें डाउनलोड करें](https://huggingface.co/docs/huggingface_hub/en/guides/download) हब से।\\n- [फ़ाइलें अपलोड करें](https://huggingface.co/docs/huggingface_hub/en/guides/upload) हब पर।\\n- [अपनी रिपॉजिटरी प्रबंधित करें](https://huggingface.co/docs/huggingface_hub/en/guides/repository)।\\n- तैनात मॉडलों पर [अनुमान चलाएँ](https://huggingface.co/docs/huggingface_hub/en/guides/inference)।\\n- मॉडल, डेटासेट और स्पेस के लिए [खोज](https://huggingface.co/docs/huggingface_hub/en/guides/search)।\\n- [मॉडल कार्ड साझा करें](https://huggingface.co/docs/huggingface_hub/en/guides/model-cards) अपने मॉडलों का दस्तावेजीकरण करने के लिए।\\n- [समुदाय के साथ जुड़ें](https://huggingface.co/docs/huggingface_hub/en/guides/community) पीआर और टिप्पणियों के माध्यम से।\\n\\n## स्थापना\\n\\n[pip](https://pypi.org/project/huggingface-hub/) के साथ `huggingface_hub` पैकेज इंस्टॉल करें:\\n\\n```bash\\npip install huggingface_hub'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 3456}, page_content='```\\n\\nयदि आप चाहें, तो आप इसे [conda](https://huggingface.co/docs/huggingface_hub/en/installation#install-with-conda) से भी इंस्टॉल कर सकते हैं।\\n\\nपैकेज को डिफ़ॉल्ट रूप से न्यूनतम रखने के लिए, `huggingface_hub` कुछ उपयोग मामलों के लिए उपयोगी वैकल्पिक निर्भरता के साथ आता है। उदाहरण के लिए, यदि आप अनुमान के लिए संपूर्ण अनुभव चाहते हैं, तो चलाएँ:\\n\\n```bash\\npip install huggingface_hub[inference]\\n```\\n\\nअधिक इंस्टॉलेशन और वैकल्पिक निर्भरता जानने के लिए, [इंस्टॉलेशन गाइड](https://huggingface.co/docs/huggingface_hub/en/installation) देखें।\\n\\n## जल्दी शुरू\\n\\n### फ़ाइलें डाउनलोड करें\\n\\nएकल फ़ाइल डाउनलोड करें\\n\\n```py\\nfrom huggingface_hub import hf_hub_download\\n\\nhf_hub_download(repo_id=\"tiiuae/falcon-7b-instruct\", filename=\"config.json\")\\n```\\n\\nया एक संपूर्ण भंडार\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_download(\"stabilityai/stable-diffusion-2-1\")'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 4317}, page_content='```\\n\\nफ़ाइलें स्थानीय कैश फ़ोल्डर में डाउनलोड की जाएंगी. [this_guide] में अधिक विवरण (https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache)।\\n\\n### लॉग इन करें\\n\\nHugging Face Hub एप्लिकेशन को प्रमाणित करने के लिए टोकन का उपयोग करता है (देखें [docs](https://huggingface.co/docs/hub/security-tokens))। अपनी मशीन में लॉगिन करने के लिए, निम्नलिखित सीएलआई चलाएँ:\\n\\n```bash\\nhuggingface-cli login\\n# या कृपया इसे एक पर्यावरण चर के रूप में निर्दिष्ट करें।\\nhuggingface-cli login --token $HUGGINGFACE_TOKEN\\n```\\n\\n### एक रिपॉजिटरी बनाएं\\n\\n```py\\nfrom huggingface_hub import create_repo\\n\\ncreate_repo(repo_id=\"super-cool-model\")\\n```\\n\\n### फाइलें अपलोड करें\\n\\nएकल फ़ाइल अपलोड करें\\n\\n```py\\nfrom huggingface_hub import upload_file\\n\\nupload_file(\\n    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\",\\n    path_in_repo=\"README.md\",\\n    repo_id=\"lysandre/test-model\",\\n)'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 5174}, page_content='```\\n\\nया एक संपूर्ण फ़ोल्डर\\n\\n```py\\nfrom huggingface_hub import upload_folder\\n\\nupload_folder(\\n    folder_path=\"/path/to/local/space\",\\n    repo_id=\"username/my-cool-space\",\\n    repo_type=\"space\",\\n)'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 5369}, page_content='```\\n\\n[अपलोड गाइड](https://huggingface.co/docs/huggingface_hub/en/guides/upload) में विवरण के लिए।\\n\\n## हब से एकीकरण।\\n\\nहम मुफ्त मॉडल होस्टिंग और वर्जनिंग प्रदान करने के लिए शानदार ओपन सोर्स एमएल लाइब्रेरीज़ के साथ साझेदारी कर रहे हैं। आप मौजूदा एकीकरण [यहां](https://huggingface.co/docs/hub/libraries) पा सकते हैं।\\n\\nफायदे ये हैं:\\n\\n- पुस्तकालयों और उनके उपयोगकर्ताओं के लिए निःशुल्क मॉडल या डेटासेट होस्टिंग।\\n- गिट-आधारित दृष्टिकोण के कारण, बहुत बड़ी फ़ाइलों के साथ भी अंतर्निहित फ़ाइल संस्करणिंग।\\n- सभी मॉडलों के लिए होस्टेड अनुमान एपीआई सार्वजनिक रूप से उपलब्ध है।\\n- अपलोड किए गए मॉडलों के साथ खेलने के लिए इन-ब्राउज़र विजेट।\\n- कोई भी आपकी लाइब्रेरी के लिए एक नया मॉडल अपलोड कर सकता है, उन्हें मॉडल को खोजने योग्य बनाने के लिए बस संबंधित टैग जोड़ना होगा।\\n- तेज़ डाउनलोड! हम डाउनलोड को जियो-रेप्लिकेट करने के लिए क्लाउडफ्रंट (एक सीडीएन) का उपयोग करते हैं ताकि वे दुनिया में कहीं से भी तेजी से चमक सकें।\\n- उपयोग आँकड़े और अधिक सुविधाएँ आने वाली हैं।'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/README_hi.md', 'start_index': 6317}, page_content='यदि आप अपनी लाइब्रेरी को एकीकृत करना चाहते हैं, तो चर्चा शुरू करने के लिए बेझिझक एक मुद्दा खोलें। हमने ❤️ के साथ एक [चरण-दर-चरण मार्गदर्शिका](https://huggingface.co/docs/hub/adding-a-library) लिखी, जिसमें दिखाया गया कि यह एकीकरण कैसे करना है।\\n\\n## योगदान (सुविधा अनुरोध, बग, आदि) का अति स्वागत है 💙💚💛💜🧡❤️\\n\\nयोगदान के लिए हर किसी का स्वागत है और हम हर किसी के योगदान को महत्व देते हैं। कोड समुदाय की मदद करने का एकमात्र तरीका नहीं है।\\nप्रश्नों का उत्तर देना, दूसरों की मदद करना, उन तक पहुंचना और दस्तावेज़ों में सुधार करना समुदाय के लिए बेहद मूल्यवान है।\\nहमने संक्षेप में बताने के लिए एक [योगदान मार्गदर्शिका](https://github.com/huggingface/huggingface_hub/blob/main/CONTRIBUTING.md) लिखी है\\nइस भंडार में योगदान करने की शुरुआत कैसे करें।'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# LXMERT\\n\\n## Overview'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 746}, page_content='-->\\n\\n# LXMERT\\n\\n## Overview\\n\\nThe LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders\\n(one for the vision modality, one for the language modality, and then one to fuse both modalities) pretrained using a\\ncombination of masked language modeling, visual-language text alignment, ROI-feature regression, masked\\nvisual-attribute modeling, masked visual-object modeling, and visual-question answering objectives. The pretraining\\nconsists of multiple multi-modal datasets: MSCOCO, Visual-Genome + Visual-Genome Question Answering, VQA 2.0, and GQA.\\n\\nThe abstract from the paper is the following:'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 1502}, page_content='*Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly,\\nthe alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality\\nEncoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we\\nbuild a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language\\nencoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language\\nsemantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative\\npretraining tasks: masked language modeling, masked object prediction (feature regression and label classification),\\ncross-modality matching, and image question answering. These tasks help in learning both intra-modality and'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 2422}, page_content='cross-modality relationships. After fine-tuning from our pretrained parameters, our model achieves the state-of-the-art\\nresults on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our\\npretrained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR, and improve the previous\\nbest result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel\\nmodel components and pretraining strategies significantly contribute to our strong results; and also present several\\nattention visualizations for the different encoders*'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 3063}, page_content='This model was contributed by [eltoto1219](https://huggingface.co/eltoto1219). The original code can be found [here](https://github.com/airsplay/lxmert).\\n\\n## Usage tips'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 3218}, page_content='## Usage tips\\n\\n- Bounding boxes are not necessary to be used in the visual feature embeddings, any kind of visual-spacial features\\n  will work.\\n- Both the language hidden states and the visual hidden states that LXMERT outputs are passed through the\\n  cross-modality layer, so they contain information from both modalities. To access a modality that only attends to\\n  itself, select the vision/language hidden states from the first input in the tuple.\\n- The bidirectional cross-modality encoder attention only returns attention values when the language modality is used\\n  as the input and the vision modality is used as the context vector. Further, while the cross-modality encoder\\n  contains self-attention for each respective modality and cross-attention, only the cross attention is returned and\\n  both self attention outputs are disregarded.\\n\\n## Resources\\n\\n- [Question answering task guide](../tasks/question_answering)\\n\\n## LxmertConfig\\n\\n[[autodoc]] LxmertConfig\\n\\n## LxmertTokenizer'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/model_doc/lxmert.md', 'start_index': 4143}, page_content='## LxmertConfig\\n\\n[[autodoc]] LxmertConfig\\n\\n## LxmertTokenizer\\n\\n[[autodoc]] LxmertTokenizer\\n\\n## LxmertTokenizerFast\\n\\n[[autodoc]] LxmertTokenizerFast\\n\\n## Lxmert specific outputs\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertModelOutput\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForPreTrainingOutput\\n\\n[[autodoc]] models.lxmert.modeling_lxmert.LxmertForQuestionAnsweringOutput\\n\\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertModelOutput\\n\\n[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\\n\\n<frameworkcontent>\\n<pt>\\n\\n## LxmertModel\\n\\n[[autodoc]] LxmertModel\\n    - forward\\n\\n## LxmertForPreTraining\\n\\n[[autodoc]] LxmertForPreTraining\\n    - forward\\n\\n## LxmertForQuestionAnswering\\n\\n[[autodoc]] LxmertForQuestionAnswering\\n    - forward\\n\\n</pt>\\n<tf>\\n\\n## TFLxmertModel\\n\\n[[autodoc]] TFLxmertModel\\n    - call\\n\\n## TFLxmertForPreTraining\\n\\n[[autodoc]] TFLxmertForPreTraining\\n    - call\\n\\n</tf>\\n</frameworkcontent>'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 1}, page_content=\"The tokenization pipeline\\n\\nWhen calling `Tokenizer.encode` or\\n`Tokenizer.encode_batch`, the input\\ntext(s) go through the following pipeline:\\n\\n-   `normalization`\\n-   `pre-tokenization`\\n-   `model`\\n-   `post-processing`\\n\\nWe'll see in details what happens during each of those steps in detail,\\nas well as when you want to `decode <decoding>` some token ids, and how the 🤗 Tokenizers library allows you\\nto customize each of those steps to your needs. If you're already\\nfamiliar with those steps and want to learn by seeing some code, jump to\\n`our BERT from scratch example <example>`.\\n\\nFor the examples that require a `Tokenizer` we will use the tokenizer we trained in the\\n`quicktour`, which you can load with:\"),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 711}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START reload_tokenizer\",\\n\"end-before\": \"END reload_tokenizer\",\\n\"dedent\": 12}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_reload_tokenizer\",\\n\"end-before\": \"END pipeline_reload_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START reload_tokenizer\",\\n\"end-before\": \"END reload_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\n## Normalization'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 1465}, page_content='## Normalization\\n\\nNormalization is, in a nutshell, a set of operations you apply to a raw\\nstring to make it less random or \"cleaner\". Common operations include\\nstripping whitespace, removing accented characters or lowercasing all\\ntext. If you\\'re familiar with [Unicode\\nnormalization](https://unicode.org/reports/tr15), it is also a very\\ncommon normalization operation applied in most tokenizers.\\n\\nEach normalization operation is represented in the 🤗 Tokenizers library\\nby a `Normalizer`, and you can combine\\nseveral of those by using a `normalizers.Sequence`. Here is a normalizer applying NFD Unicode normalization\\nand removing accents as an example:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 2118}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START setup_normalizer\",\\n\"end-before\": \"END setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_setup_normalizer\",\\n\"end-before\": \"END pipeline_setup_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START setup_normalizer\",\\n\"end-before\": \"END setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nYou can manually test that normalizer by applying it to any string:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 2871}, page_content='You can manually test that normalizer by applying it to any string:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START test_normalizer\",\\n\"end-before\": \"END test_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_test_normalizer\",\\n\"end-before\": \"END pipeline_test_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START test_normalizer\",\\n\"end-before\": \"END test_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nWhen building a `Tokenizer`, you can\\ncustomize its normalizer by just changing the corresponding attribute:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 3796}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START replace_normalizer\",\\n\"end-before\": \"END replace_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_replace_normalizer\",\\n\"end-before\": \"END pipeline_replace_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START replace_normalizer\",\\n\"end-before\": \"END replace_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nOf course, if you change the way a tokenizer applies normalization, you\\nshould probably retrain it from scratch afterward.\\n\\n## Pre-Tokenization'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 4685}, page_content='## Pre-Tokenization\\n\\nPre-tokenization is the act of splitting a text into smaller objects\\nthat give an upper bound to what your tokens will be at the end of\\ntraining. A good way to think of this is that the pre-tokenizer will\\nsplit your text into \"words\" and then, your final tokens will be parts\\nof those words.\\n\\nAn easy way to pre-tokenize inputs is to split on spaces and\\npunctuations, which is done by the\\n`pre_tokenizers.Whitespace`\\npre-tokenizer:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 5139}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START setup_pre_tokenizer\",\\n\"end-before\": \"END setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_setup_pre_tokenizer\",\\n\"end-before\": \"END pipeline_setup_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START setup_pre_tokenizer\",\\n\"end-before\": \"END setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 5910}, page_content='The output is a list of tuples, with each tuple containing one word and\\nits span in the original sentence (which is used to determine the final\\n`offsets` of our `Encoding`). Note that splitting on\\npunctuation will split contractions like `\"I\\'m\"` in this example.\\n\\nYou can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that will\\nsplit on space, punctuation and digits, separating numbers in their\\nindividual digits:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 6364}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START combine_pre_tokenizer\",\\n\"end-before\": \"END combine_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_combine_pre_tokenizer\",\\n\"end-before\": \"END pipeline_combine_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START combine_pre_tokenizer\",\\n\"end-before\": \"END combine_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nAs we saw in the `quicktour`, you can\\ncustomize the pre-tokenizer of a `Tokenizer` by just changing the corresponding attribute:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 7277}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START replace_pre_tokenizer\",\\n\"end-before\": \"END replace_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_replace_pre_tokenizer\",\\n\"end-before\": \"END pipeline_replace_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START replace_pre_tokenizer\",\\n\"end-before\": \"END replace_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nOf course, if you change the way the pre-tokenizer, you should probably\\nretrain your tokenizer from scratch afterward.\\n\\n## Model'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 8180}, page_content='## Model\\n\\nOnce the input texts are normalized and pre-tokenized, the\\n`Tokenizer` applies the model on the\\npre-tokens. This is the part of the pipeline that needs training on your\\ncorpus (or that has been trained if you are using a pretrained\\ntokenizer).\\n\\nThe role of the model is to split your \"words\" into tokens, using the\\nrules it has learned. It\\'s also responsible for mapping those tokens to\\ntheir corresponding IDs in the vocabulary of the model.\\n\\nThis model is passed along when intializing the\\n`Tokenizer` so you already know how to\\ncustomize this part. Currently, the 🤗 Tokenizers library supports:\\n\\n-   `models.BPE`\\n-   `models.Unigram`\\n-   `models.WordLevel`\\n-   `models.WordPiece`\\n\\nFor more details about each model and its behavior, you can check\\n[here](components#models)\\n\\n## Post-Processing\\n\\nPost-processing is the last step of the tokenization pipeline, to\\nperform any additional transformation to the\\n`Encoding` before it\\'s returned, like\\nadding potential special tokens.'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 9170}, page_content='As we saw in the quick tour, we can customize the post processor of a\\n`Tokenizer` by setting the\\ncorresponding attribute. For instance, here is how we can post-process\\nto make the inputs suitable for the BERT model:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START setup_processor\",\\n\"end-before\": \"END setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_setup_processor\",\\n\"end-before\": \"END pipeline_setup_processor\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START setup_processor\",\\n\"end-before\": \"END setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 10134}, page_content=\"Note that contrarily to the pre-tokenizer or the normalizer, you don't\\nneed to retrain a tokenizer after changing its post-processor.\\n\\n## All together: a BERT tokenizer from scratch\\n\\nLet's put all those pieces together to build a BERT tokenizer. First,\\nBERT relies on WordPiece, so we instantiate a new\\n`Tokenizer` with this model:\"),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 10467}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_tokenizer\",\\n\"end-before\": \"END bert_setup_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_tokenizer\",\\n\"end-before\": \"END bert_setup_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_tokenizer\",\\n\"end-before\": \"END bert_setup_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThen we know that BERT preprocesses texts by removing accents and\\nlowercasing. We also use a unicode normalizer:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 11340}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_normalizer\",\\n\"end-before\": \"END bert_setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_normalizer\",\\n\"end-before\": \"END bert_setup_normalizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_normalizer\",\\n\"end-before\": \"END bert_setup_normalizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nThe pre-tokenizer is just splitting on whitespace and punctuation:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 12105}, page_content='The pre-tokenizer is just splitting on whitespace and punctuation:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_pre_tokenizer\",\\n\"end-before\": \"END bert_setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_pre_tokenizer\",\\n\"end-before\": \"END bert_setup_pre_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_pre_tokenizer\",\\n\"end-before\": \"END bert_setup_pre_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nAnd the post-processing uses the template we saw in the previous\\nsection:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 12956}, page_content='And the post-processing uses the template we saw in the previous\\nsection:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_setup_processor\",\\n\"end-before\": \"END bert_setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_setup_processor\",\\n\"end-before\": \"END bert_setup_processor\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_setup_processor\",\\n\"end-before\": \"END bert_setup_processor\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nWe can use this tokenizer and train on it on wikitext like in the\\n`quicktour`:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 13790}, page_content='We can use this tokenizer and train on it on wikitext like in the\\n`quicktour`:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_train_tokenizer\",\\n\"end-before\": \"END bert_train_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_train_tokenizer\",\\n\"end-before\": \"END bert_train_tokenizer\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_train_tokenizer\",\\n\"end-before\": \"END bert_train_tokenizer\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\n## Decoding'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 14629}, page_content=\"## Decoding\\n\\nOn top of encoding the input texts, a `Tokenizer` also has an API for decoding, that is converting IDs\\ngenerated by your model back to a text. This is done by the methods\\n`Tokenizer.decode` (for one predicted text) and `Tokenizer.decode_batch` (for a batch of predictions).\\n\\nThe `decoder` will first convert the IDs back to tokens\\n(using the tokenizer's vocabulary) and remove all special tokens, then\\njoin those tokens with spaces:\"),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 15076}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START test_decoding\",\\n\"end-before\": \"END test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START pipeline_test_decoding\",\\n\"end-before\": \"END pipeline_test_decoding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START test_decoding\",\\n\"end-before\": \"END test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 15811}, page_content='If you used a model that added special characters to represent subtokens\\nof a given \"word\" (like the `\"##\"` in\\nWordPiece) you will need to customize the `decoder` to treat\\nthem properly. If we take our previous `bert_tokenizer` for instance the\\ndefault decoding will give:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 16085}, page_content='<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_test_decoding\",\\n\"end-before\": \"END bert_test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_test_decoding\",\\n\"end-before\": \"END bert_test_decoding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_test_decoding\",\\n\"end-before\": \"END bert_test_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>\\n\\nBut by changing it to a proper decoder, we get:'),\n",
              " Document(metadata={'source': 'huggingface/tokenizers/blob/main/docs/source-doc-builder/pipeline.mdx', 'start_index': 16832}, page_content='But by changing it to a proper decoder, we get:\\n\\n<tokenizerslangcontent>\\n<python>\\n<literalinclude>\\n{\"path\": \"../../bindings/python/tests/documentation/test_pipeline.py\",\\n\"language\": \"python\",\\n\"start-after\": \"START bert_proper_decoding\",\\n\"end-before\": \"END bert_proper_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</python>\\n<rust>\\n<literalinclude>\\n{\"path\": \"../../tokenizers/tests/documentation.rs\",\\n\"language\": \"rust\",\\n\"start-after\": \"START bert_proper_decoding\",\\n\"end-before\": \"END bert_proper_decoding\",\\n\"dedent\": 4}\\n</literalinclude>\\n</rust>\\n<node>\\n<literalinclude>\\n{\"path\": \"../../bindings/node/examples/documentation/pipeline.test.ts\",\\n\"language\": \"js\",\\n\"start-after\": \"START bert_proper_decoding\",\\n\"end-before\": \"END bert_proper_decoding\",\\n\"dedent\": 8}\\n</literalinclude>\\n</node>\\n</tokenizerslangcontent>'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 1}, page_content=\"TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https://paperswithcode.com/method/resnet) that aim to boost accuracy while maintaining GPU training and inference efficiency.  They contain several design tricks including a SpaceToDepth stem, [Anti-Alias downsampling](https://paperswithcode.com/method/anti-alias-downsampling), In-Place Activated BatchNorm, Blocks selection and [squeeze-and-excitation layers](https://paperswithcode.com/method/squeeze-and-excitation-block).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('tresnet_l', pretrained=True)\\nmodel.eval()\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 639}, page_content='```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 1376}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 2129}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tresnet_l`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model('tresnet_l', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 2721}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{ridnik2020tresnet,\\n      title={TResNet: High Performance GPU-Dedicated Architecture}, \\n      author={Tal Ridnik and Hussam Lawen and Asaf Noy and Emanuel Ben Baruch and Gilad Sharir and Itamar Friedman},\\n      year={2020},\\n      eprint={2003.13630},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 3423}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 3428}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: TResNet\\n  Paper:\\n    Title: 'TResNet: High Performance GPU-Dedicated Architecture'\\n    URL: https://paperswithcode.com/paper/tresnet-high-performance-gpu-dedicated\\nModels:\\n- Name: tresnet_l\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 10873416792\\n    Parameters: 53456696\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_l\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 4305}, page_content=\"Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L267\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_81_5-235b486c.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81.49%\\n      Top 5 Accuracy: 95.62%\\n- Name: tresnet_l_448\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 43488238584\\n    Parameters: 53456696\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 5204}, page_content=\"- AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_l_448\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L285\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_448-940d0cd1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82.26%\\n      Top 5 Accuracy: 95.98%\\n- Name: tresnet_m\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 5733048064\\n    Parameters: 41282200\\n    File Size: 125861314\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 6096}, page_content=\"- Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    Training Time: < 24 hours\\n    ID: tresnet_m\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L261\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_80_8-dbc13962.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.8%\\n      Top 5 Accuracy: 94.86%\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 7002}, page_content=\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.8%\\n      Top 5 Accuracy: 94.86%\\n- Name: tresnet_m_448\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 22929743104\\n    Parameters: 29278464\\n    File Size: 125861314\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_m_448\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L279\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 7982}, page_content=\"Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_448-bc359d10.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 81.72%\\n      Top 5 Accuracy: 95.57%\\n- Name: tresnet_xl\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 15162534034\\n    Parameters: 75646610\\n    File Size: 314378965\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_xl\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 8875}, page_content=\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L273\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_xl_82_0-a2d51b00.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82.05%\\n      Top 5 Accuracy: 95.93%\\n- Name: tresnet_xl_448\\n  In Collection: TResNet\\n  Metadata:\\n    FLOPs: 60641712730\\n    Parameters: 75646610\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Convolution\\n    - Anti-Alias Downsampling\\n    - Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Residual Connection\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/tresnet.md', 'start_index': 9798}, page_content=\"- AutoAugment\\n    - Cutout\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA 100 GPUs\\n    ID: tresnet_xl_448\\n    LR: 0.01\\n    Epochs: 300\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/tresnet.py#L291\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_448-940d0cd1.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83.06%\\n      Top 5 Accuracy: 96.19%\\n-->\"),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-configure.md', 'start_index': 1}, page_content='Configure the Dataset Viewer\\n\\nThe Dataset Viewer supports many [data files formats](./datasets-adding#file-formats), from text to tabular and from image to audio formats.\\nIt also separates the train/validation/test splits based on file and folder names.\\n\\nTo configure the Dataset Viewer for your dataset, first make sure your dataset is in a [supported data format](./datasets-adding#files-formats).\\n\\n## Configure dropdowns for splits or subsets\\n\\nIn the Dataset Viewer you can view the [train/validation/test](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) splits of datasets, and sometimes additionally choose between multiple subsets (e.g. one per language).\\n\\nTo define those dropdowns, you can name the data files or their folder after their split names (train/validation/test).\\nIt is also possible to customize your splits manually using YAML.'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-configure.md', 'start_index': 875}, page_content=\"For more information, feel free to check out the documentation on [Data files Configuration](./datasets-data-files-configuration).\\n\\n## Disable the viewer\\n\\nThe dataset viewer can be disabled. To do this, add a YAML section to the dataset's `README.md` file (create one if it does not already exist) and add a `viewer` property with the value `false`.\"),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-configure.md', 'start_index': 1226}, page_content='```\\n---\\nviewer: false\\n---\\n```\\n\\nNote that the viewer is always disabled on the private datasets.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 0}, page_content='Author: [@vasudevgupta7](https://github.com/thevasudevgupta/)\\n\\n## Intro\\n\\nIn this project, we fine-tuned [**BigBird**](https://arxiv.org/abs/2007.14062) on [**natural-questions**](https://huggingface.co/datasets/natural_questions) dataset for **question-answering** task on long documents. **BigBird**, is a **sparse-attention based transformer** which extends Transformer based models, such as BERT to much **longer sequences**.\\n\\nRead more about BigBird at https://huggingface.co/blog/big-bird\\n\\n## Fine-tuning\\n\\n**Setup**\\n\\nYou need to install jax yourself by following the official docs ([refer this](https://github.com/google/jax#installation)). Other requirements for this project can be installed by running following command:\\n\\n```shell\\npip3 install -qr requirements.txt'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 773}, page_content=\"```\\n\\n**Download & prepare dataset**\\n\\nThe Natural Questions corpus contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question. This corpus takes ~100 GB on disk. We have used HuggingFace datasets to download & process the dataset.\\n\\n```shell\\n# just run following CMD\\npython3 prepare_natural_questions.py\\n\\n# this will download the whole dataset from HuggingFace Hub & will make it ready for training\\n# this script takes ~3 hours to process the dataset\\n```\\n\\n**Launch Training**\\n\\nWe have trained on Cloud's TPU v3-8. Each epoch took around 4.5 hours and the model got converged in just 2 epochs. You can see complete training args in [this script](bigbird_flax.py).\\n\\n```shell\\n# just run following CMD\\npython3 train.py\\n\\n# In case, you want to try hparams tuning, you can run wandb sweep\\nwandb sweep --project=bigbird sweep_flax.yaml\\nwandb agent <agent-id-obtained-by-above-CMD>\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 1755}, page_content='```\\n\\n## Evaluation\\n\\nOur evaluation script is different from the original script and we are evaluating sequences with length up to 4096 for simplicity. We managed to get the **EM score of ~55.2** using our evaluation script.\\n\\n```shell\\n# download validation-dataset first\\nmkdir natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/natural_questions-validation.arrow -P natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/dataset_info.json -P natural-questions-validation\\nwget https://huggingface.co/datasets/vasudevgupta/natural-questions-validation/resolve/main/state.json -P natural-questions-validation\\n\\n# simply run following command\\npython3 evaluate.py'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/examples/research_projects/jax-projects/big_bird/README.md', 'start_index': 2548}, page_content='```\\n\\nYou can find our checkpoint on HuggingFace Hub ([see this](https://huggingface.co/vasudevgupta/flax-bigbird-natural-questions)). In case you are interested in PyTorch BigBird fine-tuning, you can refer to [this repositary](https://github.com/thevasudevgupta/bigbird).'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 0}, page_content='--\\ntitle: Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\nthumbnail: /blog/assets/30_clip_rsicd/clip_schematic.png\\nauthors:\\n- user: arampacha\\n  guest: true\\n- user: devv\\n  guest: true\\n- user: goutham794\\n  guest: true\\n- user: cataluna84\\n  guest: true\\n- user: ghosh-r\\n  guest: true\\n- user: sujitpal\\n  guest: true\\n---\\n\\n# Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\n\\n\\n\\n## Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\n\\n<img src=\"/blog/assets/30_clip_rsicd/clip-rsicd-header-image.png\"/>\\n\\nIn July this year, [Hugging Face](https://huggingface.co/) organized a [Flax/JAX Community Week](https://github.com/huggingface/transformers/blob/master/examples/research_projects/jax-projects/README.md), and invited the community to submit projects to train Hugging Face [transformers](https://github.com/huggingface/transformers) models in the areas of Natural Language Processing (NLP) and Computer Vision (CV).'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 968}, page_content='Participants used Tensor Processing Units (TPUs) with [Flax](https://github.com/google/flax) and [JAX](https://github.com/google/jax). JAX is a linear algebra library (like `numpy`) that can do automatic differentiation ([Autograd](https://github.com/hips/autograd)) and compile down to [XLA](https://www.tensorflow.org/xla), and Flax is a neural network library and ecosystem for JAX. TPU compute time was provided free by [Google Cloud](https://cloud.google.com/), who co-sponsored the event.\\n\\nOver the next two weeks, teams participated in lectures from Hugging Face and Google, trained one or more models using JAX/Flax, shared them with the community, and provided a  [Hugging Face Spaces](https://huggingface.co/spaces) demo showcasing the capabilities of their model. Approximately 100 teams participated in the event, and it resulted in 170 models and 36 demos.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 1839}, page_content='Our team, like probably many others, is a distributed one, spanning 12 time zones. Our common thread is that we all belong to the [TWIML Slack Channel](https://twimlai.slack.com/), where we came together based on a shared interest in Artificial Intelligence (AI) and Machine Learning (ML) topics.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 2138}, page_content='We fine-tuned the [CLIP Network from OpenAI](https://openai.comclip/) with satellite images and captions from the [RSICD dataset](https://github.com/201528014227051/RSICD_optimal). The CLIP network learns visual concepts by being trained with image and caption pairs in a self-supervised manner, by using text paired with images found across the Internet. During inference, the model can predict the most relevant image given a text description or the most relevant text description given an image. CLIP is powerful enough to be used in zero-shot manner on everyday images. However, we felt that satellite images were sufficiently different from everyday images that it would be useful to fine-tune CLIP with them. Our intuition turned out to be correct, as the evaluation results (described below) shows. In this post, we describe details of our training and evaluation process, and our plans for future work on this project.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 3066}, page_content='The goal of our project was to provide a useful service and demonstrate how to use CLIP for practical use cases. Our model can be used by applications to search through large collections of satellite images using textual queries. Such queries could describe the image in totality (for example, beach, mountain, airport, baseball field, etc) or search or mention specific geographic or man-made features within these images. CLIP can similarly be fine-tuned for other domains as well, as shown by the [medclip-demo team](https://huggingface.co/spaces/flax-community/medclip-demo) for medical images.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 3666}, page_content='The ability to search through large collections of images using text queries is an immensely powerful feature, and can be used as much for social good as for malign purposes. Possible applications include national defense and anti-terrorism activities, the ability to spot and address effects of climate change before they become unmanageable, etc. Unfortunately, this power can also be misused, such as for military and police surveillance by authoritarian nation-states, so it does raise some ethical questions as well.\\n\\nYou can read about the project on our [project page](https://github.com/arampacha/CLIP-rsicd), download our [trained model](https://huggingface.co/flax-community/clip-rsicd-v2) to use for inference on your own data, or see it in action on our [demo](https://huggingface.co/spaces/sujitpal/clip-rsicd-demo).\\n\\n\\n### Training\\n\\n#### Dataset'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 4498}, page_content='### Training\\n\\n#### Dataset\\n\\nWe fine-tuned the CLIP model primarily with the [RSICD dataset](https://github.com/201528014227051/RSICD_optimal). This dataset consists of about 10,000 images collected from Google Earth, Baidu Map, MapABC, and Tianditu. It is provided freely to the research community to advance remote sensing captioning via [Exploring Models and Data for Remote Sensing Image Caption Generation](https://arxiv.org/abs/1712.0783) (Lu et al, 2017). The images are (224, 224) RGB images at various resolutions, and each image has up to 5 captions associated with it.\\n\\n<img src=\"/blog/assets/30_clip_rsicd/rsicd-images-sampling.png\"/>\\n<center><i>Some examples of images from the RSICD dataset</i></center>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 5216}, page_content='In addition, we used the [UCM Dataset](https://mega.nz/folder/wCpSzSoS#RXzIlrv--TDt3ENZdKN8JA) and the [Sydney dataset](https://mega.nz/folder/pG4yTYYA#4c4buNFLibryZnlujsrwEQ) for training, The UCM dataset is based on the UC Merced Land Use dataset. It consists of 2100 images belonging to 21 classes (100 images per class), and each image has 5 captions. The Sydney dataset contains images of Sydney, Australia from Google Earth. It contains 613 images belonging to 7 classes. Images are (500, 500) RGB and provides 5 captions for each image. We used these additional datasets because we were not sure if the RSICD dataset would be large enough to fine-tune CLIP.\\n\\n\\n#### Model'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 5883}, page_content='#### Model\\n\\nOur model is just the fine-tuned version of the original CLIP model shown below. Inputs to the model are a batch of captions and a batch of images passed through the CLIP text encoder and image encoder respectively. The training process uses [contrastive learning](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607) to learn a joint embedding representation of image and captions. In this embedding space, images and their respective captions are pushed close together, as are similar images and similar captions. Conversely, images and captions for different images, or dissimilar images and captions, are likely to be pushed further apart.\\n\\n<img src=\"/blog/assets/30_clip_rsicd/clip_schematic.png\"/>\\n<center><i>CLIP Training and Inference (Image Credit: CLIP: Connecting Text and Images (https://openai.comclip/))</i></center>\\n\\n\\n#### Data Augmentation'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 6755}, page_content=\"#### Data Augmentation\\n\\nIn order to regularize our dataset and prevent overfitting due to the size of the dataset, we used both image and text augmentation.\\n\\nImage augmentation was done inline using built-in transforms from Pytorch's [Torchvision](https://pytorch.org/vision/stable/index.html) package. The transformations used were Random Cropping, Random Resizing and Cropping, Color Jitter, and Random Horizontal and Vertical flipping.\\n\\nWe augmented the text with backtranslation to generate captions for images with less than 5 unique captions per image. The [Marian MT]((https://huggingface.co/transformers/model_doc/marian.html)) family of models from Hugging Face was used to translate the existing captions into French, Spanish, Italian, and Portuguese and back to English to fill out the captions for these images.\\n\\nAs shown in these loss plots below, image augmentation reduced overfitting significantly, and text and image augmentation reduced overfitting even further.\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 7737}, page_content='<img src=\"/blog/assets/30_clip_rsicd/image-augment-loss.png\"/>\\n<img src=\"/blog/assets/30_clip_rsicd/image-text-aug-loss.png\"/>\\n<center><i>Evaluation and Training loss plots comparing (top) no augmentation vs image augmentation, and (bottom) image augmentation vs text+image augmentation</i></center>\\n\\n\\n### Evaluation\\n\\n#### Metrics\\n\\nA subset of the RSICD test set was used for evaluation. We found 30 categories of images in this subset. The evaluation was done by comparing each image with a set of 30 caption sentences of the form `\"An aerial photograph of {category}\"`. The model produced a ranked list of the 30 captions, from most relevant to least relevant. Categories corresponding to captions with the top k scores (for k=1, 3, 5, and 10) were compared with the category provided via the image file name. The scores are averaged over the entire set of images used for evaluation and reported for various values of k, as shown below.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 8678}, page_content='The `baseline` model represents the pre-trained `openai/clip-vit-base-path32` CLIP model. This model was fine-tuned with captions and images from the RSICD dataset, which resulted in a significant performance boost, as shown below.\\n\\nOur best model was trained with image and text augmentation, with batch size 1024 (128 on each of the 8 TPU cores), and the Adam optimizer with learning rate 5e-6. We trained our second base model with the same hyperparameters, except that we used the Adafactor optimizer with learning rate 1e-4. You can download either model from their model repos linked to in the table below.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 9292}, page_content='| Model-name                               | k=1   | k=3   | k=5   | k=10  |\\n| ---------------------------------------- | ----- | ----- | ----- | ----- |\\n| baseline                                 | 0.572 | 0.745 | 0.837 | 0.939 |\\n| bs128x8-lr1e-4-augs/ckpt-2               | 0.819 | 0.950 | 0.974 | 0.994 |\\n| bs128x8-lr1e-4-imgaugs/ckpt-2            | 0.812 | 0.942 | 0.970 | 0.991 |\\n| [bs128x8-lr1e-4-imgaugs-textaugs/ckpt-4](https://huggingface.co/flax-community/clip-rsicd)<sup>2</sup>   | 0.843 | 0.958 | 0.977 | 0.993 |\\n| bs128x8-lr5e-5-imgaugs-textaugs/ckpt-8   | 0.831 | 0.959 | 0.977 | 0.994 |\\n| bs128x8-lr5e-5-imgaugs/ckpt-4            | 0.746 | 0.906 | 0.956 | 0.989 |\\n| bs128x8-lr5e-5-imgaugs-textaugs-2/ckpt-4 | 0.811 | 0.945 | 0.972 | 0.993 |\\n| bs128x8-lr5e-5-imgaugs-textaugs-3/ckpt-5 | 0.823 | 0.946 | 0.971 | 0.992 |\\n| bs128x8-lr5e-5-wd02/ckpt-4               | 0.820 | 0.946 | 0.965 | 0.990 |'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 10126}, page_content='| bs128x8-lr5e-5-wd02/ckpt-4               | 0.820 | 0.946 | 0.965 | 0.990 |\\n| [bs128x8-lr5e-6-adam/ckpt-1](https://huggingface.co/flax-community/clip-rsicd-v2)<sup>1</sup> | **0.883** | **0.968** | **0.982** | **0.998** |'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 10351}, page_content='_1 - our best model, 2 - our second best model_\\n\\n\\n#### Demo\\n\\nYou can access the [CLIP-RSICD Demo](https://huggingface.co/spaces/sujitpal/clip-rsicd-demo) here. It uses our fine-tuned CLIP model to provide the following functionality:\\n\\n* Text to Image search\\n* Image to Image search\\n* Find text feature in image\\n\\nThe first two functionalities use the RSICD test set as its image corpus. They are encoded using our best fine-tuned CLIP model and stored in a [NMSLib](https://github.com/nmslib/nmslib) index which allows Approximate Nearest Neighbor based retrieval. For text-to-image and image-to-image search respectively, the query text or image are encoded with our model and matched against the image vectors in the corpus. For the third functionality, we divide the incoming image into patches and encode them, encode the queried text feature, match the text vector with each image patch vector, and return the probability of finding the feature in each patch.\\n\\n### Future Work'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/fine-tune-clip-rsicd.md', 'start_index': 11316}, page_content='### Future Work\\n\\nWe are grateful that we have been given an opportunity to further refine our model. Some ideas we have for future work are as follows:\\n\\n1. Construct a sequence to sequence model using a CLIP encoder and a GPT-3 decoder and train it for image captioning.\\n2. Fine-tune the model on more image caption pairs from other datasets and investigate if we can improve its performance.\\n3. Investigate how fine-tuning affects the performance of model on non-RSICD image caption pairs.\\n4. Investigate the capability of the fine-tuned model to classify outside the categories it has been fine-tuned on.\\n5. Evaluate the model using other criteria such as image classification.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# 🤗 Transformers Notebooks\\n\\nYou can find here a list of the official notebooks provided by Hugging Face.\\n\\nAlso, we would like to list here interesting content created by the community.\\nIf you wrote some notebook(s) leveraging 🤗 Transformers and would like to be listed here, please open a\\nPull Request so it can be included under the Community notebooks.\\n\\n\\n## Hugging Face\\'s notebooks 🤗'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 950}, page_content=\"## Hugging Face's notebooks 🤗\\n\\n### Documentation notebooks\\n\\nYou can open any page of the documentation as a notebook in Colab (there is a button directly on said pages) but they are also listed here if you need them:\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 1168}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [Quicktour of the library](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)  | A presentation of the various APIs in Transformers |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/en/transformers_doc/quicktour.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 1816}, page_content='| [Summary of the tasks](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)  | How to run the models of the Transformers library task by task |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/task_summary.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 2380}, page_content='| [Preprocessing data](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)  | How to use a tokenizer to preprocess your data |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 2929}, page_content='| [Fine-tuning a pretrained model](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)  | How to use the Trainer to fine-tune a pretrained model |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/training.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 3483}, page_content='| [Summary of the tokenizers](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)  | The differences between the tokenizers algorithm |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 4053}, page_content='| [Multilingual models](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)  | How to use the multilingual models of the library |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/multilingual.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 4605}, page_content='### PyTorch Examples\\n\\n#### Natural Language Processing[[pytorch-nlp]]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 4676}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 5309}, page_content='| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 5877}, page_content='| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 6475}, page_content='| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 7078}, page_content='| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/main/examples/token_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 7705}, page_content='| [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SQUAD. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 8291}, page_content='| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SWAG. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 8864}, page_content='| [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/main/examples/translation.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on WMT. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/translation.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 9420}, page_content='| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/examples/summarization.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on XSUM. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 9985}, page_content='| [How to train a language model from scratch](https://github.com/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)| Highlight all the steps to effectively train Transformer model on custom data | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/01_how_to_train.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 10549}, page_content='| [How to generate text](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)| How to use different decoding methods for language generation with transformers | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 11102}, page_content='| [How to generate text (with constraints)](https://github.com/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)| How to guide language generation with user-provided constraints | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/blog/blob/main/notebooks/53_constrained_beam_search.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 11682}, page_content='| [Reformer](https://github.com/huggingface/blog/blob/main/notebooks/03_reformer.ipynb)| How Reformer pushes the limits of language modeling | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/blog/blob/main/notebooks/03_reformer.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/patrickvonplaten/blog/blob/main/notebooks/03_reformer.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 12185}, page_content='#### Computer Vision[[pytorch-cv]]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 12221}, page_content='| Notebook                                                                                                                                                                   | Description                                                                                                            |                                                                                                                                                                                                            |   |'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 12726}, page_content='|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------:|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 13235}, page_content='| [How to fine-tune a model on image classification (Torchvision)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                   | Show how to preprocess the data using Torchvision and fine-tune any pretrained Vision model on Image Classification    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)                 | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 13923}, page_content='| [How to fine-tune a model on image classification (Albumentations)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) | Show how to preprocess the data using Albumentations and fine-tune any pretrained Vision model on Image Classification | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)  | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 14626}, page_content='| [How to fine-tune a model on image classification (Kornia)](https://github.com/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)                 | Show how to preprocess the data using Kornia and fine-tune any pretrained Vision model on Image Classification         | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)          | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 15321}, page_content='| [How to perform zero-shot object detection with OWL-ViT](https://github.com/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)          | Show how to perform zero-shot object detection on images with text queries                                             | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/zeroshot_object_detection_with_owlvit.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 16026}, page_content='| [How to fine-tune an image captioning model](https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)                                      | Show how to fine-tune BLIP for image captioning on a custom dataset                                                    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_captioning_blip.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 16715}, page_content='| [How to build an image similarity system with Transformers](https://github.com/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)                            | Show how to build an image similarity system                                                                           | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)                     | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_similarity.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 17399}, page_content='| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)                     | Show how to preprocess the data and fine-tune a pretrained SegFormer model on Semantic Segmentation                    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 18088}, page_content='| [How to fine-tune a VideoMAE model on video classification](https://github.com/huggingface/notebooks/blob/main/examples/video_classification.ipynb)          | Show how to preprocess the data and fine-tune a pretrained VideoMAE model on Video Classification                      | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/video_classification.ipynb)                | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/video_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 18762}, page_content='#### Audio[[pytorch-audio]]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 18791}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to fine-tune a speech recognition model in English](https://github.com/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)| Show how to preprocess the data and fine-tune a pretrained Speech model on TIMIT | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 19492}, page_content='| [How to fine-tune a speech recognition model in any language](https://github.com/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)| Show how to preprocess the data and fine-tune a multi-lingually pretrained speech model on Common Voice | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 20162}, page_content='| [How to fine-tune a model on audio classification](https://github.com/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)| Show how to preprocess the data and fine-tune a pretrained Speech model on Keyword Spotting | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 20774}, page_content='#### Biological Sequences[[pytorch-bio]]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 20816}, page_content='| Notebook     | Description                                                                             |   |   |\\n|:----------|:----------------------------------------------------------------------------------------|:-------------|------:|\\n| [How to fine-tune a pre-trained protein model](https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) | See how to tokenize proteins and fine-tune a large pre-trained protein \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) |'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 21679}, page_content='| [How to generate protein folds](https://github.com/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) | See how to go from protein sequence to a full protein model and PDB file                | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_folding.ipynb) |'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 22255}, page_content='| [How to fine-tune a Nucleotide Transformer model](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) | See how to tokenize DNA and fine-tune a large pre-trained DNA \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) |'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 22930}, page_content='| [Fine-tune a Nucleotide Transformer model with LoRA](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) | Train even larger DNA models in a memory-efficient way | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling_with_peft.ipynb) |'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 23616}, page_content='#### Other modalities[[pytorch-other]]\\n\\n| Notebook     | Description                                                                             |   |   |\\n|:----------|:----------------------------------------------------------------------------------------|:-------------|------:|\\n| [Probabilistic Time Series Forecasting](https://github.com/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) | See how to train Time Series Transformer on a custom dataset                            | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/time-series-transformers.ipynb) |\\n\\n#### Utility notebooks[[pytorch-utility]]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 24510}, page_content='#### Utility notebooks[[pytorch-utility]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to export model to ONNX](https://github.com/huggingface/notebooks/blob/main/examples/onnx-export.ipynb)| Highlight how to export and run inference workloads through ONNX |\\n| [How to use Benchmarks](https://github.com/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| How to benchmark models with transformers | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/benchmark.ipynb)|\\n\\n### TensorFlow Examples\\n\\n#### Natural Language Processing[[tensorflow-nlp]]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 25412}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [Train your tokenizer](https://github.com/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)  | How to train and use your very own tokenizer  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tokenizer_training.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 26045}, page_content='| [Train your language model](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)   | How to easily start using transformers  |[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling_from_scratch-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 26622}, page_content='| [How to fine-tune a model on text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 27229}, page_content='| [How to fine-tune a model on language modeling](https://github.com/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a causal or masked LM task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 27841}, page_content='| [How to fine-tune a model on token classification](https://github.com/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on a token classification task (NER, PoS). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 28477}, page_content='| [How to fine-tune a model on question answering](https://github.com/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SQUAD. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 29072}, page_content='| [How to fine-tune a model on multiple choice](https://github.com/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on SWAG. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 29654}, page_content='| [How to fine-tune a model on translation](https://github.com/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on WMT. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 30219}, page_content='| [How to fine-tune a model on summarization](https://github.com/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| Show how to preprocess the data and fine-tune a pretrained model on XSUM. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 30794}, page_content='#### Computer Vision[[tensorflow-cv]]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 30833}, page_content='| Notebook                                                                                                                                                 | Description                                                                                         |   |   |\\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------|:-------------|------:|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 31382}, page_content='| [How to fine-tune a model on image classification](https://github.com/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)            | Show how to preprocess the data and fine-tune any pretrained Vision model on Image Classification   | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/image_classification-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 32022}, page_content='| [How to fine-tune a SegFormer model on semantic segmentation](https://github.com/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb) | Show how to preprocess the data and fine-tune a pretrained SegFormer model on Semantic Segmentation | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/semantic_segmentation-tf.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 32666}, page_content='#### Biological Sequences[[tensorflow-bio]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to fine-tune a pre-trained protein model](https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | See how to tokenize proteins and fine-tune a large pre-trained protein \"language\" model | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb) |\\n\\n#### Utility notebooks[[tensorflow-utility]]'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 33443}, page_content=\"#### Utility notebooks[[tensorflow-utility]]\\n\\n| Notebook     |      Description      |   |                                                                                                                                                                                      |\\n|:----------|:-------------|:-------------|------:|\\n| [How to train TF/Keras models on TPU](https://github.com/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | See how to train at high speed on Google's TPU hardware | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) | [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb) |\\n\\n### Optimum notebooks\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 34320}, page_content='### Optimum notebooks\\n\\n🤗  [Optimum](https://github.com/huggingface/optimum) is an extension of 🤗 Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardwares.'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 34557}, page_content='| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|\\n| [How to quantize a model with ONNX Runtime for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| Show how to apply static and dynamic quantization on a model using [ONNX Runtime](https://github.com/microsoft/onnxruntime) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 35385}, page_content='| [How to quantize a model with Intel Neural Compressor for text classification](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| Show how to apply static, dynamic and aware training quantization on a model using [Intel Neural Compressor (INC)](https://github.com/intel/neural-compressor) for any GLUE task. | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_inc.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 36158}, page_content='| [How to fine-tune a model on text classification with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)| Show how to preprocess the data and fine-tune a model on any GLUE task using [ONNX Runtime](https://github.com/microsoft/onnxruntime). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/text_classification_ort.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 36838}, page_content='| [How to fine-tune a model on summarization with ONNX Runtime](https://github.com/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)| Show how to preprocess the data and fine-tune a model on XSUM using [ONNX Runtime](https://github.com/microsoft/onnxruntime). | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/examples/summarization_ort.ipynb)|'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/notebooks/README.md', 'start_index': 37486}, page_content='## Community notebooks:\\n\\nMore notebooks developed by the community are available [here](https://hf.co/docs/transformers/community#community-notebooks).'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 0}, page_content='--\\ntitle: \"Multivariate Probabilistic Time Series Forecasting with Informer\" \\nthumbnail: /blog/assets/134_informer/thumbnail.png\\nauthors:\\n- user: elisim\\n  guest: true\\n- user: nielsr\\n- user: kashif\\n---\\n\\n# Multivariate Probabilistic Time Series Forecasting with Informer\\n\\n\\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multivariate_informer.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n## Introduction'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 617}, page_content=\"## Introduction\\n\\nA few months ago we introduced the [Time Series Transformer](https://huggingface.co/blog/time-series-transformers), which is the vanilla Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)) applied to forecasting, and showed an example for the **univariate** probabilistic forecasting task (i.e. predicting each time series' 1-d distribution individually). In this post we introduce the _Informer_ model ([Zhou, Haoyi, et al., 2021](https://arxiv.org/abs/2012.07436)), AAAI21 best paper which is [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in 🤗 Transformers. We will show how to use the Informer model for the **multivariate** probabilistic forecasting task, i.e., predicting the distribution of a future **vector** of time-series target values. Note that this will also work for the vanilla Time Series Transformer model.\\n\\n##  Multivariate Probabilistic Time Series Forecasting\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 1517}, page_content='##  Multivariate Probabilistic Time Series Forecasting\\n\\nAs far as the modeling aspect of probabilistic forecasting is concerned, the Transformer/Informer will require no change when dealing with multivariate time series. In both the univariate and multivariate setting, the model will receive a sequence of vectors and thus the only change is on the output or emission side.\\n\\nModeling the full joint conditional distribution of high dimensional data can get computationally expensive and thus methods resort to some approximation of the distribution, the easiest being to model the data as an independent distribution from the same family, or some low-rank approximation to the full covariance, etc. Here we will just resort to the independent (or diagonal) emissions which are supported for the families of distributions we have implemented [here](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils).\\n\\n## Informer - Under The Hood'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 2445}, page_content=\"## Informer - Under The Hood\\n\\nBased on the vanilla Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), Informer employs two major improvements. To understand these improvements, let's recall the drawbacks of the vanilla Transformer:\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 2699}, page_content=\"1. **Quadratic computation of canonical self-attention:** The vanilla Transformer has a computational complexity of \\\\\\\\(O(T^2 D)\\\\\\\\) where \\\\\\\\(T\\\\\\\\) is the time series length and \\\\\\\\(D\\\\\\\\) is the dimension of the hidden states. For long sequence time-series forecasting (also known as the _LSTF problem_), this might be really computationally expensive. To solve this problem, Informer employs a new self-attention mechanism called _ProbSparse_ attention, which has \\\\\\\\(O(T \\\\log T)\\\\\\\\) time and space complexity.\\n1. **Memory bottleneck when stacking layers:** When stacking \\\\\\\\(N\\\\\\\\) encoder/decoder layers, the vanilla Transformer has a memory usage of \\\\\\\\(O(N T^2)\\\\\\\\), which limits the model's capacity for long sequences. Informer uses a _Distilling_ operation, for reducing the input size between layers into its half slice. By doing so, it reduces the whole memory usage to be \\\\\\\\(O(N\\\\cdot T \\\\log T)\\\\\\\\).\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 3597}, page_content='As you can see, the motivation for the Informer model is similar to Longformer ([Beltagy et el., 2020](https://arxiv.org/abs/2004.05150)), Sparse Transformer ([Child et al., 2019](https://arxiv.org/abs/1904.10509)) and other NLP papers for reducing the quadratic complexity of the self-attention mechanism **when the input sequence is long**. Now, let\\'s dive into _ProbSparse_ attention and the _Distilling_ operation with code examples. \\n\\n### ProbSparse Attention\\n\\nThe main idea of ProbSparse is that the canonical self-attention scores form a long-tail distribution, where the \"active\" queries lie in the \"head\" scores and \"lazy\" queries lie in the \"tail\" area. By \"active\" query we mean a query \\\\\\\\(q_i\\\\\\\\) such that the dot-product \\\\\\\\(\\\\langle q_i,k_i \\\\rangle\\\\\\\\) **contributes** to the major attention, whereas a \"lazy\" query forms a dot-product which generates **trivial** attention. Here, \\\\\\\\(q_i\\\\\\\\) and \\\\\\\\(k_i\\\\\\\\) are the \\\\\\\\(i\\\\\\\\)-th rows in \\\\\\\\(Q\\\\\\\\) and \\\\\\\\(K\\\\\\\\) attention matrices respectively.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 4595}, page_content='| ![informer_full_vs_sparse_attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/informer_full_vs_sparse_attention.png) |\\n|:--:|\\n| Vanilla self attention vs ProbSparse attention from [Autoformer (Wu, Haixu, et al., 2021)](https://wuhaixu2016.github.io/pdf/NeurIPS2021_Autoformer.pdf) |\\n\\nGiven the idea of \"active\" and \"lazy\" queries, the ProbSparse attention selects the \"active\" queries, and creates a reduced query matrix \\\\\\\\(Q_{reduced}\\\\\\\\) which is used to calculate the attention weights in \\\\\\\\(O(T \\\\log T)\\\\\\\\). Let\\'s see this more in detail with a code example. \\n    \\nRecall the canonical self-attention formula:\\n\\n$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 5261}, page_content='$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nWhere \\\\\\\\(Q\\\\in \\\\mathbb{R}^{L_Q \\\\times d}\\\\\\\\), \\\\\\\\(K\\\\in \\\\mathbb{R}^{L_K \\\\times d}\\\\\\\\) and \\\\\\\\(V\\\\in \\\\mathbb{R}^{L_V \\\\times d}\\\\\\\\). Note that in practice, the input length of queries and keys are typically equivalent in the self-attention computation, i.e. \\\\\\\\(L_Q = L_K = T\\\\\\\\) where \\\\\\\\(T\\\\\\\\) is the time series length. Therefore, the \\\\\\\\(QK^T\\\\\\\\) multiplication takes \\\\\\\\(O(T^2 \\\\cdot d)\\\\\\\\) computational complexity. In ProbSparse attention, our goal is to create a new \\\\\\\\(Q_{reduce}\\\\\\\\) matrix and define:\\n\\n$$\\n\\\\textrm{ProbSparseAttention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{Q_{reduce}K^T}{\\\\sqrt{d_k}} )V\\n$$'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 5835}, page_content='$$\\n\\\\textrm{ProbSparseAttention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{Q_{reduce}K^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nwhere the \\\\\\\\(Q_{reduce}\\\\\\\\) matrix only selects the Top  \\\\\\\\(u\\\\\\\\) \"active\" queries. Here, \\\\\\\\(u = c \\\\cdot \\\\log L_Q\\\\\\\\) and \\\\\\\\(c\\\\\\\\) called the _sampling factor_ hyperparameter for the ProbSparse attention. Since \\\\\\\\(Q_{reduce}\\\\\\\\) selects only the Top \\\\\\\\(u\\\\\\\\) queries, its size is \\\\\\\\(c\\\\cdot \\\\log L_Q \\\\times d\\\\\\\\), so the multiplication \\\\\\\\(Q_{reduce}K^T\\\\\\\\) takes only \\\\\\\\(O(L_K \\\\log L_Q) = O(T \\\\log T)\\\\\\\\).\\n\\nThis is good! But how can we select the \\\\\\\\(u\\\\\\\\) \"active\" queries to create \\\\\\\\(Q_{reduce}\\\\\\\\)? Let\\'s define the _Query Sparsity Measurement_.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 6473}, page_content='#### Query Sparsity Measurement\\nQuery Sparsity Measurement \\\\\\\\(M(q_i, K)\\\\\\\\) is used for selecting the \\\\\\\\(u\\\\\\\\) \"active\" queries \\\\\\\\(q_i\\\\\\\\) in \\\\\\\\(Q\\\\\\\\) to create \\\\\\\\(Q_{reduce}\\\\\\\\). In theory, the dominant \\\\\\\\(\\\\langle q_i,k_i \\\\rangle\\\\\\\\) pairs encourage the \"active\" \\\\\\\\(q_i\\\\\\\\)\\'s probability distribution **away** from the uniform distribution as can be seen in the figure below. Hence, the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the actual queries distribution and the uniform distribution is used to define the sparsity measurement. \\n\\n| ![informer_probsparse](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/informer_probsparse.png) | \\n|:--:|\\n| The illustration of ProbSparse Attention from official [repository](https://github.com/zhouhaoyi/Informer2020)|\\n\\n\\nIn practice, the measurement is defined as:\\n\\n$$\\nM(q_i, K) = \\\\max_j \\\\frac{q_ik_j^T}{\\\\sqrt{d}}-\\\\frac{1}{L_k} \\\\sum_{j=1}^{L_k}\\\\frac{q_ik_j^T}{\\\\sqrt{d}}\\n$$'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 7474}, page_content='The important thing to understand here is when \\\\\\\\(M(q_i, K)\\\\\\\\) is larger, the query \\\\\\\\(q_i\\\\\\\\) should be in \\\\\\\\(Q_{reduce}\\\\\\\\) and vice versa.\\n\\nBut how can we calculate the term \\\\\\\\(q_ik_j^T\\\\\\\\) in non-quadratic time? Recall that most of the dot-product \\\\\\\\(\\\\langle q_i,k_i \\\\rangle\\\\\\\\) generate either way the trivial attention (i.e. long-tail distribution property), so it is enough to randomly sample a subset of keys from \\\\\\\\(K\\\\\\\\), which will be called `K_sample` in the code.\\n\\nNow, we are ready to see the code of `probsparse_attention`:\\n    \\n```python\\nfrom torch import nn\\nimport math\\n\\n\\ndef probsparse_attention(query_states, key_states, value_states, sampling_factor=5):\\n    \"\"\"\\n    Compute the probsparse self-attention.\\n    Input shape: Batch x Time x Channel'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 8239}, page_content='Note the additional `sampling_factor` input.\\n    \"\"\"\\n    # get input sizes with logs\\n    L_K = key_states.size(1)\\n    L_Q = query_states.size(1)\\n    log_L_K = np.ceil(np.log1p(L_K)).astype(\"int\").item()\\n    log_L_Q = np.ceil(np.log1p(L_Q)).astype(\"int\").item()\\n\\n    # calculate a subset of samples to slice from K and create Q_K_sample\\n    U_part = min(sampling_factor * L_Q * log_L_K, L_K)\\n\\n    # create Q_K_sample (the q_i * k_j^T term in the sparsity measurement)\\n    index_sample = torch.randint(0, L_K, (U_part,))\\n    K_sample = key_states[:, index_sample, :]\\n    Q_K_sample = torch.bmm(query_states, K_sample.transpose(1, 2))\\n\\n    # calculate the query sparsity measurement with Q_K_sample\\n    M = Q_K_sample.max(dim=-1)[0] - torch.div(Q_K_sample.sum(dim=-1), L_K)\\n\\n    # calculate u to find the Top-u queries under the sparsity measurement\\n    u = min(sampling_factor * log_L_Q, L_Q)\\n    M_top = M.topk(u, sorted=False)[1]'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 9174}, page_content='# calculate Q_reduce as query_states[:, M_top]\\n    dim_for_slice = torch.arange(query_states.size(0)).unsqueeze(-1)\\n    Q_reduce = query_states[dim_for_slice, M_top]  # size: c*log_L_Q x channel\\n\\n    # and now, same as the canonical\\n    d_k = query_states.size(-1)\\n    attn_scores = torch.bmm(Q_reduce, key_states.transpose(-2, -1))  # Q_reduce x K^T\\n    attn_scores = attn_scores / math.sqrt(d_k)\\n    attn_probs = nn.functional.softmax(attn_scores, dim=-1)\\n    attn_output = torch.bmm(attn_probs, value_states)\\n\\n    return attn_output, attn_scores'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 9723}, page_content='```\\nNote that in the implementation, \\\\\\\\(U_{part}\\\\\\\\) contain \\\\\\\\(L_Q\\\\\\\\) in the calculation, for stability issues (see [this disccusion](https://discuss.huggingface.co/t/probsparse-attention-in-informer/34428) for more information).\\n\\nWe did it! Please be aware that this is only a partial implementation of the `probsparse_attention`, and the full implementation can be found in 🤗 Transformers.\\n\\n### Distilling\\n\\nBecause of the ProbSparse self-attention, the encoder’s feature map has some redundancy that can be removed. Therefore,\\nthe distilling operation is used to reduce the input size between encoder layers into its half slice, thus in theory removing this redundancy. In practice, Informer\\'s \"distilling\" operation just adds 1D convolution layers with max pooling between each of the encoder layers. Let \\\\\\\\(X_n\\\\\\\\) be the output of the \\\\\\\\(n\\\\\\\\)-th encoder layer, the distilling operation is then defined as:\\n\\n\\n$$\\nX_{n+1} = \\\\textrm{MaxPool} ( \\\\textrm{ELU}(\\\\textrm{Conv1d}(X_n))\\n$$'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 10635}, page_content=\"$$\\nX_{n+1} = \\\\textrm{MaxPool} ( \\\\textrm{ELU}(\\\\textrm{Conv1d}(X_n))\\n$$\\n\\n\\nLet's see this in code:\\n    \\n```python\\nfrom torch import nn\\n\\n# ConvLayer is a class with forward pass applying ELU and MaxPool1d\\ndef informer_encoder_forward(x_input, num_encoder_layers=3, distil=True):\\n    # Initialize the convolution layers\\n    if distil:\\n        conv_layers = nn.ModuleList([ConvLayer() for _ in range(num_encoder_layers - 1)])\\n        conv_layers.append(None)\\n    else:\\n        conv_layers = [None] * num_encoder_layers\\n    \\n    # Apply conv_layer between each encoder_layer\\n    for encoder_layer, conv_layer in zip(encoder_layers, conv_layers):\\n        output = encoder_layer(x_input)\\n        if conv_layer is not None:\\n            output = conv_layer(loutput)\\n    \\n    return output\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 11413}, page_content=\"```\\n    \\nBy reducing the input of each layer by two, we get a memory usage of \\\\\\\\(O(N\\\\cdot T \\\\log T)\\\\\\\\) instead of \\\\\\\\(O(N\\\\cdot T^2)\\\\\\\\) where \\\\\\\\(N\\\\\\\\) is the number of encoder/decoder layers. This is what we wanted!\\n    \\nThe Informer model in [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in the 🤗 Transformers library, and simply called `InformerModel`. In the sections below, we will show how to train this model on a custom multivariate time-series dataset.\\n\\n\\n## Set-up Environment\\n\\nFirst, let's install the necessary libraries: 🤗 Transformers, 🤗 Datasets, 🤗 Evaluate, 🤗 Accelerate and [GluonTS](https://github.com/awslabs/gluonts).\\n\\nAs we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches.\\n\\n\\n```python\\n!pip install -q transformers datasets evaluate accelerate gluonts ujson\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 12328}, page_content='```\\n\\n## Load Dataset\\n\\nIn this blog post, we\\'ll use the `traffic_hourly` dataset, which is available on the [Hugging Face Hub](https://huggingface.co/datasets/monash_tsf). This dataset contains the San Francisco Traffic dataset used by [Lai et al. (2017)](https://arxiv.org/abs/1703.07015). It contains 862 hourly time series showing the road occupancy rates in the range \\\\\\\\([0, 1]\\\\\\\\) on the San Francisco Bay area freeways from 2015 to 2016.\\n\\nThis dataset is part of the [Monash Time Series Forecasting](https://forecastingdata.org/) repository, a collection of time series datasets from a number of domains. It can be viewed as the [GLUE benchmark](https://gluebenchmark.com/) of time series forecasting.\\n\\n\\n```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"monash_tsf\", \"traffic_hourly\")'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 13136}, page_content='```\\n\\nAs can be seen, the dataset contains 3 splits: train, validation and test.\\n\\n\\n```python\\ndataset\\n\\n>>> DatasetDict({\\n        train: Dataset({\\n            features: [\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'],\\n            num_rows: 862\\n        })\\n        test: Dataset({\\n            features: [\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'],\\n            num_rows: 862\\n        })\\n        validation: Dataset({\\n            features: [\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'],\\n            num_rows: 862\\n        })\\n    })\\n```\\n\\nEach example contains a few keys, of which `start` and `target` are the most important ones. Let us have a look at the first time series in the dataset:\\n\\n\\n```python\\ntrain_example = dataset[\"train\"][0]\\ntrain_example.keys()\\n\\n>>> dict_keys([\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'])'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 14048}, page_content='```\\n\\nThe `start` simply indicates the start of the time series (as a datetime), and the `target` contains the actual values of the time series.\\n\\nThe `start` will be useful to add time related features to the time series values, as extra input to the model (such as \"month of year\"). Since we know the frequency of the data is `hourly`, we know for instance that the second value has the timestamp `2015-01-01 01:00:01`, `2015-01-01 02:00:01`, etc.\\n\\n\\n```python\\nprint(train_example[\"start\"])\\nprint(len(train_example[\"target\"]))\\n\\n>>> 2015-01-01 00:00:01\\n    17448'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 14609}, page_content='```\\n\\nThe validation set contains the same data as the training set, just for a `prediction_length` longer amount of time. This allows us to validate the model\\'s predictions against the ground truth.\\n\\nThe test set is again one `prediction_length` longer data compared to the validation set (or some multiple of `prediction_length` longer data compared to the training set for testing on multiple rolling windows).\\n\\n\\n```python\\nvalidation_example = dataset[\"validation\"][0]\\nvalidation_example.keys()\\n\\n>>> dict_keys([\\'start\\', \\'target\\', \\'feat_static_cat\\', \\'feat_dynamic_real\\', \\'item_id\\'])\\n```\\n\\nThe initial values are exactly the same as the corresponding training example. However, this example has `prediction_length=48` (48 hours, or 2 days) additional values compared to the training example. Let us verify it.\\n\\n\\n```python\\nfreq = \"1H\"\\nprediction_length = 48\\n\\nassert len(train_example[\"target\"]) + prediction_length == len(\\n    dataset[\"validation\"][0][\"target\"]\\n)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 15571}, page_content='```\\n\\nLet\\'s visualize this:\\n\\n\\n```python\\nimport matplotlib.pyplot as plt\\n\\nnum_of_samples = 150\\n\\nfigure, axes = plt.subplots()\\naxes.plot(train_example[\"target\"][-num_of_samples:], color=\"blue\")\\naxes.plot(\\n    validation_example[\"target\"][-num_of_samples - prediction_length :],\\n    color=\"red\",\\n    alpha=0.5,\\n)\\n\\nplt.show()\\n```\\n    \\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_22_0.png)\\n    \\n\\nLet\\'s split up the data:\\n\\n\\n```python\\ntrain_dataset = dataset[\"train\"]\\ntest_dataset = dataset[\"test\"]'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 16124}, page_content='```\\n\\n## Update `start` to `pd.Period`\\n\\nThe first thing we\\'ll do is convert the `start` feature of each time series to a pandas `Period` index using the data\\'s `freq`:\\n\\n\\n```python\\nfrom functools import lru_cache\\n\\nimport pandas as pd\\nimport numpy as np\\n\\n\\n@lru_cache(10_000)\\ndef convert_to_pandas_period(date, freq):\\n    return pd.Period(date, freq)\\n\\n\\ndef transform_start_field(batch, freq):\\n    batch[\"start\"] = [convert_to_pandas_period(date, freq) for date in batch[\"start\"]]\\n    return batch\\n```\\n\\nWe now use `datasets`\\' [`set_transform`](https://huggingface.co/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.set_transform) functionality to do this on-the-fly in place:\\n\\n\\n```python\\nfrom functools import partial\\n\\ntrain_dataset.set_transform(partial(transform_start_field, freq=freq))\\ntest_dataset.set_transform(partial(transform_start_field, freq=freq))'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 17003}, page_content=\"```\\n\\nNow, let's convert the dataset into a multivariate time series using the `MultivariateGrouper` from GluonTS. This grouper will convert the individual 1-dimensional time series into a single 2D matrix.\\n\\n\\n```python\\nfrom gluonts.dataset.multivariate_grouper import MultivariateGrouper\\n\\nnum_of_variates = len(train_dataset)\\n\\ntrain_grouper = MultivariateGrouper(max_target_dim=num_of_variates)\\ntest_grouper = MultivariateGrouper(\\n    max_target_dim=num_of_variates,\\n    num_test_dates=len(test_dataset) // num_of_variates, # number of rolling test windows\\n)\\n\\nmulti_variate_train_dataset = train_grouper(train_dataset)\\nmulti_variate_test_dataset = test_grouper(test_dataset)\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 17677}, page_content='```\\n\\nNote that the target is now 2-dimensional, where the first dimension is the number of variates (number of time series) and the second is the time series values (time dimension): \\n\\n\\n```python\\nmulti_variate_train_example = multi_variate_train_dataset[0]\\nprint(\"multi_variate_train_example[\"target\"].shape =\", multi_variate_train_example[\"target\"].shape)\\n\\n>>> multi_variate_train_example[\"target\"].shape = (862, 17448)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 18098}, page_content=\"```\\n\\n## Define the Model\\n\\nNext, let's instantiate a model. The model will be trained from scratch, hence we won't use the `from_pretrained` method here, but rather randomly initialize the model from a [`config`](https://huggingface.co/docs/transformers/main/en/model_doc/informer#transformers.InformerConfig).\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 18409}, page_content='We specify a couple of additional parameters to the model:\\n- `prediction_length` (in our case, `48` hours): this is the horizon that the decoder of the Informer will learn to predict for;\\n- `context_length`: the model will set the `context_length` (input of the encoder) equal to the `prediction_length`, if no `context_length` is specified;\\n- `lags` for a given frequency: these specify an efficient \"look back\" mechanism, where we concatenate values from the past to the current values as additional features, e.g. for a `Daily` frequency we might consider a look back of `[1, 7, 30, ...]` or for `Minute` data we might consider `[1, 30, 60, 60*24, ...]` etc.;\\n- the number of time features: in our case, this will be `5` as we\\'ll add `HourOfDay`, `DayOfWeek`, ..., and `Age` features (see below).\\n\\nLet us check the default lags provided by GluonTS for the given frequency (\"hourly\"):\\n\\n\\n```python\\nfrom gluonts.time_feature import get_lags_for_frequency'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 19298}, page_content='```python\\nfrom gluonts.time_feature import get_lags_for_frequency\\n\\nlags_sequence = get_lags_for_frequency(freq)\\nprint(lags_sequence)\\n\\n>>> [1, 2, 3, 4, 5, 6, 7, 23, 24, 25, 47, 48, 49, 71, 72, 73, 95, 96, 97, 119, 120, \\n     121, 143, 144, 145, 167, 168, 169, 335, 336, 337, 503, 504, 505, 671, 672, 673, 719, 720, 721]'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 19617}, page_content='```\\n\\nThis means that this would look back up to 721 hours (~30 days) for each time step, as additional features. However, the resulting feature vector would end up being of size `len(lags_sequence)*num_of_variates` which for our case will be 34480! This is not going to work so we will use our own sensible lags.\\n\\nLet us also check the default time features which GluonTS provides us:\\n\\n\\n```python\\nfrom gluonts.time_feature import time_features_from_frequency_str\\n\\ntime_features = time_features_from_frequency_str(freq)\\nprint(time_features)\\n\\n>>> [<function hour_of_day at 0x7f3809539240>, <function day_of_week at 0x7f3809539360>, <function day_of_month at 0x7f3809539480>, <function day_of_year at 0x7f38095395a0>]'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 20332}, page_content='```\\n\\nIn this case, there are four additional features, namely \"hour of day\", \"day of week\", \"day of month\" and \"day of year\". This means that for each time step, we\\'ll add these features as a scalar values. For example, consider the timestamp `2015-01-01 01:00:01`. The four additional features will be:\\n\\n\\n```python\\nfrom pandas.core.arrays.period import period_array\\n\\ntimestamp = pd.Period(\"2015-01-01 01:00:01\", freq=freq)\\ntimestamp_as_index = pd.PeriodIndex(data=period_array([timestamp]))\\nadditional_features = [\\n    (time_feature.__name__, time_feature(timestamp_as_index))\\n    for time_feature in time_features\\n]\\nprint(dict(additional_features))\\n\\n>>> {\\'hour_of_day\\': array([-0.45652174]), \\'day_of_week\\': array([0.]), \\'day_of_month\\': array([-0.5]), \\'day_of_year\\': array([-0.5])}'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 21115}, page_content='```\\n\\nNote that hours and days are encoded as values between `[-0.5, 0.5]` from GluonTS. For more information about `time_features`, please see [this](https://github.com/awslabs/gluonts/blob/dev/src/gluonts/time_feature/_base.py). Besides those 4 features, we\\'ll also add an \"age\" feature as we\\'ll see later on in the data transformations.\\n\\nWe now have everything to define the model:\\n\\n\\n```python\\nfrom transformers import InformerConfig, InformerForPrediction'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 21501}, page_content='```python\\nfrom transformers import InformerConfig, InformerForPrediction\\n\\nconfig = InformerConfig(\\n    # in the multivariate setting, input_size is the number of variates in the time series per time step\\n    input_size=num_of_variates,\\n    # prediction length:\\n    prediction_length=prediction_length,\\n    # context length:\\n    context_length=prediction_length * 2,\\n    # lags value copied from 1 week before:\\n    lags_sequence=[1, 24 * 7],\\n    # we\\'ll add 5 time features (\"hour_of_day\", ..., and \"age\"):\\n    num_time_features=len(time_features) + 1,\\n    \\n    # informer params:\\n    dropout=0.1,\\n    encoder_layers=6,\\n    decoder_layers=4,\\n    # project input from num_of_variates*len(lags_sequence)+num_time_features to:\\n    d_model=64,\\n)\\n\\nmodel = InformerForPrediction(config)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 22281}, page_content=\"```\\n\\nBy default, the model uses a diagonal Student-t distribution (but this is [configurable](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils)):\\n\\n\\n```python\\nmodel.config.distribution_output\\n\\n>>> 'student_t'\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 22516}, page_content=\"```\\n\\n## Define Transformations\\n\\nNext, we define the transformations for the data, in particular for the creation of the time features (based on the dataset or universal ones).\\n\\nAgain, we'll use the GluonTS library for this. We define a `Chain` of transformations (which is a bit comparable to `torchvision.transforms.Compose` for images). It allows us to combine several transformations into a single pipeline.\\n\\n\\n```python\\nfrom gluonts.time_feature import TimeFeature\\nfrom gluonts.dataset.field_names import FieldName\\nfrom gluonts.transform import (\\n    AddAgeFeature,\\n    AddObservedValuesIndicator,\\n    AddTimeFeatures,\\n    AsNumpyArray,\\n    Chain,\\n    ExpectedNumInstanceSampler,\\n    InstanceSplitter,\\n    RemoveFields,\\n    SelectFields,\\n    SetField,\\n    TestSplitSampler,\\n    Transformation,\\n    ValidationSplitSampler,\\n    VstackFeatures,\\n    RenameFields,\\n)\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 23381}, page_content='```\\n\\nThe transformations below are annotated with comments, to explain what they do. At a high level, we will iterate over the individual time series of our dataset and add/remove fields or features:\\n\\n\\n```python\\nfrom transformers import PretrainedConfig\\n\\n\\ndef create_transformation(freq: str, config: PretrainedConfig) -> Transformation:\\n    # create list of fields to remove later\\n    remove_field_names = []\\n    if config.num_static_real_features == 0:\\n        remove_field_names.append(FieldName.FEAT_STATIC_REAL)\\n    if config.num_dynamic_real_features == 0:\\n        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)\\n    if config.num_static_categorical_features == 0:\\n        remove_field_names.append(FieldName.FEAT_STATIC_CAT)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 24125}, page_content='return Chain(\\n        # step 1: remove static/dynamic fields if not specified\\n        [RemoveFields(field_names=remove_field_names)]\\n        # step 2: convert the data to NumPy (potentially not needed)\\n        + (\\n            [\\n                AsNumpyArray(\\n                    field=FieldName.FEAT_STATIC_CAT,\\n                    expected_ndim=1,\\n                    dtype=int,\\n                )\\n            ]\\n            if config.num_static_categorical_features > 0\\n            else []\\n        )\\n        + (\\n            [\\n                AsNumpyArray(\\n                    field=FieldName.FEAT_STATIC_REAL,\\n                    expected_ndim=1,\\n                )\\n            ]\\n            if config.num_static_real_features > 0\\n            else []\\n        )\\n        + [\\n            AsNumpyArray(\\n                field=FieldName.TARGET,\\n                # we expect an extra dim for the multivariate case:\\n                expected_ndim=1 if config.input_size == 1 else 2,\\n            ),'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 25046}, page_content=\"expected_ndim=1 if config.input_size == 1 else 2,\\n            ),\\n            # step 3: handle the NaN's by filling in the target with zero\\n            # and return the mask (which is in the observed values)\\n            # true for observed values, false for nan's\\n            # the decoder uses this mask (no loss is incurred for unobserved values)\\n            # see loss_weights inside the xxxForPrediction model\\n            AddObservedValuesIndicator(\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName.OBSERVED_VALUES,\\n            ),\\n            # step 4: add temporal features based on freq of the dataset\\n            # these serve as positional encodings\\n            AddTimeFeatures(\\n                start_field=FieldName.START,\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName.FEAT_TIME,\\n                time_features=time_features_from_frequency_str(freq),\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 25927}, page_content='time_features=time_features_from_frequency_str(freq),\\n                pred_length=config.prediction_length,\\n            ),\\n            # step 5: add another temporal feature (just a single number)\\n            # tells the model where in the life the value of the time series is\\n            # sort of running counter\\n            AddAgeFeature(\\n                target_field=FieldName.TARGET,\\n                output_field=FieldName.FEAT_AGE,\\n                pred_length=config.prediction_length,\\n                log_scale=True,\\n            ),\\n            # step 6: vertically stack all the temporal features into the key FEAT_TIME\\n            VstackFeatures(\\n                output_field=FieldName.FEAT_TIME,\\n                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]\\n                + (\\n                    [FieldName.FEAT_DYNAMIC_REAL]\\n                    if config.num_dynamic_real_features > 0\\n                    else []\\n                ),\\n            ),'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 26853}, page_content='else []\\n                ),\\n            ),\\n            # step 7: rename to match HuggingFace names\\n            RenameFields(\\n                mapping={\\n                    FieldName.FEAT_STATIC_CAT: \"static_categorical_features\",\\n                    FieldName.FEAT_STATIC_REAL: \"static_real_features\",\\n                    FieldName.FEAT_TIME: \"time_features\",\\n                    FieldName.TARGET: \"values\",\\n                    FieldName.OBSERVED_VALUES: \"observed_mask\",\\n                }\\n            ),\\n        ]\\n    )'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 27372}, page_content='```\\n\\n## Define `InstanceSplitter`\\n\\nFor training/validation/testing we next create an `InstanceSplitter` which is used to sample windows from the dataset (as, remember, we can\\'t pass the entire history of values to the model due to time- and memory constraints).\\n\\nThe instance splitter samples random `context_length` sized and subsequent `prediction_length` sized windows from the data, and appends a `past_` or `future_` key to any temporal keys in `time_series_fields` for the respective windows. The instance splitter can be configured into three different modes:\\n1. `mode=\"train\"`: Here we sample the context and prediction length windows randomly from the dataset given to it (the training dataset)\\n2. `mode=\"validation\"`: Here we sample the very last context length window and prediction window from the dataset given to it (for the back-testing or validation likelihood calculations)\\n3. `mode=\"test\"`: Here we sample the very last context length window only (for the prediction use case)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 28369}, page_content='```python\\nfrom gluonts.transform.sampler import InstanceSampler\\nfrom typing import Optional\\n\\n\\ndef create_instance_splitter(\\n    config: PretrainedConfig,\\n    mode: str,\\n    train_sampler: Optional[InstanceSampler] = None,\\n    validation_sampler: Optional[InstanceSampler] = None,\\n) -> Transformation:\\n    assert mode in [\"train\", \"validation\", \"test\"]\\n\\n    instance_sampler = {\\n        \"train\": train_sampler\\n        or ExpectedNumInstanceSampler(\\n            num_instances=1.0, min_future=config.prediction_length\\n        ),\\n        \"validation\": validation_sampler\\n        or ValidationSplitSampler(min_future=config.prediction_length),\\n        \"test\": TestSplitSampler(),\\n    }[mode]'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 29061}, page_content='return InstanceSplitter(\\n        target_field=\"values\",\\n        is_pad_field=FieldName.IS_PAD,\\n        start_field=FieldName.START,\\n        forecast_start_field=FieldName.FORECAST_START,\\n        instance_sampler=instance_sampler,\\n        past_length=config.context_length + max(config.lags_sequence),\\n        future_length=config.prediction_length,\\n        time_series_fields=[\"time_features\", \"observed_mask\"],\\n    )'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 29479}, page_content='```\\n\\n## Create DataLoaders\\n\\nNext, it\\'s time to create the DataLoaders, which allow us to have batches of (input, output) pairs - or in other words (`past_values`, `future_values`).\\n\\n\\n```python\\nfrom typing import Iterable\\n\\nimport torch\\nfrom gluonts.itertools import Cached, Cyclic\\nfrom gluonts.dataset.loader import as_stacked_batches\\n\\n\\ndef create_train_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n    num_batches_per_epoch: int,\\n    shuffle_buffer_length: Optional[int] = None,\\n    cache_data: bool = True,\\n    **kwargs,\\n) -> Iterable:\\n    PREDICTION_INPUT_NAMES = [\\n        \"past_time_features\",\\n        \"past_values\",\\n        \"past_observed_mask\",\\n        \"future_time_features\",\\n    ]\\n    if config.num_static_categorical_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\\n\\n    if config.num_static_real_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 30438}, page_content='TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [\\n        \"future_values\",\\n        \"future_observed_mask\",\\n    ]\\n\\n    transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(data, is_train=True)\\n    if cache_data:\\n        transformed_data = Cached(transformed_data)\\n\\n    # we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \"train\")'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 30857}, page_content='# the instance splitter will sample a window of\\n    # context length + lags + prediction length (from all the possible transformed time series, 1 in our case)\\n    # randomly from within the target time series and return an iterator.\\n    stream = Cyclic(transformed_data).stream()\\n    training_instances = instance_splitter.apply(stream)\\n    \\n    return as_stacked_batches(\\n        training_instances,\\n        batch_size=batch_size,\\n        shuffle_buffer_length=shuffle_buffer_length,\\n        field_names=TRAINING_INPUT_NAMES,\\n        output_type=torch.tensor,\\n        num_batches_per_epoch=num_batches_per_epoch,\\n    )'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 31477}, page_content='```\\n\\n\\n```python\\ndef create_backtest_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n    **kwargs,\\n):\\n    PREDICTION_INPUT_NAMES = [\\n        \"past_time_features\",\\n        \"past_values\",\\n        \"past_observed_mask\",\\n        \"future_time_features\",\\n    ]\\n    if config.num_static_categorical_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\\n\\n    if config.num_static_real_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\\n\\n    transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(data)\\n\\n    # we create a Validation Instance splitter which will sample the very last\\n    # context window seen during training only for the encoder.\\n    instance_sampler = create_instance_splitter(config, \"validation\")'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 32319}, page_content='# we apply the transformations in train mode\\n    testing_instances = instance_sampler.apply(transformed_data, is_train=True)\\n    \\n    return as_stacked_batches(\\n        testing_instances,\\n        batch_size=batch_size,\\n        output_type=torch.tensor,\\n        field_names=PREDICTION_INPUT_NAMES,\\n    )\\n\\ndef create_test_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n    batch_size: int,\\n    **kwargs,\\n):\\n    PREDICTION_INPUT_NAMES = [\\n        \"past_time_features\",\\n        \"past_values\",\\n        \"past_observed_mask\",\\n        \"future_time_features\",\\n    ]\\n    if config.num_static_categorical_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_categorical_features\")\\n\\n    if config.num_static_real_features > 0:\\n        PREDICTION_INPUT_NAMES.append(\"static_real_features\")\\n\\n    transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(data, is_train=False)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 33247}, page_content='# We create a test Instance splitter to sample the very last\\n    # context window from the dataset provided.\\n    instance_sampler = create_instance_splitter(config, \"test\")\\n\\n    # We apply the transformations in test mode\\n    testing_instances = instance_sampler.apply(transformed_data, is_train=False)\\n    \\n    return as_stacked_batches(\\n        testing_instances,\\n        batch_size=batch_size,\\n        output_type=torch.tensor,\\n        field_names=PREDICTION_INPUT_NAMES,\\n    )'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 33728}, page_content=\"```\\n\\n\\n```python\\ntrain_dataloader = create_train_dataloader(\\n    config=config,\\n    freq=freq,\\n    data=multi_variate_train_dataset,\\n    batch_size=256,\\n    num_batches_per_epoch=100,\\n    num_workers=2,\\n)\\n\\ntest_dataloader = create_backtest_dataloader(\\n    config=config,\\n    freq=freq,\\n    data=multi_variate_test_dataset,\\n    batch_size=32,\\n)\\n```\\n\\nLet's check the first batch:\\n\\n\\n```python\\nbatch = next(iter(train_dataloader))\\nfor k, v in batch.items():\\n    print(k, v.shape, v.type())\\n\\n>>> past_time_features torch.Size([256, 264, 5]) torch.FloatTensor\\n    past_values torch.Size([256, 264, 862]) torch.FloatTensor\\n    past_observed_mask torch.Size([256, 264, 862]) torch.FloatTensor\\n    future_time_features torch.Size([256, 48, 5]) torch.FloatTensor\\n    future_values torch.Size([256, 48, 862]) torch.FloatTensor\\n    future_observed_mask torch.Size([256, 48, 862]) torch.FloatTensor\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 34613}, page_content=\"```\\n\\nAs can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the case for NLP models), but rather `past_values`, along with `past_observed_mask`, `past_time_features` and `static_real_features`.\\n\\nThe decoder inputs consist of `future_values`, `future_observed_mask` and `future_time_features`. The `future_values` can be seen as the equivalent of `decoder_input_ids` in NLP.\\n\\nWe refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/informer#transformers.InformerModel.forward.past_values) for a detailed explanation for each of them.\\n\\n## Forward Pass\\n\\nLet's perform a single forward pass with the batch we just created:\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 35209}, page_content='## Forward Pass\\n\\nLet\\'s perform a single forward pass with the batch we just created:\\n\\n\\n```python\\n# perform forward pass\\noutputs = model(\\n    past_values=batch[\"past_values\"],\\n    past_time_features=batch[\"past_time_features\"],\\n    past_observed_mask=batch[\"past_observed_mask\"],\\n    static_categorical_features=batch[\"static_categorical_features\"]\\n    if config.num_static_categorical_features > 0\\n    else None,\\n    static_real_features=batch[\"static_real_features\"]\\n    if config.num_static_real_features > 0\\n    else None,\\n    future_values=batch[\"future_values\"],\\n    future_time_features=batch[\"future_time_features\"],\\n    future_observed_mask=batch[\"future_observed_mask\"],\\n    output_hidden_states=True,\\n)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 35922}, page_content='```\\n\\n\\n```python\\nprint(\"Loss:\", outputs.loss.item())\\n\\n>>> Loss: -1071.5718994140625'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 36005}, page_content=\"```\\n\\nNote that the model is returning a loss. This is possible as the decoder automatically shifts the `future_values` one position to the right in order to have the labels. This allows computing a loss between the predicted values and the labels. The loss is the negative log-likelihood of the predicted distribution with respect to the ground truth values and tends to negative infinity.\\n\\nAlso note that the decoder uses a causal mask to not look into the future as the values it needs to predict are in the `future_values` tensor.\\n\\n## Train the Model\\n\\nIt's time to train the model! We'll use a standard PyTorch training loop.\\n\\nWe will use the 🤗 [Accelerate](https://huggingface.co/docs/accelerate/index) library here, which automatically places the model, optimizer and dataloader on the appropriate `device`.\\n\\n\\n```python\\nfrom accelerate import Accelerator\\nfrom torch.optim import AdamW\\n\\nepochs = 25\\nloss_history = []\\n\\naccelerator = Accelerator()\\ndevice = accelerator.device\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 36896}, page_content='epochs = 25\\nloss_history = []\\n\\naccelerator = Accelerator()\\ndevice = accelerator.device\\n\\nmodel.to(device)\\noptimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\\n\\nmodel, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_dataloader,\\n)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 37196}, page_content='model.train()\\nfor epoch in range(epochs):\\n    for idx, batch in enumerate(train_dataloader):\\n        optimizer.zero_grad()\\n        outputs = model(\\n            static_categorical_features=batch[\"static_categorical_features\"].to(device)\\n            if config.num_static_categorical_features > 0\\n            else None,\\n            static_real_features=batch[\"static_real_features\"].to(device)\\n            if config.num_static_real_features > 0\\n            else None,\\n            past_time_features=batch[\"past_time_features\"].to(device),\\n            past_values=batch[\"past_values\"].to(device),\\n            future_time_features=batch[\"future_time_features\"].to(device),\\n            future_values=batch[\"future_values\"].to(device),\\n            past_observed_mask=batch[\"past_observed_mask\"].to(device),\\n            future_observed_mask=batch[\"future_observed_mask\"].to(device),\\n        )\\n        loss = outputs.loss'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 38118}, page_content='# Backpropagation\\n        accelerator.backward(loss)\\n        optimizer.step()\\n\\n        loss_history.append(loss.item())\\n        if idx % 100 == 0:\\n            print(loss.item())\\n\\n>>> -1081.978515625\\n    ...\\n    -2877.723876953125'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 38348}, page_content='```\\n\\n```python\\n# view training\\nloss_history = np.array(loss_history).reshape(-1)\\nx = range(loss_history.shape[0])\\nplt.figure(figsize=(10, 5))\\nplt.plot(x, loss_history, label=\"train\")\\nplt.title(\"Loss\", fontsize=15)\\nplt.legend(loc=\"upper right\")\\nplt.xlabel(\"iteration\")\\nplt.ylabel(\"nll\")\\nplt.show()'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 38645}, page_content=\"```\\n\\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_62_0.png)\\n    \\n\\n## Inference\\n\\nAt inference time, it's recommended to use the `generate()` method for autoregressive generation, similar to NLP models.\\n\\nForecasting involves getting data from the test instance sampler, which will sample the very last `context_length` sized window of values from each time series in the dataset, and pass it to the model. Note that we pass `future_time_features`, which are known ahead of time, to the decoder.\\n\\nThe model will autoregressively sample a certain number of values from the predicted distribution and pass them back to the decoder to return the prediction outputs:\\n\\n\\n```python\\nmodel.eval()\\n\\nforecasts_ = []\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 39368}, page_content='```python\\nmodel.eval()\\n\\nforecasts_ = []\\n\\nfor batch in test_dataloader:\\n    outputs = model.generate(\\n        static_categorical_features=batch[\"static_categorical_features\"].to(device)\\n        if config.num_static_categorical_features > 0\\n        else None,\\n        static_real_features=batch[\"static_real_features\"].to(device)\\n        if config.num_static_real_features > 0\\n        else None,\\n        past_time_features=batch[\"past_time_features\"].to(device),\\n        past_values=batch[\"past_values\"].to(device),\\n        future_time_features=batch[\"future_time_features\"].to(device),\\n        past_observed_mask=batch[\"past_observed_mask\"].to(device),\\n    )\\n    forecasts_.append(outputs.sequences.cpu().numpy())'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 40081}, page_content=\"```\\n\\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `input_size`). \\n\\nIn this case, we get `100` possible values for the next `48` hours for each of the `862` time series (for each example in the batch which is of size `1` since we only have a single multivariate time series):\\n\\n\\n```python\\nforecasts_[0].shape\\n\\n>>> (1, 100, 48, 862)\\n```\\n\\nWe'll stack them vertically, to get forecasts for all time-series in the test dataset (just in case there are more time series in the test set):\\n\\n\\n```python\\nforecasts = np.vstack(forecasts_)\\nprint(forecasts.shape)\\n\\n>>> (1, 100, 48, 862)\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 40701}, page_content='```\\n\\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values present in the test set. For that, we\\'ll use the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library, which includes the [MASE](https://huggingface.co/spaces/evaluate-metric/mase) and [sMAPE](https://huggingface.co/spaces/evaluate-metric/smape) metrics.\\n\\nWe calculate both metrics for each time series variate in the dataset:\\n\\n\\n```python\\nfrom evaluate import load\\nfrom gluonts.time_feature import get_seasonality\\n\\nmase_metric = load(\"evaluate-metric/mase\")\\nsmape_metric = load(\"evaluate-metric/smape\")\\n\\nforecast_median = np.median(forecasts, 1).squeeze(0).T\\n\\nmase_metrics = []\\nsmape_metrics = []'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 41315}, page_content='forecast_median = np.median(forecasts, 1).squeeze(0).T\\n\\nmase_metrics = []\\nsmape_metrics = []\\n\\nfor item_id, ts in enumerate(test_dataset):\\n    training_data = ts[\"target\"][:-prediction_length]\\n    ground_truth = ts[\"target\"][-prediction_length:]\\n    mase = mase_metric.compute(\\n        predictions=forecast_median[item_id],\\n        references=np.array(ground_truth),\\n        training=np.array(training_data),\\n        periodicity=get_seasonality(freq),\\n    )\\n    mase_metrics.append(mase[\"mase\"])\\n\\n    smape = smape_metric.compute(\\n        predictions=forecast_median[item_id],\\n        references=np.array(ground_truth),\\n    )\\n    smape_metrics.append(smape[\"smape\"])'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 41981}, page_content='```\\n\\n\\n```python\\nprint(f\"MASE: {np.mean(mase_metrics)}\")\\n\\n>>> MASE: 1.1913437728068093\\n\\nprint(f\"sMAPE: {np.mean(smape_metrics)}\")\\n\\n>>> sMAPE: 0.5322665081607634\\n```\\n\\n\\n```python\\nplt.scatter(mase_metrics, smape_metrics, alpha=0.2)\\nplt.xlabel(\"MASE\")\\nplt.ylabel(\"sMAPE\")\\nplt.show()'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 42259}, page_content='```\\n\\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_73_0.png)\\n    \\nTo plot the prediction for any time series variate with respect the ground truth test data we define the following helper:\\n\\n\\n```python\\nimport matplotlib.dates as mdates\\n\\n\\ndef plot(ts_index, mv_index):\\n    fig, ax = plt.subplots()\\n\\n    index = pd.period_range(\\n        start=multi_variate_test_dataset[ts_index][FieldName.START],\\n        periods=len(multi_variate_test_dataset[ts_index][FieldName.TARGET]),\\n        freq=multi_variate_test_dataset[ts_index][FieldName.START].freq,\\n    ).to_timestamp()\\n\\n    ax.xaxis.set_minor_locator(mdates.HourLocator())\\n\\n    ax.plot(\\n        index[-2 * prediction_length :],\\n        multi_variate_test_dataset[ts_index][\"target\"][mv_index, -2 * prediction_length :],\\n        label=\"actual\",\\n    )'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 43121}, page_content='ax.plot(\\n        index[-prediction_length:],\\n        forecasts[ts_index, ..., mv_index].mean(axis=0),\\n        label=\"mean\",\\n    )\\n    ax.fill_between(\\n        index[-prediction_length:],\\n        forecasts[ts_index, ..., mv_index].mean(0)\\n        - forecasts[ts_index, ..., mv_index].std(axis=0),\\n        forecasts[ts_index, ..., mv_index].mean(0)\\n        + forecasts[ts_index, ..., mv_index].std(axis=0),\\n        alpha=0.2,\\n        interpolate=True,\\n        label=\"+/- 1-std\",\\n    )\\n    ax.legend()\\n    fig.autofmt_xdate()'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 43644}, page_content='```\\n\\nFor example:\\n\\n\\n```python\\nplot(0, 344)'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 43687}, page_content='```\\n\\n![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/informer/output_77_0.png)\\n    \\n\\n## Conclusion\\n\\nHow do we compare against other models? The [Monash Time Series Repository](https://forecastingdata.org/#results) has a comparison table of test set MASE metrics which we can add to:\\n\\n|Dataset | \\tSES| \\tTheta | \\tTBATS| \\tETS\\t| (DHR-)ARIMA| \\tPR|\\tCatBoost |\\tFFNN\\t| DeepAR | \\tN-BEATS | \\tWaveNet|  Transformer (uni.) | **Informer (mv. our)**| \\n|:------------------:|:-----------------:|:--:|:--:|:--:|:--:|:--:|:--:|:---:|:---:|:--:|:--:|:--:|:--:|\\n|Traffic Hourly | 1.922\\t| 1.922\\t| 2.482 |\\t2.294|\\t2.535|\\t1.281|\\t1.571\\t|0.892|\\t0.825\\t|1.100|\\t1.066\\t| **0.821** | 1.191 |'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/informer.md', 'start_index': 44392}, page_content='As can be seen, and perhaps surprising to some, the multivariate forecasts are typically _worse_ than the univariate ones, the reason being the difficulty in estimating the cross-series correlations/relationships. The additional variance added by the estimates often harms the resulting forecasts or the model learns spurious correlations. We refer to [this paper](https://openreview.net/forum?id=GpW327gxLTF) for further reading. Multivariate models tend to work well when trained on a lot of data.\\n\\nSo the vanilla Transformer still performs best here! In the future, we hope to better benchmark these models in a central place to ease reproducing the results of several papers. Stay tuned for more!\\n\\n## Resources\\n\\nWe recommend to check out the [Informer docs](https://huggingface.co/docs/transformers/main/en/model_doc/informer) and the [example notebook](https://github.com/huggingface/notebooks/blob/main/examples/multivariate_informer.ipynb) linked at the top of this blog post.'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 1}, page_content='SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https://www.paperswithcode.com/method/resnet) that employs [squeeze-and-excitation blocks](https://paperswithcode.com/method/squeeze-and-excitation-block) to enable the network to perform dynamic channel-wise feature recalibration.\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model(\\'seresnet152d\\', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert(\\'RGB\\')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 967}, page_content='```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])\\n```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 1939}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](https://rwightman.github.io/pytorch-image-models/feature_extraction/), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n```python\\nmodel = timm.create_model('seresnet152d', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 2537}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](https://rwightman.github.io/pytorch-image-models/scripts/) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{hu2019squeezeandexcitation,\\n      title={Squeeze-and-Excitation Networks}, \\n      author={Jie Hu and Li Shen and Samuel Albanie and Gang Sun and Enhua Wu},\\n      year={2019},\\n      eprint={1709.01507},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 3190}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 3195}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Networks\\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\\nModels:\\n- Name: seresnet152d\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 20161904304\\n    Parameters: 66840000\\n    File Size: 268144497\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet152d\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 152\\n    Dropout: 0.2\\n    Crop Pct: '0.94'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '256'\\n    Interpolation: bicubic\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 4102}, page_content=\"Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '256'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1206\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet152d_ra2-04464dd2.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 83.74%\\n      Top 5 Accuracy: 96.77%\\n- Name: seresnet50\\n  In Collection: SE ResNet\\n  Metadata:\\n    FLOPs: 5285062320\\n    Parameters: 28090000\\n    File Size: 112621903\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Bottleneck Residual Block\\n    - Convolution\\n    - Global Average Pooling\\n    - Max Pooling\\n    - ReLU\\n    - Residual Block\\n    - Residual Connection\\n    - Softmax\\n    - Squeeze-and-Excitation Block\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/docs/models/se-resnet.md', 'start_index': 4990}, page_content=\"- Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 8x NVIDIA Titan X GPUs\\n    ID: seresnet50\\n    LR: 0.6\\n    Epochs: 100\\n    Layers: 50\\n    Dropout: 0.2\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bicubic\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/a7f95818e44b281137503bcf4b3e3e94d8ffa52f/timm/models/resnet.py#L1180\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet50_ra_224-8efdb4bb.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.26%\\n      Top 5 Accuracy: 95.07%\\n-->\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Kandinsky 2.2\\n\\nKandinsky 2.2 is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Vladimir Arkhipkin](https://github.com/oriBetelgeuse), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey), and [Denis Dimitrov](https://github.com/denndimitrov).'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 994}, page_content=\"The description from it's GitHub page is:\\n\\n*Kandinsky 2.2 brings substantial improvements upon its predecessor, Kandinsky 2.1, by introducing a new, more powerful image encoder - CLIP-ViT-G and the ControlNet support. The switch to CLIP-ViT-G as the image encoder significantly increases the model's capability to generate more aesthetic pictures and better understand text, thus enhancing the model's overall performance. The addition of the ControlNet mechanism allows the model to effectively control the process of generating images. This leads to more accurate and visually appealing outputs and opens new possibilities for text-guided image manipulation.*\\n\\nThe original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).\\n\\n<Tip>\\n\\nCheck out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.\\n\\n</Tip>\\n\\n<Tip>\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 1974}, page_content='</Tip>\\n\\n<Tip>\\n\\nMake sure to check out the schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## KandinskyV22PriorPipeline\\n\\n[[autodoc]] KandinskyV22PriorPipeline\\n\\t- all\\n\\t- __call__\\n\\t- interpolate\\n\\n## KandinskyV22Pipeline\\n\\n[[autodoc]] KandinskyV22Pipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22CombinedPipeline\\n\\n[[autodoc]] KandinskyV22CombinedPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22ControlnetPipeline\\n\\n[[autodoc]] KandinskyV22ControlnetPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22PriorEmb2EmbPipeline\\n\\n[[autodoc]] KandinskyV22PriorEmb2EmbPipeline\\n\\t- all\\n\\t- __call__\\n\\t- interpolate\\n\\n## KandinskyV22Img2ImgPipeline\\n\\n[[autodoc]] KandinskyV22Img2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22Img2ImgCombinedPipeline'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/kandinsky_v22.md', 'start_index': 2873}, page_content='[[autodoc]] KandinskyV22Img2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22Img2ImgCombinedPipeline\\n\\n[[autodoc]] KandinskyV22Img2ImgCombinedPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22ControlnetImg2ImgPipeline\\n\\n[[autodoc]] KandinskyV22ControlnetImg2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22InpaintPipeline\\n\\n[[autodoc]] KandinskyV22InpaintPipeline\\n\\t- all\\n\\t- __call__\\n\\n## KandinskyV22InpaintCombinedPipeline\\n\\n[[autodoc]] KandinskyV22InpaintCombinedPipeline\\n\\t- all\\n\\t- __call__'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 0}, page_content='--\\ntitle: \"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\"\\nthumbnail: blog/assets/156_huggylingo/Huggy_Lingo.png\\nauthors:\\n- user: davanstrien\\n---\\n\\n## Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\n\\n\\n\\n**tl;dr**: We\\'re using machine learning to detect the language of Hub datasets with no language metadata, and [librarian-bots](https://huggingface.co/librarian-bots) to make pull requests to add this metadata. \\n\\nThe Hugging Face Hub has become the repository where the community shares machine learning models, datasets, and applications. As the number of datasets grows, metadata becomes increasingly important as a tool for finding the right resource for your use case.\\n\\nIn this blog post, I\\'m excited to share some early experiments which seek to use machine learning to improve the metadata for datasets hosted on the Hugging Face Hub.\\n\\n### Language Metadata for Datasets on the Hub'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 925}, page_content='### Language Metadata for Datasets on the Hub\\n\\nThere are currently ~50K public datasets on the Hugging Face Hub. Metadata about the language used in a dataset can be specified using a [YAML](https://en.wikipedia.org/wiki/YAML) field at the top of the [dataset card](https://huggingface.co/docs/datasets/upload_dataset#create-a-dataset-card).\\n\\nAll public datasets specify 1,716 unique languages via a language tag in their metadata. Note that some of them will be the result of languages being specified in different ways i.e. `en` vs `eng` vs `english` vs `English`. \\n\\nFor example, the [IMDB dataset](https://huggingface.co/datasets/imdb) specifies `en` in the YAML metadata (indicating English):\\n\\n<p align=\"center\"> \\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_metadata.png\" alt=\"Screenshot of YAML metadata\"><br> \\n<em>Section of the YAML metadata for the IMDB dataset</em> \\n </p>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 1877}, page_content='It is perhaps unsurprising that English is by far the most common language for datasets on the Hub, with around 19% of datasets on the Hub listing their language as `en` (not including any variations of `en`, so the actual percentage is likely much higher).\\n\\n <p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_freq.png\" alt=\"Distribution of language tags\"><br> \\n     <em>The frequency and percentage frequency for datasets on the Hugging Face Hub</em> \\n </p> \\n\\n\\nWhat does the distribution of languages look like if we exclude English? We can see that there is a grouping of a few dominant languages and after that there is a pretty smooth fall in the frequencies at which languages appear.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 2655}, page_content='<p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_freq_distribution.png\" alt=\"Distribution of language tags\"><br> \\n     <em>Distribution of language tags for datasets on the hub excluding English.</em> \\n </p> \\n\\nHowever, there is a major caveat to this. Most datasets (around 87%) do not specify the language used; only approximately 13% of datasets include language information in their metadata.\\n\\n\\n<p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/has_lang_info_bar.png\" alt=\"Barchart\"><br> \\n     <em>The percent of datasets which have language metadata. True indicates language metadata is specified, False means no language data is listed. No card data means that there isn\\'t any metadata or it couldn\\'t be loaded by the `huggingface_hub` Python library.</em> \\n</p> \\n\\n#### Why is Language Metadata Important?'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 3591}, page_content=\"#### Why is Language Metadata Important?\\n\\nLanguage metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. \\n\\nCurrently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. \\n\\nMany people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data.\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 4421}, page_content='If we switch to the task of finding relevant machine learning models, knowing what languages were included in the training data for a model can help us find models for the language we are interested in. This relies on the dataset specifying this information. \\n\\nFinally, knowing what languages are represented on the Hub (and which are not), helps us understand the language biases of the Hub and helps inform community efforts to address gaps in particular languages. \\n\\n### Predicting the Languages of Datasets Using Machine Learning\\n\\nWe’ve already seen that many of the datasets on the Hugging Face Hub haven’t included metadata for the language used. However, since these datasets are already shared openly, perhaps we can look at the dataset and try to identify the language using machine learning.\\n\\n#### Getting the Data \\n\\nOne way we could access some examples from a dataset is by using the datasets library to download the datasets i.e. \\n\\n```python\\nfrom datasets import load_dataset'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 5366}, page_content='```python\\nfrom datasets import load_dataset\\n\\ndataset = load_dataset(\"biglam/on_the_books\")'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 5457}, page_content='```\\n\\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole dataset. We could instead try to load a sample of the dataset. However, depending on how the dataset was created, we might still end up downloading more data than we’d need onto the machine we’re working on. \\n\\nLuckily, many datasets on the Hub are available via the [datasets server](https://huggingface.co/docs/datasets-server/index). The datasets server is an API that allows us to access datasets hosted on the Hub without downloading the dataset locally. The Datasets Server powers the Datasets Viewer preview you will see for many datasets hosted on the Hub.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 6117}, page_content='For this first experiment with predicting language for datasets, we define a list of column names and data types likely to contain textual content i.e. `text` or `prompt` column names and `string` features are likely to be relevant `image` is not. This means we can avoid predicting the language for datasets where language information is less relevant, for example, image classification datasets. We use the Datasets Server to get 20 rows of text data to pass to a machine learning model (we could modify this to take more or fewer examples from the dataset). \\n\\nThis approach means that for the majority of datasets on the Hub we can quickly request the contents of likely text columns for the first 20 rows in a dataset. \\n\\n#### Predicting the Language of a Dataset'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 6842}, page_content='#### Predicting the Language of a Dataset \\n\\nOnce we have some examples of text from a dataset, we need to predict the language. There are various options here, but for this work, we used the [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model created by [Meta](https://huggingface.co/facebook) as part of the [No Language Left Behind](https://ai.facebook.com/research/no-language-left-behind/) work. This model can detect 217 languages which will likely represent the majority of languages for datasets hosted on the Hub. \\n\\nWe pass 20 examples to the model representing rows from a dataset. This results in 20 individual language predictions (one per row) for each dataset.  \\n\\nOnce we have these predictions, we do some additional filtering to determine if we will accept the predictions as a metadata suggestion. This roughly consists of:'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 7756}, page_content='- Grouping the predictions for each dataset by language: some datasets return predictions for multiple languages. We group these predictions by the language predicted i.e. if a dataset returns predictions for English and Dutch, we group the English and Dutch predictions together. \\n- For datasets with multiple languages predicted, we count how many predictions we have for each language. If a language is predicted less than 20% of the time, we discard this prediction. i.e. if we have 18 predictions for English and only 2 for Dutch we discard the Dutch predictions. \\n- We calculate the mean score for all predictions for a language. If the mean score associated with a languages prediction is below 80% we discard this prediction.\\n\\n <p align=\"center\"> \\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/prediction-flow.png\" alt=\"Prediction workflow\"><br> \\n     <em>Diagram showing how predictions are handled.</em> \\n </p>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 8746}, page_content=\"Once we’ve done this filtering, we have a further step of deciding how to use these predictions. The fastText language prediction model returns predictions as an [ISO 639-3](https://en.wikipedia.org/wiki/ISO_639-3) code (an international standard for language codes) along with a script type. i.e. `kor_Hang` is the ISO 693-3 language code for Korean (kor) + Hangul script (Hang) a [ISO 15924](https://en.wikipedia.org/wiki/ISO_15924) code representing the script of a language.\\n\\nWe discard the script information since this isn't currently captured consistently as metadata on the Hub and, where possible, we convert the language prediction returned by the model from [ISO 639-3](https://en.wikipedia.org/wiki/ISO_639-3) to [ISO 639-1](https://en.wikipedia.org/wiki/ISO_639-1) language codes. This is largely done because these language codes have better support in the Hub UI for navigating datasets.\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 9651}, page_content=\"For some ISO 639-3 codes, there is no ISO 639-1 equivalent. For these cases we manually specify a mapping if we deem it to make sense, for example Standard Arabic (`arb`) is mapped to Arabic (`ar`). Where an obvious mapping is not possible, we currently don't suggest metadata for this dataset. In future iterations of this work we may take a different approach. It is important to recognise this approach does come with downsides, since it reduces the diversity of languages which might be suggested and also relies on subjective judgments about what languages can be mapped to others. \\n\\nBut the process doesn't stop here. After all, what use is predicting the language of the datasets if we can't share that information with the rest of the community?\\n\\n### Using Librarian-Bot to Update Metadata\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 10406}, page_content=\"### Using Librarian-Bot to Update Metadata\\n\\nTo ensure this valuable language metadata is incorporated back into the Hub, we turn to Librarian-Bot! Librarian-Bot takes the language predictions generated by Meta's [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model and opens pull requests to add this information to the metadata of each respective dataset. \\n\\nThis system not only updates the datasets with language information, but also does it swiftly and efficiently, without requiring manual work from humans. If the owner of a repo decided to approve and merge the pull request, then the language metadata becomes available for all users, significantly enhancing the usability of the Hugging Face Hub. You can keep track of what the librarian-bot is doing [here](https://huggingface.co/librarian-bot/activity/community)! \\n\\n#### Next Steps\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/huggy-lingo.md', 'start_index': 11305}, page_content=\"#### Next Steps \\n\\nAs the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.\\n\\nWith the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. \\n\\nAs the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md', 'start_index': 0}, page_content='!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Trainer'),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md', 'start_index': 746}, page_content=\"-->\\n\\n# Trainer\\n\\nThe [`Trainer`] class provides an API for feature-complete training in PyTorch, and it supports distributed training on multiple GPUs/TPUs, mixed precision for [NVIDIA GPUs](https://nvidia.github.io/apex/), [AMD GPUs](https://rocm.docs.amd.com/en/latest/rocm.html), and [`torch.amp`](https://pytorch.org/docs/stable/amp.html) for PyTorch. [`Trainer`] goes hand-in-hand with the [`TrainingArguments`] class, which offers a wide range of options to customize how a model is trained. Together, these two classes provide a complete training API.\\n\\n[`Seq2SeqTrainer`] and [`Seq2SeqTrainingArguments`] inherit from the [`Trainer`] and [`TrainingArgument`] classes and they're adapted for training models for sequence-to-sequence tasks such as summarization or translation.\\n\\n<Tip warning={true}>\\n\\nThe [`Trainer`] class is optimized for 🤗 Transformers models and can have surprising behaviors\\nwhen used with other models. When using it with your own model, make sure:\"),\n",
              " Document(metadata={'source': 'huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md', 'start_index': 1722}, page_content='- your model always return tuples or subclasses of [`~utils.ModelOutput`]\\n- your model can compute the loss if a `labels` argument is provided and that loss is returned as the first\\n  element of the tuple (if your model returns tuples)\\n- your model can accept multiple label arguments (use `label_names` in [`TrainingArguments`] to indicate their name to the [`Trainer`]) but none of them should be named `\"label\"`\\n\\n</Tip>\\n\\n## Trainer[[api-reference]]\\n\\n[[autodoc]] Trainer\\n    - all\\n\\n## Seq2SeqTrainer\\n\\n[[autodoc]] Seq2SeqTrainer\\n    - evaluate\\n    - predict\\n\\n## TrainingArguments\\n\\n[[autodoc]] TrainingArguments\\n    - all\\n\\n## Seq2SeqTrainingArguments\\n\\n[[autodoc]] Seq2SeqTrainingArguments\\n    - all'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: upload_button_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 81}, page_content='```\\nimport gradio as gr'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 106}, page_content='with gr.Blocks() as demo:\\n    \\n    with gr.Row():\\n        with gr.Column():\\n            upload_btn = gr.UploadButton(label=\"Upload Single File\", file_count=\"single\")\\n        with gr.Column():\\n            output_file_1 = gr.File(label=\"Upload Single File Output\", file_count=\"single\")\\n            num_load_btn_1 = gr.Number(label=\"# Load Upload Single File\", value=0)\\n            output_click_1 = gr.Number(label=\"# Click Upload Single File Output\", value=0)\\n            upload_btn.upload(lambda s,n: (s, n + 1), [upload_btn, num_load_btn_1], [output_file_1, num_load_btn_1])\\n            upload_btn.click(lambda n: (n + 1), output_click_1, [output_click_1])\\n    with gr.Row():\\n        with gr.Column():\\n            upload_btn_multiple = gr.UploadButton(label=\"Upload Multiple Files\", file_count=\"multiple\")\\n        with gr.Column():\\n            output_file_2 = gr.File(label=\"Upload Multiple Files Output\", file_count=\"multiple\")'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 950}, page_content='output_file_2 = gr.File(label=\"Upload Multiple Files Output\", file_count=\"multiple\")\\n            num_load_btn_2 = gr.Number(label=\"# Load Upload Multiple Files\", value=0)\\n            output_click_2 = gr.Number(label=\"# Click Upload Multiple Files Output\", value=0)\\n            upload_btn_multiple.upload(lambda s,n: (s, n + 1), [upload_btn_multiple, num_load_btn_2], [output_file_2, num_load_btn_2])\\n            upload_btn_multiple.click(lambda n: (n + 1), output_click_2, [output_click_2])'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 1443}, page_content='if __name__ == \"__main__\":\\n    demo.launch()'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/upload_button_component_events/run.ipynb', 'start_index': 1488}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 1}, page_content='Normalization and pre-tokenization[[normalization-and-pre-tokenization]]\\n\\n<CourseFloatingBanner chapter={6}\\n  classNames=\"absolute z-10 right-0 top-0\"\\n  notebooks={[\\n    {label: \"Google Colab\", value: \"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb\"},\\n    {label: \"Aws Studio\", value: \"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb\"},\\n]} />\\n\\nBefore we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we\\'ll first take a look at the preprocessing that each tokenizer applies to text. Here\\'s a high-level overview of the steps in the tokenization pipeline:'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 795}, page_content='<div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg\" alt=\"The tokenization pipeline.\">\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg\" alt=\"The tokenization pipeline.\">\\n</div>\\n\\nBefore splitting a text into subtokens (according to its model), the tokenizer performs two steps: _normalization_ and _pre-tokenization_.\\n\\n## Normalization[[normalization]]\\n\\n<Youtube id=\"4IIC2jI9CaU\"/>\\n\\nThe normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you\\'re familiar with [Unicode normalization](http://www.unicode.org/reports/tr15/) (such as NFC or NFKC), this is also something the tokenizer may apply.'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 1722}, page_content='The 🤗 Transformers `tokenizer` has an attribute called `backend_tokenizer` that provides access to the underlying tokenizer from the 🤗 Tokenizers library:\\n\\n```py\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\nprint(type(tokenizer.backend_tokenizer))'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 2028}, page_content='```\\n\\n```python out\\n<class \\'tokenizers.Tokenizer\\'>\\n```\\n\\nThe `normalizer` attribute of the `tokenizer` object has a `normalize_str()` method that we can use to see how the normalization is performed:\\n\\n```py\\nprint(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\\n```\\n\\n```python out\\n\\'hello how are u?\\''),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 2351}, page_content='```\\n\\nIn this example, since we picked the `bert-base-uncased` checkpoint, the normalization applied lowercasing and removed the accents. \\n\\n<Tip>\\n\\n✏️ **Try it out!** Load a tokenizer from the `bert-base-cased` checkpoint and pass the same example to it. What are the main differences you can see between the cased and uncased versions of the tokenizer?\\n\\n</Tip>\\n\\n## Pre-tokenization[[pre-tokenization]]\\n\\n<Youtube id=\"grlLV8AIXug\"/>\\n\\nAs we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That\\'s where the pre-tokenization step comes in. As we saw in [Chapter 2](/course/chapter2), a word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training.'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 3227}, page_content='To see how a fast tokenizer performs pre-tokenization, we can use the `pre_tokenize_str()` method of the `pre_tokenizer` attribute of the `tokenizer` object:\\n\\n```py\\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 3475}, page_content='```\\n\\n```python out\\n[(\\'Hello\\', (0, 5)), (\\',\\', (5, 6)), (\\'how\\', (7, 10)), (\\'are\\', (11, 14)), (\\'you\\', (16, 19)), (\\'?\\', (19, 20))]\\n```\\n\\nNotice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between `are` and `you` to account for that.\\n\\nSince we\\'re using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer:\\n\\n```py\\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 4243}, page_content='```\\n\\nit will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a `Ġ` symbol, enabling it to recover the original spaces if we decode the tokens:\\n\\n```python out\\n[(\\'Hello\\', (0, 5)), (\\',\\', (5, 6)), (\\'Ġhow\\', (6, 10)), (\\'Ġare\\', (10, 14)), (\\'Ġ\\', (14, 15)), (\\'Ġyou\\', (15, 19)),\\n (\\'?\\', (19, 20))]\\n```\\n\\nAlso note that unlike the BERT tokenizer, this tokenizer does not ignore the double space.\\n\\nFor a last example, let\\'s have a look at the T5 tokenizer, which is based on the SentencePiece algorithm:\\n\\n```py\\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\\ntokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\\n```\\n\\n```python out\\n[(\\'▁Hello,\\', (0, 6)), (\\'▁how\\', (7, 10)), (\\'▁are\\', (11, 14)), (\\'▁you?\\', (16, 20))]'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 5025}, page_content=\"```\\n\\nLike the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (`_`), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before `Hello`) and ignored the double space between `are` and `you`.\\n\\nNow that we've seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We'll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we'll examine how the three main algorithms used for subword tokenization work.\\n\\n## SentencePiece[[sentencepiece]]\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 5671}, page_content=\"## SentencePiece[[sentencepiece]]\\n\\n[SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `▁`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter7/7)), it doesn't even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).\\n\\nThe other main feature of SentencePiece is *reversible tokenization*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_`s with spaces -- this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.\\n\\n## Algorithm overview[[algorithm-overview]]\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 6601}, page_content=\"## Algorithm overview[[algorithm-overview]]\\n\\nIn the following sections, we'll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here's a quick overview of how they each work. Don't hesitate to come back to this table after reading each of the next sections if it doesn't make sense to you yet.\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 7035}, page_content='Model | BPE | WordPiece | Unigram\\n:----:|:---:|:---------:|:------:\\nTraining | Starts from a small vocabulary and learns rules to merge tokens |  Starts from a small vocabulary and learns rules to merge tokens | Starts from a large vocabulary and learns rules to remove tokens\\nTraining step | Merges the tokens corresponding to the most common pair | Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent | Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus\\nLearns | Merge rules and a vocabulary | Just a vocabulary | A vocabulary with a score for each token'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 7750}, page_content='Encoding | Splits a word into characters and applies the merges learned during training | Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word | Finds the most likely split into tokens, using the scores learned during training'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/chapters/en/chapter6/4.mdx', 'start_index': 8050}, page_content=\"Now let's dive into BPE!\"),\n",
              " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/howto/map_pools.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Map pools\\n\\nMap pools allow you to instantiate multiple versions of your environment on the backend, the enables higher \\nthroughput with parallelization of interaction in simulations and embodied environments.\\nUsing map pools is simple with 🤗 Simulate. First define a function that will generate your environment, we call each environment instance a \"map\".'),\n",
              " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/howto/map_pools.mdx', 'start_index': 946}, page_content='```\\ndef generate_map(index):\\n    root = sm.Asset(name=f\"root_{index}\")\\n    root += sm.Box(\\n        name=f\"floor_{index}\",\\n        position=[0, -0.05, 0],\\n        scaling=[10, 0.1, 10],\\n        material=sm.Material.BLUE,\\n        with_collider=True,\\n    )\\n    root += sm.Box(\\n        name=f\"wall1_{index}\",\\n        position=[-1, 0.5, 0],\\n        scaling=[0.1, 1, 5.1],\\n        material=sm.Material.GRAY75,\\n        with_collider=True,\\n    )\\n    root += sm.Box(\\n        name=f\"wall2_{index}\",\\n        position=[1, 0.5, 0],\\n        scaling=[0.1, 1, 5.1],\\n        material=sm.Material.GRAY75,\\n        with_collider=True,\\n    )\\n    root += sm.Box(\\n        name=f\"wall3_{index}\",\\n        position=[0, 0.5, 4.5],\\n        scaling=[5.9, 1, 0.1],\\n        material=sm.Material.GRAY75,\\n        with_collider=True,\\n    )\\n\\n    # add actors, sensors, reward functions etc ...\\n\\n    return root'),\n",
              " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/howto/map_pools.mdx', 'start_index': 1822}, page_content='```\\n\\nYou can then provide the `generate_map` method as an argument to the `sm.ParallelRLEnv` class, which will instantiate `n_maps`. \\nTraining with a subset of the maps is possible using the `n_show` option. At each environment reset, it cycles through to the next map.\\n\\n[[autodoc]] ParallelRLEnv'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 1}, page_content='@gradio/imageeditor\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6809](https://github.com/gradio-app/gradio/pull/6809) [`1401d99`](https://github.com/gradio-app/gradio/commit/1401d99ade46d87da75b5f5808a3354c49f1d1ea) - Fix `ImageEditor` interaction story.  Thanks [@hannahblair](https://github.com/hannahblair)!\\n\\n## 0.1.5\\n\\n### Fixes\\n\\n- [#6799](https://github.com/gradio-app/gradio/pull/6799) [`c352811`](https://github.com/gradio-app/gradio/commit/c352811f76d4126613ece0a584f8c552fdd8d1f6) - Adds docstrings for `gr.WaveformOptions`, `gr.Brush`, and `gr.Eraser`, fixes examples for `ImageEditor`, and allows individual images to be used as the initial `value` for `ImageEditor`.  Thanks [@abidlabs](https://github.com/abidlabs)!\\n\\n## 0.1.4\\n\\n### Patch Changes'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 715}, page_content='## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https://github.com/gradio-app/gradio/commit/5d51fbce7826da840a2fd4940feb5d9ad6f1bc5a), [`34f9431`](https://github.com/gradio-app/gradio/commit/34f943101bf7dd6b8a8974a6131c1ed7c4a0dac0)]:\\n  - @gradio/upload@0.5.4\\n  - @gradio/client@0.9.1\\n  - @gradio/image@0.5.1\\n\\n## 0.1.3\\n\\n### Patch Changes'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 1071}, page_content='- Updated dependencies [[`6a9151d`](https://github.com/gradio-app/gradio/commit/6a9151d5c9432c724098da7d88a539aaaf5ffe88), [`21cfb0a`](https://github.com/gradio-app/gradio/commit/21cfb0acc309bb1a392f4d8a8e42f6be864c5978), [`d76bcaa`](https://github.com/gradio-app/gradio/commit/d76bcaaaf0734aaf49a680f94ea9d4d22a602e70), [`67ddd40`](https://github.com/gradio-app/gradio/commit/67ddd40b4b70d3a37cb1637c33620f8d197dbee0), [`053bec9`](https://github.com/gradio-app/gradio/commit/053bec98be1127e083414024e02cf0bebb0b5142), [`bdf81fe`](https://github.com/gradio-app/gradio/commit/bdf81fead86e1d5a29e6b036f1fff677f6480e6b), [`4d1cbbc`](https://github.com/gradio-app/gradio/commit/4d1cbbcf30833ef1de2d2d2710c7492a379a9a00), [`5177132`](https://github.com/gradio-app/gradio/commit/5177132d718c77f6d47869b4334afae6380394cb)]:\\n  - @gradio/image@0.5.0\\n  - @gradio/upload@0.5.3\\n  - @gradio/client@0.9.0\\n  - @gradio/wasm@0.4.0\\n  - @gradio/icons@0.3.2\\n  - @gradio/atoms@0.4.0\\n  - @gradio/statustracker@0.4.2'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 2066}, page_content='## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b639e04`](https://github.com/gradio-app/gradio/commit/b639e040741e6c0d9104271c81415d7befbd8cf3), [`206af31`](https://github.com/gradio-app/gradio/commit/206af31d7c1a31013364a44e9b40cf8df304ba50)]:\\n  - @gradio/image@0.4.2\\n  - @gradio/icons@0.3.1\\n  - @gradio/atoms@0.3.1\\n  - @gradio/statustracker@0.4.1\\n  - @gradio/upload@0.5.2\\n\\n## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`71f1a1f99`](https://github.com/gradio-app/gradio/commit/71f1a1f9931489d465c2c1302a5c8d768a3cd23a)]:\\n  - @gradio/client@0.8.2\\n  - @gradio/image@0.4.1\\n  - @gradio/upload@0.5.1\\n\\n## 0.1.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))\\n\\nA brand new component, completely separate from `Image` that provides simple editing capabilities.'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 2896}, page_content='A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n- Set background images from file uploads, webcam, or just paste!\\n- Crop images with an improved cropping UI. App authors can event set specific crop size, or crop ratios (`1:1`, etc)\\n- Paint on top of any image (or no image) and erase any mistakes!\\n- The ImageEditor supports layers, confining draw and erase actions to that layer.\\n- More flexible access to data. The image component returns a composite image representing the final state of the canvas as well as providing the background and all layers as individual images.\\n- Fully customisable. All features can be enabled and disabled. Even the brush color swatches can be customised.\\n\\n<video src=\"https://user-images.githubusercontent.com/12937446/284027169-31188926-fd16-4a1c-8718-998e7aae4695.mp4\" autoplay muted></video>\\n\\n```py'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 3777}, page_content='```py\\n\\ndef fn(im):\\n    im[\"composite\"] # the full canvas\\n    im[\"background\"] # the background image\\n    im[\"layers\"] # a list of individual layers\\n\\n\\nim = gr.ImageEditor(\\n    # decide which sources you\\'d like to accept\\n    sources=[\"upload\", \"webcam\", \"clipboard\"],\\n    # set a cropsize constraint, can either be a ratio or a concrete [width, height]\\n    crop_size=\"1:1\",\\n    # enable crop (or disable it)\\n    transforms=[\"crop\"],\\n    # customise the brush\\n    brush=Brush(\\n      default_size=\"25\", # or leave it as \\'auto\\'\\n      color_mode=\"fixed\", # \\'fixed\\' hides the user swatches and colorpicker, \\'defaults\\' shows it\\n      default_color=\"hotpink\", # html names are supported\\n      colors=[\\n        \"rgba(0, 150, 150, 1)\", # rgb(a)\\n        \"#fff\", # hex rgb\\n        \"hsl(360, 120, 120)\" # in fact any valid colorstring\\n      ]\\n    ),\\n    brush=Eraser(default_size=\"25\")\\n)'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/js/imageeditor/CHANGELOG.md', 'start_index': 4652}, page_content='```\\n\\nThanks [@pngwn](https://github.com/pngwn)!\\n\\n### Fixes\\n\\n- [#6502](https://github.com/gradio-app/gradio/pull/6502) [`070f71c93`](https://github.com/gradio-app/gradio/commit/070f71c933d846ce8e2fe11cdd9bc0f3f897f29f) - Ensure image editor crop and draw cursor works as expected when the scroll position changes. Thanks [@pngwn](https://github.com/pngwn)!\\n\\n# @gradio/image'),\n",
              " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': 1}, page_content='How to contribute to simulate?\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg)](CODE_OF_CONDUCT.md)\\n\\nSimulation environments is an open source project, so all contributions and suggestions are welcome.\\n\\nYou can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements, \\nimproving the documentation, fixing bugs,...\\n\\nMany thanks in advance to every contributor.\\n\\nIn order to facilitate healthy, constructive behavior in an open and inclusive community, we all respect and abide by \\nour [code of conduct](CODE_OF_CONDUCT.md).\\n\\n## How to work on an open Issue?\\nYou have the list of open Issues at: https://github.com/huggingface/simulate/issues\\n\\nSome of them may have the label `help wanted`: that means that any contributor is welcomed!\\n\\nIf you would like to work on any of the open Issues:'),\n",
              " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': 834}, page_content=\"If you would like to work on any of the open Issues:\\n\\n1. Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page.\\n\\n2. You can self-assign it by commenting on the Issue page with one of the keywords: `#take` or `#self-assign`.\\n\\n3. Work on your self-assigned issue and eventually create a Pull Request.\\n\\n## How to create a Pull Request?\\n1. Fork the [repository](https://github.com/huggingface/simulate) by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account.\\n\\n2. Clone your fork to your local disk, and add the base repository as a remote:\\n\\n\\t```bash\\n\\tgit clone git@github.com:<your Github handle>/simulate.git\\n\\tcd simulate\\n\\tgit remote add upstream https://github.com/huggingface/simulate.git\"),\n",
              " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': 1670}, page_content='```\\n\\n3. Create a new branch to hold your development changes:\\n\\n\\t```bash\\n\\tgit checkout -b a-descriptive-name-for-my-changes\\n\\t```\\n\\n\\t**do not** work on the `main` branch.\\n\\n4. Set up a development environment by running the following command in a virtual environment:\\n\\n\\t```bash\\n\\tpip install -e \".[dev]\"\\n\\t```\\n\\n   (If simulate was already installed in the virtual environment, remove\\n   it with `pip uninstall simulate` before reinstalling it in editable\\n   mode with the `-e` flag.)\\n\\n5. Develop the features on your branch. If you want to add a dataset see more in-detail instructions in the section [*How to add a dataset*](#how-to-add-a-dataset). \\n\\n6. Format your code. Run black and isort so that your newly added files look nice with the following command:\\n\\n\\t```bash\\n\\tmake style\\n\\t```\\n\\n7. Once you\\'re happy with your dataset script file, add your changes and make a commit to record your changes locally:\\n\\n\\t```bash\\n\\tgit add simulate/<your_dataset_name>\\n\\tgit commit'),\n",
              " Document(metadata={'source': 'huggingface/simulate/blob/main/CONTRIBUTING.md', 'start_index': 2634}, page_content='```\\n\\n\\tIt is a good idea to sync your copy of the code with the original\\n\\trepository regularly. This way you can quickly account for changes:\\n\\n\\t```bash\\n\\tgit fetch upstream\\n\\tgit rebase upstream/main\\n    ```\\n\\n   Push the changes to your account using:\\n\\n   ```bash\\n   git push -u origin a-descriptive-name-for-my-changes\\n   ```\\n\\n8. Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review.\\n\\n## Code of conduct\\n\\nThis project adheres to the HuggingFace [code of conduct](CODE_OF_CONDUCT.md). \\nBy participating, you are expected to uphold this code.'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/chatinterface_system_prompt/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: chatinterface_system_prompt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport time\\n\\ndef echo(message, history, system_prompt, tokens):\\n    response = f\"System prompt: {system_prompt}\\\\n Message: {message}.\"\\n    for i in range(min(len(response), int(tokens))):\\n        time.sleep(0.05)\\n        yield response[: i+1]\\n\\ndemo = gr.ChatInterface(echo, \\n                        additional_inputs=[\\n                            gr.Textbox(\"You are helpful AI.\", label=\"System Prompt\"), \\n                            gr.Slider(10, 100)\\n                        ]\\n                       )\\n\\nif __name__ == \"__main__\":\\n    demo.queue().launch()\\n```'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 1}, page_content=\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial Network (CSPNet) approach to [ResNeXt](https://paperswithcode.com/method/resnext). The CSPNet partitions the feature map of the base layer into two parts and then merges them through a cross-stage hierarchy. The use of a split and merge strategy allows for more gradient flow through the network.\\n\\n## How do I use this model on an image?\\n\\nTo load a pretrained model:\\n\\n```py\\n>>> import timm\\n>>> model = timm.create_model('cspresnext50', pretrained=True)\\n>>> model.eval()\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 572}, page_content='```\\n\\nTo load and preprocess the image:\\n\\n```py \\n>>> import urllib\\n>>> from PIL import Image\\n>>> from timm.data import resolve_data_config\\n>>> from timm.data.transforms_factory import create_transform\\n\\n>>> config = resolve_data_config({}, model=model)\\n>>> transform = create_transform(**config)\\n\\n>>> url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\n>>> urllib.request.urlretrieve(url, filename)\\n>>> img = Image.open(filename).convert(\\'RGB\\')\\n>>> tensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n\\n```py\\n>>> import torch\\n>>> with torch.no_grad():\\n...     out = model(tensor)\\n>>> probabilities = torch.nn.functional.softmax(out[0], dim=0)\\n>>> print(probabilities.shape)\\n>>> # prints: torch.Size([1000])'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 1367}, page_content='```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n>>> # Get imagenet class mappings\\n>>> url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\n>>> urllib.request.urlretrieve(url, filename) \\n>>> with open(\"imagenet_classes.txt\", \"r\") as f:\\n...     categories = [s.strip() for s in f.readlines()]\\n\\n>>> # Print top categories per image\\n>>> top5_prob, top5_catid = torch.topk(probabilities, 5)\\n>>> for i in range(top5_prob.size(0)):\\n...     print(categories[top5_catid[i]], top5_prob[i].item())\\n>>> # prints class names and probabilities like:\\n>>> # [(\\'Samoyed\\', 0.6425196528434753), (\\'Pomeranian\\', 0.04062102362513542), (\\'keeshond\\', 0.03186424449086189), (\\'white wolf\\', 0.01739676296710968), (\\'Eskimo dog\\', 0.011717947199940681)]'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 2161}, page_content=\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspresnext50`. You can find the IDs in the model summaries at the top of this page.\\n\\nTo extract image features with this model, follow the [timm feature extraction examples](../feature_extraction), just change the name of the model you want to use.\\n\\n## How do I finetune this model?\\n\\nYou can finetune any of the pre-trained models just by changing the classifier (the last layer).\\n\\n```py\\n>>> model = timm.create_model('cspresnext50', pretrained=True, num_classes=NUM_FINETUNE_CLASSES)\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 2714}, page_content=\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscript](https://github.com/rwightman/pytorch-image-models/blob/master/train.py) to use your dataset.\\n\\n## How do I train this model?\\n\\nYou can follow the [timm recipe scripts](../scripts) for training a new model afresh.\\n\\n## Citation\\n\\n```BibTeX\\n@misc{wang2019cspnet,\\n      title={CSPNet: A New Backbone that can Enhance Learning Capability of CNN}, \\n      author={Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh},\\n      year={2019},\\n      eprint={1911.11929},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CV}\\n}\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 3385}, page_content='```'),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 3390}, page_content=\"<!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNeXt\\n  Paper:\\n    Title: 'CSPNet: A New Backbone that can Enhance Learning Capability of CNN'\\n    URL: https://paperswithcode.com/paper/cspnet-a-new-backbone-that-can-enhance\\nModels:\\n- Name: cspresnext50\\n  In Collection: CSP ResNeXt\\n  Metadata:\\n    FLOPs: 3962945536\\n    Parameters: 20570000\\n    File Size: 82562887\\n    Architecture:\\n    - 1x1 Convolution\\n    - Batch Normalization\\n    - Convolution\\n    - Global Average Pooling\\n    - Grouped Convolution\\n    - Max Pooling\\n    - ReLU\\n    - ResNeXt Block\\n    - Residual Connection\\n    - Softmax\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Label Smoothing\\n    - Polynomial Learning Rate Decay\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training Resources: 1x GPU\\n    ID: cspresnext50\\n    LR: 0.1\\n    Layers: 50\\n    Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 128\\n    Image Size: '224'\\n    Weight Decay: 0.005\"),\n",
              " Document(metadata={'source': 'huggingface/pytorch-image-models/blob/main/hfdocs/source/models/csp-resnext.mdx', 'start_index': 4289}, page_content=\"Momentum: 0.9\\n    Batch Size: 128\\n    Image Size: '224'\\n    Weight Decay: 0.005\\n    Interpolation: bilinear\\n    Training Steps: 8000000\\n  Code: https://github.com/rwightman/pytorch-image-models/blob/d8e69206be253892b2956341fea09fdebfaae4e3/timm/models/cspnet.py#L430\\n  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/cspresnext50_ra_224-648b4713.pth\\n  Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80.05%\\n      Top 5 Accuracy: 94.94%\\n-->\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# MultiDiffusion\\n\\n[MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation](https://huggingface.co/papers/2302.08113) is by Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\\n\\nThe abstract from the paper is:'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 816}, page_content='*Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals,'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 1710}, page_content='user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.*'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 1869}, page_content=\"You can find additional information about MultiDiffusion on the [project page](https://multidiffusion.github.io/), [original codebase](https://github.com/omerbt/MultiDiffusion), and try it out in a [demo](https://huggingface.co/spaces/weizmannscience/MultiDiffusion).\\n\\n## Tips\\n\\nWhile calling [`StableDiffusionPanoramaPipeline`], it's possible to specify the `view_batch_size` parameter to be > 1.\\nFor some GPUs with high performance, this can speedup the generation process and increase VRAM usage.\\n\\nTo generate panorama-like images make sure you pass the width parameter accordingly. We recommend a width value of 2048 which is the default.\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 2512}, page_content='Circular padding is applied to ensure there are no stitching artifacts when working with panoramas to ensure a seamless transition from the rightmost part to the leftmost part. By enabling circular padding (set `circular_padding=True`), the operation applies additional crops after the rightmost point of the image, allowing the model to \"see” the transition from the rightmost part to the leftmost part. This helps maintain visual consistency in a 360-degree sense and creates a proper “panorama” that can be viewed using 360-degree panorama viewers. When decoding latents in Stable Diffusion, circular padding is applied to ensure that the decoded latents match in the RGB space.\\n\\nFor example, without circular padding, there is a stitching artifact (default):\\n![img](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/indoor_%20no_circular_padding.png)'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/panorama.md', 'start_index': 3396}, page_content='But with circular padding, the right and the left parts are matching (`circular_padding=True`):\\n![img](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/indoor_%20circular_padding.png)\\n\\n<Tip>\\n\\nMake sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## StableDiffusionPanoramaPipeline\\n[[autodoc]] StableDiffusionPanoramaPipeline\\n\\t- __call__\\n\\t- all\\n\\n## StableDiffusionPipelineOutput\\n[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput'),\n",
              " Document(metadata={'source': 'huggingface/simulate/blob/main/docs/source/api/actors.mdx', 'start_index': 0}, page_content='!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Actors\\n\\n[[autodoc]] SimpleActor\\n\\n[[autodoc]] EgocentricCameraActor\\n\\n\\nUnder construction 🚧.'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02c_preprocess-sentence-pairs-pt.md', 'start_index': 0}, page_content='ow to preprocess pairs of sentences? We have seen how to tokenize single sentences and batch them together in the \"Batching inputs together\" video. If this code look unfamiliar to you, be sure to check that video again! Here we will focus on tasks that classify pairs of sentences. For instance, we may want to classify whether two texts are paraphrases or not. Here is an example taken from the Quora Question Pairs dataset, which focuses on identifying duplicate questions. In the first pair, the two questions are duplicates; in the second, they are not. Another pair classification problem is when we want to know if two sentences are logically related or not (a problem called Natural Language Inference or NLI). In this example taken from the MultiNLI dataset, we have a pair of sentences for each possible label: contradiction, neutral or entailment (which is a fancy way of saying the first sentence implies the second). So classifying pairs of sentences is a problem worth studying. In fact,'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02c_preprocess-sentence-pairs-pt.md', 'start_index': 908}, page_content=\"implies the second). So classifying pairs of sentences is a problem worth studying. In fact, in the GLUE benchmark (which is an academic benchmark for text classification), 8 of the 10 datasets are focused on tasks using pairs of sentences. That's why models like BERT are often pretrained with a dual objective: on top of the language modeling objective, they often have an objective related to sentence pairs. For instance, during pretraining, BERT is shown pairs of sentences and must predict both the value of randomly masked tokens and whether the second sentence follows from the first. Fortunately, the tokenizer from the Transformers library has a nice API to deal with pairs of sentences: you just have to pass them as two arguments to the tokenizer. On top of the input IDs and the attention mask we studied already, it returns a new field called token type IDs, which tells the model which tokens belong to the first sentence and which ones belong to the second sentence. Zooming in a\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/subtitles/en/raw/chapter3/02c_preprocess-sentence-pairs-pt.md', 'start_index': 1809}, page_content='tokens belong to the first sentence and which ones belong to the second sentence. Zooming in a little bit, here are the input IDs, aligned with the tokens they correspond to, their respective token type ID and attention mask. We can see the tokenizer also added special tokens so we have a CLS token, the tokens from the first sentence, a SEP token, the tokens from the second sentence, and a final SEP token. If we have several pairs of sentences, we can tokenize them together by passing the list of first sentences, then the list of second sentences and all the keyword arguments we studied already, like padding=True. Zooming in at the result, we can see how the tokenizer added padding to the second pair of sentences, to make the two outputs the same length, and properly dealt with token type IDS and attention masks for the two sentences. This is then all ready to pass through our model!'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 0}, page_content='!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Download files from the Hub\\n\\nThe `huggingface_hub` library provides functions to download files from the repositories\\nstored on the Hub. You can use these functions independently or integrate them into your\\nown library, making it more convenient for your users to interact with the Hub. This\\nguide will show you how to:\\n\\n* Download and cache a single file.\\n* Download and cache an entire repository.\\n* Download files to a local folder. \\n\\n## Download a single file\\n\\nThe [`hf_hub_download`] function is the main function for downloading files from the Hub.\\nIt downloads the remote file, caches it on disk (in a version-aware way), and returns its local file path.\\n\\n<Tip>'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 835}, page_content='<Tip>\\n\\nThe returned filepath is a pointer to the HF local cache. Therefore, it is important to not modify the file to avoid\\nhaving a corrupted cache. If you are interested in getting to know more about how files are cached, please refer to our\\n[caching guide](./manage-cache).\\n\\n</Tip>\\n\\n### From latest version\\n\\nSelect the file to download using the `repo_id`, `repo_type` and `filename` parameters. By default, the file will\\nbe considered as being part of a `model` repo.\\n\\n```python\\n>>> from huggingface_hub import hf_hub_download\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\")\\n\\'/root/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade/config.json\\'\\n\\n# Download from a dataset\\n>>> hf_hub_download(repo_id=\"google/fleurs\", filename=\"fleurs.py\", repo_type=\"dataset\")\\n\\'/root/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34/fleurs.py\\''),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 1794}, page_content='```\\n\\n### From specific version\\n\\nBy default, the latest version from the `main` branch is downloaded. However, in some cases you want to download a file\\nat a particular version (e.g. from a specific branch, a PR, a tag or a commit hash).\\nTo do so, use the `revision` parameter:\\n\\n```python\\n# Download from the `v1.0` tag\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"v1.0\")\\n\\n# Download from the `test-branch` branch\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"test-branch\")\\n\\n# Download from Pull Request #3\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"refs/pr/3\")\\n\\n# Download from a specific commit hash\\n>>> hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\", revision=\"877b84a8f93f2d619faa2a6e514a32beef88ab0a\")'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 2640}, page_content='```\\n\\n**Note:** When using the commit hash, it must be the full-length hash instead of a 7-character commit hash.\\n\\n### Construct a download URL\\n\\nIn case you want to construct the URL used to download a file from a repo, you can use [`hf_hub_url`] which returns a URL.\\nNote that it is used internally by [`hf_hub_download`].\\n\\n## Download an entire repository\\n\\n[`snapshot_download`] downloads an entire repository at a given revision. It uses internally [`hf_hub_download`] which\\nmeans all downloaded files are also cached on your local disk. Downloads are made concurrently to speed-up the process.\\n\\nTo download a whole repository, just pass the `repo_id` and `repo_type`:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\")\\n\\'/home/lysandre/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade\\''),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 3544}, page_content='# Or from a dataset\\n>>> snapshot_download(repo_id=\"google/fleurs\", repo_type=\"dataset\")\\n\\'/home/lysandre/.cache/huggingface/hub/datasets--google--fleurs/snapshots/199e4ae37915137c555b1765c01477c216287d34\\''),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 3748}, page_content='```\\n\\n[`snapshot_download`] downloads the latest revision by default. If you want a specific repository revision, use the\\n`revision` parameter:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", revision=\"refs/pr/1\")'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 4026}, page_content='```\\n\\n### Filter files to download\\n\\n[`snapshot_download`] provides an easy way to download a repository. However, you don\\'t always want to download the\\nentire content of a repository. For example, you might want to prevent downloading all `.bin` files if you know you\\'ll\\nonly use the `.safetensors` weights. You can do that using `allow_patterns` and `ignore_patterns` parameters.\\n\\nThese parameters accept either a single pattern or a list of patterns. Patterns are Standard Wildcards (globbing\\npatterns) as documented [here](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm). The pattern matching is\\nbased on [`fnmatch`](https://docs.python.org/3/library/fnmatch.html).\\n\\nFor example, you can use `allow_patterns` to only download JSON configuration files:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", allow_patterns=\"*.json\")'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 4932}, page_content='```\\n\\nOn the other hand, `ignore_patterns` can exclude certain files from being downloaded. The\\nfollowing example ignores the `.msgpack` and `.h5` file extensions:\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\", ignore_patterns=[\"*.msgpack\", \"*.h5\"])\\n```\\n\\nFinally, you can combine both to precisely filter your download. Here is an example to download all json and markdown\\nfiles except `vocab.json`.\\n\\n```python\\n>>> from huggingface_hub import snapshot_download\\n>>> snapshot_download(repo_id=\"gpt2\", allow_patterns=[\"*.md\", \"*.json\"], ignore_patterns=\"vocab.json\")'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 5561}, page_content='```\\n\\n## Download file(s) to local folder\\n\\nThe recommended (and default) way to download files from the Hub is to use the [cache-system](./manage-cache).\\nYou can define your cache location by setting `cache_dir` parameter (both in [`hf_hub_download`] and [`snapshot_download`]).'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 5840}, page_content='However, in some cases you want to download files and move them to a specific folder. This is useful to get a workflow\\ncloser to what `git` commands offer. You can do that using the `local_dir` and `local_dir_use_symlinks` parameters:\\n- `local_dir` must be a path to a folder on your system. The downloaded files will keep the same file structure as in the\\nrepo. For example if `filename=\"data/train.csv\"` and `local_dir=\"path/to/folder\"`, then the returned filepath will be\\n`\"path/to/folder/data/train.csv\"`.\\n- `local_dir_use_symlinks` defines how the file must be saved in your local folder.\\n  - The default behavior (`\"auto\"`) is to duplicate small files (<5MB) and use symlinks for bigger files. Symlinks allow\\n    to optimize both bandwidth and disk usage. However manually editing a symlinked file might corrupt the cache, hence\\n    the duplication for small files. The 5MB threshold can be configured with the `HF_HUB_LOCAL_DIR_AUTO_SYMLINK_THRESHOLD`\\n    environment variable.'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 6803}, page_content=\"environment variable.\\n  - If `local_dir_use_symlinks=True` is set, all files are symlinked for an optimal disk space optimization. This is\\n    for example useful when downloading a huge dataset with thousands of small files.\\n  - Finally, if you don't want symlinks at all you can disable them (`local_dir_use_symlinks=False`). The cache directory\\n    will still be used to check wether the file is already cached or not. If already cached, the file is **duplicated**\\n    from the cache (i.e. saves bandwidth but increases disk usage). If the file is not already cached, it will be\\n    downloaded and moved directly to the local dir. This means that if you need to reuse it somewhere else later, it\\n    will be **re-downloaded**.\"),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 7533}, page_content='Here is a table that summarizes the different options to help you choose the parameters that best suit your use case.'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 7652}, page_content='<!-- Generated with https://www.tablesgenerator.com/markdown_tables -->\\n| Parameters | File already cached | Returned path | Can read path? | Can save to path? | Optimized bandwidth | Optimized disk usage |\\n|---|:---:|:---:|:---:|:---:|:---:|:---:|\\n| `local_dir=None` |  | symlink in cache | ✅ | ❌<br>_(save would corrupt the cache)_ | ✅ | ✅ |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=\"auto\"` |  | file or symlink in folder | ✅ | ✅ _(for small files)_ <br> ⚠️ _(for big files do not resolve path before saving)_ | ✅ | ✅ |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=True` |  | symlink in folder | ✅ | ⚠️<br>_(do not resolve path before saving)_ | ✅ | ✅ |\\n| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=False` | No | file in folder | ✅ | ✅ | ❌<br>_(if re-run, file is re-downloaded)_ | ⚠️<br>(multiple copies if ran in multiple folders) |'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 8532}, page_content='| `local_dir=\"path/to/folder\"`<br>`local_dir_use_symlinks=False` | Yes | file in folder | ✅ | ✅ | ⚠️<br>_(file has to be cached first)_ | ❌<br>_(file is duplicated)_ |'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 8701}, page_content='**Note:** if you are on a Windows machine, you need to enable developer mode or run `huggingface_hub` as admin to enable\\nsymlinks. Check out the [cache limitations](../guides/manage-cache#limitations) section for more details.\\n\\n## Download from the CLI\\n\\nYou can use the `huggingface-cli download` command from the terminal to directly download files from the Hub.\\nInternally, it uses the same [`hf_hub_download`] and [`snapshot_download`] helpers described above and prints the\\nreturned path to the terminal.\\n\\n```bash\\n>>> huggingface-cli download gpt2 config.json\\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 9378}, page_content='```\\n\\nYou can download multiple files at once which displays a progress bar and returns the snapshot path in which the files\\nare located:\\n\\n```bash\\n>>> huggingface-cli download gpt2 config.json model.safetensors\\nFetching 2 files: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 23831.27it/s]\\n/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10'),\n",
              " Document(metadata={'source': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/download.md', 'start_index': 9790}, page_content='```\\n\\nFor more details about the CLI download command, please refer to the [CLI guide](./cli#huggingface-cli-download).\\n\\n## Faster downloads\\n\\nIf you are running on a machine with high bandwidth, you can increase your download speed with [`hf_transfer`](https://github.com/huggingface/hf_transfer), a Rust-based library developed to speed up file transfers with the Hub. To enable it, install the package (`pip install hf_transfer`) and set `HF_HUB_ENABLE_HF_TRANSFER=1` as an environment variable.\\n\\n<Tip>\\n\\nProgress bars are supported in `hf_transfer` starting from version `0.1.4`. Consider upgrading (`pip install -U hf-transfer`) if you plan to enable faster downloads.\\n\\n</Tip>\\n\\n<Tip warning={true}>\\n\\n`hf_transfer` is a power user tool! It is tested and production-ready, but it lacks user-friendly features like advanced error handling or proxies. For more details, please take a look at this [section](https://huggingface.co/docs/huggingface_hub/hf_transfer).\\n\\n</Tip>'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 1}, page_content='Two types of value-based methods [[two-types-value-based-methods]]\\n\\nIn value-based methods,\\xa0**we learn a value function**\\xa0that\\xa0**maps a state to the expected value of being at that state.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/vbm-1.jpg\" alt=\"Value Based Methods\"/>\\n\\nThe value of a state is the\\xa0**expected discounted return**\\xa0the agent can get if it\\xa0**starts at that state and then acts according to our policy.**\\n\\n<Tip>\\nBut what does it mean to act according to our policy? After all, we don\\'t have a policy in value-based methods since we train a value function and not a policy.\\n</Tip>\\n\\nRemember that the goal of an\\xa0**RL agent is to have an optimal policy π\\\\*.**\\n\\nTo find the optimal policy, we learned about two different methods:'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 737}, page_content='To find the optimal policy, we learned about two different methods:\\n\\n- *Policy-based methods:*\\xa0**Directly train the policy**\\xa0to select what action to take given a state (or a probability distribution over actions at that state). In this case, we\\xa0**don\\'t have a value function.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-2.jpg\" alt=\"Two RL approaches\"/>\\n\\nThe policy takes a state as input and outputs what action to take at that state (deterministic policy: a policy that output one action given a state, contrary to stochastic policy that output a probability distribution over actions).\\n\\nAnd consequently,\\xa0**we don\\'t define by hand the behavior of our policy; it\\'s the training that will define it.**\\n\\n- *Value-based methods:*\\xa0**Indirectly, by training a value function**\\xa0that outputs the value of a state or a state-action pair. Given this value function, our policy\\xa0**will take an action.**'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 1712}, page_content='Since the policy is not trained/learned,\\xa0**we need to specify its behavior.**\\xa0For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward,\\xa0**we\\'ll create a Greedy Policy.**\\n\\n<figure>\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-3.jpg\" alt=\"Two RL approaches\"/>\\n  <figcaption>Given a state, our action-value function (that we train) outputs the value of each action at that state. Then, our pre-defined Greedy Policy selects the action that will yield the highest value given a state or a state action pair.</figcaption>\\n</figure>\\n\\nConsequently, whatever method you use to solve your problem,\\xa0**you will have a policy**. In the case of value-based methods, you don\\'t train the policy: your policy\\xa0**is just a simple pre-specified function**\\xa0(for instance, the Greedy Policy) that\\xa0uses the values given by the value-function to select its actions.'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 2699}, page_content='So the difference is:\\n\\n- In policy-based training,\\xa0**the optimal policy (denoted π\\\\*) is found by training the policy directly.**\\n- In value-based training,\\xa0**finding an optimal value function (denoted Q\\\\* or V\\\\*, we\\'ll study the difference below) leads to having an optimal policy.**\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link between value and policy\"/>\\n\\nIn fact, most of the time, in value-based methods, you\\'ll use\\xa0**an Epsilon-Greedy Policy**\\xa0that handles the exploration/exploitation trade-off; we\\'ll talk about this when we talk about Q-Learning in the second part of this unit.\\n\\n\\nAs we mentioned above, we have two types of value-based functions:\\n\\n## The state-value function [[state-value-function]]\\n\\nWe write the state value function under a policy π like this:'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 3505}, page_content='We write the state value function under a policy π like this:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-1.jpg\" alt=\"State value function\"/>\\n\\nFor each state, the state-value function outputs the expected return if the agent **starts at that state** and then follows the policy forever afterward (for all future timesteps, if you prefer).\\n\\n<figure>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/state-value-function-2.jpg\" alt=\"State value function\"/>\\n  <figcaption>If we take the state with value -7: it\\'s the expected return starting at that state and taking actions according to our policy (greedy policy), so right, right, right, down, down, right, right.</figcaption>\\n</figure>\\n\\n## The action-value function [[action-value-function]]'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 4331}, page_content='## The action-value function [[action-value-function]]\\n\\nIn the action-value function, for each state and action pair, the action-value function\\xa0**outputs the expected return**\\xa0if the agent starts in that state, takes that action, and then follows the policy forever after.\\n\\nThe value of taking action \\\\\\\\(a\\\\\\\\) in state \\\\\\\\(s\\\\\\\\) under a policy \\\\\\\\(π\\\\\\\\) is:\\n\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-1.jpg\" alt=\"Action State value function\"/>\\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/action-state-value-function-2.jpg\" alt=\"Action State value function\"/>\\n\\n\\nWe see that the difference is:\\n\\n- For the state-value function, we calculate\\xa0**the value of a state \\\\\\\\(S_t\\\\\\\\)**\\n- For the action-value function, we calculate\\xa0**the value of the state-action pair ( \\\\\\\\(S_t, A_t\\\\\\\\) ) hence the value of taking that action at that state.**'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit2/two-types-value-based-methods.mdx', 'start_index': 5309}, page_content='<figure>\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-types.jpg\" alt=\"Two types of value function\"/>\\n  <figcaption>\\nNote: We didn\\'t fill all the state-action pairs for the example of Action-value function</figcaption>\\n</figure>\\n\\nIn either case, whichever value function we choose (state-value or action-value function),\\xa0**the returned value is the expected return.**\\n\\nHowever, the problem is that\\xa0**to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.**\\n\\nThis can be a computationally expensive process, and that\\'s\\xa0**where the Bellman equation comes in to help us.**'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1}, page_content=\"The Hugging Face Course\\n\\nThis repo contains the content that's used to create the **[Hugging Face course](https://huggingface.co/course/chapter1/1)**. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you'll learn how to use the [Hugging Face](https://huggingface.co/) ecosystem — [🤗 Transformers](https://github.com/huggingface/transformers), [🤗 Datasets](https://github.com/huggingface/datasets), [🤗 Tokenizers](https://github.com/huggingface/tokenizers), and [🤗 Accelerate](https://github.com/huggingface/accelerate) — as well as the [Hugging Face Hub](https://huggingface.co/models). It's completely free and open-source!\\n\\n## 🌎 Languages and translations\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 737}, page_content='| Language                                                                      | Source                                                                             | Authors                                                                                                                                                                                                                                                                                                                                                  |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1251}, page_content='|:------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 1765}, page_content='| [English](https://huggingface.co/course/en/chapter1/1)                        | [`chapters/en`](https://github.com/huggingface/course/tree/main/chapters/en)       | [@sgugger](https://github.com/sgugger), [@lewtun](https://github.com/lewtun), [@LysandreJik](https://github.com/LysandreJik), [@Rocketknight1](https://github.com/Rocketknight1), [@sashavor](https://github.com/sashavor), [@osanseviero](https://github.com/osanseviero), [@SaulLu](https://github.com/SaulLu), [@lvwerra](https://github.com/lvwerra) |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 2279}, page_content='| [Bengali](https://huggingface.co/course/bn/chapter1/1) (WIP)                  | [`chapters/bn`](https://github.com/huggingface/course/tree/main/chapters/bn)       | [@avishek-018](https://github.com/avishek-018), [@eNipu](https://github.com/eNipu)                                                                                                                                                                                                                                                                       |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 2793}, page_content='| [German](https://huggingface.co/course/de/chapter1/1) (WIP)                   | [`chapters/de`](https://github.com/huggingface/course/tree/main/chapters/de)       | [@JesperDramsch](https://github.com/JesperDramsch), [@MarcusFra](https://github.com/MarcusFra), [@fabridamicelli](https://github.com/fabridamicelli)                                                                                                                                                                                                                                                          |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 3360}, page_content='| [Spanish](https://huggingface.co/course/es/chapter1/1) (WIP)                  | [`chapters/es`](https://github.com/huggingface/course/tree/main/chapters/es)       | [@camartinezbu](https://github.com/camartinezbu), [@munozariasjm](https://github.com/munozariasjm), [@fordaz](https://github.com/fordaz)                                                                                                                                                                                                                 |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 3874}, page_content='| [Persian](https://huggingface.co/course/fa/chapter1/1) (WIP)                  | [`chapters/fa`](https://github.com/huggingface/course/tree/main/chapters/fa)       | [@jowharshamshiri](https://github.com/jowharshamshiri), [@schoobani](https://github.com/schoobani)                                                                                                                                                                                                                                                       |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 4388}, page_content='| [French](https://huggingface.co/course/fr/chapter1/1)                         | [`chapters/fr`](https://github.com/huggingface/course/tree/main/chapters/fr)       | [@lbourdois](https://github.com/lbourdois), [@ChainYo](https://github.com/ChainYo), [@melaniedrevet](https://github.com/melaniedrevet), [@abdouaziz](https://github.com/abdouaziz)                                                                                                                                                                       |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 4902}, page_content='| [Gujarati](https://huggingface.co/course/gu/chapter1/1) (WIP)                 | [`chapters/gu`](https://github.com/huggingface/course/tree/main/chapters/gu)       | [@pandyaved98](https://github.com/pandyaved98)                                                                                                                                                                                                                                                                                                           |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 5416}, page_content='| [Hebrew](https://huggingface.co/course/he/chapter1/1) (WIP)                   | [`chapters/he`](https://github.com/huggingface/course/tree/main/chapters/he)       | [@omer-dor](https://github.com/omer-dor)                                                                                                                                                                                                                                                                                                                 |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 5930}, page_content='| [Hindi](https://huggingface.co/course/hi/chapter1/1) (WIP)                    | [`chapters/hi`](https://github.com/huggingface/course/tree/main/chapters/hi)       | [@pandyaved98](https://github.com/pandyaved98)                                                                                                                                                                                                                                                                                                           |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 6444}, page_content='| [Bahasa Indonesia](https://huggingface.co/course/id/chapter1/1) (WIP)                   | [`chapters/id`](https://github.com/huggingface/course/tree/main/chapters/id)       | [@gstdl](https://github.com/gstdl)                                                                                                                                                                                                                                                                                                           |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 6956}, page_content='| [Italian](https://huggingface.co/course/it/chapter1/1) (WIP)                  | [`chapters/it`](https://github.com/huggingface/course/tree/main/chapters/it)       | [@CaterinaBi](https://github.com/CaterinaBi), [@ClonedOne](https://github.com/ClonedOne),    [@Nolanogenn](https://github.com/Nolanogenn), [@EdAbati](https://github.com/EdAbati), [@gdacciaro](https://github.com/gdacciaro)                                                                                                                                                                  |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 7508}, page_content='| [Japanese](https://huggingface.co/course/ja/chapter1/1) (WIP)                 | [`chapters/ja`](https://github.com/huggingface/course/tree/main/chapters/ja)       | [@hiromu166](https://github.com/@hiromu166), [@younesbelkada](https://github.com/@younesbelkada), [@HiromuHota](https://github.com/@HiromuHota)                                                                                                                                                                                                       |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 8019}, page_content='| [Korean](https://huggingface.co/course/ko/chapter1/1) (WIP)                   | [`chapters/ko`](https://github.com/huggingface/course/tree/main/chapters/ko)       | [@Doohae](https://github.com/Doohae), [@wonhyeongseo](https://github.com/wonhyeongseo), [@dlfrnaos19](https://github.com/dlfrnaos19), [@nsbg](https://github.com/nsbg)                                                                                                                                                                                                                                                                                                                     |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 8663}, page_content='| [Portuguese](https://huggingface.co/course/pt/chapter1/1) (WIP)               | [`chapters/pt`](https://github.com/huggingface/course/tree/main/chapters/pt)       | [@johnnv1](https://github.com/johnnv1), [@victorescosta](https://github.com/victorescosta), [@LincolnVS](https://github.com/LincolnVS)                                                                                                                                                                                                                   |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 9177}, page_content='| [Russian](https://huggingface.co/course/ru/chapter1/1) (WIP)                  | [`chapters/ru`](https://github.com/huggingface/course/tree/main/chapters/ru)       | [@pdumin](https://github.com/pdumin), [@svv73](https://github.com/svv73)                                                                                                                                                                                                                                                                                 |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 9691}, page_content='| [Thai](https://huggingface.co/course/th/chapter1/1) (WIP)                     | [`chapters/th`](https://github.com/huggingface/course/tree/main/chapters/th)       | [@peeraponw](https://github.com/peeraponw), [@a-krirk](https://github.com/a-krirk), [@jomariya23156](https://github.com/jomariya23156), [@ckingkan](https://github.com/ckingkan)                                                                                                                                                                         |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 10205}, page_content='| [Turkish](https://huggingface.co/course/tr/chapter1/1) (WIP)                  | [`chapters/tr`](https://github.com/huggingface/course/tree/main/chapters/tr)       | [@tanersekmen](https://github.com/tanersekmen), [@mertbozkir](https://github.com/mertbozkir), [@ftarlaci](https://github.com/ftarlaci), [@akkasayaz](https://github.com/akkasayaz)                                                                                                                                                                       |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 10719}, page_content='| [Vietnamese](https://huggingface.co/course/vi/chapter1/1)               | [`chapters/vi`](https://github.com/huggingface/course/tree/main/chapters/vi)       | [@honghanhh](https://github.com/honghanhh)                                                                                                                                                                                                                                                                                                               |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 11227}, page_content='| [Chinese (simplified)](https://huggingface.co/course/zh-CN/chapter1/1)  | [`chapters/zh-CN`](https://github.com/huggingface/course/tree/main/chapters/zh-CN) | [@zhlhyx](https://github.com/zhlhyx), [petrichor1122](https://github.com/petrichor1122), [@1375626371](https://github.com/1375626371)                                                                                                                                                                                                                    |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 11735}, page_content='| [Chinese (traditional)](https://huggingface.co/course/zh-TW/chapter1/1) (WIP) | [`chapters/zh-TW`](https://github.com/huggingface/course/tree/main/chapters/zh-TW) | [@davidpeng86](https://github.com/davidpeng86)                                                                                                                                                                                                                                                                                                           |'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 12251}, page_content=\"### Translating the course into your language\\n\\nAs part of our mission to democratise machine learning, we'd love to have the course available in many more languages! Please follow the steps below if you'd like to help translate the course into your language 🙏.\\n\\n**🗞️ Open an issue**\\n\\nTo get started, navigate to the [_Issues_](https://github.com/huggingface/course/issues) page of this repo and check if anyone else has opened an issue for your language. If not, open a new issue by selecting the _Translation template_ from the _New issue_ button.\\n\\nOnce an issue is created, post a comment to indicate which chapters you'd like to work on and we'll add your name to the list.\\n\\n**🗣 Join our Discord**\\n\\nSince it can be difficult to discuss translation details quickly over GitHub issues, we have created dedicated channels for each language on our Discord server. If you'd like to join, follow the instructions at this channel 👉: [https://discord.gg/JfAtkvEtRb](https://discord.gg/JfAtkvEtRb)\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 13244}, page_content=\"**🍴 Fork the repository**\\n\\nNext, you'll need to [fork this repo](https://docs.github.com/en/get-started/quickstart/fork-a-repo). You can do this by clicking on the **Fork** button on the top-right corner of this repo's page.\\n\\nOnce you've forked the repo, you'll want to get the files on your local machine for editing. You can do that by cloning the fork with Git as follows:\\n\\n```bash\\ngit clone https://github.com/YOUR-USERNAME/course\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 13679}, page_content=\"```\\n\\n**📋 Copy-paste the English files with a new language code**\\n\\nThe course files are organised under a main directory:\\n\\n* [`chapters`](https://github.com/huggingface/course/tree/main/chapters): all the text and code snippets associated with the course.\\n\\nYou'll only need to copy the files in the [`chapters/en`](https://github.com/huggingface/course/tree/main/chapters/en) directory, so first navigate to your fork of the repo and run the following:\\n\\n```bash\\ncd ~/path/to/course\\ncp -r chapters/en/CHAPTER-NUMBER chapters/LANG-ID/CHAPTER-NUMBER\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 14225}, page_content=\"```\\n\\nHere, `CHAPTER-NUMBER` refers to the chapter you'd like to work on and `LANG-ID` should be one of the ISO 639-1 or ISO 639-2 language codes -- see [here](https://www.loc.gov/standards/iso639-2/php/code_list.php) for a handy table.\\n\\n**✍️ Start translating**\\n\\nNow comes the fun part - translating the text! The first thing we recommend is translating the part of the `_toctree.yml` file that corresponds to your chapter. This file is used to render the table of contents on the website and provide the links to the Colab notebooks. The only fields you should change are the `title`, ones -- for example, here are the parts of `_toctree.yml` that we'd translate for [Chapter 0](https://huggingface.co/course/chapter0/1?fw=pt):\\n\\n```yaml\\n- title: 0. Setup # Translate this!\\n  sections:\\n  - local: chapter0/1 # Do not change this!\\n    title: Introduction # Translate this!\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 15097}, page_content=\"```\\n\\n> 🚨 Make sure the `_toctree.yml` file only contains the sections that have been translated! Otherwise you won't be able to build the content on the website or locally (see below how).\\n\\n\\nOnce you have translated the `_toctree.yml` file, you can start translating the [MDX](https://mdxjs.com/) files associated with your chapter.\\n\\n> 🙋 If the `_toctree.yml` file doesn't yet exist for your language, you can simply create one by copy-pasting from the English version and deleting the sections that aren't related to your chapter. Just make sure it exists in the `chapters/LANG-ID/` directory!\\n\\n**👷\\u200d♂️ Build the course locally**\\n\\nOnce you're happy with your changes, you can preview how they'll look by first installing the [`doc-builder`](https://github.com/huggingface/doc-builder) tool that we use for building all documentation at Hugging Face:\\n\\n```\\npip install hf-doc-builder\\n```\\n\\n```\\ndoc-builder preview course ../course/chapters/LANG-ID --not_python_module\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 15979}, page_content='```\\n\\n```\\ndoc-builder preview course ../course/chapters/LANG-ID --not_python_module\\n```\\n\\n**`preview` command does not work with Windows.\\n\\nThis will build and render the course on [http://localhost:3000/](http://localhost:3000/). Although the content looks much nicer on the Hugging Face website, this step will still allow you to check that everything is formatted correctly.\\n\\n**🚀 Submit a pull request**\\n\\nIf the translations look good locally, the final step is to prepare the content for a pull request. Here, the first think to check is that the files are formatted correctly. For that you can run:\\n\\n```\\npip install -r requirements.txt\\nmake style'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 16581}, page_content=\"```\\npip install -r requirements.txt\\nmake style\\n```\\n\\nOnce that's run, commit any changes, open a pull request, and tag [@lewtun](https://github.com/lewtun) for a review. Congratulations, you've now completed your first translation 🥳!\\n\\n> 🚨 To build the course on the website, double-check your language code exists in `languages` field of the `build_documentation.yml` and `build_pr_documentation.yml` files in the `.github` folder. If not, just add them in their alphabetical order.\\n\\n## 📔 Jupyter notebooks\\n\\nThe Jupyter notebooks containing all the code from the course are hosted on the [`huggingface/notebooks`](https://github.com/huggingface/notebooks) repo. If you wish to generate them locally, first install the required dependencies:\\n\\n```bash\\npython -m pip install -r requirements.txt\\n```\\n\\nThen run the following script:\\n\\n```bash\\npython utils/generate_notebooks.py --output_dir nbs\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 17469}, page_content='```\\n\\nThis script extracts all the code snippets from the chapters and stores them as notebooks in the `nbs` folder (which is ignored by Git by default).\\n\\n## ✍️ Contributing a new chapter\\n\\n> Note: we are not currently accepting community contributions for new chapters. These instructions are for the Hugging Face authors.\\n\\nAdding a new chapter to the course is quite simple:'),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 17792}, page_content=\"Adding a new chapter to the course is quite simple:\\n\\n1. Create a new directory under `chapters/en/chapterX`, where `chapterX` is the chapter you'd like to add.\\n2. Add numbered MDX files `sectionX.mdx` for each section. If you need to include images, place them in the [huggingface-course/documentation-images](https://huggingface.co/datasets/huggingface-course/documentation-images) repository and use the [HTML Images Syntax](https://www.w3schools.com/html/html_images.asp) with the path `https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/{langY}/{chapterX}/{your-image.png}`.\\n3. Update the `_toctree.yml` file to include your chapter sections -- this information will render the table of contents on the website. If your section involves both the PyTorch and TensorFlow APIs of `transformers`, make sure you include links to both Colabs in the `colab` field.\"),\n",
              " Document(metadata={'source': 'huggingface/course/blob/main/README.md', 'start_index': 18689}, page_content='If you get stuck, check out one of the existing chapters -- this will often show you the expected syntax.\\n\\nOnce you are happy with the content, open a pull request and tag [@lewtun](https://github.com/lewtun) for a review. We recommend adding the first chapter draft as a single pull request -- the team will then provide feedback internally to iterate on the content 🤗!\\n\\n## 🙌 Acknowledgements\\n\\nThe structure of this repo and README are inspired by the wonderful [Advanced NLP with spaCy](https://github.com/ines/spacy-course) course.'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 1}, page_content='The advantages and disadvantages of policy-gradient methods\\n\\nAt this point, you might ask, \"but Deep Q-Learning is excellent! Why use policy-gradient methods?\". To answer this question, let\\'s study the **advantages and disadvantages of policy-gradient methods**.\\n\\n## Advantages\\n\\nThere are multiple advantages over value-based methods. Let\\'s see some of them:\\n\\n### The simplicity of integration\\n\\nWe can estimate the policy directly without storing additional data (action values).\\n\\n### Policy-gradient methods can learn a stochastic policy\\n\\nPolicy-gradient methods can\\xa0**learn a stochastic policy while value functions can\\'t**.\\n\\nThis has two consequences:\\n\\n1. We **don\\'t need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent explores\\xa0**the state space without always taking the same trajectory.**'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 878}, page_content='2. We also get rid of the problem of **perceptual aliasing**. Perceptual aliasing is when two states seem (or are) the same but need different actions.\\n\\nLet\\'s take an example: we have an intelligent vacuum cleaner whose goal is to suck the dust and avoid killing the hamsters.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster1.jpg\" alt=\"Hamster 1\"/>\\n</figure>\\n\\nOur vacuum cleaner can only perceive where the walls are.\\n\\nThe problem is that the **two red (colored) states are aliased states because the agent perceives an upper and lower wall for each**.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster2.jpg\" alt=\"Hamster 1\"/>\\n</figure>'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 1754}, page_content='Under a deterministic policy, the policy will either always move right when in a red state or always move left. **Either case will cause our agent to get stuck and never suck the dust**.\\n\\nUnder a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic policy** (\"greedy epsilon strategy\"). Consequently, our agent can **spend a lot of time before finding the dust**.\\n\\nOn the other hand, an optimal stochastic policy **will randomly move left or right in red (colored) states**. Consequently, **it will not be stuck and will reach the goal state with a high probability**.\\n\\n<figure class=\"image table text-center m-0 w-full\">\\n  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg\" alt=\"Hamster 1\"/>\\n</figure>\\n\\n### Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces'),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 2664}, page_content=\"The problem with Deep Q-learning is that their **predictions assign a score (maximum expected future reward) for each possible action**, at each time step, given the current state.\\n\\nBut what if we have an infinite possibility of actions?\\n\\nFor instance, with a self-driving car, at each state, you can have a (near) infinite choice of actions (turning the wheel at 15°, 17.2°, 19,4°, honking, etc.). **We'll need to output a Q-value for each possible action**! And **taking the max action of a continuous output is an optimization problem itself**!\\n\\nInstead, with policy-gradient methods, we output a\\xa0**probability distribution over actions.**\\n\\n### Policy-gradient methods have better convergence properties\"),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 3308}, page_content=\"### Policy-gradient methods have better convergence properties\\n\\nIn value-based methods, we use an aggressive operator to **change the value function: we take the maximum over Q-estimates**.\\nConsequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.\\n\\nFor instance, if during the training, the best action was left (with a Q-value of 0.22) and the training step after it's right (since the right Q-value becomes 0.23), we dramatically changed the policy since now the policy will take most of the time right instead of left.\\n\\nOn the other hand, in policy-gradient methods, stochastic policy action preferences (probability of taking action) **change smoothly over time**.\\n\\n## Disadvantages\\n\\nNaturally, policy-gradient methods also have some disadvantages:\"),\n",
              " Document(metadata={'source': 'huggingface/deep-rl-class/blob/main/units/en/unit4/advantages-disadvantages.mdx', 'start_index': 4117}, page_content=\"## Disadvantages\\n\\nNaturally, policy-gradient methods also have some disadvantages:\\n\\n- **Frequently, policy-gradient methods converges to a local maximum instead of a global optimum.**\\n- Policy-gradient goes slower,\\xa0**step by step: it can take longer to train (inefficient).**\\n- Policy-gradient can have high variance. We'll see in the actor-critic unit why, and how we can solve this problem.\\n\\n👉 If you want to go deeper into the advantages and disadvantages of policy-gradient methods, [you can check this video](https://youtu.be/y3oqOjHilio).\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/shap_e.md', 'start_index': 0}, page_content='!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n-->\\n\\n# Shap-E\\n\\nThe Shap-E model was proposed in [Shap-E: Generating Conditional 3D Implicit Functions](https://huggingface.co/papers/2305.02463) by Alex Nichol and Heewoo Jun from [OpenAI](https://github.com/openai).\\n\\nThe abstract from the paper is:'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/shap_e.md', 'start_index': 797}, page_content='The abstract from the paper is:\\n\\n*We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space.*'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/docs/source/en/api/pipelines/shap_e.md', 'start_index': 1719}, page_content='The original codebase can be found at [openai/shap-e](https://github.com/openai/shap-e).\\n\\n<Tip>\\n\\nSee the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.\\n\\n</Tip>\\n\\n## ShapEPipeline\\n[[autodoc]] ShapEPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ShapEImg2ImgPipeline\\n[[autodoc]] ShapEImg2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## ShapEPipelineOutput\\n[[autodoc]] pipelines.shap_e.pipeline_shap_e.ShapEPipelineOutput'),\n",
              " Document(metadata={'source': 'gradio-app/gradio/blob/main/demo/streaming_wav2vec/run.ipynb', 'start_index': 1}, page_content='Gradio Demo: streaming_wav2vec\\n\\n\\n```\\n!pip install -q gradio torch transformers \\n```\\n\\n\\n```\\nfrom transformers import pipeline\\nimport gradio as gr\\nimport time\\n\\np = pipeline(\"automatic-speech-recognition\")\\n\\ndef transcribe(audio, state=\"\"):\\n    time.sleep(2)\\n    text = p(audio)[\"text\"]\\n    state += text + \" \"\\n    return state, state\\n\\ndemo = gr.Interface(\\n    fn=transcribe, \\n    inputs=[\\n        gr.Audio(sources=[\"microphone\"], type=\"filepath\", streaming=True), \\n        \"state\"\\n    ],\\n    outputs=[\\n        \"textbox\",\\n        \"state\"\\n    ],\\n    live=True\\n)\\n\\nif __name__ == \"__main__\":\\n    demo.launch()\\n\\n```'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 1}, page_content=\"Adding a Sign-In with HF button to your Space\\n\\nYou can enable a built-in sign-in flow in your Space by seamlessly creating and associating an [OAuth/OpenID connect](https://developer.okta.com/blog/2019/10/21/illustrated-guide-to-oauth-and-oidc) app so users can log in with their HF account.\\n\\nThis enables new use cases for your Space. For instance, when combined with [Persistent Storage](https://huggingface.co/docs/hub/spaces-storage), a generative AI Space could allow users to log in to access their previous generations, only accessible to them.\\n\\n<Tip>\\n\\nThis guide will take you through the process of integrating a *Sign-In with HF* button into any Space. If you're seeking a fast and simple method to implement this in a **Gradio** Space, take a look at its [built-in integration](https://www.gradio.app/guides/sharing-your-app#o-auth-login-via-hugging-face).\\n\\n</Tip>\\n\\n<Tip>\"),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 870}, page_content='</Tip>\\n\\n<Tip>\\n\\nYou can also use the HF OAuth flow to create a \"Sign in with HF\" flow in any website or App, outside of Spaces. [Read our general OAuth page](./oauth).\\n\\n</Tip>\\n\\n## Create an OAuth app\\n\\nAll you need to do is add `hf_oauth: true` to your Space\\'s metadata inside your `README.md` file.\\n\\nHere\\'s an example of metadata for a Gradio Space:\\n\\n```yaml\\ntitle: Gradio Oauth Test\\nemoji: 🏆\\ncolorFrom: pink\\ncolorTo: pink\\nsdk: gradio\\nsdk_version: 3.40.0\\npython_version: 3.10.6\\napp_file: app.py\\n\\nhf_oauth: true\\n# optional, see \"Scopes\" below. \"openid profile\" is always included.\\nhf_oauth_scopes:\\n - read-repos\\n - write-repos\\n - manage-repos\\n - inference-api'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 1528}, page_content='```\\n\\nYou can check out the [configuration reference docs](./spaces-config-reference) for more information.\\n\\nThis will add the following [environment variables](https://huggingface.co/docs/hub/spaces-overview#helper-environment-variables) to your space:\\n\\n- `OAUTH_CLIENT_ID`: the client ID of your OAuth app (public)\\n- `OAUTH_CLIENT_SECRET`: the client secret of your OAuth app\\n- `OAUTH_SCOPES`: scopes accessible by your OAuth app.\\n- `OPENID_PROVIDER_URL`: The URL of the OpenID provider. The OpenID metadata will be available at [`{OPENID_PROVIDER_URL}/.well-known/openid-configuration`](https://huggingface.co/.well-known/openid-configuration).\\n\\nAs for any other environment variable, you can use them in your code by using `os.getenv(\"OAUTH_CLIENT_ID\")`, for example.\\n\\n## Redirect URLs \\n\\nYou can use any redirect URL you want, as long as it targets your Space.'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 2300}, page_content=\"## Redirect URLs \\n\\nYou can use any redirect URL you want, as long as it targets your Space.\\n\\nNote that `SPACE_HOST` is [available](https://huggingface.co/docs/hub/spaces-overview#helper-environment-variables) as an environment variable.\\n\\nFor example, you can use `https://{SPACE_HOST}/login/callback` as a redirect URI.\\n\\n## Scopes\\n\\nThe following scopes are always included for Spaces:\\n\\n- `openid`: Get the ID token in addition to the access token.\\n- `profile`: Get the user's profile information (username, avatar, etc.)\\n\\nThose scopes are optional and can be added by setting `hf_oauth_scopes` in your Space's metadata:\"),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 2822}, page_content=\"Those scopes are optional and can be added by setting `hf_oauth_scopes` in your Space's metadata:\\n\\n- `email`: Get the user's email address.\\n- `read-repos`: Get read access to the user's personal repos.\\n- `write-repos`: Get write access to the user's personal repos. Does not grant read access on its own, you need to include `read-repos` as well.\\n- `manage-repos`: Get access to a repo's settings. Also grants repo creation and deletion.\\n- `inference-api`: Get access to the [Inference API](https://huggingface.co/docs/api-inference/index), you will be able to make inference requests on behalf of the user.\\n\\n## Adding the button to your Space\"),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 3431}, page_content='## Adding the button to your Space\\n\\nYou now have all the information to add a \"Sign-in with HF\" button to your Space. Some libraries ([Python](https://github.com/lepture/authlib), [NodeJS](https://github.com/panva/node-openid-client)) can help you implement the OpenID/OAuth protocol. Gradio also provides **built-in support**, making implementing the Sign-in with HF button a breeze; you can [check out the associated guide](https://www.gradio.app/guides/sharing-your-app#o-auth-login-via-hugging-face).\\n\\nBasically, you need to:'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 3937}, page_content='Basically, you need to:\\n\\n- Redirect the user to `https://huggingface.co/oauth/authorize?redirect_uri={REDIRECT_URI}&scope=openid%20profile&client_id={CLIENT_ID}&state={STATE}`, where `STATE` is a random string that you will need to verify later.\\n- Handle the callback on `/auth/callback` or `/login/callback` (or your own custom callback URL) and verify the `state` parameter.\\n- Use the `code` query parameter to get an access token and id token from `https://huggingface.co/oauth/token` (POST request with `client_id`, `code`, `grant_type=authorization_code` and `redirect_uri` as form data, and with `Authorization: Basic {base64(client_id:client_secret)}` as a header).\\n\\n<Tip warning={true}>\\n\\nYou should use `target=_blank` on the button to open the sign-in page in a new tab, unless you run the space outside its `iframe`. Otherwise, you might encounter issues with cookies on some browsers.\\n\\n</Tip>\\n\\n## Examples:'),\n",
              " Document(metadata={'source': 'huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md', 'start_index': 4834}, page_content='</Tip>\\n\\n## Examples:\\n\\n- [Gradio test app](https://huggingface.co/spaces/Wauplin/gradio-oauth-test)\\n- [Hugging Chat (NodeJS/SvelteKit)](https://huggingface.co/spaces/huggingchat/chat-ui)\\n- [Inference Widgets (Auth.js/SvelteKit)](https://huggingface.co/spaces/huggingfacejs/inference-widgets), uses the `inference-api` scope to make inference requests on behalf of the user.'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 0}, page_content='!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\n# 🧨 Diffusers Examples\\n\\nDiffusers examples are a collection of scripts to demonstrate how to effectively use the `diffusers` library\\nfor a variety of use cases involving training or fine-tuning.'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 788}, page_content='**Note**: If you are looking for **official** examples on how to use `diffusers` for inference, please have a look at [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines).\\n\\nOur examples aspire to be **self-contained**, **easy-to-tweak**, **beginner-friendly** and for **one-purpose-only**.\\nMore specifically, this means:'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 1159}, page_content='- **Self-contained**: An example script shall only depend on \"pip-install-able\" Python packages that can be found in a `requirements.txt` file. Example scripts shall **not** depend on any local files. This means that one can simply download an example script, *e.g.* [train_unconditional.py](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/train_unconditional.py), install the required dependencies, *e.g.* [requirements.txt](https://github.com/huggingface/diffusers/blob/main/examples/unconditional_image_generation/requirements.txt) and execute the example script.'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 1769}, page_content=\"- **Easy-to-tweak**: While we strive to present as many use cases as possible, the example scripts are just that - examples. It is expected that they won't work out-of-the box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs. To help you with that, most of the examples fully expose the preprocessing of the data and the training loop to allow you to tweak and edit them as required.\\n- **Beginner-friendly**: We do not aim for providing state-of-the-art training scripts for the newest models, but rather examples that can be used as a way to better understand diffusion models and how to use them with the `diffusers` library. We often purposefully leave out certain state-of-the-art methods if we consider them too complex for beginners.\"),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 2573}, page_content='- **One-purpose-only**: Examples should show one task and one task only. Even if a task is from a modeling point of view very similar, *e.g.* image super-resolution and image modification tend to use the same model and training method, we want examples to showcase only one task to keep them as readable and easy-to-understand as possible.'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 2914}, page_content='We provide **official** examples that cover the most popular tasks of diffusion models.\\n*Official* examples are **actively** maintained by the `diffusers` maintainers and we try to rigorously follow our example philosophy as defined above.\\nIf you feel like another important example should exist, we are more than happy to welcome a [Feature Request](https://github.com/huggingface/diffusers/issues/new?assignees=&labels=&template=feature_request.md&title=) or directly a [Pull Request](https://github.com/huggingface/diffusers/compare) from you!\\n\\nTraining examples show how to pretrain or fine-tune diffusion models for a variety of tasks. Currently we support:'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 3578}, page_content='| Task | 🤗 Accelerate | 🤗 Datasets | Colab\\n|---|---|:---:|:---:|\\n| [**Unconditional Image Generation**](./unconditional_image_generation) | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb)\\n| [**Text-to-Image fine-tuning**](./text_to_image) | ✅ | ✅ |\\n| [**Textual Inversion**](./textual_inversion) | ✅ | - | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)\\n| [**Dreambooth**](./dreambooth) | ✅ | - | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb)\\n| [**ControlNet**](./controlnet) | ✅ | ✅ | -\\n| [**InstructPix2Pix**](./instruct_pix2pix) | ✅ | ✅ | -'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 4503}, page_content='| [**InstructPix2Pix**](./instruct_pix2pix) | ✅ | ✅ | -\\n| [**Reinforcement Learning for Control**](https://github.com/huggingface/diffusers/blob/main/examples/reinforcement_learning/run_diffusers_locomotion.py)                    | - | - | coming soon.'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 4757}, page_content='## Community\\n\\nIn addition, we provide **community** examples, which are examples added and maintained by our community.\\nCommunity examples can consist of both *training* examples or *inference* pipelines.\\nFor such examples, we are more lenient regarding the philosophy defined above and also cannot guarantee to provide maintenance for every issue.\\nExamples that are useful for the community, but are either not yet deemed popular or not yet following our above philosophy should go into the [community examples](https://github.com/huggingface/diffusers/tree/main/examples/community) folder. The community folder therefore includes training examples and inference pipelines.\\n**Note**: Community examples can be a [great first contribution](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) to show to the community how you like to use `diffusers` 🪄.\\n\\n## Research Projects'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 5661}, page_content='## Research Projects\\n\\nWe also provide **research_projects** examples that are maintained by the community as defined in the respective research project folders. These examples are useful and offer the extended capabilities which are complementary to the official examples. You may refer to [research_projects](https://github.com/huggingface/diffusers/tree/main/examples/research_projects) for details.\\n\\n## Important note\\n\\nTo make sure you can successfully run the latest versions of the example scripts, you have to **install the library from source** and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:\\n```bash\\ngit clone https://github.com/huggingface/diffusers\\ncd diffusers\\npip install .'),\n",
              " Document(metadata={'source': 'huggingface/diffusers/blob/main/examples/README.md', 'start_index': 6417}, page_content='```\\nThen cd in the example folder of your choice and run\\n```bash\\npip install -r requirements.txt\\n```'),\n",
              " Document(metadata={'source': 'huggingface/peft/blob/main/examples/image_classification/README.md', 'start_index': 1}, page_content='Fine-tuning for image classification using LoRA and 🤗 PEFT\\n\\n## Vision Transformer model from transformers\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/image_classification/image_classification_peft_lora.ipynb) \\n\\nWe provide a notebook (`image_classification_peft_lora.ipynb`) where we learn how to use [LoRA](https://arxiv.org/abs/2106.09685) from 🤗 PEFT to fine-tune an image classification model by ONLY using **0.7%** of the original trainable parameters of the model. \\n\\nLoRA adds low-rank \"update matrices\" to certain blocks in the underlying model (in this case the attention blocks) and ONLY trains those matrices during fine-tuning. During inference, these update matrices are _merged_ with the original model parameters. For more details, check out the [original LoRA paper](https://arxiv.org/abs/2106.09685). \\n\\n## PoolFormer model from timm'),\n",
              " Document(metadata={'source': 'huggingface/peft/blob/main/examples/image_classification/README.md', 'start_index': 930}, page_content='## PoolFormer model from timm\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/image_classification/image_classification_timm_peft_lora.ipynb) \\n\\nThe notebook `image_classification_timm_peft_lora.ipynb` showcases fine-tuning an image classification model using from the [timm](https://huggingface.co/docs/timm/index) library. Again, LoRA is used to reduce the numberof trainable parameters to a fraction of the total.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 0}, page_content='--\\ntitle: \"Generating Human-level Text with Contrastive Search in Transformers 🤗\"\\nthumbnail: /blog/assets/115_introducing_contrastive_search/thumbnail.png\\nauthors:\\n- user: GMFTBY\\n---\\n\\n# Generating Human-level Text with Contrastive Search in Transformers 🤗\\n\\n\\n****\\n\\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\\n</a>\\n\\n### 1. Introduction:'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 510}, page_content='### 1. Introduction:\\n\\nNatural language generation (i.e. text generation) is one of the core tasks in natural language processing (NLP). In this blog, we introduce the current state-of-the-art decoding method, ___Contrastive Search___, for neural text generation. Contrastive search is originally proposed in _\"A Contrastive Framework for Neural Text Generation\"_ <a href=\\'#references\\'>[1]</a> ([[Paper]](https://arxiv.org/abs/2202.06417)[[Official Implementation]](https://github.com/yxuansu/SimCTG)) at NeurIPS 2022. Moreover, in this follow-up work,  _\"Contrastive Search Is What You Need For Neural Text Generation\"_ <a href=\\'#references\\'>[2]</a> ([[Paper]](https://arxiv.org/abs/2210.14140) [[Official Implementation]](https://github.com/yxuansu/Contrastive_Search_Is_What_You_Need)), the authors further demonstrate that contrastive search can generate human-level text using **off-the-shelf** language models across **16** languages.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 1451}, page_content=\"**[Remark]** For users who are not familiar with text generation, please refer more details to [this blog post](https://huggingface.co/blog/how-to-generate).\\n\\n****\\n\\n<span id='demo'/>\\n\\n### 2. Hugging Face 🤗 Demo of Contrastive Search:\\n\\nContrastive Search is now available on 🤗 `transformers`, both on PyTorch and TensorFlow. You can interact with the examples shown in this blog post using your framework of choice in [this Colab notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/115_introducing_contrastive_search.ipynb), which is linked at the top. We have also built this awesome [demo](https://huggingface.co/spaces/joaogante/contrastive_search_generation) which directly compares contrastive search with other popular decoding methods (e.g. beam search, top-k sampling <a href='#references'>[3]</a>, and nucleus sampling <a href='#references'>[4]</a>).\\n\\n****\\n\\n<span id='installation'/>\\n\\n### 3. Environment Installation:\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 2347}, page_content='****\\n\\n<span id=\\'installation\\'/>\\n\\n### 3. Environment Installation:\\n\\nBefore running the experiments in the following sections, please install the update-to-date version of `transformers` as\\n```yaml\\npip install torch\\npip install \"transformers==4.24.0\"'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 2596}, page_content=\"```\\n\\n****\\n\\n<span id='problems_of_decoding_methods'/>\\n\\n### 4. Problems of Existing Decoding Methods:\\n\\nDecoding methods can be divided into two categories: (i) deterministic methods and (ii) stochastic methods. Let's discuss both!\\n\\n\\n<span id='deterministic_methods'/>\\n\\n#### 4.1. Deterministic Methods:\\n\\nDeterministic methods, e.g. greedy search and beam search, generate text by selecting the text continuation with the highest likelihood measured by the language model. However, as widely discussed in previous studies <a href='#references'>[3]</a><a href='#references'>[4]</a>, deterministic methods often lead to the problem of _model degeneration_, i.e., the generated text is unnatural and contains undesirable repetitions.\\n\\nBelow, let's see an example of generated text from greedy search using GPT-2 model.\\n\\n```python\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 3409}, page_content='```python\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\'gpt2-large\\')\\ninput_ids = tokenizer(\\'DeepMind Company is\\', return_tensors=\\'pt\\').input_ids\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2-large\\')\\n\\noutput = model.generate(input_ids, max_length=128)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 3828}, page_content=\"```\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeepMind Company is a leading AI research company, with a focus on deep learning and deep\\nlearning-based systems.\\n\\nThe company's research is focused on the development of deep learning-based systems that\\ncan learn from large amounts of data, and that can be used to solve real-world problems.\\n\\nDeepMind's research is also used by the UK government to develop new technologies for the\\nUK's National Health Service.\\n\\nDeepMind's research is also used by the UK government to develop new technologies for the\\nUK's National Health Service.\\n\\nDeepMind's research is also used by the UK government to develop new technologies\\n----------------------------------------------------------------------------------------------------\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 4721}, page_content=\"```\\n</details>\\n\\n**[Remark]** From the result generated by greedy search, we can see obvious pattern of repetitions.\\n\\n<span id='stochastic_methods'/>\\n\\n#### 4.2. Stochastic Methods:\\n\\nTo address the issues posed by deterministic methods, stochastic methods generate text by introducing randomness during the decoding process. Two widely-used stochastic methods are (i) top-k sampling <a href='#references'>[3]</a> and (ii) nucleus sampling (also called top-p sampling) <a href='#references'>[4]</a>.\\n\\nBelow, we illustrate an example of generated text by nucleus sampling (p=0.95) using the GPT-2 model.\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\\n\\ntokenizer = AutoTokenizer.from_pretrained('gpt2-large')\\ninput_ids = tokenizer('DeepMind Company is', return_tensors='pt').input_ids\\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-large')\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 5589}, page_content='torch.manual_seed(0.)\\noutput = model.generate(input_ids, do_sample=True, max_length=128, top_p=0.95, top_k=0)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 5813}, page_content='```\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeepMind Company is a leading provider of AI-based research, development, and delivery of\\nAI solutions for security, infrastructure, machine learning, communications, and so on.\"\\n\\n\\'AI is not journalism\\'\\n\\nWorse still was the message its researchers hoped would reach the world\\'s media — that it\\nwas not really research, but rather a get-rich-quick scheme to profit from living forces\\'\\nignorance.\\n\\n\"The thing is, we know that people don\\'t consciously assess the value of the others\\'\\ninformation. They understand they will get the same on their own.\"\\n\\nOne example? Given the details of today\\n----------------------------------------------------------------------------------------------------'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 6677}, page_content=\"```\\n</details>\\n\\n**[Remark]** While nucleus sampling can generate text free of repetitions, the semantic coherence of the generated text is not well-maintained. For instance, the generated phrase _'AI is not journalism'_ is incoherent with respect to the given prefix, i.e. _'DeepMind Company'_.\\n\\nWe note that this semantic inconsistency problem can partially be remedied by lowering the temperature. However, reducing the temperature brings nucleus sampling closer to greedy search, which can be seen as a trade-off between greedy search and nucleus sampling. Generally, it is challenging to find a prompt and model-independent temperature that avoids both the pitfalls of greedy search and nucleus sampling.\\n\\n****\\n\\n<span id='contrastive_search'/>\\n\\n### 5. Contrastive Search:\\n\\nIn this section, we introduce a new decoding method, ___Contrastive Search___, in details.\\n\\n<span id='contrastive_objective'/>\\n\\n#### 5.1. Decoding Objective:\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 7546}, page_content='<span id=\\'contrastive_objective\\'/>\\n\\n#### 5.1. Decoding Objective:\\n\\nGiven the prefix text \\\\\\\\(x_{< t}\\\\\\\\), the selection of the output token \\\\\\\\(x_{t}\\\\\\\\) follows\\n\\n<center class=\"half\">\\n    <img src=\"assets/115_introducing_contrastive_search/formulation.png\" width=\"750\"/>\\n</center>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 7825}, page_content=\"where \\\\\\\\(V^{(k)}\\\\\\\\) is the set of top-k predictions from the language model's probability distribution \\\\\\\\(p_{\\\\theta}(v|x_{< t})\\\\\\\\). The first term, i.e. _model confidence_, is the probability of the candidate \\\\\\\\(v\\\\\\\\) predicted by the language model. The second term, _degeneration penalty_, measures how discriminative of \\\\\\\\(v\\\\\\\\) with respect to the previous context \\\\\\\\( x_{< t}\\\\\\\\) and the function \\\\\\\\(s(\\\\cdot, \\\\cdot)\\\\\\\\) computes the cosine similarity between the token representations. More specifically, the degeneration penalty is defined as the maximum cosine similarity between the token representation of \\\\\\\\(v\\\\\\\\), i.e. \\\\\\\\(h_{v}\\\\\\\\), and that of all tokens in the context \\\\\\\\(x_{< t}\\\\\\\\). Here, the candidate representation \\\\\\\\(h_{v}\\\\\\\\) is computed by the language model given the concatenation of \\\\\\\\(x_{< t}\\\\\\\\) and \\\\\\\\(v\\\\\\\\). Intuitively, a larger degeneration penalty of \\\\\\\\(v\\\\\\\\) means it is more similar (in the representation space) to the context, therefore more likely leading to the problem of\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 8730}, page_content='(in the representation space) to the context, therefore more likely leading to the problem of model degeneration. The hyperparameter \\\\\\\\(\\\\alpha\\\\\\\\) regulates the importance of these two components. When \\\\\\\\(\\\\alpha=0\\\\\\\\), contrastive search degenerates to the vanilla greedy search.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 9009}, page_content='**[Remark]** When generating output, contrastive search jointly considers (i) the probability predicted by the language model to maintain the semantic coherence between the generated text and the prefix text; and (ii) the similarity with respect to the previous context to avoid model degeneration.\\n\\n\\n<span id=\\'contrastive_generation\\'/>\\n\\n#### 5.2. Generating Text with Contrastive Search:\\n\\nBelow, we use the same prefix text (i.e. _\"DeepMind Company is\"_) as in Section <a href=\\'#deterministic_methods\\'>4.1</a> and <a href=\\'#stochastic_methods\\'>4.2</a>, and generate the text with contrastive search (k=4 and \\\\\\\\(\\\\alpha=0.6\\\\\\\\)). To fully demonstrate the superior capability of contrastive search, we let the language model generate a **long** document with **512** tokens as\\n\\n```python\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 9784}, page_content='```python\\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\\n\\nmodel_name = \\'gpt2-large\\'\\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\\nmodel = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id)\\nmodel.eval()\\n\\n# prepare the prefix\\nprefix_text = r\\'DeepMind Company is\\'\\ninput_ids = tokenizer(prefix_text, return_tensors=\\'pt\\').input_ids\\n\\n# generate the result with contrastive search\\noutput = model.generate(input_ids, penalty_alpha=0.6, top_k=4, max_length=512)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 10398}, page_content='```\\n\\nThe arguments are as follows:\\n* `--top_k`: The hyperparameter \\\\\\\\(k\\\\\\\\) in contrastive search.\\n* `--penalty_alpha`: The hyperparameter \\\\\\\\(\\\\alpha\\\\\\\\) in contrastive search.\\n\\n<details open>\\n<summary><b>Model Output:</b></summary>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 10629}, page_content=\"```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nDeepMind Company is a leader in artificial intelligence (AI). We have a long history of working\\nwith companies such as Google, Facebook, Amazon, and Microsoft to build products that improve\\npeople's lives, and today we are excited to announce that DeepMind's AlphaGo program has won the\\ngame of Go, becoming the first program to defeat a professional Go player.\\n\\nThe victory is a testament to the power of deep learning, and to the incredible work of our\\nresearch team, which has been at the forefront of AI research for the past five years. AlphaGo\\nis one of the most advanced Go programs ever created, and its performance is an important step\\ntowards the goal of human-level AI.\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 11424}, page_content='\"This is the culmination of a decade of hard work,\" said Andy Ng, co-founder and CTO of DeepMind.\\n\"We are thrilled to have achieved this milestone and look forward to continuing to develop AI that\\ncan be used in a wide range of applications and to help people live better lives.\"\\n\\nDeepMind\\'s work on Go began in 2010, when it began to train a neural network to play Go using\\nmillions of games played by top Go players around the world. Since then, the team has refined the\\nalgorithm, adding more and more layers of reinforcement learning to make it better at recognizing\\npatterns and making decisions based on those patterns. In the past year and a half, the team has\\nmade significant progress in the game, winning a record-tying 13 games in a row to move into the\\ntop four of the world rankings.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 12222}, page_content='\"The game of Go is a complex game in which players have to be very careful not to overextend their\\nterritory, and this is something that we have been able to improve over and over again,\" said\\nDr. Demis Hassabis, co-founder and Chief Scientific Officer of DeepMind. \"We are very proud of our\\nteam\\'s work, and we hope that it will inspire others to take the next step in their research and\\napply the same techniques to other problems.\"'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 12658}, page_content='In addition to the win in Go, DeepMind has also developed an AI system that can learn to play a\\nnumber of different games, including poker, Go, and chess. This AI system, called Tarsier, was\\ndeveloped in partnership with Carnegie Mellon University and the University of California,\\nBerkeley, and is being used to teach computer vision and machine learning to identify objects in\\nimages and recognize speech in natural language. Tarsier has been trained to play the game of Go\\nand other games on a\\n----------------------------------------------------------------------------------------------------'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 13256}, page_content='```\\n</details>\\n\\n**[Remark]** We see that the generated text is of exceptionally high quality. The entire document is grammatically fluent as well as semantically coherent. Meanwhile, the generated text also well maintains its factually correctness. For instance, in the first paragraph, it elaborates _\"AlphaGo\"_ as the _\"first program to defeat a professional Go player\"_.\\n\\n\\n<span id=\\'contrastive_visual_demonstration\\'/>\\n\\n#### 5.3. Visual Demonstration of Contrastive Search:'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 13679}, page_content='#### 5.3. Visual Demonstration of Contrastive Search:\\n\\nTo better understand how contrastive search works, we provide a visual comparison between greedy search (<a href=\\'#deterministic_methods\\'>Section 4.1</a>) and contrastive search. Specifically, we visualize the token similarity matrix of the generated text from greedy search and contrastive search, respectively. The similarity between two tokens is defined as the cosine similarity between their token representations (i.e. the hidden states of the last transformer layer). The results of greedy search (top) and contrastive search (bottom) are shown in the Figure below.\\n\\n<center class=\"half\">\\n    <img src=\"assets/115_introducing_contrastive_search/greedy_search_visualization.png\" width=\"400\"/>\\n    <img src=\"assets/115_introducing_contrastive_search/contrastive_search_visualization.png\" width=\"400\"/>\\n</center>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 14552}, page_content=\"**[Remark]** From the result of greedy search, we see high similarity scores in the off-diagonal entries which clearly indicates the generated repetitions by greedy search. On the contrary, in the result of contrastive search, the high similarity scores mostly appear in the diagonal entries which verifies that the degeneration problem is successfully addressed. This nice property of contrastive search is achieved by the introduction of degeneration penalty (see <a href='#contrastive_objective'>Section 5.1</a>) during the decoding process.\\n\\n\\n****\\n\\n<span id='more_examples'/>\\n\\n### 6. More Generated Examples:\\n\\nIn this section, we provide more generated examples to compare different decoding methods.\\n\\n<span id='gpt2_example_one'/>\\n\\n#### 6.1. Example One - GPT-2:\\n\\nIn this part, we use GPT-2 to generate text with the prefix text from the original [OpenAI blog](https://openai.com/blog/better-language-models/) that announced the release of GPT-2.\"),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 15505}, page_content='> _In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English._\\n\\n\\n<details open>\\n<summary><b> Load the language model and prepare the prefix text:</b></summary>\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, GPT2LMHeadModel\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\'gpt2-large\\')\\nmodel = GPT2LMHeadModel.from_pretrained(\\'gpt2-large\\')\\n\\nprefix_text = r\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\\ninput_ids = tokenizer(prefix_text, return_tensors=\\'pt\\').input_ids'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 16343}, page_content='```\\n</details>\\n\\n<span id=\\'gpt2_greedy_example_one\\'/>\\n\\n##### 6.1.1. Generating Text with Greedy Search:\\n\\n<details>\\n<summary><b>Code: [click to expand]</b></summary>\\n\\n```python\\noutput = model.generate(input_ids, max_length=512)\\nprint(\"Output:\\\\n\" + 100 * \\'-\\')\\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\\nprint(\"\" + 100 * \\'-\\')\\n```\\n</details>\\n\\n<details>\\n<summary><b>Model Output: [click to expand]</b></summary>'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 16768}, page_content='```\\nOutput:\\n----------------------------------------------------------------------------------------------------\\nIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously\\nunexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact\\nthat the unicorns spoke perfect English.\\n\\nThe researchers, led by Dr. David R. Williams of the University of California, Santa Cruz,\\ndiscovered the unicorns in the Andes Mountains of Peru. The area is known for its unique geology\\nand is home to a number of rare species of animals.\\n\\nThe researchers found the unicorns in the Andes Mountains of Peru.\\n\\n\"We were surprised to find that the unicorns were able to communicate with each other,\" Williams\\nsaid. \"We were also surprised to find that they were able to communicate in English.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 17733}, page_content='\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"'),\n",
              " Document(metadata={'source': 'huggingface/blog/blob/main/introducing-csearch.md', 'start_index': 18656}, page_content='The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago.\\n\\n\"The Incas were the first people to use the Andes Mountains as a refuge from the harsh climate of\\nthe Andes,\" Williams said. \"They were also the first people to use the Andes Mountains as a place\\nto hunt and gather food.\"\\n\\nThe researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\narea around 2,000 years ago\\n----------------------------------------------------------------------------------------------------'),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "docs_processed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ebf312",
      "metadata": {
        "id": "f2ebf312"
      },
      "source": [
        "We also have to keep in mind that when embedding documents, we will use an embedding model that accepts a certain maximum sequence length max_seq_length.\n",
        "\n",
        "So we should make sure that our chunk sizes are below this limit because any longer chunk will be truncated before processing, thus losing relevancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e41ffb",
      "metadata": {
        "id": "96e41ffb"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "print(f\"Model's max seq length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fc63de",
      "metadata": {
        "id": "f1fc63de"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer  = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ffdf4e",
      "metadata": {
        "id": "86ffdf4e"
      },
      "outputs": [],
      "source": [
        "# Plot the distribution of document lengths, counted as the number of tokens\n",
        "\n",
        "figure = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991cea81",
      "metadata": {
        "id": "991cea81"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "def split_documents(\n",
        "  chunk_size: int,\n",
        "  kb_article:List[LangchainDocument],\n",
        "  tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME\n",
        ") -> List[LangchainDocument] :\n",
        "    \"\"\"\n",
        "    Split document into chunks of maximum size 'chunk size' token and return a list of unique documents\n",
        "    \"\"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap = int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators = MARKDOWN_SEPARATORS\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in kb_article:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # remove duplicates\n",
        "\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content]=True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7a7d20",
      "metadata": {
        "id": "5f7a7d20"
      },
      "outputs": [],
      "source": [
        "docs_processed = split_documents(\n",
        "    512,     # We choose a chunk size=512 as per our model embedding model's max sent size\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49eed0e",
      "metadata": {
        "id": "d49eed0e"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "lengths = [len(tokenizer.encode(doc.page_content))  for doc in docs_processed]\n",
        "\n",
        "figure = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.ylabel(\"Number of tokens\")\n",
        "plt.xlabel(\"Number of documents\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abfdd6a4",
      "metadata": {
        "id": "abfdd6a4"
      },
      "outputs": [],
      "source": [
        "docs_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e447894",
      "metadata": {
        "id": "2e447894"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a114ea",
      "metadata": {
        "id": "79a114ea"
      },
      "source": [
        "Building vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f13c324",
      "metadata": {
        "id": "8f13c324"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    # model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DB = FAISS.from_documents(\n",
        "    docs_processed,\n",
        "    embedding_model,\n",
        "    distance_strategy=DistanceStrategy.COSINE\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8daa833a973842d4bcf51d641b525c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0b98e5c6fd74f4aba05a765a9cc25c2",
              "IPY_MODEL_20bac1eb87e546fe80b705572e7ce6fb",
              "IPY_MODEL_2749260d715c4408a894549cc15f97aa"
            ],
            "layout": "IPY_MODEL_40138f5daedb484791ef9db0ee615bf9"
          }
        },
        "e0b98e5c6fd74f4aba05a765a9cc25c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24ab44f9a52649ecbf59e40a09f9ec29",
            "placeholder": "​",
            "style": "IPY_MODEL_8667c1faeaa14f3cb7b7f0f0d4bb4abd",
            "value": "README.md: 100%"
          }
        },
        "20bac1eb87e546fe80b705572e7ce6fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_299bf678d127401fbcd81534a396c508",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4f5b60aa5da4a49be4666df1ac26132",
            "value": 21
          }
        },
        "2749260d715c4408a894549cc15f97aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2b9712805314000898bc88f30d264e2",
            "placeholder": "​",
            "style": "IPY_MODEL_11b52f3a62074587a232e9fa1e30410b",
            "value": " 21.0/21.0 [00:00&lt;00:00, 664B/s]"
          }
        },
        "40138f5daedb484791ef9db0ee615bf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ab44f9a52649ecbf59e40a09f9ec29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8667c1faeaa14f3cb7b7f0f0d4bb4abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "299bf678d127401fbcd81534a396c508": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4f5b60aa5da4a49be4666df1ac26132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2b9712805314000898bc88f30d264e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b52f3a62074587a232e9fa1e30410b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf0250a9faf7405898110a5c406c2dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9323248e19f548c48378f5f479918932",
              "IPY_MODEL_3edfcad451af4d35847e807046b5f4e5",
              "IPY_MODEL_07137072bfbe4464b2c5f330f708081e"
            ],
            "layout": "IPY_MODEL_8b9e9b9d6ed04d01aca7b0e30561e4c6"
          }
        },
        "9323248e19f548c48378f5f479918932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1901fe2c2124c0f9f53d3eade8ae253",
            "placeholder": "​",
            "style": "IPY_MODEL_fee63f6ccb394cd0925fab6c4e91bf29",
            "value": "huggingface_doc.csv: 100%"
          }
        },
        "3edfcad451af4d35847e807046b5f4e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a5dfda493d64501b2af0191b9863610",
            "max": 21954601,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_957f2a92d522436aa7d073ba9462fc87",
            "value": 21954601
          }
        },
        "07137072bfbe4464b2c5f330f708081e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e0527e7d6f5483381b0dc2d2ae7ae18",
            "placeholder": "​",
            "style": "IPY_MODEL_a0b661c6c963431ab30d6dac4c2997f2",
            "value": " 22.0M/22.0M [00:01&lt;00:00, 20.4MB/s]"
          }
        },
        "8b9e9b9d6ed04d01aca7b0e30561e4c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1901fe2c2124c0f9f53d3eade8ae253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee63f6ccb394cd0925fab6c4e91bf29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a5dfda493d64501b2af0191b9863610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "957f2a92d522436aa7d073ba9462fc87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e0527e7d6f5483381b0dc2d2ae7ae18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b661c6c963431ab30d6dac4c2997f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53c5c9e241e544cfa0844cefc0338bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efef862da9f14a68ad6096f6d1fad96a",
              "IPY_MODEL_fcd723eccb094fa5ba0f60458dbfdb0f",
              "IPY_MODEL_644a5bd9135a472ca6a579a8c84f8d8d"
            ],
            "layout": "IPY_MODEL_e6bdc3910262419d972dcbedb3f1f10b"
          }
        },
        "efef862da9f14a68ad6096f6d1fad96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c601c3b75524259b0fdb6dc01d1ec13",
            "placeholder": "​",
            "style": "IPY_MODEL_62ea973c8c5148d4949ef364f459dcdc",
            "value": "Generating train split: 100%"
          }
        },
        "fcd723eccb094fa5ba0f60458dbfdb0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b4c683e7a4d499981aab71e11f718b5",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8244350fb0f84a1a81515438ff17683f",
            "value": 2647
          }
        },
        "644a5bd9135a472ca6a579a8c84f8d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e47fa6b2e958445b8b56e8152270adba",
            "placeholder": "​",
            "style": "IPY_MODEL_1fea0aa1281c4399ad71a89bdaf7cadd",
            "value": " 2647/2647 [00:01&lt;00:00, 2673.44 examples/s]"
          }
        },
        "e6bdc3910262419d972dcbedb3f1f10b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c601c3b75524259b0fdb6dc01d1ec13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ea973c8c5148d4949ef364f459dcdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b4c683e7a4d499981aab71e11f718b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8244350fb0f84a1a81515438ff17683f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e47fa6b2e958445b8b56e8152270adba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fea0aa1281c4399ad71a89bdaf7cadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7b36c5159044d1c94b1cafcd3424432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad142b13902049ba9eee833cdc43161c",
              "IPY_MODEL_2398aa7200d44829bab7d82b35342647",
              "IPY_MODEL_eb1f72436f824abbac6d5b37f176536f"
            ],
            "layout": "IPY_MODEL_bb4f4951df6b489dac57d217401e8803"
          }
        },
        "ad142b13902049ba9eee833cdc43161c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92a6b77111ea4352865545c0d1af6b85",
            "placeholder": "​",
            "style": "IPY_MODEL_b52df4b76d2b4058b1ef937ea2cf5669",
            "value": "100%"
          }
        },
        "2398aa7200d44829bab7d82b35342647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ff9c0f0660b43c48a75aba071f9d8bb",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d32baef2d4ee4c8bae4a5451b7b67ede",
            "value": 2647
          }
        },
        "eb1f72436f824abbac6d5b37f176536f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75d2e8c5db494d748a08fc1925cd3f05",
            "placeholder": "​",
            "style": "IPY_MODEL_10d49d9a7a1a4ac79230af5d73e3aaae",
            "value": " 2647/2647 [00:00&lt;00:00, 8059.45it/s]"
          }
        },
        "bb4f4951df6b489dac57d217401e8803": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92a6b77111ea4352865545c0d1af6b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b52df4b76d2b4058b1ef937ea2cf5669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ff9c0f0660b43c48a75aba071f9d8bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d32baef2d4ee4c8bae4a5451b7b67ede": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75d2e8c5db494d748a08fc1925cd3f05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10d49d9a7a1a4ac79230af5d73e3aaae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}